# Latest Large Language Model Attack Papers
**update at 2025-05-22 16:55:29**

翻译来自 https://cloud.tencent.com/document/product/551/15619

## **1. Keep Security! Benchmarking Security Policy Preservation in Large Language Model Contexts Against Indirect Attacks in Question Answering**

保持安全！针对问题解答中的间接攻击，对大型语言模型上下文中的安全策略保留进行基准测试 cs.CL

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2505.15805v1) [paper-pdf](http://arxiv.org/pdf/2505.15805v1)

**Authors**: Hwan Chang, Yumin Kim, Yonghyun Jun, Hwanhee Lee

**Abstract**: As Large Language Models (LLMs) are increasingly deployed in sensitive domains such as enterprise and government, ensuring that they adhere to user-defined security policies within context is critical-especially with respect to information non-disclosure. While prior LLM studies have focused on general safety and socially sensitive data, large-scale benchmarks for contextual security preservation against attacks remain lacking. To address this, we introduce a novel large-scale benchmark dataset, CoPriva, evaluating LLM adherence to contextual non-disclosure policies in question answering. Derived from realistic contexts, our dataset includes explicit policies and queries designed as direct and challenging indirect attacks seeking prohibited information. We evaluate 10 LLMs on our benchmark and reveal a significant vulnerability: many models violate user-defined policies and leak sensitive information. This failure is particularly severe against indirect attacks, highlighting a critical gap in current LLM safety alignment for sensitive applications. Our analysis reveals that while models can often identify the correct answer to a query, they struggle to incorporate policy constraints during generation. In contrast, they exhibit a partial ability to revise outputs when explicitly prompted. Our findings underscore the urgent need for more robust methods to guarantee contextual security.

摘要: 随着大型语言模型（LLM）越来越多地部署在企业和政府等敏感领域，确保它们在上下文中遵守用户定义的安全策略至关重要，尤其是在信息不披露方面。虽然之前的LLM研究重点关注一般安全和社会敏感数据，但仍然缺乏针对攻击的上下文安全保护的大规模基准。为了解决这个问题，我们引入了一个新颖的大规模基准数据集CoPriva，以评估LLM在问答中对上下文保密政策的遵守情况。我们的数据集源自现实背景，包括明确的政策和查询，旨在作为寻求违禁信息的直接和具有挑战性的间接攻击。我们在我们的基准上评估了10个LLM，并揭示了一个重大漏洞：许多模型违反了用户定义的策略并泄露了敏感信息。对于间接攻击，这种故障尤其严重，凸显了当前针对敏感应用的LLM安全调整中的关键差距。我们的分析表明，虽然模型通常可以识别查询的正确答案，但它们很难在生成过程中纳入政策约束。相比之下，它们在明确提示时表现出修改输出的部分能力。我们的研究结果强调迫切需要更强大的方法来保证上下文安全。



## **2. Reverse Engineering Human Preferences with Reinforcement Learning**

利用强化学习反向工程人类偏好 cs.CL

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2505.15795v1) [paper-pdf](http://arxiv.org/pdf/2505.15795v1)

**Authors**: Lisa Alazraki, Tan Yi-Chern, Jon Ander Campos, Maximilian Mozes, Marek Rei, Max Bartolo

**Abstract**: The capabilities of Large Language Models (LLMs) are routinely evaluated by other LLMs trained to predict human preferences. This framework--known as LLM-as-a-judge--is highly scalable and relatively low cost. However, it is also vulnerable to malicious exploitation, as LLM responses can be tuned to overfit the preferences of the judge. Previous work shows that the answers generated by a candidate-LLM can be edited post hoc to maximise the score assigned to them by a judge-LLM. In this study, we adopt a different approach and use the signal provided by judge-LLMs as a reward to adversarially tune models that generate text preambles designed to boost downstream performance. We find that frozen LLMs pipelined with these models attain higher LLM-evaluation scores than existing frameworks. Crucially, unlike other frameworks which intervene directly on the model's response, our method is virtually undetectable. We also demonstrate that the effectiveness of the tuned preamble generator transfers when the candidate-LLM and the judge-LLM are replaced with models that are not used during training. These findings raise important questions about the design of more reliable LLM-as-a-judge evaluation settings. They also demonstrate that human preferences can be reverse engineered effectively, by pipelining LLMs to optimise upstream preambles via reinforcement learning--an approach that could find future applications in diverse tasks and domains beyond adversarial attacks.

摘要: 大型语言模型（LLM）的能力通常由其他经过训练以预测人类偏好的LLM进行评估。这个框架-被称为LLM作为法官-具有高度可扩展性和相对较低的成本。然而，它也容易受到恶意利用，因为LLM响应可以被调整以过度适应法官的偏好。以前的工作表明，候选人LLM生成的答案可以事后编辑，以最大限度地提高法官LLM分配给他们的分数。在这项研究中，我们采用了一种不同的方法，并使用judge-LLM提供的信号作为奖励，以对抗性地调整模型，这些模型生成旨在提高下游性能的文本前置码。我们发现，使用这些模型流水线化的冻结LLM比现有框架获得更高的LLM评估分数。至关重要的是，与直接干预模型响应的其他框架不同，我们的方法几乎无法检测。我们还证明，当候选LLM和判断LLM被训练期间未使用的模型替换时，调整后的前同步码生成器的有效性会转移。这些发现提出了更可靠的法学硕士作为一个法官的评价设置的设计的重要问题。他们还证明，人类的偏好可以有效地进行逆向工程，通过流水线LLM来优化上游的优化，这种方法可以在对抗性攻击之外的各种任务和领域中找到未来的应用。



## **3. Scalable Defense against In-the-wild Jailbreaking Attacks with Safety Context Retrieval**

通过安全上下文检索针对野外越狱攻击的可扩展防御 cs.CR

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2505.15753v1) [paper-pdf](http://arxiv.org/pdf/2505.15753v1)

**Authors**: Taiye Chen, Zeming Wei, Ang Li, Yisen Wang

**Abstract**: Large Language Models (LLMs) are known to be vulnerable to jailbreaking attacks, wherein adversaries exploit carefully engineered prompts to induce harmful or unethical responses. Such threats have raised critical concerns about the safety and reliability of LLMs in real-world deployment. While existing defense mechanisms partially mitigate such risks, subsequent advancements in adversarial techniques have enabled novel jailbreaking methods to circumvent these protections, exposing the limitations of static defense frameworks. In this work, we explore defending against evolving jailbreaking threats through the lens of context retrieval. First, we conduct a preliminary study demonstrating that even a minimal set of safety-aligned examples against a particular jailbreak can significantly enhance robustness against this attack pattern. Building on this insight, we further leverage the retrieval-augmented generation (RAG) techniques and propose Safety Context Retrieval (SCR), a scalable and robust safeguarding paradigm for LLMs against jailbreaking. Our comprehensive experiments demonstrate how SCR achieves superior defensive performance against both established and emerging jailbreaking tactics, contributing a new paradigm to LLM safety. Our code will be available upon publication.

摘要: 众所周知，大型语言模型（LLM）很容易受到越狱攻击，其中对手利用精心设计的提示来引发有害或不道德的反应。此类威胁引发了人们对LLM在现实世界部署中的安全性和可靠性的严重担忧。虽然现有的防御机制部分减轻了此类风险，但对抗技术的后续进步使新型越狱方法能够规避这些保护，暴露了静态防御框架的局限性。在这项工作中，我们探索通过上下文检索的视角抵御不断变化的越狱威胁。首先，我们进行了一项初步研究，证明即使是针对特定越狱的最少一组安全一致的示例也可以显着增强针对这种攻击模式的鲁棒性。在这一见解的基础上，我们进一步利用检索增强生成（RAG）技术并提出安全上下文检索（SR），这是一种针对LLM越狱的可扩展且强大的保护范式。我们全面的实验展示了可控硅如何在针对既定和新兴越狱策略的情况下实现卓越的防御性能，为LLM安全性贡献了新的范式。我们的代码将在发布后提供。



## **4. Shaping the Safety Boundaries: Understanding and Defending Against Jailbreaks in Large Language Models**

塑造安全边界：理解和防御大型语言模型中的越狱 cs.CL

17 pages, 9 figures

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2412.17034v2) [paper-pdf](http://arxiv.org/pdf/2412.17034v2)

**Authors**: Lang Gao, Jiahui Geng, Xiangliang Zhang, Preslav Nakov, Xiuying Chen

**Abstract**: Jailbreaking in Large Language Models (LLMs) is a major security concern as it can deceive LLMs to generate harmful text. Yet, there is still insufficient understanding of how jailbreaking works, which makes it hard to develop effective defense strategies. We aim to shed more light into this issue: we conduct a detailed large-scale analysis of seven different jailbreak methods and find that these disagreements stem from insufficient observation samples. In particular, we introduce \textit{safety boundary}, and we find that jailbreaks shift harmful activations outside that safety boundary, where LLMs are less sensitive to harmful information. We also find that the low and the middle layers are critical in such shifts, while deeper layers have less impact. Leveraging on these insights, we propose a novel defense called \textbf{Activation Boundary Defense} (ABD), which adaptively constrains the activations within the safety boundary. We further use Bayesian optimization to selectively apply the defense method to the low and the middle layers. Our experiments on several benchmarks show that ABD achieves an average DSR of over 98\% against various forms of jailbreak attacks, with less than 2\% impact on the model's general capabilities.

摘要: 大型语言模型（LLM）中的越狱是一个主要的安全问题，因为它可能会欺骗LLM生成有害文本。然而，人们对越狱的运作方式仍然缺乏足够的了解，这使得制定有效的防御策略变得困难。我们的目标是更多地了解这个问题：我们对七种不同的越狱方法进行了详细的大规模分析，发现这些分歧源于观察样本不足。特别是，我们引入了\textit{safety boundary}，我们发现越狱将有害激活转移到安全边界之外，而LLM对有害信息不太敏感。我们还发现，低层和中层在此类转变中至关重要，而较深层的影响较小。利用这些见解，我们提出了一种名为\textBF{Activation Boundary Defense}（ABD）的新型防御，它自适应地将激活限制在安全边界内。我们进一步使用Bayesian优化来选择性地将防御方法应用于低层和中层。我们在多个基准测试上的实验表明，ABD针对各种形式的越狱攻击，平均DSR超过98%，对模型的一般能力影响不到2%。



## **5. Alignment Under Pressure: The Case for Informed Adversaries When Evaluating LLM Defenses**

压力下的一致：评估LLM防御时知情对手的理由 cs.CR

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2505.15738v1) [paper-pdf](http://arxiv.org/pdf/2505.15738v1)

**Authors**: Xiaoxue Yang, Bozhidar Stevanoski, Matthieu Meeus, Yves-Alexandre de Montjoye

**Abstract**: Large language models (LLMs) are rapidly deployed in real-world applications ranging from chatbots to agentic systems. Alignment is one of the main approaches used to defend against attacks such as prompt injection and jailbreaks. Recent defenses report near-zero Attack Success Rates (ASR) even against Greedy Coordinate Gradient (GCG), a white-box attack that generates adversarial suffixes to induce attacker-desired outputs. However, this search space over discrete tokens is extremely large, making the task of finding successful attacks difficult. GCG has, for instance, been shown to converge to local minima, making it sensitive to initialization choices. In this paper, we assess the future-proof robustness of these defenses using a more informed threat model: attackers who have access to some information about the alignment process. Specifically, we propose an informed white-box attack leveraging the intermediate model checkpoints to initialize GCG, with each checkpoint acting as a stepping stone for the next one. We show this approach to be highly effective across state-of-the-art (SOTA) defenses and models. We further show our informed initialization to outperform other initialization methods and show a gradient-informed checkpoint selection strategy to greatly improve attack performance and efficiency. Importantly, we also show our method to successfully find universal adversarial suffixes -- single suffixes effective across diverse inputs. Our results show that, contrary to previous beliefs, effective adversarial suffixes do exist against SOTA alignment-based defenses, that these can be found by existing attack methods when adversaries exploit alignment knowledge, and that even universal suffixes exist. Taken together, our results highlight the brittleness of current alignment-based methods and the need to consider stronger threat models when testing the safety of LLMs.

摘要: 大型语言模型（LLM）被快速部署在从聊天机器人到代理系统的实际应用中。对齐是用于防御诸如即时注入和越狱等攻击的主要方法之一。最近的防御报告甚至对贪婪坐标梯度（GCG）的攻击成功率（ASR）接近于零，GCG是一种白盒攻击，生成对抗性后缀以诱导攻击者期望的输出。然而，这种在离散令牌上的搜索空间非常大，使得找到成功攻击的任务变得困难。例如，GCG已被证明收敛到局部极小值，使其对初始化选择敏感。在本文中，我们使用一个更明智的威胁模型来评估这些防御系统的面向未来的鲁棒性：可以访问有关对齐过程的一些信息的攻击者。具体来说，我们提出了一种知情白盒攻击，利用中间模型检查点来初始化GCG，每个检查点都充当下一个检查点的垫脚石。我们证明这种方法在最先进的（SOTA）防御和模型中非常有效。我们进一步展示了我们的知情初始化，以优于其他初始化方法，并展示了一种基于梯度的检查点选择策略，以极大地提高攻击性能和效率。重要的是，我们还展示了成功找到通用对抗后缀的方法--在不同输入中有效的单个后缀。我们的结果表明，与之前的观点相反，针对基于SOTA匹配的防御，确实存在有效的对抗性后缀，当对手利用对齐知识时，这些后缀可以通过现有的攻击方法找到，甚至存在通用后缀。总而言之，我们的结果凸显了当前基于环境的方法的脆弱性，以及在测试LLM的安全性时需要考虑更强的威胁模型。



## **6. SQL Injection Jailbreak: A Structural Disaster of Large Language Models**

SQL注入越狱：大型语言模型的结构灾难 cs.CR

Accepted by findings of ACL 2025

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2411.01565v6) [paper-pdf](http://arxiv.org/pdf/2411.01565v6)

**Authors**: Jiawei Zhao, Kejiang Chen, Weiming Zhang, Nenghai Yu

**Abstract**: Large Language Models (LLMs) are susceptible to jailbreak attacks that can induce them to generate harmful content. Previous jailbreak methods primarily exploited the internal properties or capabilities of LLMs, such as optimization-based jailbreak methods and methods that leveraged the model's context-learning abilities. In this paper, we introduce a novel jailbreak method, SQL Injection Jailbreak (SIJ), which targets the external properties of LLMs, specifically, the way LLMs construct input prompts. By injecting jailbreak information into user prompts, SIJ successfully induces the model to output harmful content. For open-source models, SIJ achieves near 100% attack success rates on five well-known LLMs on the AdvBench and HEx-PHI, while incurring lower time costs compared to previous methods. For closed-source models, SIJ achieves an average attack success rate over 85% across five models in the GPT and Doubao series. Additionally, SIJ exposes a new vulnerability in LLMs that urgently requires mitigation. To address this, we propose a simple adaptive defense method called Self-Reminder-Key to counter SIJ and demonstrate its effectiveness through experimental results. Our code is available at https://github.com/weiyezhimeng/SQL-Injection-Jailbreak.

摘要: 大型语言模型（LLM）容易受到越狱攻击，从而导致它们生成有害内容。之前的越狱方法主要利用LLM的内部属性或功能，例如基于优化的越狱方法和利用模型上下文学习能力的方法。本文中，我们介绍了一种新颖的越狱方法--SQL注入越狱（SIJ），它针对的是LLM的外部属性，具体来说是LLM构建输入提示的方式。通过将越狱信息注入用户提示中，SIJ成功诱导模型输出有害内容。对于开源模型，SIJ在AdvBench和HEx-PHI上的五个知名LLM上实现了接近100%的攻击成功率，同时与之前的方法相比，时间成本更低。对于闭源型号，SIJ在GPT和抖音系列的五种型号中的平均攻击成功率超过85%。此外，SIJ暴露了LLM中的一个新漏洞，迫切需要缓解。为了解决这个问题，我们提出了一种名为Self-Reminder-Key的简单自适应防御方法来对抗SIJ，并通过实验结果证明其有效性。我们的代码可在https://github.com/weiyezhimeng/SQL-Injection-Jailbreak上获取。



## **7. Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data Could Be Secretly Stolen!**

在开源LLM上进行微调时要小心：您的微调数据可能会被秘密窃取！ cs.CL

19 pages

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2505.15656v1) [paper-pdf](http://arxiv.org/pdf/2505.15656v1)

**Authors**: Zhexin Zhang, Yuhao Sun, Junxiao Yang, Shiyao Cui, Hongning Wang, Minlie Huang

**Abstract**: Fine-tuning on open-source Large Language Models (LLMs) with proprietary data is now a standard practice for downstream developers to obtain task-specific LLMs. Surprisingly, we reveal a new and concerning risk along with the practice: the creator of the open-source LLMs can later extract the private downstream fine-tuning data through simple backdoor training, only requiring black-box access to the fine-tuned downstream model. Our comprehensive experiments, across 4 popularly used open-source models with 3B to 32B parameters and 2 downstream datasets, suggest that the extraction performance can be strikingly high: in practical settings, as much as 76.3% downstream fine-tuning data (queries) out of a total 5,000 samples can be perfectly extracted, and the success rate can increase to 94.9% in more ideal settings. We also explore a detection-based defense strategy but find it can be bypassed with improved attack. Overall, we highlight the emergency of this newly identified data breaching risk in fine-tuning, and we hope that more follow-up research could push the progress of addressing this concerning risk. The code and data used in our experiments are released at https://github.com/thu-coai/Backdoor-Data-Extraction.

摘要: 对具有专有数据的开源大型语言模型（LLM）进行微调现在已成为下游开发人员获取特定任务LLM的标准实践。令人惊讶的是，我们在实践中揭示了一个新的且令人担忧的风险：开源LLM的创建者稍后可以通过简单的后门训练提取私有下游微调数据，只需要黑匣子访问微调下游模型。我们对4个常用的3B至32 B参数开源模型和2个下游数据集进行了全面的实验，表明提取性能可以非常高：在实际环境中，总共5，000个样本中，多达76.3%的下游微调数据（查询）可以被完美提取，在更理想的环境中，成功率可以提高到94.9%。我们还探索了基于检测的防御策略，但发现可以通过改进的攻击来绕过它。总体而言，我们强调了这种新发现的数据泄露风险在微调中的紧迫性，我们希望更多的后续研究能够推动解决这一相关风险的进展。我们实验中使用的代码和数据发布在https://github.com/thu-coai/Backdoor-Data-Extraction上。



## **8. SEA: Low-Resource Safety Alignment for Multimodal Large Language Models via Synthetic Embeddings**

SEA：通过合成嵌入实现多模式大型语言模型的低资源安全性对齐 cs.CL

Accepted in ACL 2025 Main Track

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2502.12562v2) [paper-pdf](http://arxiv.org/pdf/2502.12562v2)

**Authors**: Weikai Lu, Hao Peng, Huiping Zhuang, Cen Chen, Ziqian Zeng

**Abstract**: Multimodal Large Language Models (MLLMs) have serious security vulnerabilities.While safety alignment using multimodal datasets consisting of text and data of additional modalities can effectively enhance MLLM's security, it is costly to construct these datasets. Existing low-resource security alignment methods, including textual alignment, have been found to struggle with the security risks posed by additional modalities. To address this, we propose Synthetic Embedding augmented safety Alignment (SEA), which optimizes embeddings of additional modality through gradient updates to expand textual datasets. This enables multimodal safety alignment training even when only textual data is available. Extensive experiments on image, video, and audio-based MLLMs demonstrate that SEA can synthesize a high-quality embedding on a single RTX3090 GPU within 24 seconds. SEA significantly improves the security of MLLMs when faced with threats from additional modalities. To assess the security risks introduced by video and audio, we also introduced a new benchmark called VA-SafetyBench. High attack success rates across multiple MLLMs validate its challenge. Our code and data will be available at https://github.com/ZeroNLP/SEA.

摘要: 多模式大型语言模型（MLLM）存在严重的安全漏洞。虽然使用由文本和其他模式数据组成的多模式数据集进行安全对齐可以有效增强MLLM的安全性，但构建这些数据集的成本很高。现有的低资源安全对齐方法（包括文本对齐）被发现难以应对额外模式带来的安全风险。为了解决这个问题，我们提出了合成嵌入增强安全对齐（SEA），它通过梯度更新来优化额外模式的嵌入以扩展文本数据集。即使只有文本数据可用，这也可以实现多模式安全对齐训练。基于图像、视频和音频的MLLM的广泛实验表明，SEA可以在24秒内在单个RTX 3090图形处理器上合成高质量嵌入。SEA在面临来自其他模式的威胁时显着提高了MLLM的安全性。为了评估视频和音频带来的安全风险，我们还引入了名为VA-SafetyBench的新基准。多个MLLM的高攻击成功率证实了其挑战。我们的代码和数据可在https://github.com/ZeroNLP/SEA上获取。



## **9. Silent Leaks: Implicit Knowledge Extraction Attack on RAG Systems through Benign Queries**

Silent Leaks：通过Benign Buttons对RAG系统进行隐式知识提取攻击 cs.CR

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2505.15420v1) [paper-pdf](http://arxiv.org/pdf/2505.15420v1)

**Authors**: Yuhao Wang, Wenjie Qu, Yanze Jiang, Zichen Liu, Yue Liu, Shengfang Zhai, Yinpeng Dong, Jiaheng Zhang

**Abstract**: Retrieval-Augmented Generation (RAG) systems enhance large language models (LLMs) by incorporating external knowledge bases, but they are vulnerable to privacy risks from data extraction attacks. Existing extraction methods typically rely on malicious inputs such as prompt injection or jailbreaking, making them easily detectable via input- or output-level detection. In this paper, we introduce Implicit Knowledge Extraction Attack (IKEA), which conducts knowledge extraction on RAG systems through benign queries. IKEA first leverages anchor concepts to generate queries with the natural appearance, and then designs two mechanisms to lead to anchor concept thoroughly 'explore' the RAG's privacy knowledge: (1) Experience Reflection Sampling, which samples anchor concepts based on past query-response patterns to ensure the queries' relevance to RAG documents; (2) Trust Region Directed Mutation, which iteratively mutates anchor concepts under similarity constraints to further exploit the embedding space. Extensive experiments demonstrate IKEA's effectiveness under various defenses, surpassing baselines by over 80% in extraction efficiency and 90% in attack success rate. Moreover, the substitute RAG system built from IKEA's extractions consistently outperforms those based on baseline methods across multiple evaluation tasks, underscoring the significant privacy risk in RAG systems.

摘要: 检索增强生成（RAG）系统通过整合外部知识库来增强大型语言模型（LLM），但它们很容易受到数据提取攻击的隐私风险。现有的提取方法通常依赖于恶意输入，例如提示注入或越狱，使得它们可以通过输入或输出级检测轻松检测到。本文引入了隐式知识提取攻击（IKEA），它通过良性查询对RAG系统进行知识提取。宜家首先利用锚概念生成具有自然外观的查询，然后设计了两种机制来引导锚概念彻底“探索”RAG的隐私知识：（1）体验反射采样，基于过去的查询-响应模式对锚概念进行采样，以确保查询与RAG文档的相关性;（2）信任区域定向突变，在相似性约束下迭代突变锚概念，以进一步利用嵌入空间。大量实验证明了宜家在各种防御下的有效性，提取效率超过基线80%，攻击成功率超过基线90%。此外，根据宜家提取物构建的替代RAG系统在多个评估任务中始终优于基于基线方法的系统，这凸显了RAG系统中存在的巨大隐私风险。



## **10. Audio Jailbreak: An Open Comprehensive Benchmark for Jailbreaking Large Audio-Language Models**

Audio Jailbreak：一个用于越狱大型音频语言模型的开放综合基准测试 cs.SD

We release AJailBench, including both static and optimized  adversarial data, to facilitate future research:  https://github.com/mbzuai-nlp/AudioJailbreak

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2505.15406v1) [paper-pdf](http://arxiv.org/pdf/2505.15406v1)

**Authors**: Zirui Song, Qian Jiang, Mingxuan Cui, Mingzhe Li, Lang Gao, Zeyu Zhang, Zixiang Xu, Yanbo Wang, Chenxi Wang, Guangxian Ouyang, Zhenhao Chen, Xiuying Chen

**Abstract**: The rise of Large Audio Language Models (LAMs) brings both potential and risks, as their audio outputs may contain harmful or unethical content. However, current research lacks a systematic, quantitative evaluation of LAM safety especially against jailbreak attacks, which are challenging due to the temporal and semantic nature of speech. To bridge this gap, we introduce AJailBench, the first benchmark specifically designed to evaluate jailbreak vulnerabilities in LAMs. We begin by constructing AJailBench-Base, a dataset of 1,495 adversarial audio prompts spanning 10 policy-violating categories, converted from textual jailbreak attacks using realistic text to speech synthesis. Using this dataset, we evaluate several state-of-the-art LAMs and reveal that none exhibit consistent robustness across attacks. To further strengthen jailbreak testing and simulate more realistic attack conditions, we propose a method to generate dynamic adversarial variants. Our Audio Perturbation Toolkit (APT) applies targeted distortions across time, frequency, and amplitude domains. To preserve the original jailbreak intent, we enforce a semantic consistency constraint and employ Bayesian optimization to efficiently search for perturbations that are both subtle and highly effective. This results in AJailBench-APT, an extended dataset of optimized adversarial audio samples. Our findings demonstrate that even small, semantically preserved perturbations can significantly reduce the safety performance of leading LAMs, underscoring the need for more robust and semantically aware defense mechanisms.

摘要: 大型音频语言模型（LAMs）的兴起带来了潜在的风险，因为它们的音频输出可能包含有害或不道德的内容。然而，目前的研究缺乏一个系统的，定量的评估LAM的安全性，特别是对越狱攻击，这是具有挑战性的，由于语音的时间和语义的性质。为了弥补这一差距，我们引入AJailBench，这是第一个专门用于评估LAM中越狱漏洞的基准测试。我们首先构建AJailBench-Base，这是一个包含1，495个对抗性音频提示的数据集，涵盖10个违反策略的类别，从使用真实文本的文本越狱攻击转换为语音合成。使用该数据集，我们评估了几种最先进的LAM，并发现没有一种在攻击中表现出一致的鲁棒性。为了进一步加强越狱测试并模拟更真实的攻击条件，我们提出了一种生成动态对抗变体的方法。我们的音频微扰工具包（APT）在时间、频率和幅度域中应用有针对性的失真。为了保留最初的越狱意图，我们强制执行语义一致性约束并采用Bayesian优化来有效地搜索微妙且高效的扰动。这会产生AJailBench-APT，这是一个优化的对抗性音频样本的扩展数据集。我们的研究结果表明，即使是很小的、在语义上保留的扰动也会显着降低领先LAM的安全性能，这凸显了对更强大和语义感知的防御机制的需求。



## **11. RePPL: Recalibrating Perplexity by Uncertainty in Semantic Propagation and Language Generation for Explainable QA Hallucination Detection**

RePPL：通过语义传播和语言生成中的不确定性重新校准困惑，以实现可解释QA幻觉检测 cs.CL

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2505.15386v1) [paper-pdf](http://arxiv.org/pdf/2505.15386v1)

**Authors**: Yiming Huang, Junyan Zhang, Zihao Wang, Biquan Bie, Xuming Hu, Yi R., Fung, Xinlei He

**Abstract**: Large Language Models (LLMs) have become powerful, but hallucinations remain a vital obstacle to their trustworthy use. While previous works improved the capability of hallucination detection by measuring uncertainty, they all lack the ability to explain the provenance behind why hallucinations occur, i.e., which part of the inputs tends to trigger hallucinations. Recent works on the prompt attack indicate that uncertainty exists in semantic propagation, where attention mechanisms gradually fuse local token information into high-level semantics across layers. Meanwhile, uncertainty also emerges in language generation, due to its probability-based selection of high-level semantics for sampled generations. Based on that, we propose RePPL to recalibrate uncertainty measurement by these two aspects, which dispatches explainable uncertainty scores to each token and aggregates in Perplexity-style Log-Average form as total score. Experiments show that our method achieves the best comprehensive detection performance across various QA datasets on advanced models (average AUC of 0.833), and our method is capable of producing token-level uncertainty scores as explanations for the hallucination. Leveraging these scores, we preliminarily find the chaotic pattern of hallucination and showcase its promising usage.

摘要: 大型语言模型（LLM）已经变得强大，但幻觉仍然是其值得信赖使用的重要障碍。虽然之前的作品通过测量不确定性来提高了幻觉检测的能力，但它们都缺乏解释幻觉发生背后来源的能力，即这部分输入往往会引发幻觉。最近关于提示攻击的研究表明，语义传播中存在不确定性，其中注意力机制逐渐将本地令牌信息融合到跨层的高级语义中。与此同时，由于语言生成对采样世代的高级语义基于概率选择，因此也出现了不确定性。在此基础上，我们提出RePPL通过这两个方面重新校准不确定性测量，将可解释的不确定性分数分配到每个代币，并以困惑式的Log-Average形式汇总为总分。实验表明，我们的方法在高级模型上的各种QA数据集中实现了最佳的综合检测性能（平均曲线下面积为0.833），并且我们的方法能够产生符号级的不确定性分数作为幻觉的解释。利用这些分数，我们初步发现了幻觉的混乱模式，并展示了其有希望的用途。



## **12. PoisonArena: Uncovering Competing Poisoning Attacks in Retrieval-Augmented Generation**

PoisonArena：揭露检索增强一代中的竞争中毒攻击 cs.IR

29 pages

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2505.12574v2) [paper-pdf](http://arxiv.org/pdf/2505.12574v2)

**Authors**: Liuji Chen, Xiaofang Yang, Yuanzhuo Lu, Jinghao Zhang, Xin Sun, Qiang Liu, Shu Wu, Jing Dong, Liang Wang

**Abstract**: Retrieval-Augmented Generation (RAG) systems, widely used to improve the factual grounding of large language models (LLMs), are increasingly vulnerable to poisoning attacks, where adversaries inject manipulated content into the retriever's corpus. While prior research has predominantly focused on single-attacker settings, real-world scenarios often involve multiple, competing attackers with conflicting objectives. In this work, we introduce PoisonArena, the first benchmark to systematically study and evaluate competing poisoning attacks in RAG. We formalize the multi-attacker threat model, where attackers vie to control the answer to the same query using mutually exclusive misinformation. PoisonArena leverages the Bradley-Terry model to quantify each method's competitive effectiveness in such adversarial environments. Through extensive experiments on the Natural Questions and MS MARCO datasets, we demonstrate that many attack strategies successful in isolation fail under competitive pressure. Our findings highlight the limitations of conventional evaluation metrics like Attack Success Rate (ASR) and F1 score and underscore the need for competitive evaluation to assess real-world attack robustness. PoisonArena provides a standardized framework to benchmark and develop future attack and defense strategies under more realistic, multi-adversary conditions. Project page: https://github.com/yxf203/PoisonArena.

摘要: 检索增强生成（RAG）系统，广泛用于改善大型语言模型（LLM）的事实基础，越来越容易受到中毒攻击，其中对手将操纵的内容注入检索器的语料库。虽然以前的研究主要集中在单个攻击者的设置，但现实世界的场景往往涉及多个相互竞争的攻击者，这些攻击者的目标相互冲突。在这项工作中，我们介绍PoisonArena，第一个基准系统地研究和评估竞争中毒攻击在RAG。我们形式化的多攻击者威胁模型，攻击者争夺控制答案相同的查询使用互斥的错误信息。PoisonArena利用Bradley-Terry模型来量化每种方法在此类对抗环境中的竞争有效性。通过对Natural Questions和MS MARCO数据集的广泛实验，我们证明了许多孤立成功的攻击策略在竞争压力下失败。我们的研究结果强调了攻击成功率（SVR）和F1评分等传统评估指标的局限性，并强调了竞争性评估来评估现实世界攻击稳健性的必要性。PoisonArena提供了一个标准化的框架，可以在更现实的多对手条件下基准和开发未来的攻击和防御策略。项目页面：https://github.com/yxf203/PoisonArena。



## **13. Your Language Model Can Secretly Write Like Humans: Contrastive Paraphrase Attacks on LLM-Generated Text Detectors**

您的语言模型可以像人类一样秘密写作：对LLM生成的文本检测器的对比重述攻击 cs.CL

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2505.15337v1) [paper-pdf](http://arxiv.org/pdf/2505.15337v1)

**Authors**: Hao Fang, Jiawei Kong, Tianqu Zhuang, Yixiang Qiu, Kuofeng Gao, Bin Chen, Shu-Tao Xia, Yaowei Wang, Min Zhang

**Abstract**: The misuse of large language models (LLMs), such as academic plagiarism, has driven the development of detectors to identify LLM-generated texts. To bypass these detectors, paraphrase attacks have emerged to purposely rewrite these texts to evade detection. Despite the success, existing methods require substantial data and computational budgets to train a specialized paraphraser, and their attack efficacy greatly reduces when faced with advanced detection algorithms. To address this, we propose \textbf{Co}ntrastive \textbf{P}araphrase \textbf{A}ttack (CoPA), a training-free method that effectively deceives text detectors using off-the-shelf LLMs. The first step is to carefully craft instructions that encourage LLMs to produce more human-like texts. Nonetheless, we observe that the inherent statistical biases of LLMs can still result in some generated texts carrying certain machine-like attributes that can be captured by detectors. To overcome this, CoPA constructs an auxiliary machine-like word distribution as a contrast to the human-like distribution generated by the LLM. By subtracting the machine-like patterns from the human-like distribution during the decoding process, CoPA is able to produce sentences that are less discernible by text detectors. Our theoretical analysis suggests the superiority of the proposed attack. Extensive experiments validate the effectiveness of CoPA in fooling text detectors across various scenarios.

摘要: 学术抄袭等大型语言模型（LLM）的滥用推动了识别LLM生成文本的检测器的发展。为了绕过这些检测器，出现了故意重写这些文本以逃避检测的重述攻击。尽管取得了成功，但现有方法需要大量的数据和计算预算来训练专门的解释器，并且当面对先进的检测算法时，它们的攻击功效会大大降低。为了解决这个问题，我们提出了\textBF{Co} ntrasive\textBF{P}araphrase \textBF{A}ttack（CoPA），这是一种免训练方法，可以使用现成的LLM有效地欺骗文本检测器。第一步是仔细编写指令，鼓励LLM生成更多类似人类的文本。尽管如此，我们观察到LLM固有的统计偏差仍然会导致一些生成的文本携带某些可以被检测器捕获的类似机器的属性。为了克服这个问题，CoPA构建了一个辅助的类似机器的单词分布，与LLM生成的类似人类的分布形成对比。通过在解码过程中从类人分布中减去类机器模式，CoPA能够生成文本检测器难以识别的句子。我们的理论分析表明了拟议攻击的优越性。大量实验验证了CoPA在各种场景中欺骗文本检测器的有效性。



## **14. Towards Zero-Shot Differential Morphing Attack Detection with Multimodal Large Language Models**

利用多模式大型语言模型实现零镜头差异变形攻击检测 cs.CV

Accepted at IEEE International Conference on Automatic Face and  Gesture Recognition (FG 2025)

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2505.15332v1) [paper-pdf](http://arxiv.org/pdf/2505.15332v1)

**Authors**: Ria Shekhawat, Hailin Li, Raghavendra Ramachandra, Sushma Venkatesh

**Abstract**: Leveraging the power of multimodal large language models (LLMs) offers a promising approach to enhancing the accuracy and interpretability of morphing attack detection (MAD), especially in real-world biometric applications. This work introduces the use of LLMs for differential morphing attack detection (D-MAD). To the best of our knowledge, this is the first study to employ multimodal LLMs to D-MAD using real biometric data. To effectively utilize these models, we design Chain-of-Thought (CoT)-based prompts to reduce failure-to-answer rates and enhance the reasoning behind decisions. Our contributions include: (1) the first application of multimodal LLMs for D-MAD using real data subjects, (2) CoT-based prompt engineering to improve response reliability and explainability, (3) comprehensive qualitative and quantitative benchmarking of LLM performance using data from 54 individuals captured in passport enrollment scenarios, and (4) comparative analysis of two multimodal LLMs: ChatGPT-4o and Gemini providing insights into their morphing attack detection accuracy and decision transparency. Experimental results show that ChatGPT-4o outperforms Gemini in detection accuracy, especially against GAN-based morphs, though both models struggle under challenging conditions. While Gemini offers more consistent explanations, ChatGPT-4o is more resilient but prone to a higher failure-to-answer rate.

摘要: 利用多模式大型语言模型（LLM）的力量提供了一种有希望的方法来增强变形攻击检测（MAD）的准确性和可解释性，特别是在现实世界的生物识别应用中。这项工作介绍了使用LLM进行差异变形攻击检测（D-MAD）。据我们所知，这是第一项使用真实生物识别数据将多模式LLM用于D-MAD的研究。为了有效地利用这些模型，我们设计了基于思想链（CoT）的提示，以降低未回答率并增强决策背后的推理。我们的贡献包括：（1）使用真实数据对象首次应用多模式LLM进行D-MAD，（2）基于CoT的提示工程以提高响应可靠性和可解释性，（3）使用护照登记场景中捕获的54名个人的数据对LLM性能进行全面的定性和定量基准测试，（4）两种多模式LLM的比较分析：ChatGPT-4 o和Gemini提供了有关其变形攻击检测准确性和决策透明度的见解。实验结果表明，ChatGPT-4 o在检测准确性方面优于Gemini，尤其是针对基于GAN的变形，尽管这两种模型在具有挑战性的条件下都很困难。虽然Gemini提供了更一致的解释，但ChatGPT-4 o更有弹性，但容易出现更高的失败率。



## **15. Improving LLM First-Token Predictions in Multiple-Choice Question Answering via Prefilling Attack**

通过预填充攻击改进多项选择题回答中的LLM第一令牌预测 cs.CL

13 pages, 5 figures, 7 tables

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2505.15323v1) [paper-pdf](http://arxiv.org/pdf/2505.15323v1)

**Authors**: Silvia Cappelletti, Tobia Poppi, Samuele Poppi, Zheng-Xin Yong, Diego Garcia-Olano, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara

**Abstract**: Large Language Models (LLMs) are increasingly evaluated on multiple-choice question answering (MCQA) tasks using *first-token probability* (FTP), which selects the answer option whose initial token has the highest likelihood. While efficient, FTP can be fragile: models may assign high probability to unrelated tokens (*misalignment*) or use a valid token merely as part of a generic preamble rather than as a clear answer choice (*misinterpretation*), undermining the reliability of symbolic evaluation. We propose a simple solution: the *prefilling attack*, a structured natural-language prefix (e.g., "*The correct option is:*") prepended to the model output. Originally explored in AI safety, we repurpose prefilling to steer the model to respond with a clean, valid option, without modifying its parameters. Empirically, the FTP with prefilling strategy substantially improves accuracy, calibration, and output consistency across a broad set of LLMs and MCQA benchmarks. It outperforms standard FTP and often matches the performance of open-ended generation approaches that require full decoding and external classifiers, while being significantly more efficient. Our findings suggest that prefilling is a simple, robust, and low-cost method to enhance the reliability of FTP-based evaluation in multiple-choice settings.

摘要: 大型语言模型（LLM）越来越多地使用 * 第一令牌概率 *（TP）对多项选择题回答（MCQA）任务进行评估，该概率选择初始令牌可能性最高的答案选项。虽然高效，但RTP可能很脆弱：模型可能会将高概率分配给不相关的标记（* 未对准 *），或者仅将有效标记用作通用前序的一部分，而不是作为明确的答案选择（* 误解 *），从而破坏了符号评估的可靠性。我们提出了一个简单的解决方案：* 预填充攻击 *，结构化自然语言前置（例如，“* 正确的选项是：*”）前置于模型输出。我们最初在人工智能安全性方面进行探索，重新利用预填充，以引导模型以干净、有效的选项做出响应，而无需修改其参数。从经验上看，具有预填充策略的RTP大大提高了一系列LLM和MCQA基准的准确性、校准和输出一致性。它的性能优于标准的RTP，并且通常与需要完全解码和外部分类器的开放式生成方法的性能相匹配，同时效率明显更高。我们的研究结果表明，预填充是一种简单、稳健且低成本的方法，可以增强多项选择设置中基于STP的评估的可靠性。



## **16. Securing RAG: A Risk Assessment and Mitigation Framework**

保护RAG：风险评估和缓解框架 cs.CR

8 pages, 3 figures, Sara Ott and Lukas Ammann contributed equally.  This work has been submitted to the IEEE for possible publication

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2505.08728v2) [paper-pdf](http://arxiv.org/pdf/2505.08728v2)

**Authors**: Lukas Ammann, Sara Ott, Christoph R. Landolt, Marco P. Lehmann

**Abstract**: Retrieval Augmented Generation (RAG) has emerged as the de facto industry standard for user-facing NLP applications, offering the ability to integrate data without re-training or fine-tuning Large Language Models (LLMs). This capability enhances the quality and accuracy of responses but also introduces novel security and privacy challenges, particularly when sensitive data is integrated. With the rapid adoption of RAG, securing data and services has become a critical priority. This paper first reviews the vulnerabilities of RAG pipelines, and outlines the attack surface from data pre-processing and data storage management to integration with LLMs. The identified risks are then paired with corresponding mitigations in a structured overview. In a second step, the paper develops a framework that combines RAG-specific security considerations, with existing general security guidelines, industry standards, and best practices. The proposed framework aims to guide the implementation of robust, compliant, secure, and trustworthy RAG systems.

摘要: 检索增强生成（RAG）已成为面向用户的NLP应用程序事实上的行业标准，提供集成数据的能力，无需重新训练或微调大型语言模型（LLM）。这种能力增强了响应的质量和准确性，但也带来了新的安全和隐私挑战，特别是在集成敏感数据时。随着RAG的迅速采用，保护数据和服务已成为首要任务。本文首先回顾了RAG管道的漏洞，概述了从数据预处理、数据存储管理到与LLM集成的攻击面。然后，在结构化概述中将识别的风险与相应的缓解措施配对。第二步，本文开发了一个框架，该框架将RAG特定的安全考虑因素与现有的通用安全准则、行业标准和最佳实践相结合。拟议的框架旨在指导稳健、合规、安全且值得信赖的RAG系统的实施。



## **17. Blind Spot Navigation: Evolutionary Discovery of Sensitive Semantic Concepts for LVLMs**

盲点导航：LVLM敏感语义概念的进化发现 cs.CV

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2505.15265v1) [paper-pdf](http://arxiv.org/pdf/2505.15265v1)

**Authors**: Zihao Pan, Yu Tong, Weibin Wu, Jingyi Wang, Lifeng Chen, Zhe Zhao, Jiajia Wei, Yitong Qiao, Zibin Zheng

**Abstract**: Adversarial attacks aim to generate malicious inputs that mislead deep models, but beyond causing model failure, they cannot provide certain interpretable information such as ``\textit{What content in inputs make models more likely to fail?}'' However, this information is crucial for researchers to specifically improve model robustness. Recent research suggests that models may be particularly sensitive to certain semantics in visual inputs (such as ``wet,'' ``foggy''), making them prone to errors. Inspired by this, in this paper we conducted the first exploration on large vision-language models (LVLMs) and found that LVLMs indeed are susceptible to hallucinations and various errors when facing specific semantic concepts in images. To efficiently search for these sensitive concepts, we integrated large language models (LLMs) and text-to-image (T2I) models to propose a novel semantic evolution framework. Randomly initialized semantic concepts undergo LLM-based crossover and mutation operations to form image descriptions, which are then converted by T2I models into visual inputs for LVLMs. The task-specific performance of LVLMs on each input is quantified as fitness scores for the involved semantics and serves as reward signals to further guide LLMs in exploring concepts that induce LVLMs. Extensive experiments on seven mainstream LVLMs and two multimodal tasks demonstrate the effectiveness of our method. Additionally, we provide interesting findings about the sensitive semantics of LVLMs, aiming to inspire further in-depth research.

摘要: 对抗性攻击旨在生成误导深度模型的恶意输入，但除了导致模型失败之外，它们无法提供某些可解释的信息，例如'\textit{输入中的哪些内容使模型更有可能失败？}”“然而，这些信息对于研究人员专门提高模型稳健性至关重要。最近的研究表明，模型可能对视觉输入中的某些语义（例如“湿”、“雾”）特别敏感，这使得它们容易出错。受此启发，本文对大型视觉语言模型（LVLM）进行了首次探索，发现LVLM在面对图像中的特定语义概念时确实容易产生幻觉和各种错误。为了有效地搜索这些敏感概念，我们集成了大型语言模型（LLM）和文本到图像（T2 I）模型，提出了一种新颖的语义进化框架。随机初始化的语义概念经过基于LLM的交叉和变异操作以形成图像描述，然后由T2 I模型将其转换为LVLM的视觉输入。LVLM在每个输入上的特定任务性能被量化为所涉及语义的适应度分数，并作为奖励信号，以进一步指导LLM探索引发LVLM的概念。对七种主流LVLM和两种多模式任务的广泛实验证明了我们方法的有效性。此外，我们还提供了有关LVLM敏感语义的有趣发现，旨在激发进一步的深入研究。



## **18. From Words to Collisions: LLM-Guided Evaluation and Adversarial Generation of Safety-Critical Driving Scenarios**

从言语到碰撞：法学硕士指导的评估和安全关键驾驶场景的对抗生成 cs.AI

New version of the paper

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2502.02145v3) [paper-pdf](http://arxiv.org/pdf/2502.02145v3)

**Authors**: Yuan Gao, Mattia Piccinini, Korbinian Moller, Amr Alanwar, Johannes Betz

**Abstract**: Ensuring the safety of autonomous vehicles requires virtual scenario-based testing, which depends on the robust evaluation and generation of safety-critical scenarios. So far, researchers have used scenario-based testing frameworks that rely heavily on handcrafted scenarios as safety metrics. To reduce the effort of human interpretation and overcome the limited scalability of these approaches, we combine Large Language Models (LLMs) with structured scenario parsing and prompt engineering to automatically evaluate and generate safety-critical driving scenarios. We introduce Cartesian and Ego-centric prompt strategies for scenario evaluation, and an adversarial generation module that modifies trajectories of risk-inducing vehicles (ego-attackers) to create critical scenarios. We validate our approach using a 2D simulation framework and multiple pre-trained LLMs. The results show that the evaluation module effectively detects collision scenarios and infers scenario safety. Meanwhile, the new generation module identifies high-risk agents and synthesizes realistic, safety-critical scenarios. We conclude that an LLM equipped with domain-informed prompting techniques can effectively evaluate and generate safety-critical driving scenarios, reducing dependence on handcrafted metrics. We release our open-source code and scenarios at: https://github.com/TUM-AVS/From-Words-to-Collisions.

摘要: 确保自动驾驶汽车的安全需要基于虚拟环境的测试，这取决于安全关键场景的稳健评估和生成。到目前为止，研究人员已经使用基于情景的测试框架，这些框架严重依赖手工制作的场景作为安全指标。为了减少人类解释的工作量并克服这些方法的有限可扩展性，我们将大型语言模型（LLM）与结构化场景解析相结合，并提示工程技术自动评估和生成对安全至关重要的驾驶场景。我们引入了用于场景评估的Cartesian和以自我为中心的提示策略，以及一个对抗生成模块，该模块修改风险诱导车辆（自我攻击者）的轨迹以创建关键场景。我们使用2D仿真框架和多个预先训练的LLM来验证我们的方法。结果表明，该评估模块能够有效地检测碰撞场景，并推断出场景安全性.与此同时，新一代模块识别高风险代理并综合现实的安全关键场景。我们的结论是，LLM配备域知情的提示技术可以有效地评估和生成安全关键的驾驶场景，减少依赖手工制作的指标。我们在https://github.com/TUM-AVS/From-Words-to-Collisions上发布我们的开源代码和场景。



## **19. Few-Shot Adversarial Low-Rank Fine-Tuning of Vision-Language Models**

视觉语言模型的少镜头对抗低级微调 cs.LG

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2505.15130v1) [paper-pdf](http://arxiv.org/pdf/2505.15130v1)

**Authors**: Sajjad Ghiasvand, Haniyeh Ehsani Oskouie, Mahnoosh Alizadeh, Ramtin Pedarsani

**Abstract**: Vision-Language Models (VLMs) such as CLIP have shown remarkable performance in cross-modal tasks through large-scale contrastive pre-training. To adapt these large transformer-based models efficiently for downstream tasks, Parameter-Efficient Fine-Tuning (PEFT) techniques like LoRA have emerged as scalable alternatives to full fine-tuning, especially in few-shot scenarios. However, like traditional deep neural networks, VLMs are highly vulnerable to adversarial attacks, where imperceptible perturbations can significantly degrade model performance. Adversarial training remains the most effective strategy for improving model robustness in PEFT. In this work, we propose AdvCLIP-LoRA, the first algorithm designed to enhance the adversarial robustness of CLIP models fine-tuned with LoRA in few-shot settings. Our method formulates adversarial fine-tuning as a minimax optimization problem and provides theoretical guarantees for convergence under smoothness and nonconvex-strong-concavity assumptions. Empirical results across eight datasets using ViT-B/16 and ViT-B/32 models show that AdvCLIP-LoRA significantly improves robustness against common adversarial attacks (e.g., FGSM, PGD), without sacrificing much clean accuracy. These findings highlight AdvCLIP-LoRA as a practical and theoretically grounded approach for robust adaptation of VLMs in resource-constrained settings.

摘要: 通过大规模对比预训练，CLIP等视觉语言模型（VLM）在跨模式任务中表现出了出色的表现。为了有效地调整这些基于变压器的大型模型以适应下游任务，LoRA等参数高效微调（PEFT）技术已成为完全微调的可扩展替代方案，尤其是在少量场景中。然而，与传统的深度神经网络一样，VLM非常容易受到对抗攻击，其中不可感知的扰动可能会显着降低模型性能。对抗训练仍然是提高PEFT模型稳健性的最有效策略。在这项工作中，我们提出了AdvCLIP-LoRA，这是第一个旨在增强在少数镜头设置中使用LoRA微调的CLIP模型的对抗鲁棒性的算法。我们的方法将对抗性微调表述为极小极大优化问题，并为光滑性和非凸强插值假设下的收敛提供理论保证。使用ViT-B/16和ViT-B/32模型的八个数据集的经验结果表明，AdvCLIP-LoRA显着提高了针对常见对抗攻击（例如，FGSM、PVD），而不会牺牲太多干净的准确性。这些发现凸显了AdvCLIP-LoRA是一种实用且具有理论依据的方法，用于在资源有限的环境中稳健地适应VLM。



## **20. AGENTFUZZER: Generic Black-Box Fuzzing for Indirect Prompt Injection against LLM Agents**

AGENTFUZER：通用黑匣子模糊处理，用于立即间接注射LLM试剂 cs.CR

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2505.05849v2) [paper-pdf](http://arxiv.org/pdf/2505.05849v2)

**Authors**: Zhun Wang, Vincent Siu, Zhe Ye, Tianneng Shi, Yuzhou Nie, Xuandong Zhao, Chenguang Wang, Wenbo Guo, Dawn Song

**Abstract**: The strong planning and reasoning capabilities of Large Language Models (LLMs) have fostered the development of agent-based systems capable of leveraging external tools and interacting with increasingly complex environments. However, these powerful features also introduce a critical security risk: indirect prompt injection, a sophisticated attack vector that compromises the core of these agents, the LLM, by manipulating contextual information rather than direct user prompts. In this work, we propose a generic black-box fuzzing framework, AgentXploit, designed to automatically discover and exploit indirect prompt injection vulnerabilities across diverse LLM agents. Our approach starts by constructing a high-quality initial seed corpus, then employs a seed selection algorithm based on Monte Carlo Tree Search (MCTS) to iteratively refine inputs, thereby maximizing the likelihood of uncovering agent weaknesses. We evaluate AgentXploit on two public benchmarks, AgentDojo and VWA-adv, where it achieves 71% and 70% success rates against agents based on o3-mini and GPT-4o, respectively, nearly doubling the performance of baseline attacks. Moreover, AgentXploit exhibits strong transferability across unseen tasks and internal LLMs, as well as promising results against defenses. Beyond benchmark evaluations, we apply our attacks in real-world environments, successfully misleading agents to navigate to arbitrary URLs, including malicious sites.

摘要: 大型语言模型（LLM）强大的规划和推理能力促进了基于代理的系统的开发，这些系统能够利用外部工具并与日益复杂的环境进行交互。然而，这些强大的功能也引入了一个严重的安全风险：间接提示注入，这是一种复杂的攻击载体，通过操纵上下文信息而不是直接用户提示来损害这些代理的核心LLM。在这项工作中，我们提出了一个通用的黑匣子模糊框架AgentXploit，旨在自动发现和利用不同LLM代理之间的间接提示注入漏洞。我们的方法首先构建高质量的初始种子库，然后采用基于蒙特卡洛树搜索（MCTS）的种子选择算法来迭代细化输入，从而最大化发现代理弱点的可能性。我们在AgentDojo和VWA-adv这两个公共基准上评估了AgentXploit，它分别对基于o3-mini和GPT-4 o的代理实现了71%和70%的成功率，几乎是基线攻击性能的两倍。此外，AgentXploit在看不见的任务和内部LLM之间具有很强的可移植性，以及对抗防御的有希望的结果。除了基准评估之外，我们还将我们的攻击应用于现实环境中，成功地误导代理导航到任意URL，包括恶意网站。



## **21. Optimizing Adaptive Attacks against Watermarks for Language Models**

优化针对语言模型水印的自适应攻击 cs.CR

To appear at the International Conference on Machine Learning  (ICML'25)

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2410.02440v2) [paper-pdf](http://arxiv.org/pdf/2410.02440v2)

**Authors**: Abdulrahman Diaa, Toluwani Aremu, Nils Lukas

**Abstract**: Large Language Models (LLMs) can be misused to spread unwanted content at scale. Content watermarking deters misuse by hiding messages in content, enabling its detection using a secret watermarking key. Robustness is a core security property, stating that evading detection requires (significant) degradation of the content's quality. Many LLM watermarking methods have been proposed, but robustness is tested only against non-adaptive attackers who lack knowledge of the watermarking method and can find only suboptimal attacks. We formulate watermark robustness as an objective function and use preference-based optimization to tune adaptive attacks against the specific watermarking method. Our evaluation shows that (i) adaptive attacks evade detection against all surveyed watermarks, (ii) training against any watermark succeeds in evading unseen watermarks, and (iii) optimization-based attacks are cost-effective. Our findings underscore the need to test robustness against adaptively tuned attacks. We release our adaptively optimized paraphrasers at https://github.com/nilslukas/ada-wm-evasion.

摘要: 大型语言模型（LLM）可能会被滥用来大规模传播不需要的内容。内容水印通过在内容中隐藏消息来阻止滥用，从而使用秘密水印密钥进行检测。稳健性是核心安全属性，表明逃避检测需要（显着）降低内容质量。已经提出了许多LLM水印方法，但鲁棒性仅针对缺乏水印方法知识并且只能发现次优攻击的非适应性攻击者进行测试。我们将水印鲁棒性制定为目标函数，并使用基于偏好的优化来调整针对特定水印方法的自适应攻击。我们的评估表明，（i）自适应攻击可以逃避对所有调查的水印的检测，（ii）针对任何水印的训练可以成功地逃避不可见的水印，（iii）基于优化的攻击具有成本效益。我们的发现强调了测试针对自适应调整攻击的稳健性的必要性。我们在https://github.com/nilslukas/ada-wm-evasion上发布了自适应优化的解释。



## **22. AudioJailbreak: Jailbreak Attacks against End-to-End Large Audio-Language Models**

AudioJailbreak：针对端到端大型音频语言模型的越狱攻击 cs.CR

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2505.14103v2) [paper-pdf](http://arxiv.org/pdf/2505.14103v2)

**Authors**: Guangke Chen, Fu Song, Zhe Zhao, Xiaojun Jia, Yang Liu, Yanchen Qiao, Weizhe Zhang

**Abstract**: Jailbreak attacks to Large audio-language models (LALMs) are studied recently, but they achieve suboptimal effectiveness, applicability, and practicability, particularly, assuming that the adversary can fully manipulate user prompts. In this work, we first conduct an extensive experiment showing that advanced text jailbreak attacks cannot be easily ported to end-to-end LALMs via text-to speech (TTS) techniques. We then propose AudioJailbreak, a novel audio jailbreak attack, featuring (1) asynchrony: the jailbreak audio does not need to align with user prompts in the time axis by crafting suffixal jailbreak audios; (2) universality: a single jailbreak perturbation is effective for different prompts by incorporating multiple prompts into perturbation generation; (3) stealthiness: the malicious intent of jailbreak audios will not raise the awareness of victims by proposing various intent concealment strategies; and (4) over-the-air robustness: the jailbreak audios remain effective when being played over the air by incorporating the reverberation distortion effect with room impulse response into the generation of the perturbations. In contrast, all prior audio jailbreak attacks cannot offer asynchrony, universality, stealthiness, or over-the-air robustness. Moreover, AudioJailbreak is also applicable to the adversary who cannot fully manipulate user prompts, thus has a much broader attack scenario. Extensive experiments with thus far the most LALMs demonstrate the high effectiveness of AudioJailbreak. We highlight that our work peeks into the security implications of audio jailbreak attacks against LALMs, and realistically fosters improving their security robustness. The implementation and audio samples are available at our website https://audiojailbreak.github.io/AudioJailbreak.

摘要: 最近研究了对大型音频语言模型（LALM）的越狱攻击，但它们的有效性、适用性和实用性达到了次优，特别是假设对手可以完全操纵用户提示。在这项工作中，我们首先进行了一项广泛的实验，表明高级文本越狱攻击无法通过文本转语音（TTC）技术轻松移植到端到端LALM。然后，我们提出AudioJailbreak，一种新颖的音频越狱攻击，其特点是：（1）狡猾：越狱音频不需要通过制作后缀的越狱音频在时间轴上与用户提示对齐;（2）通用性：通过将多个提示合并到扰动生成中，单个越狱扰动对不同的提示有效;（3）隐蔽性：越狱音频的恶意意图不会通过提出各种意图隐藏策略来提高受害者的意识;以及（4）空中鲁棒性：越狱音频在空中播放时仍然有效，通过将回响失真效应与房间脉冲响应结合起来扰动的产生。相比之下，所有先前的音频越狱攻击都无法提供灵活性、普遍性、隐蔽性或空中鲁棒性。此外，AudioJailbreak还适用于无法完全操纵用户提示的对手，因此具有更广泛的攻击场景。迄今为止，对大多数LALM的广泛实验证明了AudioJailbreak的高有效性。我们强调，我们的工作探讨了针对LALM的音频越狱攻击的安全影响，并切实促进了其安全稳健性的提高。实现和音频示例可在我们的网站https://audiojailbreak.github.io/AudioJailbreak上获取。



## **23. sudoLLM : On Multi-role Alignment of Language Models**

sudoLLM：关于语言模型的多角色对齐 cs.CL

Under review. Code and data to be released later

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2505.14607v1) [paper-pdf](http://arxiv.org/pdf/2505.14607v1)

**Authors**: Soumadeep Saha, Akshay Chaturvedi, Joy Mahapatra, Utpal Garain

**Abstract**: User authorization-based access privileges are a key feature in many safety-critical systems, but have thus far been absent from the large language model (LLM) realm. In this work, drawing inspiration from such access control systems, we introduce sudoLLM, a novel framework that results in multi-role aligned LLMs, i.e., LLMs that account for, and behave in accordance with, user access rights. sudoLLM injects subtle user-based biases into queries and trains an LLM to utilize this bias signal in order to produce sensitive information if and only if the user is authorized. We present empirical results demonstrating that this approach shows substantially improved alignment, generalization, and resistance to prompt-based jailbreaking attacks. The persistent tension between the language modeling objective and safety alignment, which is often exploited to jailbreak LLMs, is somewhat resolved with the aid of the injected bias signal. Our framework is meant as an additional security layer, and complements existing guardrail mechanisms for enhanced end-to-end safety with LLMs.

摘要: 基于用户授权的访问特权是许多安全关键系统的一个关键功能，但迄今为止在大型语言模型（LLM）领域还没有。在这项工作中，我们从此类访问控制系统中汲取灵感，引入了sudoLLM，这是一种新颖的框架，可以产生多角色对齐的LLM，即负责用户访问权限并按照用户访问权限行事的LLM。sudoLLM将微妙的基于用户的偏见注入到查询中，并训练LLM利用此偏见信号，以便在且仅在用户获得授权的情况下生成敏感信息。我们提出的经验结果表明，这种方法显示出对基于预算的越狱攻击的一致性、概括性和抵抗性大幅提高。语言建模目标和安全对齐之间的持续紧张关系（通常被用来越狱LLM）在注入的偏见信号的帮助下在一定程度上得到了解决。我们的框架旨在作为额外的安全层，并补充现有的护栏机制，通过LLM增强端到端安全性。



## **24. MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety**

MrGuard：通用LLM安全的多语言推理保障 cs.CL

Preprint

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2504.15241v2) [paper-pdf](http://arxiv.org/pdf/2504.15241v2)

**Authors**: Yahan Yang, Soham Dan, Shuo Li, Dan Roth, Insup Lee

**Abstract**: Large Language Models (LLMs) are susceptible to adversarial attacks such as jailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability is exacerbated in multilingual settings, where multilingual safety-aligned data is often limited. Thus, developing a guardrail capable of detecting and filtering unsafe content across diverse languages is critical for deploying LLMs in real-world applications. In this work, we introduce a multilingual guardrail with reasoning for prompt classification. Our method consists of: (1) synthetic multilingual data generation incorporating culturally and linguistically nuanced variants, (2) supervised fine-tuning, and (3) a curriculum-based Group Relative Policy Optimization (GRPO) framework that further improves performance. Experimental results demonstrate that our multilingual guardrail, MrGuard, consistently outperforms recent baselines across both in-domain and out-of-domain languages by more than 15%. We also evaluate MrGuard's robustness to multilingual variations, such as code-switching and low-resource language distractors in the prompt, and demonstrate that it preserves safety judgments under these challenging conditions. The multilingual reasoning capability of our guardrail enables it to generate explanations, which are particularly useful for understanding language-specific risks and ambiguities in multilingual content moderation.

摘要: 大型语言模型（LLM）容易受到诸如越狱之类的对抗性攻击，这可能会引发有害或不安全的行为。这种漏洞在多语言环境中会加剧，其中多语言安全一致的数据通常是有限的。因此，开发一个能够检测和过滤不同语言的不安全内容的护栏对于在现实世界的应用程序中部署LLM至关重要。在这项工作中，我们介绍了一种多语言护栏，具有快速分类的推理。我们的方法包括：（1）综合多语言数据生成，融合了文化和语言上的细微差别，（2）监督式微调，以及（3）进一步提高性能的基于课程的组相对政策优化（GRPO）框架。实验结果表明，我们的多语言护栏MrGuard在域内和域外语言中的表现始终优于最近的基线15%以上。我们还评估了MrGuard对多语言变体（例如提示中的代码切换和低资源语言干扰因素）的稳健性，并证明它在这些具有挑战性的条件下保留了安全判断。我们护栏的多语言推理能力使其能够生成解释，这对于理解多语言内容审核中的特定语言风险和歧义特别有用。



## **25. Char-mander Use mBackdoor! A Study of Cross-lingual Backdoor Attacks in Multilingual LLMs**

Char-mander使用mBackdoor！多语言LLM中的跨语言后门攻击研究 cs.CL

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2502.16901v2) [paper-pdf](http://arxiv.org/pdf/2502.16901v2)

**Authors**: Himanshu Beniwal, Sailesh Panda, Birudugadda Srivibhav, Mayank Singh

**Abstract**: We explore \textbf{C}ross-lingual \textbf{B}ackdoor \textbf{AT}tacks (X-BAT) in multilingual Large Language Models (mLLMs), revealing how backdoors inserted in one language can automatically transfer to others through shared embedding spaces. Using toxicity classification as a case study, we demonstrate that attackers can compromise multilingual systems by poisoning data in a single language, with rare and high-occurring tokens serving as specific, effective triggers. Our findings expose a critical vulnerability that influences the model's architecture, resulting in a concealed backdoor effect during the information flow. Our code and data are publicly available https://github.com/himanshubeniwal/X-BAT.

摘要: 我们探索了多语言大型语言模型（mLLM）中的\textBF{C}ross-lingual \textBF{B}ackdoor \textBF{AT}tacks（X-BAT），揭示了插入一种语言的后门如何通过共享嵌入空间自动传输到其他语言。使用毒性分类作为案例研究，我们证明攻击者可以通过毒害单一语言的数据来危害多语言系统，其中罕见且高出现的标记充当特定、有效的触发器。我们的研究结果暴露了影响模型架构的一个关键漏洞，导致信息流期间隐藏的后门效应。我们的代码和数据可在https://github.com/himanshubeniwal/X-BAT上公开获取。



## **26. Breaking Bad Tokens: Detoxification of LLMs Using Sparse Autoencoders**

绝命毒师代币：使用稀疏自动编码器去规范化LLM cs.CL

Preprint: 19 pages, 7 figures, 1 table

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2505.14536v1) [paper-pdf](http://arxiv.org/pdf/2505.14536v1)

**Authors**: Agam Goyal, Vedant Rathi, William Yeh, Yian Wang, Yuen Chen, Hari Sundaram

**Abstract**: Large language models (LLMs) are now ubiquitous in user-facing applications, yet they still generate undesirable toxic outputs, including profanity, vulgarity, and derogatory remarks. Although numerous detoxification methods exist, most apply broad, surface-level fixes and can therefore easily be circumvented by jailbreak attacks. In this paper we leverage sparse autoencoders (SAEs) to identify toxicity-related directions in the residual stream of models and perform targeted activation steering using the corresponding decoder vectors. We introduce three tiers of steering aggressiveness and evaluate them on GPT-2 Small and Gemma-2-2B, revealing trade-offs between toxicity reduction and language fluency. At stronger steering strengths, these causal interventions surpass competitive baselines in reducing toxicity by up to 20%, though fluency can degrade noticeably on GPT-2 Small depending on the aggressiveness. Crucially, standard NLP benchmark scores upon steering remain stable, indicating that the model's knowledge and general abilities are preserved. We further show that feature-splitting in wider SAEs hampers safety interventions, underscoring the importance of disentangled feature learning. Our findings highlight both the promise and the current limitations of SAE-based causal interventions for LLM detoxification, further suggesting practical guidelines for safer language-model deployment.

摘要: 大型语言模型（LLM）现在在面向用户的应用程序中无处不在，但它们仍然会产生不受欢迎的有毒输出，包括脏话、粗俗和贬损言论。尽管存在多种解毒方法，但大多数都适用于广泛的、表面的修复，因此很容易被越狱攻击规避。在本文中，我们利用稀疏自动编码器（SAEs）来识别模型剩余流中与毒性相关的方向，并使用相应的解码器载体执行有针对性的激活引导。我们引入了三层转向攻击性，并在GPT-2 Small和Gemma-2-2B上对其进行了评估，揭示了毒性降低和语言流利性之间的权衡。在更强的引导强度下，这些因果干预措施在将毒性降低高达20%方面超过了竞争基线，尽管根据攻击性的不同，GPT-2 Small的流畅性可能会显着下降。至关重要的是，转向后的标准NLP基准分数保持稳定，这表明模型的知识和一般能力得到了保留。我们进一步表明，更广泛的严重不良事件中的特征分裂会阻碍安全干预，强调了解开特征学习的重要性。我们的研究结果强调了LLM解毒基于CAE的因果干预措施的前景和当前的局限性，进一步为更安全的语言模型部署提出了实用指南。



## **27. Hidden Ghost Hand: Unveiling Backdoor Vulnerabilities in MLLM-Powered Mobile GUI Agents**

Hidden Ghost Hand：揭露MLLM支持的移动图形用户界面代理中的后门漏洞 cs.CL

25 pages, 10 figures, 12 Tables

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2505.14418v1) [paper-pdf](http://arxiv.org/pdf/2505.14418v1)

**Authors**: Pengzhou Cheng, Haowen Hu, Zheng Wu, Zongru Wu, Tianjie Ju, Daizong Ding, Zhuosheng Zhang, Gongshen Liu

**Abstract**: Graphical user interface (GUI) agents powered by multimodal large language models (MLLMs) have shown greater promise for human-interaction. However, due to the high fine-tuning cost, users often rely on open-source GUI agents or APIs offered by AI providers, which introduces a critical but underexplored supply chain threat: backdoor attacks. In this work, we first unveil that MLLM-powered GUI agents naturally expose multiple interaction-level triggers, such as historical steps, environment states, and task progress. Based on this observation, we introduce AgentGhost, an effective and stealthy framework for red-teaming backdoor attacks. Specifically, we first construct composite triggers by combining goal and interaction levels, allowing GUI agents to unintentionally activate backdoors while ensuring task utility. Then, we formulate backdoor injection as a Min-Max optimization problem that uses supervised contrastive learning to maximize the feature difference across sample classes at the representation space, improving flexibility of the backdoor. Meanwhile, it adopts supervised fine-tuning to minimize the discrepancy between backdoor and clean behavior generation, enhancing effectiveness and utility. Extensive evaluations of various agent models in two established mobile benchmarks show that AgentGhost is effective and generic, with attack accuracy that reaches 99.7\% on three attack objectives, and shows stealthiness with only 1\% utility degradation. Furthermore, we tailor a defense method against AgentGhost that reduces the attack accuracy to 22.1\%. Our code is available at \texttt{anonymous}.

摘要: 由多模式大型语言模型（MLLM）支持的图形用户界面（图形用户界面）代理在人际交互方面表现出了更大的前景。然而，由于微调成本很高，用户通常依赖人工智能提供商提供的开源图形界面代理或API，这引入了一个关键但未充分开发的供应链威胁：后门攻击。在这项工作中，我们首先揭示了基于MLLM的图形用户界面代理自然暴露多个交互级触发器，例如历史步骤、环境状态和任务进度。基于这一观察，我们引入了AgentGhost，这是一个用于红色团队后门攻击的有效且隐蔽的框架。具体来说，我们首先通过结合目标和交互级别来构建复合触发器，允许图形用户界面代理无意中激活后门，同时确保任务实用性。然后，我们将后门注入制定为Min-Max优化问题，该问题使用监督对比学习来最大化表示空间中样本类之间的特征差异，从而提高后门的灵活性。同时，它采用监督式微调，以最大限度地减少后门和干净行为生成之间的差异，提高有效性和实用性。对两个已建立的移动基准测试中各种代理模型的广泛评估表明，AgentGhost有效且通用，在三个攻击目标上的攻击准确率达到99.7%，并且表现出隐蔽性，仅使用1%的效用下降。此外，我们针对AgentGhost定制了一种防御方法，将攻击准确率降低至22.1%。我们的代码可在\textttt {anonymous}上获取。



## **28. Is Your Prompt Safe? Investigating Prompt Injection Attacks Against Open-Source LLMs**

您的提示安全吗？调查针对开源LLM的即时注入攻击 cs.CR

8 pages, 3 figures, EMNLP 2025 under review

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2505.14368v1) [paper-pdf](http://arxiv.org/pdf/2505.14368v1)

**Authors**: Jiawen Wang, Pritha Gupta, Ivan Habernal, Eyke Hüllermeier

**Abstract**: Recent studies demonstrate that Large Language Models (LLMs) are vulnerable to different prompt-based attacks, generating harmful content or sensitive information. Both closed-source and open-source LLMs are underinvestigated for these attacks. This paper studies effective prompt injection attacks against the $\mathbf{14}$ most popular open-source LLMs on five attack benchmarks. Current metrics only consider successful attacks, whereas our proposed Attack Success Probability (ASP) also captures uncertainty in the model's response, reflecting ambiguity in attack feasibility. By comprehensively analyzing the effectiveness of prompt injection attacks, we propose a simple and effective hypnotism attack; results show that this attack causes aligned language models, including Stablelm2, Mistral, Openchat, and Vicuna, to generate objectionable behaviors, achieving around $90$% ASP. They also indicate that our ignore prefix attacks can break all $\mathbf{14}$ open-source LLMs, achieving over $60$% ASP on a multi-categorical dataset. We find that moderately well-known LLMs exhibit higher vulnerability to prompt injection attacks, highlighting the need to raise public awareness and prioritize efficient mitigation strategies.

摘要: 最近的研究表明，大型语言模型（LLM）容易受到不同的基于预算的攻击，从而生成有害内容或敏感信息。闭源和开源LLM的这些攻击都没有得到充分的调查。本文在五个攻击基准上研究了针对$\mathBF{14}$最受欢迎的开源LLM的有效即时注入攻击。当前的指标仅考虑成功的攻击，而我们提出的攻击成功概率（ISP）还捕捉了模型响应中的不确定性，反映了攻击可行性的模糊性。通过全面分析提示注入攻击的有效性，我们提出了一种简单有效的催眠攻击;结果表明，这种攻击会导致包括Stablelm 2、Mistral、Openchat和Vicuna在内的对齐语言模型产生令人反感的行为，实现了约90美元$%的目标。它们还表明，我们的忽略前置攻击可以破坏所有$\mathBF{14}$开源LLM，在多类别数据集上实现超过60美元$%的平均利润。我们发现，中等知名的LLM对引发注入攻击的脆弱性更高，这凸显了提高公众意识并优先考虑有效的缓解策略的必要性。



## **29. Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge Distillation**

通过弱到强的知识蒸馏消除LLM的后门攻击 cs.CL

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2410.14425v2) [paper-pdf](http://arxiv.org/pdf/2410.14425v2)

**Authors**: Shuai Zhao, Xiaobao Wu, Cong-Duy Nguyen, Yanhao Jia, Meihuizi Jia, Yichao Feng, Luu Anh Tuan

**Abstract**: Parameter-efficient fine-tuning (PEFT) can bridge the gap between large language models (LLMs) and downstream tasks. However, PEFT has been proven vulnerable to malicious attacks. Research indicates that poisoned LLMs, even after PEFT, retain the capability to activate internalized backdoors when input samples contain predefined triggers. In this paper, we introduce a novel weak-to-strong unlearning algorithm to defend against backdoor attacks based on feature alignment knowledge distillation, named W2SDefense. Specifically, we first train a small-scale language model through full-parameter fine-tuning to serve as the clean teacher model. Then, this teacher model guides the large-scale poisoned student model in unlearning the backdoor, leveraging PEFT. Theoretical analysis suggests that W2SDefense has the potential to enhance the student model's ability to unlearn backdoor features, preventing the activation of the backdoor. We conduct comprehensive experiments on three state-of-the-art large language models and several different backdoor attack algorithms. Our empirical results demonstrate the outstanding performance of W2SDefense in defending against backdoor attacks without compromising model performance.

摘要: 参数高效微调（PEFT）可以弥合大型语言模型（LLM）和下游任务之间的差距。然而，PEFT已被证明容易受到恶意攻击。研究表明，即使在PEFT之后，中毒的LLM也保留在输入样本包含预定义触发器时激活内化后门的能力。本文引入了一种基于特征对齐知识提炼的新型弱到强去学习算法，名为W2 SDefense，以抵御后门攻击。具体来说，我们首先通过全参数微调训练小规模语言模型，作为干净教师模型。然后，这个教师模型引导大规模中毒学生模型利用PEFT摆脱后门。理论分析表明，W2 SDenance有潜力增强学生模型忘记后门功能的能力，防止后门被激活。我们对三种最先进的大型语言模型和几种不同的后门攻击算法进行了全面的实验。我们的实证结果证明了W2 SDenance在防御后门攻击方面具有出色的性能，而不影响模型性能。



## **30. Exploring Jailbreak Attacks on LLMs through Intent Concealment and Diversion**

探索通过意图隐瞒和转移对LLM的越狱攻击 cs.CR

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2505.14316v1) [paper-pdf](http://arxiv.org/pdf/2505.14316v1)

**Authors**: Tiehan Cui, Yanxu Mao, Peipei Liu, Congying Liu, Datao You

**Abstract**: Although large language models (LLMs) have achieved remarkable advancements, their security remains a pressing concern. One major threat is jailbreak attacks, where adversarial prompts bypass model safeguards to generate harmful or objectionable content. Researchers study jailbreak attacks to understand security and robustness of LLMs. However, existing jailbreak attack methods face two main challenges: (1) an excessive number of iterative queries, and (2) poor generalization across models. In addition, recent jailbreak evaluation datasets focus primarily on question-answering scenarios, lacking attention to text generation tasks that require accurate regeneration of toxic content. To tackle these challenges, we propose two contributions: (1) ICE, a novel black-box jailbreak method that employs Intent Concealment and divErsion to effectively circumvent security constraints. ICE achieves high attack success rates (ASR) with a single query, significantly improving efficiency and transferability across different models. (2) BiSceneEval, a comprehensive dataset designed for assessing LLM robustness in question-answering and text-generation tasks. Experimental results demonstrate that ICE outperforms existing jailbreak techniques, revealing critical vulnerabilities in current defense mechanisms. Our findings underscore the necessity of a hybrid security strategy that integrates predefined security mechanisms with real-time semantic decomposition to enhance the security of LLMs.

摘要: 尽管大型语言模型（LLM）取得了显着的进步，但其安全性仍然是一个紧迫的问题。一个主要威胁是越狱攻击，其中敌对性会促使绕过模型保护措施来生成有害或令人反感的内容。研究人员研究越狱攻击以了解LLM的安全性和稳健性。然而，现有的越狱攻击方法面临两个主要挑战：（1）迭代查询数量过多，（2）模型之间的概括性较差。此外，最近的越狱评估数据集主要关注问答场景，缺乏对需要准确再生有毒内容的文本生成任务的关注。为了应对这些挑战，我们提出了两个贡献：（1）ICE，一种新的黑盒越狱方法，采用意图隐藏和分裂，以有效地规避安全约束。ICE通过单个查询实现了高攻击成功率（ASR），显著提高了效率和跨不同模型的可移植性。(2)BiSceneEval是一个综合数据集，旨在评估LLM在问答和文本生成任务中的鲁棒性。实验结果表明，ICE优于现有的越狱技术，揭示了当前防御机制中的关键漏洞。我们的研究结果强调了混合安全策略的必要性，该策略将预定义的安全机制与实时语义分解集成在一起，以增强LLM的安全性。



## **31. Universal Acoustic Adversarial Attacks for Flexible Control of Speech-LLMs**

语音灵活控制的通用声学对抗攻击-LLM cs.CL

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2505.14286v1) [paper-pdf](http://arxiv.org/pdf/2505.14286v1)

**Authors**: Rao Ma, Mengjie Qian, Vyas Raina, Mark Gales, Kate Knill

**Abstract**: The combination of pre-trained speech encoders with large language models has enabled the development of speech LLMs that can handle a wide range of spoken language processing tasks. While these models are powerful and flexible, this very flexibility may make them more vulnerable to adversarial attacks. To examine the extent of this problem, in this work we investigate universal acoustic adversarial attacks on speech LLMs. Here a fixed, universal, adversarial audio segment is prepended to the original input audio. We initially investigate attacks that cause the model to either produce no output or to perform a modified task overriding the original prompt. We then extend the nature of the attack to be selective so that it activates only when specific input attributes, such as a speaker gender or spoken language, are present. Inputs without the targeted attribute should be unaffected, allowing fine-grained control over the model outputs. Our findings reveal critical vulnerabilities in Qwen2-Audio and Granite-Speech and suggest that similar speech LLMs may be susceptible to universal adversarial attacks. This highlights the need for more robust training strategies and improved resistance to adversarial attacks.

摘要: 预训练的语音编码器与大型语言模型的结合使得能够开发出可以处理广泛口语处理任务的语音LLM。虽然这些模型强大且灵活，但这种灵活性可能使它们更容易受到对抗攻击。为了研究这个问题的严重程度，在这项工作中，我们研究了对语音LLM的普遍声学对抗攻击。这里，固定的、通用的、对抗性的音频段被预先添加到原始输入音频上。我们最初调查导致模型不产生输出或执行覆盖原始提示的修改任务的攻击。然后，我们将攻击的性质扩展为选择性，以便只有在特定输入属性（例如说话者性别或口语）存在时，它才会激活。没有目标属性的输入应该不受影响，允许对模型输出进行细粒度控制。我们的研究结果揭示了Qwen 2-Audio和Granite-Speech中的关键漏洞，并表明类似的语音LLM可能容易受到普遍对抗攻击。这凸显了需要更强大的训练策略和提高对对抗性攻击的抵抗力。



## **32. IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems**

针对基于LLM的多代理系统的IP泄露攻击 cs.CR

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2505.12442v2) [paper-pdf](http://arxiv.org/pdf/2505.12442v2)

**Authors**: Liwen Wang, Wenxuan Wang, Shuai Wang, Zongjie Li, Zhenlan Ji, Zongyi Lyu, Daoyuan Wu, Shing-Chi Cheung

**Abstract**: The rapid advancement of Large Language Models (LLMs) has led to the emergence of Multi-Agent Systems (MAS) to perform complex tasks through collaboration. However, the intricate nature of MAS, including their architecture and agent interactions, raises significant concerns regarding intellectual property (IP) protection. In this paper, we introduce MASLEAK, a novel attack framework designed to extract sensitive information from MAS applications. MASLEAK targets a practical, black-box setting, where the adversary has no prior knowledge of the MAS architecture or agent configurations. The adversary can only interact with the MAS through its public API, submitting attack query $q$ and observing outputs from the final agent. Inspired by how computer worms propagate and infect vulnerable network hosts, MASLEAK carefully crafts adversarial query $q$ to elicit, propagate, and retain responses from each MAS agent that reveal a full set of proprietary components, including the number of agents, system topology, system prompts, task instructions, and tool usages. We construct the first synthetic dataset of MAS applications with 810 applications and also evaluate MASLEAK against real-world MAS applications, including Coze and CrewAI. MASLEAK achieves high accuracy in extracting MAS IP, with an average attack success rate of 87% for system prompts and task instructions, and 92% for system architecture in most cases. We conclude by discussing the implications of our findings and the potential defenses.

摘要: 大型语言模型（LLM）的快速发展导致了通过协作执行复杂任务的多智能体系统（MAS）的出现。然而，MAS的复杂性质，包括其架构和代理交互，引发了有关知识产权（IP）保护的严重担忧。本文介绍MASLEAK，这是一种新型攻击框架，旨在从MAS应用程序中提取敏感信息。MASLEAK针对的是实用的黑匣子设置，其中对手不了解MAS架构或代理配置。对手只能通过其公共API与MAS交互，提交攻击查询$q$并观察最终代理的输出。受计算机蠕虫传播和感染脆弱网络主机的方式的启发，MASLEAK精心设计了对抗性查询$q$，以引发、传播和保留每个MAS代理的响应，这些响应揭示了全套专有组件，包括代理数量、系统布局、系统提示、任务指令和工具使用。我们构建了包含810个应用程序的第一个MAS应用程序合成数据集，并根据现实世界的MAS应用程序（包括Coze和CrewAI）评估MASLEAK。MASLEAK在提取MAS IP方面实现了高准确性，系统提示和任务指令的平均攻击成功率为87%，大多数情况下系统架构的平均攻击成功率为92%。最后，我们讨论了我们发现的影响和潜在的防御措施。



## **33. "Haet Bhasha aur Diskrimineshun": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs**

“Haet Bhasha aur rimineshun”：代码混合印度式英语到红队法学硕士中的语音扰动 cs.CL

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2505.14226v1) [paper-pdf](http://arxiv.org/pdf/2505.14226v1)

**Authors**: Darpan Aswal, Siddharth D Jaiswal

**Abstract**: Large Language Models (LLMs) have become increasingly powerful, with multilingual and multimodal capabilities improving by the day. These models are being evaluated through audits, alignment studies and red-teaming efforts to expose model vulnerabilities towards generating harmful, biased and unfair content. Existing red-teaming efforts have previously focused on the English language, using fixed template-based attacks; thus, models continue to be susceptible to multilingual jailbreaking strategies, especially in the multimodal context. In this study, we introduce a novel strategy that leverages code-mixing and phonetic perturbations to jailbreak LLMs for both text and image generation tasks. We also introduce two new jailbreak strategies that show higher effectiveness than baseline strategies. Our work presents a method to effectively bypass safety filters in LLMs while maintaining interpretability by applying phonetic misspellings to sensitive words in code-mixed prompts. Our novel prompts achieve a 99% Attack Success Rate for text generation and 78% for image generation, with Attack Relevance Rate of 100% for text generation and 95% for image generation when using the phonetically perturbed code-mixed prompts. Our interpretability experiments reveal that phonetic perturbations impact word tokenization, leading to jailbreak success. Our study motivates increasing the focus towards more generalizable safety alignment for multilingual multimodal models, especially in real-world settings wherein prompts can have misspelt words.

摘要: 大型语言模型（LLM）变得越来越强大，多语言和多模式能力日益提高。这些模型正在通过审计、对齐研究和红色团队工作进行评估，以暴露模型在生成有害、偏见和不公平内容方面的弱点。现有的红色团队工作此前主要集中在英语上，使用固定的基于模板的攻击;因此，模型仍然容易受到多语言越狱策略的影响，尤其是在多模式环境中。在这项研究中，我们引入了一种新颖的策略，该策略利用代码混合和语音扰动来越狱文本和图像生成任务的LLM。我们还引入了两种新的越狱策略，它们的有效性比基线策略更高。我们的工作提出了一种方法，可以有效地绕过LLM中的安全过滤器，同时通过对代码混合提示中的敏感词应用语音拼写错误来保持可解释性。当使用语音干扰的代码混合提示时，我们的新颖提示的文本生成攻击成功率为99%，图像生成攻击成功率为78%，文本生成攻击相关率为100%，图像生成攻击相关率为95%。我们的可解释性实验表明，语音扰动会影响单词符号化，从而导致越狱成功。我们的研究促使人们更加关注多语言多模式模型的更通用的安全对齐，特别是在提示可能有拼写错误单词的现实环境中。



## **34. Capturing the Effects of Quantization on Trojans in Code LLMs**

捕获量化对代码LLM中特洛伊木马的影响 cs.SE

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2505.14200v1) [paper-pdf](http://arxiv.org/pdf/2505.14200v1)

**Authors**: Aftab Hussain, Sadegh AlMahdi Kazemi Zarkouei, Md Rafiqul Islam Rabin, Mohammad Amin Alipour, Sen Lin, Bowen Xu

**Abstract**: Large language models of code exhibit high capability in performing diverse software engineering tasks, such as code translation, defect detection, text-to-code generation, and code summarization. While their ability to enhance developer productivity has spurred widespread use, these models have also seen substantial growth in size, often reaching billions of parameters. This scale demands efficient memory resource usage, prompting practitioners to use optimization techniques such as model quantization. Quantization uses smaller bit representations for the model parameters, reducing the precision of the weights. In this work, we investigate the impact of quantization on the risk of data poisoning attacks on these models, specifically examining whether it mitigates or exacerbates such vulnerabilities. We focus on two large language models, Meta's Llama-2-7b and CodeLlama-7b, applied to an SQL code generation task. Additionally, we introduce a new metric for measuring trojan signals in compromised models. We find that quantization has differing effects on code-generating LLMs: while reducing precision does not significantly alter Llama-2's behavior, it boosts performance and reduces attack success rates in CodeLlama, particularly at 4-bit precision.

摘要: 大型代码语言模型在执行各种软件工程任务（例如代码翻译、缺陷检测、文本到代码生成和代码摘要）方面表现出很高的能力。虽然它们提高开发人员生产力的能力促进了广泛使用，但这些模型的规模也大幅增长，通常达到数十亿个参数。这种规模需要高效的内存资源使用，促使从业者使用模型量化等优化技术。量化对模型参数使用较小的位表示，从而降低了权重的精确度。在这项工作中，我们调查了量化对这些模型的数据中毒攻击风险的影响，特别是检查它是否缓解或加剧了此类漏洞。我们专注于两个大的语言模型，Meta的Llama-2- 7 b和CodeLlama-7 b，适用于SQL代码生成任务。此外，我们还引入了一种新的指标来测量受损模型中的特洛伊木马信号。我们发现量化对代码生成LLM有不同的影响：虽然降低精度不会显着改变Llama-2的行为，但它会提高CodeLlama的性能并降低攻击成功率，特别是在4位精度下。



## **35. Evaluating the efficacy of LLM Safety Solutions : The Palit Benchmark Dataset**

评估LLM安全解决方案的有效性：Palit基准数据集 cs.CR

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2505.13028v2) [paper-pdf](http://arxiv.org/pdf/2505.13028v2)

**Authors**: Sayon Palit, Daniel Woods

**Abstract**: Large Language Models (LLMs) are increasingly integrated into critical systems in industries like healthcare and finance. Users can often submit queries to LLM-enabled chatbots, some of which can enrich responses with information retrieved from internal databases storing sensitive data. This gives rise to a range of attacks in which a user submits a malicious query and the LLM-system outputs a response that creates harm to the owner, such as leaking internal data or creating legal liability by harming a third-party. While security tools are being developed to counter these threats, there is little formal evaluation of their effectiveness and usability. This study addresses this gap by conducting a thorough comparative analysis of LLM security tools. We identified 13 solutions (9 closed-source, 4 open-source), but only 7 were evaluated due to a lack of participation by proprietary model owners.To evaluate, we built a benchmark dataset of malicious prompts, and evaluate these tools performance against a baseline LLM model (ChatGPT-3.5-Turbo). Our results show that the baseline model has too many false positives to be used for this task. Lakera Guard and ProtectAI LLM Guard emerged as the best overall tools showcasing the tradeoff between usability and performance. The study concluded with recommendations for greater transparency among closed source providers, improved context-aware detections, enhanced open-source engagement, increased user awareness, and the adoption of more representative performance metrics.

摘要: 大型语言模型（LLM）越来越多地集成到医疗保健和金融等行业的关键系统中。用户通常可以向支持LLM的聊天机器人提交查询，其中一些可以使用从存储敏感数据的内部数据库检索的信息来丰富响应。这会引发一系列攻击，其中用户提交恶意查询，LLM系统输出对所有者造成伤害的响应，例如泄露内部数据或通过伤害第三方而产生法律责任。虽然正在开发安全工具来应对这些威胁，但对其有效性和可用性的正式评估很少。本研究通过对LLM安全工具进行彻底的比较分析来解决这一差距。我们确定了13个解决方案（9个封闭源，4个开放源），但由于缺乏专有模型所有者的参与，只评估了7个。为了评估，我们构建了恶意提示的基准数据集，并根据基线LLM模型（ChatGPT-3.5-Turbo）评估这些工具的性能。我们的结果表明，基线模型存在太多假阳性，无法用于此任务。Lakera Guard和ProtectAI LLM Guard成为展示可用性和性能之间权衡的最佳整体工具。该研究最后提出了提高闭源提供商透明度、改进上下文感知检测、增强开源参与度、提高用户意识以及采用更具代表性的性能指标的建议。



## **36. JULI: Jailbreak Large Language Models by Self-Introspection**

JULI：通过自我反省越狱大型语言模型 cs.LG

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2505.11790v2) [paper-pdf](http://arxiv.org/pdf/2505.11790v2)

**Authors**: Jesson Wang, Zhanhao Hu, David Wagner

**Abstract**: Large Language Models (LLMs) are trained with safety alignment to prevent generating malicious content. Although some attacks have highlighted vulnerabilities in these safety-aligned LLMs, they typically have limitations, such as necessitating access to the model weights or the generation process. Since proprietary models through API-calling do not grant users such permissions, these attacks find it challenging to compromise them. In this paper, we propose Jailbreaking Using LLM Introspection (JULI), which jailbreaks LLMs by manipulating the token log probabilities, using a tiny plug-in block, BiasNet. JULI relies solely on the knowledge of the target LLM's predicted token log probabilities. It can effectively jailbreak API-calling LLMs under a black-box setting and knowing only top-$5$ token log probabilities. Our approach demonstrates superior effectiveness, outperforming existing state-of-the-art (SOTA) approaches across multiple metrics.

摘要: 大型语言模型（LLM）经过安全调整训练，以防止生成恶意内容。尽管一些攻击凸显了这些安全一致的LLM中的漏洞，但它们通常具有局限性，例如需要访问模型权重或生成过程。由于通过API调用的专有模型不会向用户授予此类权限，因此这些攻击发现很难损害它们。在本文中，我们提出了使用LLM内省越狱（JULI），它通过使用一个微型插件块BiasNet操纵令牌日志概率来越狱LLM。JULI仅依赖于目标LLM预测的令牌日志概率的知识。它可以在黑匣子设置下有效越狱API调用LLM，并且仅知道最高5美元的代币日志概率。我们的方法表现出卓越的有效性，在多个指标上优于现有的最先进（SOTA）方法。



## **37. From Assistants to Adversaries: Exploring the Security Risks of Mobile LLM Agents**

从助理到对手：探索移动LLM代理的安全风险 cs.CR

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2505.12981v2) [paper-pdf](http://arxiv.org/pdf/2505.12981v2)

**Authors**: Liangxuan Wu, Chao Wang, Tianming Liu, Yanjie Zhao, Haoyu Wang

**Abstract**: The growing adoption of large language models (LLMs) has led to a new paradigm in mobile computing--LLM-powered mobile AI agents--capable of decomposing and automating complex tasks directly on smartphones. However, the security implications of these agents remain largely unexplored. In this paper, we present the first comprehensive security analysis of mobile LLM agents, encompassing three representative categories: System-level AI Agents developed by original equipment manufacturers (e.g., YOYO Assistant), Third-party Universal Agents (e.g., Zhipu AI AutoGLM), and Emerging Agent Frameworks (e.g., Alibaba Mobile Agent). We begin by analyzing the general workflow of mobile agents and identifying security threats across three core capability dimensions: language-based reasoning, GUI-based interaction, and system-level execution. Our analysis reveals 11 distinct attack surfaces, all rooted in the unique capabilities and interaction patterns of mobile LLM agents, and spanning their entire operational lifecycle. To investigate these threats in practice, we introduce AgentScan, a semi-automated security analysis framework that systematically evaluates mobile LLM agents across all 11 attack scenarios. Applying AgentScan to nine widely deployed agents, we uncover a concerning trend: every agent is vulnerable to targeted attacks. In the most severe cases, agents exhibit vulnerabilities across eight distinct attack vectors. These attacks can cause behavioral deviations, privacy leakage, or even full execution hijacking. Based on these findings, we propose a set of defensive design principles and practical recommendations for building secure mobile LLM agents. Our disclosures have received positive feedback from two major device vendors. Overall, this work highlights the urgent need for standardized security practices in the fast-evolving landscape of LLM-driven mobile automation.

摘要: 大型语言模型（LLM）的日益采用催生了移动计算领域的一种新范式--LLM支持的移动人工智能代理--能够直接在智能手机上分解和自动化复杂任务。然而，这些特工的安全影响在很大程度上仍未得到探讨。在本文中，我们首次对移动LLM代理进行了全面的安全分析，涵盖三个代表性类别：由原始设备制造商开发的系统级AI代理（例如，YOYO助理）、第三方环球代理（例如，知普AI AutoGLM）和新兴代理框架（例如，阿里巴巴移动代理）。我们首先分析移动代理的一般工作流程，并识别三个核心能力维度的安全威胁：基于语言的推理、基于图形用户界面的交互和系统级执行。我们的分析揭示了11种不同的攻击表面，所有这些都植根于移动LLM代理的独特功能和交互模式，并跨越其整个运营生命周期。为了在实践中调查这些威胁，我们引入了AgentScan，这是一个半自动安全分析框架，可以系统地评估所有11种攻击场景中的移动LLM代理。将AgentScan应用于九个广泛部署的代理，我们发现了一个令人担忧的趋势：每个代理都容易受到有针对性的攻击。在最严重的情况下，代理在八个不同的攻击载体上表现出漏洞。这些攻击可能会导致行为偏差、隐私泄露，甚至完全执行劫持。基于这些发现，我们提出了一套用于构建安全移动LLM代理的防御设计原则和实用建议。我们的披露得到了两家主要设备供应商的积极反馈。总体而言，这项工作凸显了在LLM驱动的移动自动化快速发展的环境中对标准化安全实践的迫切需要。



## **38. MirrorShield: Towards Universal Defense Against Jailbreaks via Entropy-Guided Mirror Crafting**

盾：通过熵引导镜子制作实现普遍防御越狱 cs.CR

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2503.12931v2) [paper-pdf](http://arxiv.org/pdf/2503.12931v2)

**Authors**: Rui Pu, Chaozhuo Li, Rui Ha, Litian Zhang, Lirong Qiu, Xi Zhang

**Abstract**: Defending large language models (LLMs) against jailbreak attacks is crucial for ensuring their safe deployment. Existing defense strategies typically rely on predefined static criteria to differentiate between harmful and benign prompts. However, such rigid rules fail to accommodate the inherent complexity and dynamic nature of real-world jailbreak attacks. In this paper, we focus on the novel challenge of universal defense against diverse jailbreaks. We propose a new concept ``mirror'', which is a dynamically generated prompt that reflects the syntactic structure of the input while ensuring semantic safety. The discrepancies between input prompts and their corresponding mirrors serve as guiding principles for defense. A novel defense model, MirrorShield, is further proposed to detect and calibrate risky inputs based on the crafted mirrors. Evaluated on multiple benchmark datasets and compared against ten state-of-the-art attack methods, MirrorShield demonstrates superior defense performance and promising generalization capabilities.

摘要: 保护大型语言模型（LLM）免受越狱攻击对于确保其安全部署至关重要。现有的防御策略通常依赖于预定义的静态标准来区分有害提示和良性提示。然而，这种严格的规则无法适应现实世界越狱攻击的固有复杂性和动态性质。在本文中，我们重点关注针对各种越狱的普遍防御的新颖挑战。我们提出了一个新概念“镜像”，这是一个动态生成的提示，反映输入的语法结构，同时确保语义安全。输入提示及其相应镜像之间的差异可以作为防御的指导原则。进一步提出了一种新颖的防御模型“Inbox Shield”，用于基于精心制作的镜子来检测和校准危险输入。经过多个基准数据集的评估，并与十种最先进的攻击方法进行比较，DeliverShield表现出卓越的防御性能和有前途的概括能力。



## **39. From Theft to Bomb-Making: The Ripple Effect of Unlearning in Defending Against Jailbreak Attacks**

从盗窃到炸弹制造：在防御越狱袭击时忘记学习的涟漪效应 cs.CR

19 pages

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2407.02855v3) [paper-pdf](http://arxiv.org/pdf/2407.02855v3)

**Authors**: Zhexin Zhang, Junxiao Yang, Yida Lu, Pei Ke, Shiyao Cui, Chujie Zheng, Hongning Wang, Minlie Huang

**Abstract**: Large Language Models (LLMs) are known to be vulnerable to jailbreak attacks. An important observation is that, while different types of jailbreak attacks can generate significantly different queries, they mostly result in similar responses that are rooted in the same harmful knowledge (e.g., detailed steps to make a bomb). Consequently, unlearning-based approaches have been proposed to mitigate jailbreak attacks by directly removing harmful knowledge from the model. In this paper, we identify a novel ripple effect of unlearning, wherein LLMs can implicitly unlearn harmful knowledge that was not explicitly introduced during the unlearning phase (e.g., a model unlearning the steps for theft may also implicitly unlearn the steps for making a bomb). Through over 100 experimental runs spanning multiple models, attack strategies, and defense methods, we empirically validate this phenomenon, which makes unlearning-based methods able to decrease the Attack Success Rate on unseen data from more than 70% to less than 10% with only 100 training samples. Further analysis reveals that the strong generalization ability of unlearning may stem from the intrinsic relatedness among harmful responses across harmful questions (e.g., response patterns, shared steps and actions in response, and similarity among their learned representations in the LLM). We also discuss the potential limitations of unlearning and the observed ripple effect. We hope our research could contribute to a deeper understanding of unlearning. Our code is available at https://github.com/thu-coai/SafeUnlearning.

摘要: 众所周知，大型语言模型（LLM）很容易受到越狱攻击。一个重要的观察是，虽然不同类型的越狱攻击可以生成显着不同的查询，但它们大多会导致植根于相同有害知识的类似响应（例如，制作炸弹的详细步骤）。因此，人们提出了基于非学习的方法，通过直接从模型中删除有害知识来减轻越狱攻击。在本文中，我们确定了取消学习的一种新型连锁反应，其中LLM可以隐式地取消学习阶段未明确引入的有害知识（例如，模型忘记盗窃步骤也可能隐含地忘记制造炸弹的步骤）。通过跨越多种模型、攻击策略和防御方法的100多个实验运行，我们从经验上验证了这一现象，这使得基于非学习的方法能够将未见数据的攻击成功率从70%以上降低到10%以下，只需100个训练样本。进一步的分析表明，遗忘的强大概括能力可能源于有害问题的有害反应之间的内在相关性（例如，响应模式、响应中的共享步骤和动作，以及LLM中学习到的表示之间的相似性）。我们还讨论了忘记学习的潜在局限性和观察到的连锁反应。我们希望我们的研究能够有助于更深入地理解忘记学习。我们的代码可在https://github.com/thu-coai/SafeUnlearning上获取。



## **40. PandaGuard: Systematic Evaluation of LLM Safety in the Era of Jailbreaking Attacks**

PandaGuard：越狱袭击时代LLM安全性的系统评估 cs.CR

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2505.13862v1) [paper-pdf](http://arxiv.org/pdf/2505.13862v1)

**Authors**: Guobin Shen, Dongcheng Zhao, Linghao Feng, Xiang He, Jihang Wang, Sicheng Shen, Haibo Tong, Yiting Dong, Jindong Li, Xiang Zheng, Yi Zeng

**Abstract**: Large language models (LLMs) have achieved remarkable capabilities but remain vulnerable to adversarial prompts known as jailbreaks, which can bypass safety alignment and elicit harmful outputs. Despite growing efforts in LLM safety research, existing evaluations are often fragmented, focused on isolated attack or defense techniques, and lack systematic, reproducible analysis. In this work, we introduce PandaGuard, a unified and modular framework that models LLM jailbreak safety as a multi-agent system comprising attackers, defenders, and judges. Our framework implements 19 attack methods and 12 defense mechanisms, along with multiple judgment strategies, all within a flexible plugin architecture supporting diverse LLM interfaces, multiple interaction modes, and configuration-driven experimentation that enhances reproducibility and practical deployment. Built on this framework, we develop PandaBench, a comprehensive benchmark that evaluates the interactions between these attack/defense methods across 49 LLMs and various judgment approaches, requiring over 3 billion tokens to execute. Our extensive evaluation reveals key insights into model vulnerabilities, defense cost-performance trade-offs, and judge consistency. We find that no single defense is optimal across all dimensions and that judge disagreement introduces nontrivial variance in safety assessments. We release the code, configurations, and evaluation results to support transparent and reproducible research in LLM safety.

摘要: 大型语言模型（LLM）已经取得了卓越的能力，但仍然容易受到被称为越狱的对抗性提示的影响，这可能会绕过安全对齐并引发有害的输出。尽管LLM安全研究的努力越来越多，但现有的评估往往是分散的，集中在孤立的攻击或防御技术上，缺乏系统的，可重复的分析。在这项工作中，我们引入了PandaGuard，一个统一的模块化框架，将LLM越狱安全建模为一个由攻击者，防御者和法官组成的多代理系统。我们的框架实现了19种攻击方法和12种防御机制，以及多种判断策略，所有这些都在一个灵活的插件架构中，支持多种LLM接口，多种交互模式和配置驱动的实验，从而增强了可重复性和实际部署。基于这个框架，我们开发了PandaBench，这是一个全面的基准，可评估49个LLM和各种判断方法之间的相互作用，需要超过30亿个代币来执行。我们的广泛评估揭示了对模型漏洞、国防成本-性能权衡和判断一致性的关键见解。我们发现，没有一种防御在所有维度上都是最佳的，而且判断分歧会在安全评估中引入非平凡的方差。我们发布代码、配置和评估结果，以支持LLM安全性方面的透明和可重复研究。



## **41. One Shot Dominance: Knowledge Poisoning Attack on Retrieval-Augmented Generation Systems**

一枪优势：对检索增强生成系统的知识中毒攻击 cs.CR

14pages, 4 figures

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2505.11548v2) [paper-pdf](http://arxiv.org/pdf/2505.11548v2)

**Authors**: Zhiyuan Chang, Mingyang Li, Xiaojun Jia, Junjie Wang, Yuekai Huang, Ziyou Jiang, Yang Liu, Qing Wang

**Abstract**: Large Language Models (LLMs) enhanced with Retrieval-Augmented Generation (RAG) have shown improved performance in generating accurate responses. However, the dependence on external knowledge bases introduces potential security vulnerabilities, particularly when these knowledge bases are publicly accessible and modifiable. While previous studies have exposed knowledge poisoning risks in RAG systems, existing attack methods suffer from critical limitations: they either require injecting multiple poisoned documents (resulting in poor stealthiness) or can only function effectively on simplistic queries (limiting real-world applicability). This paper reveals a more realistic knowledge poisoning attack against RAG systems that achieves successful attacks by poisoning only a single document while remaining effective for complex multi-hop questions involving complex relationships between multiple elements. Our proposed AuthChain address three challenges to ensure the poisoned documents are reliably retrieved and trusted by the LLM, even against large knowledge bases and LLM's own knowledge. Extensive experiments across six popular LLMs demonstrate that AuthChain achieves significantly higher attack success rates while maintaining superior stealthiness against RAG defense mechanisms compared to state-of-the-art baselines.

摘要: 使用检索增强生成（RAG）增强的大型语言模型（LLM）在生成准确响应方面表现出更好的性能。然而，对外部知识库的依赖会带来潜在的安全漏洞，特别是当这些知识库可公开访问和可修改时。虽然之前的研究暴露了RAG系统中的知识中毒风险，但现有的攻击方法存在严重局限性：它们要么需要注入多个有毒文档（导致隐蔽性较差），要么只能在简单化的查询上有效发挥作用（限制现实世界的适用性）。本文揭示了一种针对RAG系统的更现实的知识中毒攻击，该攻击通过仅毒害单个文档来实现成功攻击，同时对涉及多个元素之间复杂关系的复杂多跳问题仍然有效。我们提出的AuthChain解决了三个挑战，以确保中毒文档被LLM可靠地检索和信任，即使是针对大型知识库和LLM自己的知识。在六个流行的LLM上进行的广泛实验表明，与最先进的基线相比，AuthChain实现了显着更高的攻击成功率，同时保持了对RAG防御机制的卓越隐身性。



## **42. Fragments to Facts: Partial-Information Fragment Inference from LLMs**

事实片段：来自LLM的部分信息片段推断 cs.LG

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2505.13819v1) [paper-pdf](http://arxiv.org/pdf/2505.13819v1)

**Authors**: Lucas Rosenblatt, Bin Han, Robert Wolfe, Bill Howe

**Abstract**: Large language models (LLMs) can leak sensitive training data through memorization and membership inference attacks. Prior work has primarily focused on strong adversarial assumptions, including attacker access to entire samples or long, ordered prefixes, leaving open the question of how vulnerable LLMs are when adversaries have only partial, unordered sample information. For example, if an attacker knows a patient has "hypertension," under what conditions can they query a model fine-tuned on patient data to learn the patient also has "osteoarthritis?" In this paper, we introduce a more general threat model under this weaker assumption and show that fine-tuned LLMs are susceptible to these fragment-specific extraction attacks. To systematically investigate these attacks, we propose two data-blind methods: (1) a likelihood ratio attack inspired by methods from membership inference, and (2) a novel approach, PRISM, which regularizes the ratio by leveraging an external prior. Using examples from both medical and legal settings, we show that both methods are competitive with a data-aware baseline classifier that assumes access to labeled in-distribution data, underscoring their robustness.

摘要: 大型语言模型（LLM）可以通过记忆和成员资格推断攻击泄露敏感的训练数据。之前的工作主要集中在强对抗性假设上，包括攻击者访问整个样本或长、有序的前置码，这就留下了当对手仅拥有部分、无序的样本信息时，LLM有多脆弱的问题。例如，如果攻击者知道患者患有“高血压”，他们在什么情况下可以查询根据患者数据微调的模型以了解患者也患有“骨关节炎”？“在本文中，我们在这个较弱的假设下引入了一个更通用的威胁模型，并表明经过微调的LLM容易受到这些特定于片段的提取攻击。为了系统性地研究这些攻击，我们提出了两种数据盲方法：（1）受隶属推理方法启发的似然比攻击，和（2）一种新颖的方法PRism，它通过利用外部先验来规范比率。使用来自医疗和法律环境的示例，我们表明这两种方法都与数据感知基线分类器具有竞争力，该分类器假设可以访问标记的分布数据，从而强调了其稳健性。



## **43. Investigating the Vulnerability of LLM-as-a-Judge Architectures to Prompt-Injection Attacks**

调查LLM作为法官架构对预算注入攻击的脆弱性 cs.CL

**SubmitDate**: 2025-05-19    [abs](http://arxiv.org/abs/2505.13348v1) [paper-pdf](http://arxiv.org/pdf/2505.13348v1)

**Authors**: Narek Maloyan, Bislan Ashinov, Dmitry Namiot

**Abstract**: Large Language Models (LLMs) are increasingly employed as evaluators (LLM-as-a-Judge) for assessing the quality of machine-generated text. This paradigm offers scalability and cost-effectiveness compared to human annotation. However, the reliability and security of such systems, particularly their robustness against adversarial manipulations, remain critical concerns. This paper investigates the vulnerability of LLM-as-a-Judge architectures to prompt-injection attacks, where malicious inputs are designed to compromise the judge's decision-making process. We formalize two primary attack strategies: Comparative Undermining Attack (CUA), which directly targets the final decision output, and Justification Manipulation Attack (JMA), which aims to alter the model's generated reasoning. Using the Greedy Coordinate Gradient (GCG) optimization method, we craft adversarial suffixes appended to one of the responses being compared. Experiments conducted on the MT-Bench Human Judgments dataset with open-source instruction-tuned LLMs (Qwen2.5-3B-Instruct and Falcon3-3B-Instruct) demonstrate significant susceptibility. The CUA achieves an Attack Success Rate (ASR) exceeding 30\%, while JMA also shows notable effectiveness. These findings highlight substantial vulnerabilities in current LLM-as-a-Judge systems, underscoring the need for robust defense mechanisms and further research into adversarial evaluation and trustworthiness in LLM-based assessment frameworks.

摘要: 大型语言模型（LLM）越来越多地被用作评估器（LLM as-a-Judge）来评估机器生成文本的质量。与人类注释相比，该范式提供了可扩展性和成本效益。然而，此类系统的可靠性和安全性，特别是它们对对抗性操纵的鲁棒性，仍然是关键问题。本文研究了LLM as-a-Judge架构对预算注入攻击的脆弱性，其中恶意输入旨在损害法官的决策过程。我们正式化了两种主要的攻击策略：比较挖掘攻击（CUA），直接针对最终决策输出，和合理化操纵攻击（JMA），旨在改变模型生成的推理。使用贪婪坐标梯度（GCG）优化方法，我们制作附加到正在比较的一个响应上的对抗后缀。在MT-Bench Human Judgments数据集上使用开源描述调整的LLM（Qwen 2.5 - 3B-Direct和Falcon 3 - 3B-Direct）进行的实验证明了显着的易感性。CUA的攻击成功率（ASB）超过30%，而JMA也表现出显着的有效性。这些发现凸显了当前法学硕士作为法官系统中的重大漏洞，强调了强大的防御机制以及对基于法学硕士的评估框架中的对抗性评估和可信度进行进一步研究的必要性。



## **44. Concept-Level Explainability for Auditing & Steering LLM Responses**

审计和指导LLM响应的概念级解释性 cs.CL

9 pages, 7 figures, Submission to Neurips 2025

**SubmitDate**: 2025-05-19    [abs](http://arxiv.org/abs/2505.07610v2) [paper-pdf](http://arxiv.org/pdf/2505.07610v2)

**Authors**: Kenza Amara, Rita Sevastjanova, Mennatallah El-Assady

**Abstract**: As large language models (LLMs) become widely deployed, concerns about their safety and alignment grow. An approach to steer LLM behavior, such as mitigating biases or defending against jailbreaks, is to identify which parts of a prompt influence specific aspects of the model's output. Token-level attribution methods offer a promising solution, but still struggle in text generation, explaining the presence of each token in the output separately, rather than the underlying semantics of the entire LLM response. We introduce ConceptX, a model-agnostic, concept-level explainability method that identifies the concepts, i.e., semantically rich tokens in the prompt, and assigns them importance based on the outputs' semantic similarity. Unlike current token-level methods, ConceptX also offers to preserve context integrity through in-place token replacements and supports flexible explanation goals, e.g., gender bias. ConceptX enables both auditing, by uncovering sources of bias, and steering, by modifying prompts to shift the sentiment or reduce the harmfulness of LLM responses, without requiring retraining. Across three LLMs, ConceptX outperforms token-level methods like TokenSHAP in both faithfulness and human alignment. Steering tasks boost sentiment shift by 0.252 versus 0.131 for random edits and lower attack success rates from 0.463 to 0.242, outperforming attribution and paraphrasing baselines. While prompt engineering and self-explaining methods sometimes yield safer responses, ConceptX offers a transparent and faithful alternative for improving LLM safety and alignment, demonstrating the practical value of attribution-based explainability in guiding LLM behavior.

摘要: 随着大型语言模型（LLM）的广泛部署，对其安全性和一致性的担忧日益加剧。引导LLM行为（例如减轻偏见或防范越狱）的一种方法是识别提示的哪些部分影响模型输出的特定方面。令牌级归因方法提供了一个有希望的解决方案，但在文本生成方面仍然很困难，分别解释输出中每个令牌的存在，而不是整个LLM响应的底层语义。我们引入ConceptX，这是一种模型不可知的概念级解释方法，可以识别概念，即提示中语义丰富的标记，并根据输出的语义相似性为其分配重要性。与当前的代币级方法不同，ConceptX还提供通过就地代币替换来保持上下文完整性，并支持灵活的解释目标，例如性别偏见。ConceptX通过发现偏见的来源来实现审计，并通过修改提示以改变情绪或减少LLM响应的危害性来实现引导，而无需再培训。在三个LLM中，ConceptX在忠诚度和人性化方面都优于TokenSHAP等代币级方法。随机编辑的引导任务使情绪转变提高了0.252和0.131，攻击成功率从0.463降低到0.242，优于归因和重述基线。虽然及时的工程和自我解释方法有时会产生更安全的响应，但ConceptX为提高LLM安全性和一致性提供了一种透明且忠实的替代方案，展示了基于属性的解释在指导LLM行为方面的实际价值。



## **45. The Hidden Dangers of Browsing AI Agents**

浏览人工智能代理的隐藏危险 cs.CR

**SubmitDate**: 2025-05-19    [abs](http://arxiv.org/abs/2505.13076v1) [paper-pdf](http://arxiv.org/pdf/2505.13076v1)

**Authors**: Mykyta Mudryi, Markiyan Chaklosh, Grzegorz Wójcik

**Abstract**: Autonomous browsing agents powered by large language models (LLMs) are increasingly used to automate web-based tasks. However, their reliance on dynamic content, tool execution, and user-provided data exposes them to a broad attack surface. This paper presents a comprehensive security evaluation of such agents, focusing on systemic vulnerabilities across multiple architectural layers. Our work outlines the first end-to-end threat model for browsing agents and provides actionable guidance for securing their deployment in real-world environments. To address discovered threats, we propose a defense in depth strategy incorporating input sanitization, planner executor isolation, formal analyzers, and session safeguards. These measures protect against both initial access and post exploitation attack vectors. Through a white box analysis of a popular open source project, Browser Use, we demonstrate how untrusted web content can hijack agent behavior and lead to critical security breaches. Our findings include prompt injection, domain validation bypass, and credential exfiltration, evidenced by a disclosed CVE and a working proof of concept exploit.

摘要: 由大型语言模型（LLM）支持的自主浏览代理越来越多地用于自动化基于Web的任务。然而，它们对动态内容、工具执行和用户提供的数据的依赖使它们面临广泛的攻击面。本文对此类代理进行了全面的安全评估，重点关注跨多个体系结构层的系统漏洞。我们的工作概述了浏览代理的第一个端到端威胁模型，并为确保其在现实世界环境中的部署提供了可操作的指导。为了解决发现的威胁，我们提出了一种深度防御策略，其中包括输入清理、计划执行者隔离、正式分析器和会话保护措施。这些措施可以防止初始访问和利用后攻击媒介。通过对流行的开源项目“浏览器使用”的白盒分析，我们展示了不受信任的Web内容如何劫持代理行为并导致严重的安全漏洞。我们的发现包括即时注入、域验证绕过和凭证外流，并通过公开的UTE和概念利用的有效证明来证明。



## **46. "Yes, My LoRD." Guiding Language Model Extraction with Locality Reinforced Distillation**

“是的，我的爱人。“利用局部强化蒸馏提取引导语言模型 cs.CR

To appear at ACL 25 main conference

**SubmitDate**: 2025-05-19    [abs](http://arxiv.org/abs/2409.02718v3) [paper-pdf](http://arxiv.org/pdf/2409.02718v3)

**Authors**: Zi Liang, Qingqing Ye, Yanyun Wang, Sen Zhang, Yaxin Xiao, Ronghua Li, Jianliang Xu, Haibo Hu

**Abstract**: Model extraction attacks (MEAs) on large language models (LLMs) have received increasing attention in recent research. However, existing attack methods typically adapt the extraction strategies originally developed for deep neural networks (DNNs). They neglect the underlying inconsistency between the training tasks of MEA and LLM alignment, leading to suboptimal attack performance. To tackle this issue, we propose Locality Reinforced Distillation (LoRD), a novel model extraction algorithm specifically designed for LLMs. In particular, LoRD employs a newly defined policy-gradient-style training task that utilizes the responses of victim model as the signal to guide the crafting of preference for the local model. Theoretical analyses demonstrate that I) The convergence procedure of LoRD in model extraction is consistent with the alignment procedure of LLMs, and II) LoRD can reduce query complexity while mitigating watermark protection through our exploration-based stealing. Extensive experiments validate the superiority of our method in extracting various state-of-the-art commercial LLMs. Our code is available at: https://github.com/liangzid/LoRD-MEA .

摘要: 在最近的研究中，对大型语言模型（LLM）的模型提取攻击（MEAs）受到越来越多的关注。然而，现有的攻击方法通常会适应最初为深度神经网络（DNN）开发的提取策略。他们忽视了EMA和LLM对齐训练任务之间的潜在不一致性，导致攻击性能次优。为了解决这个问题，我们提出了局部强化蒸馏（LoRD），这是一种专门为LLM设计的新型模型提取算法。特别是，LoRD采用了新定义的政策梯度式培训任务，该任务利用受害者模型的反应作为信号来指导制定对本地模型的偏好。理论分析表明，I）LoRD在模型提取中的收敛过程与LLM的对齐过程一致，II）LoRD可以降低查询复杂性，同时通过我们基于探索的窃取来减轻水印保护。大量实验验证了我们的方法在提取各种最先进的商业LLM方面的优越性。我们的代码可访问：https://github.com/liangzid/LoRD-MEA。



## **47. Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?**

低等级适应是否会导致针对训练时间攻击的鲁棒性较低？ cs.LG

To appear at ICML 25

**SubmitDate**: 2025-05-19    [abs](http://arxiv.org/abs/2505.12871v1) [paper-pdf](http://arxiv.org/pdf/2505.12871v1)

**Authors**: Zi Liang, Haibo Hu, Qingqing Ye, Yaxin Xiao, Ronghua Li

**Abstract**: Low rank adaptation (LoRA) has emerged as a prominent technique for fine-tuning large language models (LLMs) thanks to its superb efficiency gains over previous methods. While extensive studies have examined the performance and structural properties of LoRA, its behavior upon training-time attacks remain underexplored, posing significant security risks. In this paper, we theoretically investigate the security implications of LoRA's low-rank structure during fine-tuning, in the context of its robustness against data poisoning and backdoor attacks. We propose an analytical framework that models LoRA's training dynamics, employs the neural tangent kernel to simplify the analysis of the training process, and applies information theory to establish connections between LoRA's low rank structure and its vulnerability against training-time attacks. Our analysis indicates that LoRA exhibits better robustness to backdoor attacks than full fine-tuning, while becomes more vulnerable to untargeted data poisoning due to its over-simplified information geometry. Extensive experimental evaluations have corroborated our theoretical findings.

摘要: 低秩自适应（LoRA）已经成为一种用于微调大型语言模型（LLM）的突出技术，这要归功于它比以前的方法具有更高的效率。虽然广泛的研究已经检查了LoRA的性能和结构特性，但其在训练时间攻击时的行为仍然没有得到充分的研究，从而带来了重大的安全风险。在本文中，我们从理论上研究了LoRA的低秩结构在微调过程中的安全性影响，在其对数据中毒和后门攻击的鲁棒性的背景下。我们提出了一个分析框架，模型LoRA的训练动态，采用神经正切内核来简化训练过程的分析，并应用信息论建立LoRA的低秩结构和它对训练时间攻击的脆弱性之间的连接。我们的分析表明，LoRA对后门攻击表现出比完全微调更好的鲁棒性，同时由于其过于简化的信息几何结构，更容易受到非目标数据中毒的影响。广泛的实验评估证实了我们的理论发现。



## **48. LLMPot: Dynamically Configured LLM-based Honeypot for Industrial Protocol and Physical Process Emulation**

LLMPot：动态配置的基于LLM的蜜罐，用于工业协议和物理流程仿真 cs.CR

**SubmitDate**: 2025-05-19    [abs](http://arxiv.org/abs/2405.05999v3) [paper-pdf](http://arxiv.org/pdf/2405.05999v3)

**Authors**: Christoforos Vasilatos, Dunia J. Mahboobeh, Hithem Lamri, Manaar Alam, Michail Maniatakos

**Abstract**: Industrial Control Systems (ICS) are extensively used in critical infrastructures ensuring efficient, reliable, and continuous operations. However, their increasing connectivity and addition of advanced features make them vulnerable to cyber threats, potentially leading to severe disruptions in essential services. In this context, honeypots play a vital role by acting as decoy targets within ICS networks, or on the Internet, helping to detect, log, analyze, and develop mitigations for ICS-specific cyber threats. Deploying ICS honeypots, however, is challenging due to the necessity of accurately replicating industrial protocols and device characteristics, a crucial requirement for effectively mimicking the unique operational behavior of different industrial systems. Moreover, this challenge is compounded by the significant manual effort required in also mimicking the control logic the PLC would execute, in order to capture attacker traffic aiming to disrupt critical infrastructure operations. In this paper, we propose LLMPot, a novel approach for designing honeypots in ICS networks harnessing the potency of Large Language Models (LLMs). LLMPot aims to automate and optimize the creation of realistic honeypots with vendor-agnostic configurations, and for any control logic, aiming to eliminate the manual effort and specialized knowledge traditionally required in this domain. We conducted extensive experiments focusing on a wide array of parameters, demonstrating that our LLM-based approach can effectively create honeypot devices implementing different industrial protocols and diverse control logic.

摘要: 工业控制系统（ICS）广泛用于关键基础设施，确保高效、可靠和连续的运营。然而，它们不断增加的连接性和添加的高级功能使它们容易受到网络威胁的影响，可能导致基本服务的严重中断。在这种情况下，蜜罐发挥着至关重要的作用，充当ICS网络内或互联网上的诱饵目标，帮助检测、记录、分析和开发针对ICS特定网络威胁的缓解措施。然而，部署ICS蜜罐具有挑战性，因为需要准确地复制工业协议和设备特征，这是有效模仿不同工业系统独特操作行为的关键要求。此外，模仿PLC将执行的控制逻辑以捕获旨在破坏关键基础设施运营的攻击者流量所需的大量手动工作使这一挑战变得更加复杂。在本文中，我们提出了LLMPot，这是一种在ICS网络中设计蜜罐的新颖方法，利用大型语言模型（LLM）的能力。LLMPot旨在自动化和优化具有供应商不可知配置的现实蜜罐的创建，并适用于任何控制逻辑，旨在消除该领域传统上所需的手动工作和专业知识。我们针对广泛的参数进行了广泛的实验，证明我们基于LLM的方法可以有效地创建实施不同工业协议和不同控制逻辑的蜜罐设备。



## **49. Forewarned is Forearmed: A Survey on Large Language Model-based Agents in Autonomous Cyberattacks**

预先警告就是预先武装：自主网络攻击中基于大型语言模型的代理的调查 cs.NI

**SubmitDate**: 2025-05-19    [abs](http://arxiv.org/abs/2505.12786v1) [paper-pdf](http://arxiv.org/pdf/2505.12786v1)

**Authors**: Minrui Xu, Jiani Fan, Xinyu Huang, Conghao Zhou, Jiawen Kang, Dusit Niyato, Shiwen Mao, Zhu Han, Xuemin, Shen, Kwok-Yan Lam

**Abstract**: With the continuous evolution of Large Language Models (LLMs), LLM-based agents have advanced beyond passive chatbots to become autonomous cyber entities capable of performing complex tasks, including web browsing, malicious code and deceptive content generation, and decision-making. By significantly reducing the time, expertise, and resources, AI-assisted cyberattacks orchestrated by LLM-based agents have led to a phenomenon termed Cyber Threat Inflation, characterized by a significant reduction in attack costs and a tremendous increase in attack scale. To provide actionable defensive insights, in this survey, we focus on the potential cyber threats posed by LLM-based agents across diverse network systems. Firstly, we present the capabilities of LLM-based cyberattack agents, which include executing autonomous attack strategies, comprising scouting, memory, reasoning, and action, and facilitating collaborative operations with other agents or human operators. Building on these capabilities, we examine common cyberattacks initiated by LLM-based agents and compare their effectiveness across different types of networks, including static, mobile, and infrastructure-free paradigms. Moreover, we analyze threat bottlenecks of LLM-based agents across different network infrastructures and review their defense methods. Due to operational imbalances, existing defense methods are inadequate against autonomous cyberattacks. Finally, we outline future research directions and potential defensive strategies for legacy network systems.

摘要: 随着大型语言模型（LLM）的不断发展，基于LLM的代理已经超越被动聊天机器人，成为能够执行复杂任务的自治网络实体，包括网络浏览、恶意代码和欺骗性内容生成以及决策。通过显着减少时间、专业知识和资源，由LLM代理策划的人工智能辅助网络攻击导致了一种称为网络威胁通货膨胀的现象，其特征是攻击成本显着降低和攻击规模显着增加。为了提供可操作的防御见解，在本调查中，我们重点关注基于LLM的代理在不同网络系统中构成的潜在网络威胁。首先，我们介绍了基于LLM的网络攻击代理的能力，其中包括执行自主攻击策略，包括侦察、记忆、推理和行动，以及促进与其他代理或人类操作员的协作操作。基于这些功能，我们研究了基于LLM的代理发起的常见网络攻击，并比较了它们在不同类型网络（包括静态，移动和无基础设施模式）中的有效性。此外，我们分析了基于LLM的代理在不同的网络基础设施的威胁瓶颈，并审查其防御方法。由于操作不平衡，现有的防御方法不足以应对自主网络攻击。最后，我们概述了未来的研究方向和潜在的防御策略的遗留网络系统。



## **50. Language Models That Walk the Talk: A Framework for Formal Fairness Certificates**

直言不讳的语言模型：正式公平证书的框架 cs.AI

**SubmitDate**: 2025-05-19    [abs](http://arxiv.org/abs/2505.12767v1) [paper-pdf](http://arxiv.org/pdf/2505.12767v1)

**Authors**: Danqing Chen, Tobias Ladner, Ahmed Rayen Mhadhbi, Matthias Althoff

**Abstract**: As large language models become integral to high-stakes applications, ensuring their robustness and fairness is critical. Despite their success, large language models remain vulnerable to adversarial attacks, where small perturbations, such as synonym substitutions, can alter model predictions, posing risks in fairness-critical areas, such as gender bias mitigation, and safety-critical areas, such as toxicity detection. While formal verification has been explored for neural networks, its application to large language models remains limited. This work presents a holistic verification framework to certify the robustness of transformer-based language models, with a focus on ensuring gender fairness and consistent outputs across different gender-related terms. Furthermore, we extend this methodology to toxicity detection, offering formal guarantees that adversarially manipulated toxic inputs are consistently detected and appropriately censored, thereby ensuring the reliability of moderation systems. By formalizing robustness within the embedding space, this work strengthens the reliability of language models in ethical AI deployment and content moderation.

摘要: 随着大型语言模型成为高风险应用程序的组成部分，确保其稳健性和公平性至关重要。尽管取得了成功，大型语言模型仍然容易受到对抗攻击，其中同义词替换等小扰动可能会改变模型预测，从而在性别偏见缓解等公平关键领域和安全关键领域带来风险，例如毒性检测。虽然已经探索了神经网络的形式验证，但其在大型语言模型中的应用仍然有限。这项工作提出了一个整体验证框架，以验证基于转换器的语言模型的稳健性，重点是确保性别公平性和不同性别相关术语的一致输出。此外，我们将这种方法扩展到毒性检测，提供正式保证，以一致地检测和适当审查敌对操纵的有毒输入，从而确保审核系统的可靠性。通过形式化嵌入空间内的鲁棒性，这项工作增强了语言模型在道德人工智能部署和内容审核中的可靠性。



