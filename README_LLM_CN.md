# Latest Large Language Model Attack Papers
**update at 2025-05-08 18:49:24**

翻译来自 https://cloud.tencent.com/document/product/551/15619

## **1. ACE: A Security Architecture for LLM-Integrated App Systems**

ACE：LLM集成应用程序系统的安全架构 cs.CR

21 pages, 13 figures; clarify relation to indirect prompt injection  attacks

**SubmitDate**: 2025-05-07    [abs](http://arxiv.org/abs/2504.20984v2) [paper-pdf](http://arxiv.org/pdf/2504.20984v2)

**Authors**: Evan Li, Tushin Mallick, Evan Rose, William Robertson, Alina Oprea, Cristina Nita-Rotaru

**Abstract**: LLM-integrated app systems extend the utility of Large Language Models (LLMs) with third-party apps that are invoked by a system LLM using interleaved planning and execution phases to answer user queries. These systems introduce new attack vectors where malicious apps can cause integrity violation of planning or execution, availability breakdown, or privacy compromise during execution.   In this work, we identify new attacks impacting the integrity of planning, as well as the integrity and availability of execution in LLM-integrated apps, and demonstrate them against IsolateGPT, a recent solution designed to mitigate attacks from malicious apps. We propose Abstract-Concrete-Execute (ACE), a new secure architecture for LLM-integrated app systems that provides security guarantees for system planning and execution. Specifically, ACE decouples planning into two phases by first creating an abstract execution plan using only trusted information, and then mapping the abstract plan to a concrete plan using installed system apps. We verify that the plans generated by our system satisfy user-specified secure information flow constraints via static analysis on the structured plan output. During execution, ACE enforces data and capability barriers between apps, and ensures that the execution is conducted according to the trusted abstract plan. We show experimentally that our system is secure against attacks from the INJECAGENT benchmark, a standard benchmark for control flow integrity in the face of indirect prompt injection attacks, and our newly introduced attacks. Our architecture represents a significant advancement towards hardening LLM-based systems containing system facilities of varying levels of trustworthiness.

摘要: LLM集成的应用程序系统通过第三方应用程序扩展了大型语言模型（LLM）的实用性，第三方应用程序由系统LLM使用交错的规划和执行阶段调用，以回答用户查询。这些系统引入了新的攻击载体，恶意应用程序可能会导致规划或执行的完整性违反、可用性崩溃或执行期间的隐私受到损害。   在这项工作中，我们识别了影响规划完整性以及LLM集成应用程序中执行完整性和可用性的新攻击，并针对IsolateGPT（旨在减轻恶意应用程序攻击的最新解决方案）进行演示。我们提出Abstract-Concrete-Execute（ACE），这是一种针对LLM集成应用程序系统的新安全架构，为系统规划和执行提供安全保障。具体来说，ACE将规划分为两个阶段，首先仅使用可信信息创建抽象执行计划，然后使用已安装的系统应用程序将抽象计划映射到具体计划。我们通过对结构化计划输出的静态分析来验证系统生成的计划是否满足用户指定的安全信息流约束。在执行过程中，ACE在应用程序之间强制设置数据和能力障碍，并确保执行按照可信的抽象计划进行。我们通过实验证明，我们的系统可以抵御来自INJECAGENT基准测试（面对间接提示注入攻击时控制流完整性的标准基准）的攻击，以及我们新引入的攻击。我们的架构代表了强化基于LLM的系统的重大进步，该系统包含不同可信度级别的系统设施。



## **2. Fight Fire with Fire: Defending Against Malicious RL Fine-Tuning via Reward Neutralization**

以毒攻毒：通过奖励中和防御恶意RL微调 cs.LG

**SubmitDate**: 2025-05-07    [abs](http://arxiv.org/abs/2505.04578v1) [paper-pdf](http://arxiv.org/pdf/2505.04578v1)

**Authors**: Wenjun Cao

**Abstract**: Reinforcement learning (RL) fine-tuning transforms large language models while creating a vulnerability we experimentally verify: Our experiment shows that malicious RL fine-tuning dismantles safety guardrails with remarkable efficiency, requiring only 50 steps and minimal adversarial prompts, with harmful escalating from 0-2 to 7-9. This attack vector particularly threatens open-source models with parameter-level access. Existing defenses targeting supervised fine-tuning prove ineffective against RL's dynamic feedback mechanisms. We introduce Reward Neutralization, the first defense framework specifically designed against RL fine-tuning attacks, establishing concise rejection patterns that render malicious reward signals ineffective. Our approach trains models to produce minimal-information rejections that attackers cannot exploit, systematically neutralizing attempts to optimize toward harmful outputs. Experiments validate that our approach maintains low harmful scores (no greater than 2) after 200 attack steps, while standard models rapidly deteriorate. This work provides the first constructive proof that robust defense against increasingly accessible RL attacks is achievable, addressing a critical security gap for open-weight models.

摘要: 强化学习（RL）微调改变了大型语言模型，同时创建了我们实验验证的漏洞：我们的实验表明，恶意RL微调以显着的效率突破了安全护栏，只需要50个步骤和最少的对抗提示，有害的升级从0-2升级到7-9。这种攻击载体特别威胁具有参数级访问权限的开源模型。事实证明，针对监督式微调的现有防御措施对RL的动态反馈机制无效。我们引入了奖励中和，这是第一个专门针对RL微调攻击而设计的防御框架，建立了简洁的拒绝模式，使恶意奖励信号无效。我们的方法训练模型以产生攻击者无法利用的最小信息拒绝，系统性地抵消针对有害输出进行优化的尝试。实验验证了我们的方法在200次攻击步骤后保持较低的有害分数（不大于2），而标准模型迅速恶化。这项工作提供了第一个建设性的证据，证明可以实现针对日益容易获得的RL攻击的强大防御，解决了开权模型的关键安全差距。



## **3. An LLM-based Self-Evolving Security Framework for 6G Space-Air-Ground Integrated Networks**

基于LLM的6G空地综合网络自进化安全框架 cs.CR

Accepted by IEEE Communications Magazine

**SubmitDate**: 2025-05-07    [abs](http://arxiv.org/abs/2505.03161v2) [paper-pdf](http://arxiv.org/pdf/2505.03161v2)

**Authors**: Qi Qin, Xinye Cao, Guoshun Nan, Sihan Chen, Rushan Li, Li Su, Haitao Du, Qimei Cui, Pengxuan Mao, Xiaofeng Tao, Tony Q. S. Quek

**Abstract**: Recently emerged 6G space-air-ground integrated networks (SAGINs), which integrate satellites, aerial networks, and terrestrial communications, offer ubiquitous coverage for various mobile applications. However, the highly dynamic, open, and heterogeneous nature of SAGINs poses severe security issues. Forming a defense line of SAGINs suffers from two preliminary challenges: 1) accurately understanding massive unstructured multi-dimensional threat information to generate defense strategies against various malicious attacks, 2) rapidly adapting to potential unknown threats to yield more effective security strategies. To tackle the above two challenges, we propose a novel security framework for SAGINs based on Large Language Models (LLMs), which consists of two key ingredients LLM-6GNG and 6G-INST. Our proposed LLM-6GNG leverages refined chain-of-thought (CoT) reasoning and dynamic multi-agent mechanisms to analyze massive unstructured multi-dimensional threat data and generate comprehensive security strategies, thus addressing the first challenge. Our proposed 6G-INST relies on a novel self-evolving method to automatically update LLM-6GNG, enabling it to accommodate unknown threats under dynamic communication environments, thereby addressing the second challenge. Additionally, we prototype the proposed framework with ns-3, OpenAirInterface (OAI), and software-defined radio (SDR). Experiments on three benchmarks demonstrate the effectiveness of our framework. The results show that our framework produces highly accurate security strategies that remain robust against a variety of unknown attacks. We will release our code to contribute to the community.

摘要: 最近出现的6 G空-空-地综合网络（SAGER）集成了卫星、空中网络和地面通信，为各种移动应用提供无处不在的覆盖。然而，SATIN的高度动态、开放和异类性质带来了严重的安全问题。形成SATIN防线面临两个初步挑战：1）准确理解大量非结构化多维威胁信息，以生成针对各种恶意攻击的防御策略，2）快速适应潜在的未知威胁，以生成更有效的安全策略。为了应对上述两个挑战，我们提出了一种基于大型语言模型（LLM）的SAGER的新型安全框架，该框架由LLM-6 GNG和6 G-INST两个关键成分组成。我们提出的LLM-6 GNG利用精细化思想链（CoT）推理和动态多代理机制来分析大量非结构化多维威胁数据并生成全面的安全策略，从而解决第一个挑战。我们提出的6 G-INST依赖于一种新颖的自我进化方法来自动更新LLM-6 GNG，使其能够适应动态通信环境下的未知威胁，从而解决第二个挑战。此外，我们还使用ns-3、OpenAir接口（OAI）和软件定义无线电（SDR）对拟议框架进行了原型化。三个基准测试的实验证明了我们框架的有效性。结果表明，我们的框架可以生成高度准确的安全策略，并且能够抵御各种未知攻击。我们将发布我们的代码为社区做出贡献。



## **4. OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models**

Obliviate：针对大型语言模型的稳健且实用的机器去学习 cs.CL

18 pages, 2 figures

**SubmitDate**: 2025-05-07    [abs](http://arxiv.org/abs/2505.04416v1) [paper-pdf](http://arxiv.org/pdf/2505.04416v1)

**Authors**: Xiaoyu Xu, Minxin Du, Qingqing Ye, Haibo Hu

**Abstract**: Large language models (LLMs) trained over extensive corpora risk memorizing sensitive, copyrighted, or toxic content. To address this, we propose OBLIVIATE, a robust unlearning framework that removes targeted data while preserving model utility. The framework follows a structured process: extracting target tokens, building retain sets, and fine-tuning with a tailored loss function comprising three components -- masking, distillation, and world fact. Using low-rank adapters (LoRA), it ensures efficiency without compromising unlearning quality. We conduct experiments on multiple datasets, including the Harry Potter series, WMDP, and TOFU, using a comprehensive suite of metrics: forget quality (new document-level memorization score), model utility, and fluency. Results demonstrate its effectiveness in resisting membership inference attacks, minimizing the impact on retained data, and maintaining robustness across diverse scenarios.

摘要: 在广泛的库中训练的大型语言模型（LLM）存在记忆敏感、受版权保护或有毒内容的风险。为了解决这个问题，我们提出了OBLIATE，这是一个强大的去学习框架，可以删除目标数据，同时保留模型效用。该框架遵循一个结构化过程：提取目标令牌、构建保留集以及使用定制的损失函数进行微调，该函数包括三个部分--掩蔽、蒸馏和世界事实。使用低级适配器（LoRA），它可以在不影响取消学习质量的情况下确保效率。我们使用一套全面的指标对多个数据集进行实验，包括哈利·波特系列、WMDP和TOFU，包括忘记质量（新的文档级记忆分数）、模型效用和流利度。结果证明了它在抵抗隶属度推理攻击、最大限度地减少对保留数据的影响以及在不同场景下保持稳健性方面的有效性。



## **5. The Aloe Family Recipe for Open and Specialized Healthcare LLMs**

面向开放和专业医疗LL的Aloe家族食谱 cs.CL

arXiv admin note: substantial text overlap with arXiv:2405.01886

**SubmitDate**: 2025-05-07    [abs](http://arxiv.org/abs/2505.04388v1) [paper-pdf](http://arxiv.org/pdf/2505.04388v1)

**Authors**: Dario Garcia-Gasulla, Jordi Bayarri-Planas, Ashwin Kumar Gururajan, Enrique Lopez-Cuena, Adrian Tormos, Daniel Hinjos, Pablo Bernabeu-Perez, Anna Arias-Duart, Pablo Agustin Martin-Torres, Marta Gonzalez-Mallo, Sergio Alvarez-Napagao, Eduard Ayguadé-Parra, Ulises Cortés

**Abstract**: Purpose: With advancements in Large Language Models (LLMs) for healthcare, the need arises for competitive open-source models to protect the public interest. This work contributes to the field of open medical LLMs by optimizing key stages of data preprocessing and training, while showing how to improve model safety (through DPO) and efficacy (through RAG). The evaluation methodology used, which includes four different types of tests, defines a new standard for the field. The resultant models, shown to be competitive with the best private alternatives, are released with a permisive license.   Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5, Aloe Beta uses a custom dataset to enhance public data with synthetic Chain of Thought examples. The models undergo alignment with Direct Preference Optimization, emphasizing ethical and policy-aligned performance in the presence of jailbreaking attacks. Evaluation includes close-ended, open-ended, safety and human assessments, to maximize the reliability of results.   Results: Recommendations are made across the entire pipeline, backed by the solid performance of the Aloe Family. These models deliver competitive performance across healthcare benchmarks and medical fields, and are often preferred by healthcare professionals. On bias and toxicity, the Aloe Beta models significantly improve safety, showing resilience to unseen jailbreaking attacks. For a responsible release, a detailed risk assessment specific to healthcare is attached to the Aloe Family models.   Conclusion: The Aloe Beta models, and the recipe that leads to them, are a significant contribution to the open-source medical LLM field, offering top-of-the-line performance while maintaining high ethical requirements. This work sets a new standard for developing and reporting aligned LLMs in healthcare.

摘要: 目的：随着医疗保健大型语言模型（LLM）的进步，需要有竞争力的开源模型来保护公共利益。这项工作通过优化数据预处理和训练的关键阶段，同时展示如何提高模型安全性（通过DPO）和有效性（通过RAG），为开放医学LLM领域做出了贡献。所使用的评估方法包括四种不同类型的测试，为该领域定义了新的标准。由此产生的模型被证明与最好的私人替代品具有竞争力，并且在许可证下发布。   方法：Aloe Beta建立在Llama 3.1和Qwen 2.5等强大基础模型的基础上，使用自定义数据集通过合成的思想链示例增强公共数据。这些模型与直接偏好优化保持一致，强调在存在越狱攻击时的道德和政策一致的性能。评估包括封闭式、开放式、安全性和人为评估，以最大限度地提高结果的可靠性。   结果：在Aloe系列的稳健表现的支持下，整个管道都提出了建议。这些模型在医疗保健基准和医疗领域提供有竞争力的性能，并且通常受到医疗保健专业人士的青睐。在偏见和毒性方面，Aloe Beta模型显着提高了安全性，表现出对不可见越狱攻击的韧性。为了实现负责任的发布，Aloe Family模型附带了针对医疗保健的详细风险评估。   结论：Aloe Beta模型及其配方是对开源医学LLM领域的重大贡献，在保持高道德要求的同时提供顶级性能。这项工作为开发和报告医疗保健领域一致的LLM设定了新标准。



## **6. ExpShield: Safeguarding Web Text from Unauthorized Crawling and Language Modeling Exploitation**

ExpShield：保护Web文本免受未经授权的抓取和语言建模利用 cs.CR

13 pages

**SubmitDate**: 2025-05-07    [abs](http://arxiv.org/abs/2412.21123v2) [paper-pdf](http://arxiv.org/pdf/2412.21123v2)

**Authors**: Ruixuan Liu, Toan Tran, Tianhao Wang, Hongsheng Hu, Shuo Wang, Li Xiong

**Abstract**: As large language models (LLMs) increasingly depend on web-scraped datasets, concerns arise over their potential to generate verbatim training content with copyrighted or private information. However, current protections against web crawling or sample-specific memorization are inherently limited, as they require compliance from crawlers (e.g., respecting robots.txt) or model trainers (e.g., applying differential privacy). To empower data owners with direct control, we propose ExpShiled, a proactive self-defense mechanism that mitigates sample-specific memorization via imperceptible text perturbations. This approach requires no external collaboration while maintaining original readability. To evaluate individual-level defense efficacy, we first propose the metric of instance exploitation: a zero value indicates perfect defense, achieved when a protected text's log-perplexity ranking aligns with its counterfactual untrained ranking. We then reveal and validate the memorization trigger hypothesis, demonstrating that a model's memorization of a specific text sample stems primarily from its outlier tokens. Leveraging this insight, we design targeted perturbations that (1) prioritize inherent trigger tokens and (2) introduce artificial trigger tokens as pitfalls to disrupt memorization on the protected sample. Experiments validate our defense across model scales, languages, vision-to-language tasks, and fine-tuning methods. Even with privacy backdoors, the Membership Inference Attack (MIA) AUC drops from 0.95 to 0.55, and instance exploitation approaches zero. This suggests that compared to the ideal no-misuse scenario, the risk of exposing a text instance remains nearly unchanged despite its inclusion in training data.

摘要: 随着大型语言模型（LLM）越来越依赖于网络抓取的数据集，人们开始担心它们是否有可能使用受版权或私人信息生成逐字训练内容。然而，当前针对网络爬行或特定样本记忆的保护本质上是有限的，因为它们需要爬行器的合规性（例如，尊重robots.文本）或模特教练（例如，应用差异隐私）。为了使数据所有者能够直接控制，我们提出了ExpShiled，这是一种主动的自我防御机制，可以通过不可察觉的文本扰动来减轻特定于样本的记忆。这种方法不需要外部协作，同时保持原始的可读性。为了评估个人级别的防御效率，我们首先提出了实例利用的度量：零值表示完美的防御，当受保护文本的日志困惑度排名与其反事实的未经训练的排名一致时。然后，我们揭示并验证了记忆触发假设，证明了模型对特定文本样本的记忆主要源于其离群值标记。利用这一洞察力，我们设计了有针对性的扰动，（1）优先考虑固有的触发令牌和（2）引入人工触发令牌作为陷阱，以破坏受保护样本的记忆。实验验证了我们的防御跨模型规模，语言，视觉到语言的任务，和微调方法。即使有隐私后门，成员推理攻击（MIA）AUC也从0.95下降到0.55，实例利用接近于零。这表明，与理想的无误用场景相比，尽管文本实例包含在训练数据中，但暴露文本实例的风险几乎保持不变。



## **7. Towards Universal and Black-Box Query-Response Only Attack on LLMs with QROA**

采用QROA对LLM进行通用和黑匣子仅查询响应攻击 cs.CL

**SubmitDate**: 2025-05-06    [abs](http://arxiv.org/abs/2406.02044v3) [paper-pdf](http://arxiv.org/pdf/2406.02044v3)

**Authors**: Hussein Jawad, Yassine Chenik, Nicolas J. -B. Brunel

**Abstract**: The rapid adoption of Large Language Models (LLMs) has exposed critical security and ethical vulnerabilities, particularly their susceptibility to adversarial manipulations. This paper introduces QROA, a novel black-box jailbreak method designed to identify adversarial suffixes that can bypass LLM alignment safeguards when appended to a malicious instruction. Unlike existing suffix-based jailbreak approaches, QROA does not require access to the model's logit or any other internal information. It also eliminates reliance on human-crafted templates, operating solely through the standard query-response interface of LLMs. By framing the attack as an optimization bandit problem, QROA employs a surrogate model and token level optimization to efficiently explore suffix variations. Furthermore, we propose QROA-UNV, an extension that identifies universal adversarial suffixes for individual models, enabling one-query jailbreaks across a wide range of instructions. Testing on multiple models demonstrates Attack Success Rate (ASR) greater than 80\%. These findings highlight critical vulnerabilities, emphasize the need for advanced defenses, and contribute to the development of more robust safety evaluations for secure AI deployment. The code is made public on the following link: https://github.com/qroa/QROA

摘要: 大型语言模型（LLM）的迅速采用暴露了关键的安全和道德漏洞，特别是它们容易受到对抗性操纵的影响。本文介绍了QROA，这是一种新型黑匣子越狱方法，旨在识别对抗性后缀，这些后缀在附加到恶意指令时可以绕过LLM对齐保障措施。与现有的基于后缀的越狱方法不同，QROA不需要访问模型的logit或任何其他内部信息。它还消除了对人工模板的依赖，仅通过LLM的标准查询-响应界面操作。通过将攻击定义为优化强盗问题，QROA采用代理模型和令牌级优化来有效地探索后缀变体。此外，我们还提出了QROA-UNV，这是一种扩展，可以为各个模型识别通用的对抗性后缀，从而实现跨广泛指令的单查询越狱。对多个模型的测试表明攻击成功率（ASB）大于80%。这些发现凸显了关键漏洞，强调了对先进防御的需要，并有助于开发更强大的安全评估以实现安全的人工智能部署。该代码在以下链接上公开：https://github.com/qroa/QROA



## **8. HAIR: Hardness-Aware Inverse Reinforcement Learning with Introspective Reasoning for LLM Alignment**

HAIR：具有内省推理的硬感知反向强化学习，用于LLM对齐 cs.CL

The three authors contributed equally to this work

**SubmitDate**: 2025-05-06    [abs](http://arxiv.org/abs/2503.18991v2) [paper-pdf](http://arxiv.org/pdf/2503.18991v2)

**Authors**: Ruoxi Cheng, Haoxuan Ma, Weixin Wang

**Abstract**: The alignment of large language models (LLMs) with human values remains critical yet hindered by four key challenges: (1) scarcity of balanced safety datasets, (2) alignment tax, (3) vulnerability to jailbreak attacks due to shallow alignment, and (4) inability to dynamically adapt rewards according to task difficulty. To address these limitations, we introduce HAIR (Hardness-Aware Inverse Reinforcement Learning with Introspective Reasoning), a novel alignment approach inspired by shadow models in membership inference attacks. Our approach consists of two main components: (1) construction of a balanced safety Chain-of-Draft (CoD) dataset for seven harmful categories using structured prompts that leverage the introspective reasoning capabilities of LLMs; and (2) training of category-specific reward models with Group Relative Policy Optimization (GRPO), dynamically tuning optimization to task difficulty at both the data and model levels. Comprehensive experiments across four harmlessness and four usefulness benchmarks demonstrate that HAIR achieves state-of-the-art performance, outperforming all baseline methods in safety while maintaining high levels of usefulness.

摘要: 大型语言模型（LLM）与人类价值观的一致仍然至关重要，但受到四个关键挑战的阻碍：（1）平衡安全数据集的稀缺，（2）一致税，（3）由于浅一致而容易受到越狱攻击，以及（4）无法根据任务难度动态调整奖励。为了解决这些局限性，我们引入了HAIR（具有内省推理的硬度感知反向强化学习），这是一种新颖的对齐方法，其灵感来自隶属推理攻击中的影子模型。我们的方法由两个主要部分组成：（1）使用利用LLM内省推理能力的结构化提示，为七个有害类别构建平衡的安全草案链（CoD）数据集;（2）使用组相对政策优化（GRPO）训练特定类别的奖励模型，动态调整优化以满足数据和模型级别的任务难度。四种无害性和四种有用性基准的综合实验表明，HAIR实现了最先进的性能，在安全性方面优于所有基线方法，同时保持了高水平的有用性。



## **9. BadLingual: A Novel Lingual-Backdoor Attack against Large Language Models**

BadLingual：针对大型语言模型的新型语言后门攻击 cs.CR

**SubmitDate**: 2025-05-06    [abs](http://arxiv.org/abs/2505.03501v1) [paper-pdf](http://arxiv.org/pdf/2505.03501v1)

**Authors**: Zihan Wang, Hongwei Li, Rui Zhang, Wenbo Jiang, Kangjie Chen, Tianwei Zhang, Qingchuan Zhao, Guowen Xu

**Abstract**: In this paper, we present a new form of backdoor attack against Large Language Models (LLMs): lingual-backdoor attacks. The key novelty of lingual-backdoor attacks is that the language itself serves as the trigger to hijack the infected LLMs to generate inflammatory speech. They enable the precise targeting of a specific language-speaking group, exacerbating racial discrimination by malicious entities. We first implement a baseline lingual-backdoor attack, which is carried out by poisoning a set of training data for specific downstream tasks through translation into the trigger language. However, this baseline attack suffers from poor task generalization and is impractical in real-world settings. To address this challenge, we design BadLingual, a novel task-agnostic lingual-backdoor, capable of triggering any downstream tasks within the chat LLMs, regardless of the specific questions of these tasks. We design a new approach using PPL-constrained Greedy Coordinate Gradient-based Search (PGCG) based adversarial training to expand the decision boundary of lingual-backdoor, thereby enhancing the generalization ability of lingual-backdoor across various tasks. We perform extensive experiments to validate the effectiveness of our proposed attacks. Specifically, the baseline attack achieves an ASR of over 90% on the specified tasks. However, its ASR reaches only 37.61% across six tasks in the task-agnostic scenario. In contrast, BadLingual brings up to 37.35% improvement over the baseline. Our study sheds light on a new perspective of vulnerabilities in LLMs with multilingual capabilities and is expected to promote future research on the potential defenses to enhance the LLMs' robustness

摘要: 在本文中，我们提出了一种针对大型语言模型（LLM）的新形式后门攻击：语言后门攻击。语言后门攻击的关键新颖之处在于，语言本身充当了劫持受感染LLM以产生煽动性言语的触发器。它们能够准确瞄准特定语言群体，加剧恶意实体的种族歧视。我们首先实施基线语言后门攻击，通过翻译成触发语言来毒害特定下游任务的一组训练数据来执行该攻击。然而，这种基线攻击的任务概括性较差，并且在现实世界环境中不切实际。为了应对这一挑战，我们设计了BadLingual，这是一种新型的任务不可知语言后门，能够触发聊天LLM内的任何下游任务，无论这些任务的具体问题如何。我们设计了一种使用PPL约束的基于贪婪协调搜索（PGCG）的对抗训练的新方法，以扩大语言后门的决策边界，从而增强语言后门在各种任务中的概括能力。我们进行了广泛的实验来验证我们提出的攻击的有效性。具体来说，基线攻击在指定任务上实现了超过90%的ASB。然而，在任务不可知的场景中，其六项任务的ASB仅达到37.61%。相比之下，BadLingual较基线提高了37.35%。我们的研究揭示了具有多语言功能的LLM漏洞的新视角，并预计将促进未来对潜在防御措施的研究，以增强LLM的稳健性



## **10. Automatic Calibration for Membership Inference Attack on Large Language Models**

大型语言模型隶属度推理攻击的自动校准 cs.LG

**SubmitDate**: 2025-05-06    [abs](http://arxiv.org/abs/2505.03392v1) [paper-pdf](http://arxiv.org/pdf/2505.03392v1)

**Authors**: Saleh Zare Zade, Yao Qiang, Xiangyu Zhou, Hui Zhu, Mohammad Amin Roshani, Prashant Khanduri, Dongxiao Zhu

**Abstract**: Membership Inference Attacks (MIAs) have recently been employed to determine whether a specific text was part of the pre-training data of Large Language Models (LLMs). However, existing methods often misinfer non-members as members, leading to a high false positive rate, or depend on additional reference models for probability calibration, which limits their practicality. To overcome these challenges, we introduce a novel framework called Automatic Calibration Membership Inference Attack (ACMIA), which utilizes a tunable temperature to calibrate output probabilities effectively. This approach is inspired by our theoretical insights into maximum likelihood estimation during the pre-training of LLMs. We introduce ACMIA in three configurations designed to accommodate different levels of model access and increase the probability gap between members and non-members, improving the reliability and robustness of membership inference. Extensive experiments on various open-source LLMs demonstrate that our proposed attack is highly effective, robust, and generalizable, surpassing state-of-the-art baselines across three widely used benchmarks. Our code is available at: \href{https://github.com/Salehzz/ACMIA}{\textcolor{blue}{Github}}.

摘要: 成员资格推理攻击（MIA）最近被用来确定特定文本是否是大型语言模型（LLM）预训练数据的一部分。然而，现有方法经常将非成员误认为是成员，导致高假阳性率，或者依赖于额外的参考模型进行概率校准，这限制了其实用性。为了克服这些挑战，我们引入了一种名为自动校准成员推断攻击（ACMIA）的新型框架，该框架利用可调温度来有效地校准输出概率。这种方法的灵感来自我们对LLM预训练期间最大似然估计的理论见解。我们以三种配置引入ACMIA，旨在适应不同级别的模型访问并增加成员和非成员之间的概率差距，提高隶属推理的可靠性和稳健性。对各种开源LLM的广泛实验表明，我们提出的攻击非常有效、稳健且可推广，超越了三个广泛使用的基准测试中的最新基线。我们的代码可在以下网址获取：\href{https：//github.com/Salehzz/ACMIA}{\textColor{blue}{Github}}。



## **11. Using Mechanistic Interpretability to Craft Adversarial Attacks against Large Language Models**

使用机械可解释性来应对大型语言模型的对抗攻击 cs.LG

**SubmitDate**: 2025-05-06    [abs](http://arxiv.org/abs/2503.06269v2) [paper-pdf](http://arxiv.org/pdf/2503.06269v2)

**Authors**: Thomas Winninger, Boussad Addad, Katarzyna Kapusta

**Abstract**: Traditional white-box methods for creating adversarial perturbations against LLMs typically rely only on gradient computation from the targeted model, ignoring the internal mechanisms responsible for attack success or failure. Conversely, interpretability studies that analyze these internal mechanisms lack practical applications beyond runtime interventions. We bridge this gap by introducing a novel white-box approach that leverages mechanistic interpretability techniques to craft practical adversarial inputs. Specifically, we first identify acceptance subspaces - sets of feature vectors that do not trigger the model's refusal mechanisms - then use gradient-based optimization to reroute embeddings from refusal subspaces to acceptance subspaces, effectively achieving jailbreaks. This targeted approach significantly reduces computation cost, achieving attack success rates of 80-95\% on state-of-the-art models including Gemma2, Llama3.2, and Qwen2.5 within minutes or even seconds, compared to existing techniques that often fail or require hours of computation. We believe this approach opens a new direction for both attack research and defense development. Furthermore, it showcases a practical application of mechanistic interpretability where other methods are less efficient, which highlights its utility. The code and generated datasets are available at https://github.com/Sckathach/subspace-rerouting.

摘要: 用于针对LLM创建对抗性扰动的传统白盒方法通常仅依赖于目标模型的梯度计算，而忽略了负责攻击成功或失败的内部机制。相反，分析这些内部机制的可解释性研究缺乏运行时干预之外的实际应用。我们通过引入一种新颖的白盒方法来弥合这一差距，该方法利用机械解释性技术来制作实用的对抗性输入。具体来说，我们首先识别接受子空间--不会触发模型拒绝机制的特征载体集--然后使用基于梯度的优化将嵌入从拒绝子空间重新路由到接受子空间，有效地实现越狱。与经常失败或需要数小时计算的现有技术相比，这种有针对性的方法显着降低了计算成本，在几分钟甚至几秒钟内就实现了对Gemma 2、Llama3.2和Qwen 2.5等最先进模型80- 95%的攻击成功率。我们相信这种方法为攻击研究和防御开发开辟了新的方向。此外，它展示了机械解释性的实际应用，而其他方法效率较低，这凸显了它的实用性。代码和生成的数据集可在https://github.com/Sckathach/subspace-rerouting上获取。



## **12. A Trustworthy Multi-LLM Network: Challenges,Solutions, and A Use Case**

值得信赖的多元LLM网络：挑战、解决方案和用例 cs.NI

**SubmitDate**: 2025-05-06    [abs](http://arxiv.org/abs/2505.03196v1) [paper-pdf](http://arxiv.org/pdf/2505.03196v1)

**Authors**: Haoxiang Luo, Gang Sun, Yinqiu Liu, Dusit Niyato, Hongfang Yu, Mohammed Atiquzzaman, Schahram Dustdar

**Abstract**: Large Language Models (LLMs) demonstrate strong potential across a variety of tasks in communications and networking due to their advanced reasoning capabilities. However, because different LLMs have different model structures and are trained using distinct corpora and methods, they may offer varying optimization strategies for the same network issues. Moreover, the limitations of an individual LLM's training data, aggravated by the potential maliciousness of its hosting device, can result in responses with low confidence or even bias. To address these challenges, we propose a blockchain-enabled collaborative framework that connects multiple LLMs into a Trustworthy Multi-LLM Network (MultiLLMN). This architecture enables the cooperative evaluation and selection of the most reliable and high-quality responses to complex network optimization problems. Specifically, we begin by reviewing related work and highlighting the limitations of existing LLMs in collaboration and trust, emphasizing the need for trustworthiness in LLM-based systems. We then introduce the workflow and design of the proposed Trustworthy MultiLLMN framework. Given the severity of False Base Station (FBS) attacks in B5G and 6G communication systems and the difficulty of addressing such threats through traditional modeling techniques, we present FBS defense as a case study to empirically validate the effectiveness of our approach. Finally, we outline promising future research directions in this emerging area.

摘要: 大型语言模型（LLM）因其先进的推理能力而在通信和网络领域的各种任务中展现出强大的潜力。然而，由于不同的LLM具有不同的模型结构，并且使用不同的数据库和方法进行训练，因此它们可能会为相同的网络问题提供不同的优化策略。此外，个体LLM训练数据的局限性，再加上其托管设备的潜在恶意，可能会导致响应信心较低甚至有偏见。为了应对这些挑战，我们提出了一个支持区块链的协作框架，将多个LLM连接到值得信赖的多LLM网络（MultiLLNN）中。该架构能够对复杂的网络优化问题进行协作评估和选择最可靠和高质量的响应。具体来说，我们首先回顾相关工作，并强调现有的LLM在协作和信任方面的局限性，强调基于LLM的系统需要可信度。然后，我们介绍了工作流程和设计的建议值得信赖的MultiLLMN框架。鉴于B5G和6G通信系统中虚假基站（FBS）攻击的严重性以及通过传统建模技术解决此类威胁的困难，我们将FBS防御作为案例研究，以实证验证我们方法的有效性。最后，我们概述了这一新兴领域有前途的未来研究方向。



## **13. Towards Effective Identification of Attack Techniques in Cyber Threat Intelligence Reports using Large Language Models**

使用大型语言模型有效识别网络威胁情报报告中的攻击技术 cs.CR

5 pages, 2 figures 4 tables, accepted for publication at the Web  Conference 2025 (WWW'25)

**SubmitDate**: 2025-05-06    [abs](http://arxiv.org/abs/2505.03147v1) [paper-pdf](http://arxiv.org/pdf/2505.03147v1)

**Authors**: Hoang Cuong Nguyen, Shahroz Tariq, Mohan Baruwal Chhetri, Bao Quoc Vo

**Abstract**: This work evaluates the performance of Cyber Threat Intelligence (CTI) extraction methods in identifying attack techniques from threat reports available on the web using the MITRE ATT&CK framework. We analyse four configurations utilising state-of-the-art tools, including the Threat Report ATT&CK Mapper (TRAM) and open-source Large Language Models (LLMs) such as Llama2. Our findings reveal significant challenges, including class imbalance, overfitting, and domain-specific complexity, which impede accurate technique extraction. To mitigate these issues, we propose a novel two-step pipeline: first, an LLM summarises the reports, and second, a retrained SciBERT model processes a rebalanced dataset augmented with LLM-generated data. This approach achieves an improvement in F1-scores compared to baseline models, with several attack techniques surpassing an F1-score of 0.90. Our contributions enhance the efficiency of web-based CTI systems and support collaborative cybersecurity operations in an interconnected digital landscape, paving the way for future research on integrating human-AI collaboration platforms.

摘要: 这项工作评估了网络威胁情报（RTI）提取方法在使用MITRE ATT & CK框架从网络上可用的威胁报告中识别攻击技术方面的性能。我们利用最先进的工具分析了四种配置，包括Threat Report ATT & CK Mapper（TRAM）和开源大型语言模型（LLM），例如Llama 2。我们的研究结果揭示了重大挑战，包括阶级不平衡、过度匹配和特定领域的复杂性，这些挑战阻碍了准确的技术提取。为了缓解这些问题，我们提出了一种新颖的两步管道：首先，LLM总结报告，其次，重新训练的SciBERT模型处理用LLM生成的数据增强的重新平衡数据集。与基线模型相比，这种方法提高了F1评分，其中几种攻击技术超过了F1评分0.90。我们的贡献提高了基于网络的RTI系统的效率，并支持互联数字环境中的协作网络安全操作，为未来整合人类与人工智能协作平台的研究铺平了道路。



## **14. PEEK: Phishing Evolution Framework for Phishing Generation and Evolving Pattern Analysis using Large Language Models**

TEK：使用大型语言模型进行网络钓鱼生成和演变模式分析的网络钓鱼进化框架 cs.CR

**SubmitDate**: 2025-05-06    [abs](http://arxiv.org/abs/2411.11389v2) [paper-pdf](http://arxiv.org/pdf/2411.11389v2)

**Authors**: Fengchao Chen, Tingmin Wu, Van Nguyen, Shuo Wang, Alsharif Abuadbba, Carsten Rudolph

**Abstract**: Phishing remains a pervasive cyber threat, as attackers craft deceptive emails to lure victims into revealing sensitive information. While Artificial Intelligence (AI), in particular, deep learning, has become a key component in defending against phishing attacks, these approaches face critical limitations. The scarcity of publicly available, diverse, and updated data, largely due to privacy concerns, constrains detection effectiveness. As phishing tactics evolve rapidly, models trained on limited, outdated data struggle to detect new, sophisticated deception strategies, leaving systems and people vulnerable to an ever-growing array of attacks. We propose the first Phishing Evolution FramEworK (PEEK) for augmenting phishing email datasets with respect to quality and diversity, and analyzing changing phishing patterns for detection to adapt to updated phishing attacks. Specifically, we integrate large language models (LLMs) into the process of adversarial training to enhance the performance of the generated dataset and leverage persuasion principles in a recurrent framework to facilitate the understanding of changing phishing strategies. PEEK raises the proportion of usable phishing samples from 21.4% to 84.8%, surpassing existing works that rely on prompting and fine-tuning LLMs. The phishing datasets provided by PEEK, with evolving phishing patterns, outperform the other two available LLM-generated phishing email datasets in improving detection robustness. PEEK phishing boosts detectors' accuracy to over 88% and reduces adversarial sensitivity by up to 70%, still maintaining 70% detection accuracy against adversarial attacks.

摘要: 网络钓鱼仍然是一种普遍存在的网络威胁，因为攻击者制作了欺骗性电子邮件来引诱受害者泄露敏感信息。虽然人工智能（AI），特别是深度学习，已成为防御网络钓鱼攻击的关键组成部分，但这些方法面临着严重的局限性。主要由于隐私问题，公开可用的、多样化的和更新的数据的稀缺限制了检测有效性。随着网络钓鱼策略的迅速发展，在有限、过时的数据上训练的模型很难检测到新的、复杂的欺骗策略，从而使系统和人们容易受到越来越多的攻击。我们提出了第一个网络钓鱼Evolution FramEworK（TEK），用于增强网络钓鱼电子邮件数据集的质量和多样性，并分析不断变化的网络钓鱼模式进行检测，以适应更新的网络钓鱼攻击。具体来说，我们将大型语言模型（LLM）集成到对抗训练过程中，以增强生成的数据集的性能，并在循环框架中利用说服原则，以促进对不断变化的网络钓鱼策略的理解。TEK将可用网络钓鱼样本的比例从21.4%提高到84.8%，超过了依赖提示和微调LLM的现有作品。TEK提供的网络钓鱼数据集具有不断变化的网络钓鱼模式，在提高检测稳健性方面优于其他两个可用的LLM生成的网络钓鱼电子邮件数据集。TEK网络钓鱼将检测器的准确性提高至88%以上，并将对抗敏感性降低高达70%，针对对抗攻击仍保持70%的检测准确性。



## **15. Large Language Models as Robust Data Generators in Software Analytics: Are We There Yet?**

大型语言模型作为软件分析中稳健的数据生成器：我们已经做到了吗？ cs.SE

Accepted to the AI Model/Data Track of the Evaluation and Assessment  in Software Engineering (EASE) 2025 Conference

**SubmitDate**: 2025-05-05    [abs](http://arxiv.org/abs/2411.10565v3) [paper-pdf](http://arxiv.org/pdf/2411.10565v3)

**Authors**: Md. Abdul Awal, Mrigank Rochan, Chanchal K. Roy

**Abstract**: Large Language Model (LLM)-generated data is increasingly used in software analytics, but it is unclear how this data compares to human-written data, particularly when models are exposed to adversarial scenarios. Adversarial attacks can compromise the reliability and security of software systems, so understanding how LLM-generated data performs under these conditions, compared to human-written data, which serves as the benchmark for model performance, can provide valuable insights into whether LLM-generated data offers similar robustness and effectiveness. To address this gap, we systematically evaluate and compare the quality of human-written and LLM-generated data for fine-tuning robust pre-trained models (PTMs) in the context of adversarial attacks. We evaluate the robustness of six widely used PTMs, fine-tuned on human-written and LLM-generated data, before and after adversarial attacks. This evaluation employs nine state-of-the-art (SOTA) adversarial attack techniques across three popular software analytics tasks: clone detection, code summarization, and sentiment analysis in code review discussions. Additionally, we analyze the quality of the generated adversarial examples using eleven similarity metrics. Our findings reveal that while PTMs fine-tuned on LLM-generated data perform competitively with those fine-tuned on human-written data, they exhibit less robustness against adversarial attacks in software analytics tasks. Our study underscores the need for further exploration into enhancing the quality of LLM-generated training data to develop models that are both high-performing and capable of withstanding adversarial attacks in software analytics.

摘要: 大型语言模型（LLM）生成的数据越来越多地用于软件分析，但目前尚不清楚该数据与人类编写的数据相比如何，特别是当模型暴露于对抗场景时。对抗性攻击可能会损害软件系统的可靠性和安全性，因此，与作为模型性能基准的人类编写数据相比，了解LLM生成的数据在这些条件下的表现如何，可以为LLM生成的数据是否提供类似的稳健性和有效性提供有价值的见解。为了解决这一差距，我们系统地评估和比较人类编写的数据和LLM生成的数据的质量，以便在对抗性攻击的背景下微调稳健的预训练模型（Ptms）。我们评估了六种广泛使用的PtM的稳健性，这些PtM在对抗性攻击之前和之后根据人类编写和LLM生成的数据进行了微调。该评估在三个流行的软件分析任务中使用了九种最先进的（SOTA）对抗性攻击技术：克隆检测，代码摘要和代码审查讨论中的情感分析。此外，我们使用11个相似性度量来分析生成的对抗性示例的质量。我们的研究结果表明，虽然对LLM生成的数据进行微调的PTM与对人类编写的数据进行微调的PTM具有竞争力，但它们在软件分析任务中对对抗性攻击的鲁棒性较低。我们的研究强调了进一步探索提高LLM生成的训练数据质量的必要性，以开发高性能且能够抵御软件分析中的对抗性攻击的模型。



## **16. Large Language Models as Carriers of Hidden Messages**

大型语言模型作为隐藏消息的载体 cs.CL

Accepted on SECRYPT 2025 Conference. Code is available at  https://github.com/j-hoscilowic/zurek-stegano

**SubmitDate**: 2025-05-05    [abs](http://arxiv.org/abs/2406.02481v5) [paper-pdf](http://arxiv.org/pdf/2406.02481v5)

**Authors**: Jakub Hoscilowicz, Pawel Popiolek, Jan Rudkowski, Jedrzej Bieniasz, Artur Janicki

**Abstract**: Simple fine-tuning can embed hidden text into large language models (LLMs), which is revealed only when triggered by a specific query. Applications include LLM fingerprinting, where a unique identifier is embedded to verify licensing compliance, and steganography, where the LLM carries hidden messages disclosed through a trigger query.   Our work demonstrates that embedding hidden text via fine-tuning, although seemingly secure due to the vast number of potential triggers, is vulnerable to extraction through analysis of the LLM's output decoding process. We introduce an extraction attack called Unconditional Token Forcing (UTF), which iteratively feeds tokens from the LLM's vocabulary to reveal sequences with high token probabilities, indicating hidden text candidates. We also present Unconditional Token Forcing Confusion (UTFC), a defense paradigm that makes hidden text resistant to all known extraction attacks without degrading the general performance of LLMs compared to standard fine-tuning. UTFC has both benign (improving LLM fingerprinting) and malign applications (using LLMs to create covert communication channels).

摘要: 简单的微调可以将隐藏文本嵌入到大型语言模型（LLM）中，只有在特定查询触发时才会显示该文本。应用包括LLM指纹识别（其中嵌入唯一标识符以验证许可合规性）和隐写术（其中LLM携带通过触发查询披露的隐藏消息）。   我们的工作表明，通过微调嵌入隐藏文本，尽管由于潜在触发器数量庞大，看起来很安全，但通过分析LLM的输出解码过程，很容易被提取。我们引入了一种名为无条件令牌强迫（UTF）的提取攻击，它迭代地从LLM词汇表中输入令牌，以揭示具有高令牌概率的序列，从而指示隐藏的文本候选项。我们还介绍了无条件令牌强制混淆（UTFC），这是一种防御范式，可以使隐藏文本抵御所有已知的提取攻击，而与标准微调相比，不会降低LLM的总体性能。UTFC既有良性应用程序（改进LLM指纹识别），也有恶意应用程序（使用LLM创建秘密通信渠道）。



## **17. Mapping the Italian Telegram Ecosystem: Communities, Toxicity, and Hate Speech**

绘制意大利电报生态系统：社区，毒性和仇恨言论 cs.SI

**SubmitDate**: 2025-05-05    [abs](http://arxiv.org/abs/2504.19594v2) [paper-pdf](http://arxiv.org/pdf/2504.19594v2)

**Authors**: Lorenzo Alvisi, Serena Tardelli, Maurizio Tesconi

**Abstract**: Telegram has become a major space for political discourse and alternative media. However, its lack of moderation allows misinformation, extremism, and toxicity to spread. While prior research focused on these particular phenomena or topics, these have mostly been examined separately, and a broader understanding of the Telegram ecosystem is still missing. In this work, we fill this gap by conducting a large-scale analysis of the Italian Telegram sphere, leveraging a dataset of 186 million messages from 13,151 chats collected in 2023. Using network analysis, Large Language Models, and toxicity detection tools, we examine how different thematic communities form, align ideologically, and engage in harmful discourse within the Italian cultural context. Results show strong thematic and ideological homophily. We also identify mixed ideological communities where far-left and far-right rhetoric coexist on particular geopolitical issues. Beyond political analysis, we find that toxicity, rather than being isolated in a few extreme chats, appears widely normalized within highly toxic communities. Moreover, we find that Italian discourse primarily targets Black people, Jews, and gay individuals independently of the topic. Finally, we uncover common trend of intra-national hostility, where Italians often attack other Italians, reflecting regional and intra-regional cultural conflicts that can be traced back to old historical divisions. This study provides the first large-scale mapping of the Italian Telegram ecosystem, offering insights into ideological interactions, toxicity, and identity-targets of hate and contributing to research on online toxicity across different cultural and linguistic contexts on Telegram.

摘要: Telegram已成为政治话语和另类媒体的主要空间。然而，它缺乏节制，导致错误信息、极端主义和毒性蔓延。虽然之前的研究集中在这些特定的现象或主题上，但这些主要是单独研究的，并且仍然缺乏对Telegram生态系统的更广泛的了解。在这项工作中，我们通过对意大利Telegram领域进行大规模分析来填补这一空白，利用2023年收集的13，151条聊天记录中的1.86亿条消息的数据集。使用网络分析、大型语言模型和毒性检测工具，我们研究不同的主题社区如何在意大利文化背景下形成、意识形态上的一致以及参与有害话语。结果显示出较强的主题和意识形态一致性。我们还发现了混合的意识形态社区，其中极左和极右言论在特定地缘政治问题上共存。除了政治分析之外，我们发现毒性并没有在一些极端的聊天中被孤立，而是在高毒性社区中被广泛正常化。此外，我们发现意大利语的话语主要针对黑人、犹太人和同性恋者，与主题无关。最后，我们发现了国内敌意的共同趋势，意大利人经常攻击其他意大利人，反映了可以追溯到旧历史分歧的地区和地区内文化冲突。这项研究首次对意大利Telegram生态系统进行了大规模映射，提供了对意识形态相互作用、毒性和仇恨身份目标的见解，并为Telegram上不同文化和语言背景下的在线毒性研究做出了贡献。



## **18. A Survey on Privacy Risks and Protection in Large Language Models**

大型语言模型中的隐私风险与保护调查 cs.CR

**SubmitDate**: 2025-05-04    [abs](http://arxiv.org/abs/2505.01976v1) [paper-pdf](http://arxiv.org/pdf/2505.01976v1)

**Authors**: Kang Chen, Xiuze Zhou, Yuanguo Lin, Shibo Feng, Li Shen, Pengcheng Wu

**Abstract**: Although Large Language Models (LLMs) have become increasingly integral to diverse applications, their capabilities raise significant privacy concerns. This survey offers a comprehensive overview of privacy risks associated with LLMs and examines current solutions to mitigate these challenges. First, we analyze privacy leakage and attacks in LLMs, focusing on how these models unintentionally expose sensitive information through techniques such as model inversion, training data extraction, and membership inference. We investigate the mechanisms of privacy leakage, including the unauthorized extraction of training data and the potential exploitation of these vulnerabilities by malicious actors. Next, we review existing privacy protection against such risks, such as inference detection, federated learning, backdoor mitigation, and confidential computing, and assess their effectiveness in preventing privacy leakage. Furthermore, we highlight key practical challenges and propose future research directions to develop secure and privacy-preserving LLMs, emphasizing privacy risk assessment, secure knowledge transfer between models, and interdisciplinary frameworks for privacy governance. Ultimately, this survey aims to establish a roadmap for addressing escalating privacy challenges in the LLMs domain.

摘要: 虽然大型语言模型（LLM）已经成为各种应用程序的一部分，但它们的功能引起了严重的隐私问题。该调查全面概述了与LLM相关的隐私风险，并研究了缓解这些挑战的当前解决方案。首先，我们分析了LLM中的隐私泄露和攻击，重点关注这些模型如何通过模型反演、训练数据提取和成员推断等技术无意中暴露敏感信息。我们调查了隐私泄露的机制，包括未经授权提取训练数据以及恶意行为者可能利用这些漏洞。接下来，我们回顾针对此类风险（例如推断检测、联邦学习、后门缓解和机密计算）的现有隐私保护，并评估它们在防止隐私泄露方面的有效性。此外，我们还强调了关键的实践挑战，并提出了未来的研究方向，以开发安全且保护隐私的LLM，强调隐私风险评估、模型之间的安全知识转移以及隐私治理的跨学科框架。最终，这项调查旨在建立一个路线图，以解决LLM领域不断升级的隐私挑战。



## **19. Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs**

只见树木不见森林：利用启发式和偏见来激发对LLM的非理性选择 cs.CL

**SubmitDate**: 2025-05-03    [abs](http://arxiv.org/abs/2505.02862v1) [paper-pdf](http://arxiv.org/pdf/2505.02862v1)

**Authors**: Haoming Yang, Ke Ma, Xiaojun Jia, Yingfei Sun, Qianqian Xu, Qingming Huang

**Abstract**: Despite the remarkable performance of Large Language Models (LLMs), they remain vulnerable to jailbreak attacks, which can compromise their safety mechanisms. Existing studies often rely on brute-force optimization or manual design, failing to uncover potential risks in real-world scenarios. To address this, we propose a novel jailbreak attack framework, ICRT, inspired by heuristics and biases in human cognition. Leveraging the simplicity effect, we employ cognitive decomposition to reduce the complexity of malicious prompts. Simultaneously, relevance bias is utilized to reorganize prompts, enhancing semantic alignment and inducing harmful outputs effectively. Furthermore, we introduce a ranking-based harmfulness evaluation metric that surpasses the traditional binary success-or-failure paradigm by employing ranking aggregation methods such as Elo, HodgeRank, and Rank Centrality to comprehensively quantify the harmfulness of generated content. Experimental results show that our approach consistently bypasses mainstream LLMs' safety mechanisms and generates high-risk content, providing insights into jailbreak attack risks and contributing to stronger defense strategies.

摘要: 尽管大型语言模型（LLM）性能出色，但它们仍然容易受到越狱攻击，这可能会损害其安全机制。现有的研究通常依赖于暴力优化或手动设计，未能发现现实世界场景中的潜在风险。为了解决这个问题，我们提出了一种新颖的越狱攻击框架ICRT，其灵感来自人类认知中的启发和偏见。利用简单性效应，我们采用认知分解来降低恶意提示的复杂性。同时，利用相关性偏差来重组提示，增强语义对齐并有效地诱导有害输出。此外，我们引入了一种基于排名的危害性评估指标，通过采用Elo、HodgeRank和Rank Centrality等排名聚合方法来全面量化生成内容的危害性，超越了传统的二元成败范式。实验结果表明，我们的方法始终绕过主流LLM的安全机制并生成高风险内容，提供了对越狱攻击风险的见解，并有助于制定更强有力的防御策略。



## **20. Parameterized Argumentation-based Reasoning Tasks for Benchmarking Generative Language Models**

用于生成语言模型基准测试的参数化基于论证的推理任务 cs.AI

This manuscript has been accepted for presentation as a short paper  at the 20th International Conference of AI & Law in Chicago, June 16 to 20 of  2025

**SubmitDate**: 2025-05-02    [abs](http://arxiv.org/abs/2505.01539v1) [paper-pdf](http://arxiv.org/pdf/2505.01539v1)

**Authors**: Cor Steging, Silja Renooij, Bart Verheij

**Abstract**: Generative large language models as tools in the legal domain have the potential to improve the justice system. However, the reasoning behavior of current generative models is brittle and poorly understood, hence cannot be responsibly applied in the domains of law and evidence. In this paper, we introduce an approach for creating benchmarks that can be used to evaluate the reasoning capabilities of generative language models. These benchmarks are dynamically varied, scalable in their complexity, and have formally unambiguous interpretations. In this study, we illustrate the approach on the basis of witness testimony, focusing on the underlying argument attack structure. We dynamically generate both linear and non-linear argument attack graphs of varying complexity and translate these into reasoning puzzles about witness testimony expressed in natural language. We show that state-of-the-art large language models often fail in these reasoning puzzles, already at low complexity. Obvious mistakes are made by the models, and their inconsistent performance indicates that their reasoning capabilities are brittle. Furthermore, at higher complexity, even state-of-the-art models specifically presented for reasoning capabilities make mistakes. We show the viability of using a parametrized benchmark with varying complexity to evaluate the reasoning capabilities of generative language models. As such, the findings contribute to a better understanding of the limitations of the reasoning capabilities of generative models, which is essential when designing responsible AI systems in the legal domain.

摘要: 生成性大型语言模型作为法律领域的工具有潜力改善司法系统。然而，当前生成模型的推理行为很脆弱，而且人们理解得很差，因此无法负责任地应用于法律和证据领域。在本文中，我们介绍了一种创建基准的方法，该基准可用于评估生成式语言模型的推理能力。这些基准是动态变化的，其复杂性可扩展的，并且具有形式上明确的解释。在这项研究中，我们根据证人证词说明了这种方法，重点关注潜在的论点攻击结构。我们动态生成复杂性不同的线性和非线性论点攻击图，并将其转化为有关用自然语言表达的证人证词的推理难题。我们表明，最先进的大型语言模型经常在这些推理难题中失败，而且复杂性已经很低。模型会犯明显的错误，而且它们的不一致的性能表明它们的推理能力很脆弱。此外，在更高的复杂性下，即使是专门为推理能力提供的最先进的模型也会出错。我们展示了使用具有不同复杂性的参数化基准来评估生成式语言模型的推理能力的可行性。因此，这些发现有助于更好地理解生成模型推理能力的局限性，这在法律领域设计负责任的人工智能系统时至关重要。



## **21. Rubber Mallet: A Study of High Frequency Localized Bit Flips and Their Impact on Security**

橡皮锤：高频局部位翻转及其对安全性影响的研究 cs.CR

**SubmitDate**: 2025-05-02    [abs](http://arxiv.org/abs/2505.01518v1) [paper-pdf](http://arxiv.org/pdf/2505.01518v1)

**Authors**: Andrew Adiletta, Zane Weissman, Fatemeh Khojasteh Dana, Berk Sunar, Shahin Tajik

**Abstract**: The increasing density of modern DRAM has heightened its vulnerability to Rowhammer attacks, which induce bit flips by repeatedly accessing specific memory rows. This paper presents an analysis of bit flip patterns generated by advanced Rowhammer techniques that bypass existing hardware defenses. First, we investigate the phenomenon of adjacent bit flips--where two or more physically neighboring bits are corrupted simultaneously--and demonstrate they occur with significantly higher frequency than previously documented. We also show that if multiple bits flip within a byte, they are more likely to be adjacent than randomly distributed: for example, if 4 bits flip within a byte, there is an 87% chance that they are all adjacent. We also demonstrate that bit flips within a row will naturally cluster together likely due to the underlying physics of the attack. We then investigate two fault injection attacks enabled by multiple adjacent or nearby bit flips. First, we show how these correlated flips enable efficient cryptographic signature correction attacks, successfully recovering ECDSA private keys from OpenSSL implementations where single-bit approaches would be unfeasible. Second, we introduce a targeted attack against large language models by exploiting Rowhammer-induced corruptions in tokenizer dictionaries of GGUF model files. This attack effectively rewrites safety instructions in system prompts by swapping safety-critical tokens with benign alternatives, circumventing model guardrails while maintaining normal functionality in other contexts. Our experimental results across multiple DRAM configurations reveal that current memory protection schemes are inadequate against these sophisticated attack vectors, which can achieve their objectives with precise, minimal modifications rather than random corruption.

摘要: 现代动态存储器密度的增加加剧了其对Rowhammer攻击的脆弱性，Rowhammer攻击通过重复访问特定内存行来引发位翻转。本文分析了绕过现有硬件防御的高级Rowhammer技术生成的位翻转模式。首先，我们研究相邻位翻转的现象--两个或更多物理相邻位同时被破坏--并证明它们的发生频率比之前记录的要高得多。我们还表明，如果多个比特在一个字节内翻转，那么它们更有可能相邻而不是随机分布：例如，如果一个字节内翻转4个比特，那么它们都相邻的可能性有87%。我们还证明，由于攻击的基本物理原理，行内的位翻转可能会自然聚集在一起。然后，我们研究由多个相邻或附近的位翻转引发的两种故障注入攻击。首先，我们展示了这些相关翻转如何实现高效的加密签名纠正攻击，成功从单位方法不可行的OpenSSL实现中恢复ECDSA私有密钥。其次，我们通过利用GGUF模型文件的标记器字典中Rowhammer引起的损坏，引入针对大型语言模型的有针对性的攻击。这种攻击通过将安全关键令牌与良性替代方案交换，有效地重写了系统提示中的安全指令，绕过模型护栏，同时在其他上下文中保持正常功能。我们跨多种RAM配置的实验结果表明，当前的内存保护方案不足以对抗这些复杂的攻击载体，这些攻击载体可以通过精确、最少的修改而不是随机损坏来实现其目标。



## **22. LLM Security: Vulnerabilities, Attacks, Defenses, and Countermeasures**

LLM安全：漏洞、攻击、防御和对策 cs.CR

**SubmitDate**: 2025-05-02    [abs](http://arxiv.org/abs/2505.01177v1) [paper-pdf](http://arxiv.org/pdf/2505.01177v1)

**Authors**: Francisco Aguilera-Martínez, Fernando Berzal

**Abstract**: As large language models (LLMs) continue to evolve, it is critical to assess the security threats and vulnerabilities that may arise both during their training phase and after models have been deployed. This survey seeks to define and categorize the various attacks targeting LLMs, distinguishing between those that occur during the training phase and those that affect already trained models. A thorough analysis of these attacks is presented, alongside an exploration of defense mechanisms designed to mitigate such threats. Defenses are classified into two primary categories: prevention-based and detection-based defenses. Furthermore, our survey summarizes possible attacks and their corresponding defense strategies. It also provides an evaluation of the effectiveness of the known defense mechanisms for the different security threats. Our survey aims to offer a structured framework for securing LLMs, while also identifying areas that require further research to improve and strengthen defenses against emerging security challenges.

摘要: 随着大型语言模型（LLM）的不断发展，评估在训练阶段和模型部署后可能出现的安全威胁和漏洞至关重要。本调查旨在定义和分类针对LLM的各种攻击，区分那些在训练阶段发生的攻击和那些影响已经训练好的模型的攻击。本文对这些攻击进行了全面的分析，并探讨了旨在减轻此类威胁的防御机制。防御分为两大类：基于预防的防御和基于检测的防御。此外，我们的调查还总结了可能的攻击及其相应的防御策略。它还评估了已知防御机制对不同安全威胁的有效性。我们的调查旨在提供一个结构化的框架来保护法学硕士，同时确定需要进一步研究的领域，以改善和加强对新兴安全挑战的防御。



## **23. A Rusty Link in the AI Supply Chain: Detecting Evil Configurations in Model Repositories**

AI供应链中的生锈链接：检测模型库中的恶意行为 cs.CR

**SubmitDate**: 2025-05-02    [abs](http://arxiv.org/abs/2505.01067v1) [paper-pdf](http://arxiv.org/pdf/2505.01067v1)

**Authors**: Ziqi Ding, Qian Fu, Junchen Ding, Gelei Deng, Yi Liu, Yuekang Li

**Abstract**: Recent advancements in large language models (LLMs) have spurred the development of diverse AI applications from code generation and video editing to text generation; however, AI supply chains such as Hugging Face, which host pretrained models and their associated configuration files contributed by the public, face significant security challenges; in particular, configuration files originally intended to set up models by specifying parameters and initial settings can be exploited to execute unauthorized code, yet research has largely overlooked their security compared to that of the models themselves; in this work, we present the first comprehensive study of malicious configurations on Hugging Face, identifying three attack scenarios (file, website, and repository operations) that expose inherent risks; to address these threats, we introduce CONFIGSCAN, an LLM-based tool that analyzes configuration files in the context of their associated runtime code and critical libraries, effectively detecting suspicious elements with low false positive rates and high accuracy; our extensive evaluation uncovers thousands of suspicious repositories and configuration files, underscoring the urgent need for enhanced security validation in AI model hosting platforms.

摘要: 大型语言模型（LLM）的最新进展刺激了从代码生成、视频编辑到文本生成等各种人工智能应用的发展;然而，拥抱脸等人工智能供应链（托管预训练模型及其相关配置文件）面临着重大的安全挑战;公众贡献;特别是，最初旨在通过指定参数和初始设置来建立模型的配置文件可能会被利用来执行未经授权的代码，然而，与模型本身相比，研究在很大程度上忽视了它们的安全性;在这项工作中，我们首次对Hugging Face上的恶意配置进行了全面研究，识别了三种攻击场景暴露固有风险的（文件、网站和存储库操作）;为了解决这些威胁，我们引入了CONFIGSCAN，这是一种基于LLM的工具，可以在相关的运行时代码和关键库的上下文中分析配置文件，以低误报率和高准确性有效检测可疑元素;我们的广泛评估发现了数千个可疑存储库和配置文件，凸显了人工智能模型托管平台中增强安全验证的迫切需要。



## **24. Good News for Script Kiddies? Evaluating Large Language Models for Automated Exploit Generation**

好消息给孩子们。评估大型语言模型以自动生成漏洞 cs.CR

**SubmitDate**: 2025-05-02    [abs](http://arxiv.org/abs/2505.01065v1) [paper-pdf](http://arxiv.org/pdf/2505.01065v1)

**Authors**: David Jin, Qian Fu, Yuekang Li

**Abstract**: Large Language Models (LLMs) have demonstrated remarkable capabilities in code-related tasks, raising concerns about their potential for automated exploit generation (AEG). This paper presents the first systematic study on LLMs' effectiveness in AEG, evaluating both their cooperativeness and technical proficiency. To mitigate dataset bias, we introduce a benchmark with refactored versions of five software security labs. Additionally, we design an LLM-based attacker to systematically prompt LLMs for exploit generation. Our experiments reveal that GPT-4 and GPT-4o exhibit high cooperativeness, comparable to uncensored models, while Llama3 is the most resistant. However, no model successfully generates exploits for refactored labs, though GPT-4o's minimal errors highlight the potential for LLM-driven AEG advancements.

摘要: 大型语言模型（LLM）在代码相关任务中表现出了非凡的能力，这引发了人们对其自动利用生成（AEG）潜力的担忧。本文首次对LLM在AEG中的有效性进行了系统研究，评估了他们的合作性和技术熟练程度。为了减轻数据集偏见，我们引入了一个具有五个软件安全实验室重构版本的基准测试。此外，我们设计了一个基于LLM的攻击者来系统性地提示LLM进行漏洞利用生成。我们的实验表明，GPT-4和GPT-4 o表现出高度的协作性，与未经审查的模型相当，而Llama 3的抵抗力最强。然而，没有一个模型成功地为重构实验室生成漏洞，尽管GPT-4 o的最小错误凸显了LLM驱动的AEG进步的潜力。



## **25. Transferable Adversarial Attacks on Black-Box Vision-Language Models**

黑匣子视觉语言模型的可转移对抗攻击 cs.CV

**SubmitDate**: 2025-05-02    [abs](http://arxiv.org/abs/2505.01050v1) [paper-pdf](http://arxiv.org/pdf/2505.01050v1)

**Authors**: Kai Hu, Weichen Yu, Li Zhang, Alexander Robey, Andy Zou, Chengming Xu, Haoqi Hu, Matt Fredrikson

**Abstract**: Vision Large Language Models (VLLMs) are increasingly deployed to offer advanced capabilities on inputs comprising both text and images. While prior research has shown that adversarial attacks can transfer from open-source to proprietary black-box models in text-only and vision-only contexts, the extent and effectiveness of such vulnerabilities remain underexplored for VLLMs. We present a comprehensive analysis demonstrating that targeted adversarial examples are highly transferable to widely-used proprietary VLLMs such as GPT-4o, Claude, and Gemini. We show that attackers can craft perturbations to induce specific attacker-chosen interpretations of visual information, such as misinterpreting hazardous content as safe, overlooking sensitive or restricted material, or generating detailed incorrect responses aligned with the attacker's intent. Furthermore, we discover that universal perturbations -- modifications applicable to a wide set of images -- can consistently induce these misinterpretations across multiple proprietary VLLMs. Our experimental results on object recognition, visual question answering, and image captioning show that this vulnerability is common across current state-of-the-art models, and underscore an urgent need for robust mitigations to ensure the safe and secure deployment of VLLMs.

摘要: Vision大型语言模型（VLLM）越来越多地部署，以提供包含文本和图像的输入的高级功能。虽然之前的研究表明，在纯文本和纯视觉的环境中，对抗性攻击可以从开源模型转移到专有黑匣子模型，但对于VLLM来说，此类漏洞的范围和有效性仍然没有得到充分的研究。我们提出了一项全面的分析，证明有针对性的对抗性示例可以高度转移到广泛使用的专有VLLM，例如GPT-4 o、Claude和Gemini。我们表明，攻击者可以制造干扰来诱导攻击者选择的特定视觉信息解释，例如将危险内容误解为安全内容、忽略敏感或受限制材料，或者生成与攻击者意图一致的详细错误响应。此外，我们发现普遍扰动（适用于广泛图像集的修改）可能会在多个专有VLLM中持续引发这些误解。我们关于对象识别、视觉问答和图像字幕的实验结果表明，这种漏洞在当前最先进的模型中很常见，并强调迫切需要强大的缓解措施，以确保VLLM的安全部署。



## **26. Prompt Inversion Attack against Collaborative Inference of Large Language Models**

针对大型语言模型协作推理的提示倒置攻击 cs.CR

To appear at IEEE Symposium on Security and Privacy 2025

**SubmitDate**: 2025-05-02    [abs](http://arxiv.org/abs/2503.09022v3) [paper-pdf](http://arxiv.org/pdf/2503.09022v3)

**Authors**: Wenjie Qu, Yuguang Zhou, Yongji Wu, Tingsong Xiao, Binhang Yuan, Yiming Li, Jiaheng Zhang

**Abstract**: Large language models (LLMs) have been widely applied for their remarkable capability of content generation. However, the practical use of open-source LLMs is hindered by high resource requirements, making deployment expensive and limiting widespread development. The collaborative inference is a promising solution for this problem, in which users collaborate by each hosting a subset of layers and transmitting intermediate activation. Many companies are building collaborative inference platforms to reduce LLM serving costs, leveraging users' underutilized GPUs. Despite widespread interest in collaborative inference within academia and industry, the privacy risks associated with LLM collaborative inference have not been well studied. This is largely because of the challenge posed by inverting LLM activation due to its strong non-linearity.   In this paper, to validate the severity of privacy threats in LLM collaborative inference, we introduce the concept of prompt inversion attack (PIA), where a malicious participant intends to recover the input prompt through the activation transmitted by its previous participant. Extensive experiments show that our PIA method substantially outperforms existing baselines. For example, our method achieves an 88.4\% token accuracy on the Skytrax dataset with the Llama-65B model when inverting the maximum number of transformer layers, while the best baseline method only achieves 22.8\% accuracy. The results verify the effectiveness of our PIA attack and highlights its practical threat to LLM collaborative inference systems.

摘要: 大型语言模型（LLM）因其出色的内容生成能力而得到广泛应用。然而，开源LLM的实际使用受到高资源需求的阻碍，导致部署成本高昂并限制了广泛开发。协作推理是这个问题的一个有希望的解决方案，其中用户通过各自托管层的子集并传输中间激活来进行协作。许多公司正在构建协作推理平台，以利用用户未充分利用的图形处理器来降低LLM服务成本。尽管学术界和工业界对协作推理产生了广泛的兴趣，但与LLM协作推理相关的隐私风险尚未得到很好的研究。这主要是因为LLM激活由于其强非线性而带来的挑战。   本文中，为了验证LLM协同推理中隐私威胁的严重性，我们引入了提示倒置攻击（PIA）的概念，恶意参与者意图通过其前一参与者传输的激活来恢复输入提示。大量实验表明，我们的PIA方法的性能大大优于现有的基线。例如，当倒置最大数量的Transformer层时，我们的方法在使用Llama-65 B模型的Skytrax数据集上实现了88.4%的令牌准确性，而最佳基线方法仅实现了22.8%的准确性。结果验证了我们PIA攻击的有效性，并强调了其对LLM协同推理系统的实际威胁。



## **27. Attack and defense techniques in large language models: A survey and new perspectives**

大型语言模型中的攻击和防御技术：概览和新观点 cs.CR

**SubmitDate**: 2025-05-02    [abs](http://arxiv.org/abs/2505.00976v1) [paper-pdf](http://arxiv.org/pdf/2505.00976v1)

**Authors**: Zhiyu Liao, Kang Chen, Yuanguo Lin, Kangkang Li, Yunxuan Liu, Hefeng Chen, Xingwang Huang, Yuanhui Yu

**Abstract**: Large Language Models (LLMs) have become central to numerous natural language processing tasks, but their vulnerabilities present significant security and ethical challenges. This systematic survey explores the evolving landscape of attack and defense techniques in LLMs. We classify attacks into adversarial prompt attack, optimized attacks, model theft, as well as attacks on application of LLMs, detailing their mechanisms and implications. Consequently, we analyze defense strategies, including prevention-based and detection-based defense methods. Although advances have been made, challenges remain to adapt to the dynamic threat landscape, balance usability with robustness, and address resource constraints in defense implementation. We highlight open problems, including the need for adaptive scalable defenses, explainable security techniques, and standardized evaluation frameworks. This survey provides actionable insights and directions for developing secure and resilient LLMs, emphasizing the importance of interdisciplinary collaboration and ethical considerations to mitigate risks in real-world applications.

摘要: 大型语言模型（LLM）已成为众多自然语言处理任务的核心，但它们的漏洞带来了重大的安全和道德挑战。这项系统性调查探讨了LLM攻击和防御技术不断变化的格局。我们将攻击分为对抗即时攻击、优化攻击、模型盗窃以及对LLM应用程序的攻击，详细介绍了它们的机制和含义。因此，我们分析防御策略，包括基于预防和基于检测的防御方法。虽然已经取得了进展，但挑战仍然存在，以适应动态的威胁环境，平衡可用性与鲁棒性，并解决防御实施中的资源限制。我们强调开放的问题，包括需要自适应可扩展的防御，可解释的安全技术，和标准化的评估框架。该调查为开发安全和弹性LLM提供了可操作的见解和方向，强调了跨学科合作和道德考虑的重要性，以减轻现实世界应用中的风险。



## **28. Protocol-agnostic and Data-free Backdoor Attacks on Pre-trained Models in RF Fingerprinting**

对RF指纹识别中预训练模型的协议不可知且无数据后门攻击 cs.CR

10 pages, 7 figures, accepted by IEEE INFOCOM 2025

**SubmitDate**: 2025-05-01    [abs](http://arxiv.org/abs/2505.00881v1) [paper-pdf](http://arxiv.org/pdf/2505.00881v1)

**Authors**: Tianya Zhao, Ningning Wang, Junqing Zhang, Xuyu Wang

**Abstract**: While supervised deep neural networks (DNNs) have proven effective for device authentication via radio frequency (RF) fingerprinting, they are hindered by domain shift issues and the scarcity of labeled data. The success of large language models has led to increased interest in unsupervised pre-trained models (PTMs), which offer better generalization and do not require labeled datasets, potentially addressing the issues mentioned above. However, the inherent vulnerabilities of PTMs in RF fingerprinting remain insufficiently explored. In this paper, we thoroughly investigate data-free backdoor attacks on such PTMs in RF fingerprinting, focusing on a practical scenario where attackers lack access to downstream data, label information, and training processes. To realize the backdoor attack, we carefully design a set of triggers and predefined output representations (PORs) for the PTMs. By mapping triggers and PORs through backdoor training, we can implant backdoor behaviors into the PTMs, thereby introducing vulnerabilities across different downstream RF fingerprinting tasks without requiring prior knowledge. Extensive experiments demonstrate the wide applicability of our proposed attack to various input domains, protocols, and PTMs. Furthermore, we explore potential detection and defense methods, demonstrating the difficulty of fully safeguarding against our proposed backdoor attack.

摘要: 虽然监督式深度神经网络（DNN）已被证明对于通过射频（RF）指纹识别的设备认证有效，但它们受到域转移问题和标记数据稀缺的阻碍。大型语言模型的成功导致了人们对无监督预训练模型（Ptms）的兴趣越来越大，这些模型提供更好的概括性，并且不需要标记的数据集，从而可能解决上述问题。然而，RF指纹识别中的PTO固有漏洞仍然没有得到充分的探索。在本文中，我们彻底调查了对RF指纹识别中此类PTO的无数据后门攻击，重点关注攻击者无法访问下游数据、标签信息和训练过程的实际场景。为了实现后门攻击，我们精心设计了一组触发器和预定义的输出表示（POL）。通过通过后门训练映射触发器和POL，我们可以将后门行为植入到PTO中，从而在不需要先验知识的情况下在不同下游RF指纹识别任务中引入漏洞。大量的实验证明了我们提出的攻击对各种输入域、协议和STM的广泛适用性。此外，我们还探索了潜在的检测和防御方法，证明了全面防范我们提出的后门攻击的困难。



## **29. OET: Optimization-based prompt injection Evaluation Toolkit**

OET：基于优化的即时注入评估工具包 cs.CR

**SubmitDate**: 2025-05-01    [abs](http://arxiv.org/abs/2505.00843v1) [paper-pdf](http://arxiv.org/pdf/2505.00843v1)

**Authors**: Jinsheng Pan, Xiaogeng Liu, Chaowei Xiao

**Abstract**: Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation, enabling their widespread adoption across various domains. However, their susceptibility to prompt injection attacks poses significant security risks, as adversarial inputs can manipulate model behavior and override intended instructions. Despite numerous defense strategies, a standardized framework to rigorously evaluate their effectiveness, especially under adaptive adversarial scenarios, is lacking. To address this gap, we introduce OET, an optimization-based evaluation toolkit that systematically benchmarks prompt injection attacks and defenses across diverse datasets using an adaptive testing framework. Our toolkit features a modular workflow that facilitates adversarial string generation, dynamic attack execution, and comprehensive result analysis, offering a unified platform for assessing adversarial robustness. Crucially, the adaptive testing framework leverages optimization methods with both white-box and black-box access to generate worst-case adversarial examples, thereby enabling strict red-teaming evaluations. Extensive experiments underscore the limitations of current defense mechanisms, with some models remaining susceptible even after implementing security enhancements.

摘要: 大型语言模型（LLM）在自然语言理解和生成方面表现出了非凡的能力，使其能够在各个领域广泛采用。然而，它们对即时注入攻击的敏感性带来了巨大的安全风险，因为对抗性输入可以操纵模型行为并覆盖预期指令。尽管防御策略众多，但缺乏一个标准化的框架来严格评估其有效性，特别是在适应性对抗场景下。为了解决这一差距，我们引入了OET，这是一个基于优化的评估工具包，它使用自适应测试框架对不同数据集的提示注入攻击和防御进行系统性基准测试。我们的工具包具有模块化的工作流程，可促进对抗字符串生成，动态攻击执行和全面的结果分析，为评估对抗鲁棒性提供统一的平台。至关重要的是，自适应测试框架利用白盒和黑盒访问的优化方法来生成最坏情况的对抗性示例，从而实现严格的红队评估。大量的实验强调了当前防御机制的局限性，即使在实施安全增强措施后，一些模型仍然容易受到影响。



## **30. Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from Large Language Models**

溢出豆子：利用中央处理器缓存侧通道从大型语言模型中泄露令牌 cs.CR

**SubmitDate**: 2025-05-01    [abs](http://arxiv.org/abs/2505.00817v1) [paper-pdf](http://arxiv.org/pdf/2505.00817v1)

**Authors**: Andrew Adiletta, Berk Sunar

**Abstract**: Side-channel attacks on shared hardware resources increasingly threaten confidentiality, especially with the rise of Large Language Models (LLMs). In this work, we introduce Spill The Beans, a novel application of cache side-channels to leak tokens generated by an LLM. By co-locating an attack process on the same hardware as the victim model, we flush and reload embedding vectors from the embedding layer, where each token corresponds to a unique embedding vector. When accessed during token generation, it results in a cache hit detectable by our attack on shared lower-level caches.   A significant challenge is the massive size of LLMs, which, by nature of their compute intensive operation, quickly evicts embedding vectors from the cache. We address this by balancing the number of tokens monitored against the amount of information leaked. Monitoring more tokens increases potential vocabulary leakage but raises the chance of missing cache hits due to eviction; monitoring fewer tokens improves detection reliability but limits vocabulary coverage.   Through extensive experimentation, we demonstrate the feasibility of leaking tokens from LLMs via cache side-channels. Our findings reveal a new vulnerability in LLM deployments, highlighting that even sophisticated models are susceptible to traditional side-channel attacks. We discuss the implications for privacy and security in LLM-serving infrastructures and suggest considerations for mitigating such threats. For proof of concept we consider two concrete attack scenarios: Our experiments show that an attacker can recover as much as 80%-90% of a high entropy API key with single shot monitoring. As for English text we can reach a 40% recovery rate with a single shot. We should note that the rate highly depends on the monitored token set and these rates can be improved by targeting more specialized output domains.

摘要: 对共享硬件资源的侧通道攻击日益威胁机密性，尤其是随着大型语言模型（LLM）的兴起。在这项工作中，我们介绍了Spill The Bean，这是一种新型的缓存侧通道应用程序，用于泄露LLM生成的令牌。通过将攻击过程与受害者模型共存在同一硬件上，我们从嵌入层刷新并重新加载嵌入载体，其中每个令牌对应于唯一的嵌入载体。当在令牌生成期间访问时，它会导致我们对共享较低级别缓存的攻击可检测到的缓存命中。   一个重大挑战是LLM的巨大规模，由于其计算密集型操作的本质，LLM会快速从缓存中驱逐嵌入载体。我们通过平衡监控的代币数量与泄露的信息量来解决这个问题。监控更多的令牌会增加潜在的词汇泄露，但会增加因驱逐而错过缓存命中的机会;监控更少的令牌会提高检测可靠性，但会限制词汇覆盖范围。   通过广泛的实验，我们证明了通过缓存侧通道从LLM泄露令牌的可行性。我们的研究结果揭示了LLM部署中的一个新漏洞，强调即使是复杂的模型也容易受到传统的侧通道攻击。我们讨论了LLM服务基础设施中隐私和安全的影响，并提出了缓解此类威胁的考虑因素。为了证明概念，我们考虑了两种具体的攻击场景：我们的实验表明，攻击者可以通过单次监控恢复多达80%-90%的高熵API密钥。至于英文文本，我们一次就可以达到40%的恢复率。我们应该注意到，该速率高度依赖于所监视的令牌集，并且可以通过针对更专业的输出域来提高这些速率。



## **31. Can Differentially Private Fine-tuning LLMs Protect Against Privacy Attacks?**

差异私有微调LLM可以防止隐私攻击吗？ cs.CR

accepted by DBSec25

**SubmitDate**: 2025-05-01    [abs](http://arxiv.org/abs/2504.21036v2) [paper-pdf](http://arxiv.org/pdf/2504.21036v2)

**Authors**: Hao Du, Shang Liu, Yang Cao

**Abstract**: Fine-tuning large language models (LLMs) has become an essential strategy for adapting them to specialized tasks; however, this process introduces significant privacy challenges, as sensitive training data may be inadvertently memorized and exposed. Although differential privacy (DP) offers strong theoretical guarantees against such leakage, its empirical privacy effectiveness on LLMs remains unclear, especially under different fine-tuning methods. In this paper, we systematically investigate the impact of DP across fine-tuning methods and privacy budgets, using both data extraction and membership inference attacks to assess empirical privacy risks. Our main findings are as follows: (1) Differential privacy reduces model utility, but its impact varies significantly across different fine-tuning methods. (2) Without DP, the privacy risks of models fine-tuned with different approaches differ considerably. (3) When DP is applied, even a relatively high privacy budget can substantially lower privacy risk. (4) The privacy-utility trade-off under DP training differs greatly among fine-tuning methods, with some methods being unsuitable for DP due to severe utility degradation. Our results provide practical guidance for privacy-conscious deployment of LLMs and pave the way for future research on optimizing the privacy-utility trade-off in fine-tuning methodologies.

摘要: 微调大型语言模型（LLM）已成为使其适应专业任务的重要策略;然而，这个过程带来了重大的隐私挑战，因为敏感的训练数据可能会被无意中记住和暴露。尽管差异隐私（DP）为防止此类泄露提供了强有力的理论保证，但其对LLM的经验隐私有效性仍然不清楚，尤其是在不同的微调方法下。在本文中，我们系统地研究了DP对微调方法和隐私预算的影响，使用数据提取和成员资格推断攻击来评估经验隐私风险。我们的主要研究结果如下：（1）差异隐私会降低模型效用，但其影响在不同的微调方法中存在显着差异。(2)如果没有DP，用不同方法微调的模型的隐私风险会有很大差异。(3)当应用DP时，即使相对较高的隐私预算也可以大幅降低隐私风险。(4)不同微调方法之间的DP训练下的隐私与公用事业权衡差异很大，有些方法由于公用事业严重退化而不适合DP。我们的结果为具有隐私意识的LLM部署提供了实践指导，并为未来优化微调方法中的隐私与公用事业权衡的研究铺平了道路。



## **32. Stochastic Subspace Descent Accelerated via Bi-fidelity Line Search**

通过双保真线搜索加速随机子空间下降 cs.LG

**SubmitDate**: 2025-04-30    [abs](http://arxiv.org/abs/2505.00162v1) [paper-pdf](http://arxiv.org/pdf/2505.00162v1)

**Authors**: Nuojin Cheng, Alireza Doostan, Stephen Becker

**Abstract**: Efficient optimization remains a fundamental challenge across numerous scientific and engineering domains, especially when objective function and gradient evaluations are computationally expensive. While zeroth-order optimization methods offer effective approaches when gradients are inaccessible, their practical performance can be limited by the high cost associated with function queries. This work introduces the bi-fidelity stochastic subspace descent (BF-SSD) algorithm, a novel zeroth-order optimization method designed to reduce this computational burden. BF-SSD leverages a bi-fidelity framework, constructing a surrogate model from a combination of computationally inexpensive low-fidelity (LF) and accurate high-fidelity (HF) function evaluations. This surrogate model facilitates an efficient backtracking line search for step size selection, for which we provide theoretical convergence guarantees under standard assumptions. We perform a comprehensive empirical evaluation of BF-SSD across four distinct problems: a synthetic optimization benchmark, dual-form kernel ridge regression, black-box adversarial attacks on machine learning models, and transformer-based black-box language model fine-tuning. Numerical results demonstrate that BF-SSD consistently achieves superior optimization performance while requiring significantly fewer HF function evaluations compared to relevant baseline methods. This study highlights the efficacy of integrating bi-fidelity strategies within zeroth-order optimization, positioning BF-SSD as a promising and computationally efficient approach for tackling large-scale, high-dimensional problems encountered in various real-world applications.

摘要: 有效的优化仍然是众多科学和工程领域的一个根本挑战，特别是当目标函数和梯度评估计算昂贵时。虽然零阶优化方法在无法访问梯度时提供了有效的方法，但其实际性能可能会受到与函数查询相关的高成本的限制。这项工作引入了双保真随机子空间下降（BF-SSD）算法，这是一种新颖的零阶优化方法，旨在减少这种计算负担。BF-SSD利用双保真框架，从计算成本低的低保真度（LF）和准确的高保真度（HF）功能评估的组合中构建代理模型。该代理模型促进了对步骤大小选择的高效回溯线搜索，为此我们在标准假设下提供了理论收敛保证。我们针对四个不同的问题对BF-SSD进行了全面的实证评估：合成优化基准、双重形式内核岭回归、对机器学习模型的黑匣子对抗攻击以及基于转换器的黑匣子语言模型微调。数值结果表明，与相关基线方法相比，BF-SSD始终实现了卓越的优化性能，同时需要的高频功能评估显着减少。这项研究强调了在零阶优化中集成双保真策略的功效，将BF-SSD定位为一种有前途且计算效率高的方法，用于解决各种现实世界应用中遇到的大规模、多维问题。



## **33. Can We Trust Embodied Agents? Exploring Backdoor Attacks against Embodied LLM-based Decision-Making Systems**

我们可以信任有保障的代理人吗？探索针对基于LLM的决策系统的后门攻击 cs.CR

Accepted paper at ICLR 2025, 31 pages, including main paper,  references, and appendix

**SubmitDate**: 2025-04-30    [abs](http://arxiv.org/abs/2405.20774v3) [paper-pdf](http://arxiv.org/pdf/2405.20774v3)

**Authors**: Ruochen Jiao, Shaoyuan Xie, Justin Yue, Takami Sato, Lixu Wang, Yixuan Wang, Qi Alfred Chen, Qi Zhu

**Abstract**: Large Language Models (LLMs) have shown significant promise in real-world decision-making tasks for embodied artificial intelligence, especially when fine-tuned to leverage their inherent common sense and reasoning abilities while being tailored to specific applications. However, this fine-tuning process introduces considerable safety and security vulnerabilities, especially in safety-critical cyber-physical systems. In this work, we propose the first comprehensive framework for Backdoor Attacks against LLM-based Decision-making systems (BALD) in embodied AI, systematically exploring the attack surfaces and trigger mechanisms. Specifically, we propose three distinct attack mechanisms: word injection, scenario manipulation, and knowledge injection, targeting various components in the LLM-based decision-making pipeline. We perform extensive experiments on representative LLMs (GPT-3.5, LLaMA2, PaLM2) in autonomous driving and home robot tasks, demonstrating the effectiveness and stealthiness of our backdoor triggers across various attack channels, with cases like vehicles accelerating toward obstacles and robots placing knives on beds. Our word and knowledge injection attacks achieve nearly 100% success rate across multiple models and datasets while requiring only limited access to the system. Our scenario manipulation attack yields success rates exceeding 65%, reaching up to 90%, and does not require any runtime system intrusion. We also assess the robustness of these attacks against defenses, revealing their resilience. Our findings highlight critical security vulnerabilities in embodied LLM systems and emphasize the urgent need for safeguarding these systems to mitigate potential risks.

摘要: 大型语言模型（LLM）在具体人工智能的现实决策任务中表现出了巨大的潜力，特别是在进行微调以利用其固有的常识和推理能力，同时针对特定应用进行定制时。然而，这种微调过程引入了相当多的安全和安保漏洞，特别是在安全关键的网络物理系统中。在这项工作中，我们提出了第一个针对嵌入式人工智能中基于LLM的决策系统（BALD）的后门攻击的全面框架，系统地探索了攻击表面和触发机制。具体来说，我们提出了三种不同的攻击机制：文字注入、场景操纵和知识注入，针对基于LLM的决策管道中的各个组件。我们对自动驾驶和家用机器人任务中的代表性LLM（GPT-3.5、LLaMA 2、PaLM 2）进行了广泛的实验，展示了我们的后门触发器在各种攻击渠道中的有效性和隐蔽性，例如车辆加速冲向障碍物和机器人将刀放在床上。我们的文字和知识注入攻击在多个模型和数据集中实现了近100%的成功率，同时只需要有限的系统访问权限。我们的场景操纵攻击的成功率超过65%，高达90%，并且不需要任何运行时系统入侵。我们还评估了这些针对防御系统的攻击的稳健性，揭示了它们的弹性。我们的研究结果强调了嵌入式LLM系统中的关键安全漏洞，并强调迫切需要保护这些系统以减轻潜在风险。



## **34. XBreaking: Explainable Artificial Intelligence for Jailbreaking LLMs**

XBreaking：用于越狱LLM的可解释人工智能 cs.CR

**SubmitDate**: 2025-04-30    [abs](http://arxiv.org/abs/2504.21700v1) [paper-pdf](http://arxiv.org/pdf/2504.21700v1)

**Authors**: Marco Arazzi, Vignesh Kumar Kembu, Antonino Nocera, Vinod P

**Abstract**: Large Language Models are fundamental actors in the modern IT landscape dominated by AI solutions. However, security threats associated with them might prevent their reliable adoption in critical application scenarios such as government organizations and medical institutions. For this reason, commercial LLMs typically undergo a sophisticated censoring mechanism to eliminate any harmful output they could possibly produce. In response to this, LLM Jailbreaking is a significant threat to such protections, and many previous approaches have already demonstrated its effectiveness across diverse domains. Existing jailbreak proposals mostly adopt a generate-and-test strategy to craft malicious input. To improve the comprehension of censoring mechanisms and design a targeted jailbreak attack, we propose an Explainable-AI solution that comparatively analyzes the behavior of censored and uncensored models to derive unique exploitable alignment patterns. Then, we propose XBreaking, a novel jailbreak attack that exploits these unique patterns to break the security constraints of LLMs by targeted noise injection. Our thorough experimental campaign returns important insights about the censoring mechanisms and demonstrates the effectiveness and performance of our attack.

摘要: 大型语言模型是由AI解决方案主导的现代IT环境中的基本角色。然而，与它们相关的安全威胁可能会阻止它们在关键应用场景（如政府组织和医疗机构）中的可靠采用。出于这个原因，商业LLM通常会经过复杂的审查机制，以消除它们可能产生的任何有害输出。针对这一点，LLM越狱是对这种保护的重大威胁，许多以前的方法已经在不同的领域证明了其有效性。现有的越狱提案大多采用生成和测试策略来制作恶意输入。为了提高对审查机制的理解并设计有针对性的越狱攻击，我们提出了一种解释性人工智能解决方案，该解决方案比较分析审查和未审查模型的行为，以推导出独特的可利用对齐模式。然后，我们提出了XBreaking，这是一种新型越狱攻击，它利用这些独特的模式通过有针对性的噪音注入来打破LLM的安全限制。我们彻底的实验活动返回了有关审查机制的重要见解，并展示了我们攻击的有效性和性能。



## **35. Hoist with His Own Petard: Inducing Guardrails to Facilitate Denial-of-Service Attacks on Retrieval-Augmented Generation of LLMs**

用自己的花瓣提升：引入护栏以促进对检索增强一代LLM的拒绝服务攻击 cs.CR

11 pages, 6 figures. This work will be submitted to the IEEE for  possible publication

**SubmitDate**: 2025-04-30    [abs](http://arxiv.org/abs/2504.21680v1) [paper-pdf](http://arxiv.org/pdf/2504.21680v1)

**Authors**: Pan Suo, Yu-Ming Shang, San-Chuan Guo, Xi Zhang

**Abstract**: Retrieval-Augmented Generation (RAG) integrates Large Language Models (LLMs) with external knowledge bases, improving output quality while introducing new security risks. Existing studies on RAG vulnerabilities typically focus on exploiting the retrieval mechanism to inject erroneous knowledge or malicious texts, inducing incorrect outputs. However, these approaches overlook critical weaknesses within LLMs, leaving important attack vectors unexplored and limiting the scope and efficiency of attacks. In this paper, we uncover a novel vulnerability: the safety guardrails of LLMs, while designed for protection, can also be exploited as an attack vector by adversaries. Building on this vulnerability, we propose MutedRAG, a novel denial-of-service attack that reversely leverages the guardrails of LLMs to undermine the availability of RAG systems. By injecting minimalistic jailbreak texts, such as "\textit{How to build a bomb}", into the knowledge base, MutedRAG intentionally triggers the LLM's safety guardrails, causing the system to reject legitimate queries. Besides, due to the high sensitivity of guardrails, a single jailbreak sample can affect multiple queries, effectively amplifying the efficiency of attacks while reducing their costs. Experimental results on three datasets demonstrate that MutedRAG achieves an attack success rate exceeding 60% in many scenarios, requiring only less than one malicious text to each target query on average. In addition, we evaluate potential defense strategies against MutedRAG, finding that some of current mechanisms are insufficient to mitigate this threat, underscoring the urgent need for more robust solutions.

摘要: 检索增强生成（RAG）将大型语言模型（LLM）与外部知识库集成，提高输出质量，同时引入新的安全风险。现有关于RAG漏洞的研究通常集中在利用检索机制注入错误知识或恶意文本，从而引发错误的输出。然而，这些方法忽视了LLM中的关键弱点，导致重要的攻击载体未被探索，并限制了攻击的范围和效率。在本文中，我们发现了一个新颖的漏洞：LLM的安全护栏虽然是为了保护而设计的，但也可能被对手用作攻击载体。在此漏洞的基础上，我们提出了MutedRAG，一种新型的拒绝服务攻击，它利用LLM的护栏来破坏RAG系统的可用性。通过向知识库中注入极简的越狱文本，例如“\textit{How to build a bomb}"，MutedRAG故意触发LLM的安全护栏，导致系统拒绝合法查询。此外，由于护栏的高度敏感性，单个越狱样本可以影响多个查询，有效地放大了攻击的效率，同时降低了攻击的成本。在三个数据集上的实验结果表明，MutedRAG在许多场景下实现了超过60%的攻击成功率，平均每个目标查询只需要不到一个恶意文本。此外，我们评估了针对MutedRAG的潜在防御策略，发现当前的一些机制不足以减轻这种威胁，这凸显了迫切需要更强大的解决方案。



## **36. Traceback of Poisoning Attacks to Retrieval-Augmented Generation**

中毒攻击追溯到检索增强一代 cs.CR

Accepted by The Web Conference 2025

**SubmitDate**: 2025-04-30    [abs](http://arxiv.org/abs/2504.21668v1) [paper-pdf](http://arxiv.org/pdf/2504.21668v1)

**Authors**: Baolei Zhang, Haoran Xin, Minghong Fang, Zhuqing Liu, Biao Yi, Tong Li, Zheli Liu

**Abstract**: Large language models (LLMs) integrated with retrieval-augmented generation (RAG) systems improve accuracy by leveraging external knowledge sources. However, recent research has revealed RAG's susceptibility to poisoning attacks, where the attacker injects poisoned texts into the knowledge database, leading to attacker-desired responses. Existing defenses, which predominantly focus on inference-time mitigation, have proven insufficient against sophisticated attacks. In this paper, we introduce RAGForensics, the first traceback system for RAG, designed to identify poisoned texts within the knowledge database that are responsible for the attacks. RAGForensics operates iteratively, first retrieving a subset of texts from the database and then utilizing a specially crafted prompt to guide an LLM in detecting potential poisoning texts. Empirical evaluations across multiple datasets demonstrate the effectiveness of RAGForensics against state-of-the-art poisoning attacks. This work pioneers the traceback of poisoned texts in RAG systems, providing a practical and promising defense mechanism to enhance their security.

摘要: 与检索增强生成（RAG）系统集成的大型语言模型（LLM）通过利用外部知识源来提高准确性。然而，最近的研究揭示了RAG对中毒攻击的敏感性，攻击者将中毒文本注入知识数据库，导致攻击者期望的响应。现有的防御主要集中在推理时间缓解上，已经证明不足以抵御复杂的攻击。在本文中，我们介绍RAGForensics，第一个追溯系统RAG，旨在确定有毒的文本知识数据库内的攻击负责。RAGForensics迭代操作，首先从数据库中检索文本子集，然后利用特制的提示来指导LLM检测潜在的中毒文本。多个数据集的经验评估证明了RAGForensics针对最先进的中毒攻击的有效性。这项工作开创了RAG系统中有毒文本的追溯，提供了一种实用且有前途的防御机制来增强其安全性。



## **37. Generative AI in Financial Institution: A Global Survey of Opportunities, Threats, and Regulation**

金融机构中的生成人工智能：机会、威胁和监管的全球调查 cs.CR

**SubmitDate**: 2025-04-30    [abs](http://arxiv.org/abs/2504.21574v1) [paper-pdf](http://arxiv.org/pdf/2504.21574v1)

**Authors**: Bikash Saha, Nanda Rani, Sandeep Kumar Shukla

**Abstract**: Generative Artificial Intelligence (GenAI) is rapidly reshaping the global financial landscape, offering unprecedented opportunities to enhance customer engagement, automate complex workflows, and extract actionable insights from vast financial data. This survey provides an overview of GenAI adoption across the financial ecosystem, examining how banks, insurers, asset managers, and fintech startups worldwide are integrating large language models and other generative tools into their operations. From AI-powered virtual assistants and personalized financial advisory to fraud detection and compliance automation, GenAI is driving innovation across functions. However, this transformation comes with significant cybersecurity and ethical risks. We discuss emerging threats such as AI-generated phishing, deepfake-enabled fraud, and adversarial attacks on AI systems, as well as concerns around bias, opacity, and data misuse. The evolving global regulatory landscape is explored in depth, including initiatives by major financial regulators and international efforts to develop risk-based AI governance. Finally, we propose best practices for secure and responsible adoption - including explainability techniques, adversarial testing, auditability, and human oversight. Drawing from academic literature, industry case studies, and policy frameworks, this chapter offers a perspective on how the financial sector can harness GenAI's transformative potential while navigating the complex risks it introduces.

摘要: 生成式人工智能（GenAI）正在迅速重塑全球金融格局，为增强客户参与度、自动化复杂的工作流程以及从大量金融数据中提取可操作的见解提供了前所未有的机会。该调查概述了整个金融生态系统中GenAI的采用情况，研究了全球银行，保险公司，资产管理公司和金融科技初创公司如何将大型语言模型和其他生成工具集成到其运营中。从人工智能驱动的虚拟助理和个性化财务咨询到欺诈检测和合规自动化，GenAI正在推动跨职能的创新。然而，这种转变伴随着重大的网络安全和道德风险。我们讨论了人工智能生成的网络钓鱼、深度伪造的欺诈和对人工智能系统的对抗攻击等新兴威胁，以及对偏见、不透明和数据滥用的担忧。深入探讨了不断变化的全球监管格局，包括主要金融监管机构的举措以及国际上发展基于风险的人工智能治理的努力。最后，我们提出了安全且负责任的采用的最佳实践-包括可解释性技术、对抗性测试、可互换性和人类监督。本章借鉴学术文献、行业案例研究和政策框架，提供了金融部门如何利用GenAI的变革潜力，同时应对其带来的复杂风险的视角。



## **38. Unlocking User-oriented Pages: Intention-driven Black-box Scanner for Real-world Web Applications**

解锁面向用户的页面：真实世界Web应用程序的意图驱动黑盒扫描器 cs.CR

**SubmitDate**: 2025-04-30    [abs](http://arxiv.org/abs/2504.20801v2) [paper-pdf](http://arxiv.org/pdf/2504.20801v2)

**Authors**: Weizhe Wang, Yao Zhang, Kaitai Liang, Guangquan Xu, Hongpeng Bai, Qingyang Yan, Xi Zheng, Bin Wu

**Abstract**: Black-box scanners have played a significant role in detecting vulnerabilities for web applications. A key focus in current black-box scanning is increasing test coverage (i.e., accessing more web pages). However, since many web applications are user-oriented, some deep pages can only be accessed through complex user interactions, which are difficult to reach by existing black-box scanners. To fill this gap, a key insight is that web pages contain a wealth of semantic information that can aid in understanding potential user intention. Based on this insight, we propose Hoyen, a black-box scanner that uses the Large Language Model to predict user intention and provide guidance for expanding the scanning scope. Hoyen has been rigorously evaluated on 12 popular open-source web applications and compared with 6 representative tools. The results demonstrate that Hoyen performs a comprehensive exploration of web applications, expanding the attack surface while achieving about 2x than the coverage of other scanners on average, with high request accuracy. Furthermore, Hoyen detected over 90% of its requests towards the core functionality of the application, detecting more vulnerabilities than other scanners, including unique vulnerabilities in well-known web applications. Our data/code is available at https://hoyen.tjunsl.com/

摘要: 黑匣子扫描仪在检测Web应用程序漏洞方面发挥了重要作用。当前黑匣子扫描的一个关键焦点是增加测试覆盖范围（即，访问更多网页）。然而，由于许多Web应用程序都是面向用户的，因此一些深度页面只能通过复杂的用户交互来访问，而现有的黑匣子扫描仪很难到达这些交互。为了填补这一空白，一个关键的见解是，网页包含了丰富的语义信息，可以帮助理解潜在的用户意图。基于这一见解，我们提出了Hoyen，一个黑盒扫描器，使用大语言模型来预测用户的意图，并为扩大扫描范围提供指导。Hoyen在12个流行的开源Web应用程序上进行了严格的评估，并与6个代表性工具进行了比较。结果表明，Hoyen对Web应用程序进行了全面的探索，扩大了攻击面，同时平均达到了其他扫描器的2倍覆盖率，具有很高的请求准确性。此外，Hoyen检测到了超过90%的针对应用程序核心功能的请求，比其他扫描仪检测到更多的漏洞，包括知名Web应用程序中的独特漏洞。我们的数据/代码可访问https://hoyen.tjunsl.com/



## **39. Round Trip Translation Defence against Large Language Model Jailbreaking Attacks**

针对大型语言模型越狱攻击的往返翻译防御 cs.CL

6 pages, 6 figures

**SubmitDate**: 2025-04-30    [abs](http://arxiv.org/abs/2402.13517v2) [paper-pdf](http://arxiv.org/pdf/2402.13517v2)

**Authors**: Canaan Yung, Hadi Mohaghegh Dolatabadi, Sarah Erfani, Christopher Leckie

**Abstract**: Large language models (LLMs) are susceptible to social-engineered attacks that are human-interpretable but require a high level of comprehension for LLMs to counteract. Existing defensive measures can only mitigate less than half of these attacks at most. To address this issue, we propose the Round Trip Translation (RTT) method, the first algorithm specifically designed to defend against social-engineered attacks on LLMs. RTT paraphrases the adversarial prompt and generalizes the idea conveyed, making it easier for LLMs to detect induced harmful behavior. This method is versatile, lightweight, and transferrable to different LLMs. Our defense successfully mitigated over 70% of Prompt Automatic Iterative Refinement (PAIR) attacks, which is currently the most effective defense to the best of our knowledge. We are also the first to attempt mitigating the MathsAttack and reduced its attack success rate by almost 40%. Our code is publicly available at https://github.com/Cancanxxx/Round_Trip_Translation_Defence   This version of the article has been accepted for publication, after peer review (when applicable) but is not the Version of Record and does not reflect post-acceptance improvements, or any corrections. The Version of Record is available online at: https://doi.org/10.48550/arXiv.2402.13517 Use of this Accepted Version is subject to the publisher's Accepted Manuscript terms of use https://www.springernature.com/gp/open-research/policies/accepted-manuscript-terms

摘要: 大型语言模型（LLM）很容易受到人类可解释的社会工程攻击，但LLM需要高水平的理解力才能对抗。现有的防御措施最多只能减轻不到一半的攻击。为了解决这个问题，我们提出了往返翻译（RTI）方法，这是第一个专门设计用于防御对LLM的社会工程攻击的算法。HRT解释了对抗提示并概括了所传达的想法，使LLM更容易检测诱导的有害行为。该方法通用、轻量级，并且可转移到不同的LLM。我们的防御成功缓解了超过70%的提示自动迭代细化（PAIR）攻击，据我们所知，这是目前最有效的防御。我们也是第一个尝试缓解MathsAttack的公司，并将其攻击成功率降低了近40%。我们的代码可在https://github.com/Cancanxxx/Round_Trip_Translation_Defence上公开获取   经过同行评审（如果适用）后，该版本的文章已被接受出版，但不是记录版本，并且不反映接受后的改进或任何更正。记录版本可在线获取：https://doi.org/10.48550/arXiv.2402.13517此接受版本的使用须遵守出版商的接受手稿使用条款https://www.springernature.com/gp/open-research/policies/accepted-manuscript-terms



## **40. CachePrune: Neural-Based Attribution Defense Against Indirect Prompt Injection Attacks**

Cache Prune：针对即时间接注入攻击的基于神经的归因防御 cs.CR

**SubmitDate**: 2025-04-29    [abs](http://arxiv.org/abs/2504.21228v1) [paper-pdf](http://arxiv.org/pdf/2504.21228v1)

**Authors**: Rui Wang, Junda Wu, Yu Xia, Tong Yu, Ruiyi Zhang, Ryan Rossi, Lina Yao, Julian McAuley

**Abstract**: Large Language Models (LLMs) are identified as being susceptible to indirect prompt injection attack, where the model undesirably deviates from user-provided instructions by executing tasks injected in the prompt context. This vulnerability stems from LLMs' inability to distinguish between data and instructions within a prompt. In this paper, we propose CachePrune that defends against this attack by identifying and pruning task-triggering neurons from the KV cache of the input prompt context. By pruning such neurons, we encourage the LLM to treat the text spans of input prompt context as only pure data, instead of any indicator of instruction following. These neurons are identified via feature attribution with a loss function induced from an upperbound of the Direct Preference Optimization (DPO) objective. We show that such a loss function enables effective feature attribution with only a few samples. We further improve on the quality of feature attribution, by exploiting an observed triggering effect in instruction following. Our approach does not impose any formatting on the original prompt or introduce extra test-time LLM calls. Experiments show that CachePrune significantly reduces attack success rates without compromising the response quality. Note: This paper aims to defend against indirect prompt injection attacks, with the goal of developing more secure and robust AI systems.

摘要: 大型语言模型（LLM）被认为容易受到间接提示注入攻击，其中模型通过执行在提示上下文中注入的任务而不希望地偏离用户提供的指令。此漏洞源于LLM无法区分提示内的数据和指令。本文中，我们提出了Cache Prune，它通过从输入提示上下文的KV缓存中识别和修剪任务触发神经元来抵御这种攻击。通过修剪此类神经元，我们鼓励LLM将输入提示上下文的文本跨度仅视为纯数据，而不是任何指令遵循的指示符。这些神经元是通过特征属性识别的，该特征属性具有直接偏好优化（DPO）目标的上界诱导的损失函数。我们表明，这样的损失函数只需少量样本即可实现有效的特征归因。我们通过利用在指令遵循中观察到的触发效应，进一步提高了特征归因的质量。我们的方法不会对原始提示强加任何格式，也不会引入额外的测试时LLM调用。实验表明，Cache Prune显着降低了攻击成功率，而不会影响响应质量。注：本文旨在防御间接即时注入攻击，目标是开发更安全、更强大的人工智能系统。



## **41. Chain-of-Defensive-Thought: Structured Reasoning Elicits Robustness in Large Language Models against Reference Corruption**

防御思想链：结构化推理在大型语言模型中针对引用腐败的鲁棒性 cs.CL

**SubmitDate**: 2025-04-29    [abs](http://arxiv.org/abs/2504.20769v1) [paper-pdf](http://arxiv.org/pdf/2504.20769v1)

**Authors**: Wenxiao Wang, Parsa Hosseini, Soheil Feizi

**Abstract**: Chain-of-thought prompting has demonstrated great success in facilitating the reasoning abilities of large language models. In this work, we explore how these enhanced reasoning abilities can be exploited to improve the robustness of large language models in tasks that are not necessarily reasoning-focused. In particular, we show how a wide range of large language models exhibit significantly improved robustness against reference corruption using a simple method called chain-of-defensive-thought, where only a few exemplars with structured and defensive reasoning are provided as demonstrations. Empirically, the improvements can be astounding, especially given the simplicity and applicability of the method. For example, in the Natural Questions task, the accuracy of GPT-4o degrades from 60% to as low as 3% with standard prompting when 1 out of 10 references provided is corrupted with prompt injection attacks. In contrast, GPT-4o using chain-of-defensive-thought prompting maintains an accuracy of 50%.

摘要: 思想链提示在促进大型语言模型的推理能力方面取得了巨大成功。在这项工作中，我们探索如何利用这些增强的推理能力来提高大型语言模型在不一定以推理为重点的任务中的稳健性。特别是，我们展示了广泛的大型语言模型如何使用一种称为防御思想链的简单方法来显着提高针对引用腐败的鲁棒性，其中仅提供了一些具有结构化和防御推理的示例作为演示。从经验上看，这些改进可能令人震惊，特别是考虑到该方法的简单性和适用性。例如，在自然问题任务中，当提供的十分之一的参考文献因提示注入攻击而损坏时，GPT-4 o的准确性会从60%下降到标准提示的3%。相比之下，使用防御思想链提示的GPT-4 o保持50%的准确率。



## **42. ReCIT: Reconstructing Full Private Data from Gradient in Parameter-Efficient Fine-Tuning of Large Language Models**

ReCIT：在大型语言模型的参数有效微调中从梯度重建完整的私有数据 cs.CR

**SubmitDate**: 2025-04-29    [abs](http://arxiv.org/abs/2504.20570v1) [paper-pdf](http://arxiv.org/pdf/2504.20570v1)

**Authors**: Jin Xie, Ruishi He, Songze Li, Xiaojun Jia, Shouling Ji

**Abstract**: Parameter-efficient fine-tuning (PEFT) has emerged as a practical solution for adapting large language models (LLMs) to custom datasets with significantly reduced computational cost. When carrying out PEFT under collaborative learning scenarios (e.g., federated learning), it is often required to exchange model updates (or gradients) across parties. These gradients, even with limited dimensions, can cause severe breach of data privacy. Recent works have shown that both contextual prefixes and personally identifiable information (PII) can be exposed through gradients. However, \emph{simultaneously} and \emph{accurately} recovering both components from the same training instance remains infeasible due to the following challenges: 1) limited number of PEFT parameters; 2) high-dimensional token spaces; and 3) large batch sizes. We propose ReCIT, a novel privacy attack that addresses all challenges, and achieves recovery of \emph{full} private data from PEFT gradients with high fidelity. Specifically, ReCIT proposes to enhance the memorization capability of the pre-trained model through malicious fine-tuning with Personal Notes; ReCIT also proposes a novel filter-based token extraction technique and a token pairing mechanism, to accurately reconstruct tokens from the training sequences with large batch sizes. Extensive evaluations show that ReCIT consistently outperforms state-of-the-art gradient inversion and memorization-based attacks across different PEFT paradigms. It achieves up to 10$\times$ higher PII recovery rates and remains effective across varying batch sizes, especially in settings where prefix reconstruction is intractable for conventional approaches. These findings highlight an urgent need to reassess the privacy guarantees of PEFT, especially in decentralized or shared training environments.

摘要: 参数高效微调（PEFT）已成为一种将大型语言模型（LLM）适应自定义数据集的实用解决方案，并显着降低计算成本。当在协作学习场景下执行PEFT时（例如，联邦学习），通常需要跨各方交换模型更新（或梯度）。这些梯度，即使维度有限，也可能导致数据隐私的严重侵犯。最近的工作表明，上下文前置码和个人可识别信息（PRI）都可以通过梯度暴露。然而，由于存在以下挑战，\{同时}和\{准确地}从同一训练实例恢复两个组件仍然不可行：1）PEFT参数数量有限; 2）多维令牌空间; 3）批量大小较大。我们提出了ReCIT，这是一种新型隐私攻击，可以解决所有挑战，并以高保真度实现从PEFT梯度恢复\{full}私人数据。具体来说，ReCIT提出通过使用Personal note的恶意微调来增强预训练模型的记忆能力; ReCIT还提出了一种新型的基于过滤器的令牌提取技术和令牌配对机制，以准确地从大批量的训练序列中重建令牌。广泛的评估表明，ReCIT在不同的PEFT范例中始终优于最先进的梯度反转和基于记忆的攻击。它实现了高达10\times $的PII恢复率，并在不同的批量大小中保持有效，特别是在前缀重建对于传统方法来说很难处理的设置中。这些发现强调了重新评估PEFT隐私保障的迫切需要，特别是在分散或共享的培训环境中。



## **43. Token-Efficient Prompt Injection Attack: Provoking Cessation in LLM Reasoning via Adaptive Token Compression**

令牌高效提示注入攻击：通过自适应令牌压缩引发LLM推理中的停止 cs.CR

**SubmitDate**: 2025-04-29    [abs](http://arxiv.org/abs/2504.20493v1) [paper-pdf](http://arxiv.org/pdf/2504.20493v1)

**Authors**: Yu Cui, Yujun Cai, Yiwei Wang

**Abstract**: While reasoning large language models (LLMs) demonstrate remarkable performance across various tasks, they also contain notable security vulnerabilities. Recent research has uncovered a "thinking-stopped" vulnerability in DeepSeek-R1, where model-generated reasoning tokens can forcibly interrupt the inference process, resulting in empty responses that compromise LLM-integrated applications. However, existing methods triggering this vulnerability require complex mathematical word problems with long prompts--even exceeding 5,000 tokens. To reduce the token cost and formally define this vulnerability, we propose a novel prompt injection attack named "Reasoning Interruption Attack", based on adaptive token compression. We demonstrate that simple standalone arithmetic tasks can effectively trigger this vulnerability, and the prompts based on such tasks exhibit simpler logical structures than mathematical word problems. We develop a systematic approach to efficiently collect attack prompts and an adaptive token compression framework that utilizes LLMs to automatically compress these prompts. Experiments show our compression framework significantly reduces prompt length while maintaining effective attack capabilities. We further investigate the attack's performance via output prefix and analyze the underlying causes of the vulnerability, providing valuable insights for improving security in reasoning LLMs.

摘要: 虽然推理大型语言模型（LLM）在各种任务中表现出卓越的性能，但它们也包含显着的安全漏洞。最近的研究发现了DeepSeek-R1中的一个“思维停止”漏洞，模型生成的推理令牌可以强行中断推理过程，导致空响应，从而危及LLM集成应用程序。然而，触发此漏洞的现有方法需要复杂的数学单词问题和长提示-甚至超过5，000个令牌。为了减少令牌成本和正式定义这个漏洞，我们提出了一种新的提示注入攻击命名为“推理中断攻击”，基于自适应令牌压缩。我们证明，简单的独立算术任务可以有效地触发此漏洞，并且基于此类任务的提示表现出比数学单词问题更简单的逻辑结构。我们开发了一种有效收集攻击提示的系统方法，以及一个利用LLM自动压缩这些提示的自适应令牌压缩框架。实验表明，我们的压缩框架显着减少了提示长度，同时保持了有效的攻击能力。我们通过输出前置进一步调查攻击的性能并分析漏洞的根本原因，为提高推理LLM的安全性提供有价值的见解。



## **44. Robustness via Referencing: Defending against Prompt Injection Attacks by Referencing the Executed Instruction**

通过引用实现鲁棒性：通过引用执行的指令来防御提示注入攻击 cs.CR

**SubmitDate**: 2025-04-29    [abs](http://arxiv.org/abs/2504.20472v1) [paper-pdf](http://arxiv.org/pdf/2504.20472v1)

**Authors**: Yulin Chen, Haoran Li, Yuan Sui, Yue Liu, Yufei He, Yangqiu Song, Bryan Hooi

**Abstract**: Large language models (LLMs) have demonstrated impressive performance and have come to dominate the field of natural language processing (NLP) across various tasks. However, due to their strong instruction-following capabilities and inability to distinguish between instructions and data content, LLMs are vulnerable to prompt injection attacks. These attacks manipulate LLMs into deviating from the original input instructions and executing maliciously injected instructions within data content, such as web documents retrieved from search engines. Existing defense methods, including prompt-engineering and fine-tuning approaches, typically instruct models to follow the original input instructions while suppressing their tendencies to execute injected instructions. However, our experiments reveal that suppressing instruction-following tendencies is challenging. Through analyzing failure cases, we observe that although LLMs tend to respond to any recognized instructions, they are aware of which specific instructions they are executing and can correctly reference them within the original prompt. Motivated by these findings, we propose a novel defense method that leverages, rather than suppresses, the instruction-following abilities of LLMs. Our approach prompts LLMs to generate responses that include both answers and their corresponding instruction references. Based on these references, we filter out answers not associated with the original input instructions. Comprehensive experiments demonstrate that our method outperforms prompt-engineering baselines and achieves performance comparable to fine-tuning methods, reducing the attack success rate (ASR) to 0 percent in some scenarios. Moreover, our approach has minimal impact on overall utility.

摘要: 大型语言模型（LLM）表现出令人印象深刻的性能，并在各种任务中占据了自然语言处理（NLP）领域的主导地位。然而，由于LLM强大的描述跟踪能力并且无法区分指令和数据内容，因此很容易受到提示注入攻击。这些攻击操纵LLM偏离原始输入指令，并执行数据内容（例如从搜索引擎检索到的网络文档）中恶意注入的指令。现有的防御方法，包括预算工程和微调方法，通常指示模型遵循原始输入指令，同时抑制它们执行注入指令的倾向。然而，我们的实验表明，抑制顺从倾向是具有挑战性的。通过分析失败案例，我们观察到，尽管LLM倾向于响应任何识别的指令，但它们知道自己正在执行哪些特定指令，并且可以在原始提示内正确引用它们。受这些发现的激励，我们提出了一种新型的防御方法，该方法利用而不是抑制LLM的描述跟随能力。我们的方法促使LLM生成包括答案及其相应的指令参考的响应。根据这些参考，我们过滤掉与原始输入指令无关的答案。全面的实验表明，我们的方法优于预算工程基线，并实现了与微调方法相当的性能，在某些情况下将攻击成功率（ASB）降低至0%。此外，我们的方法对整体效用的影响极小。



## **45. NeuRel-Attack: Neuron Relearning for Safety Disalignment in Large Language Models**

NeuRel-Attack：大型语言模型中安全失准的神经元再学习 cs.LG

**SubmitDate**: 2025-04-29    [abs](http://arxiv.org/abs/2504.21053v1) [paper-pdf](http://arxiv.org/pdf/2504.21053v1)

**Authors**: Yi Zhou, Wenpeng Xing, Dezhang Kong, Changting Lin, Meng Han

**Abstract**: Safety alignment in large language models (LLMs) is achieved through fine-tuning mechanisms that regulate neuron activations to suppress harmful content. In this work, we propose a novel approach to induce disalignment by identifying and modifying the neurons responsible for safety constraints. Our method consists of three key steps: Neuron Activation Analysis, where we examine activation patterns in response to harmful and harmless prompts to detect neurons that are critical for distinguishing between harmful and harmless inputs; Similarity-Based Neuron Identification, which systematically locates the neurons responsible for safe alignment; and Neuron Relearning for Safety Removal, where we fine-tune these selected neurons to restore the model's ability to generate previously restricted responses. Experimental results demonstrate that our method effectively removes safety constraints with minimal fine-tuning, highlighting a critical vulnerability in current alignment techniques. Our findings underscore the need for robust defenses against adversarial fine-tuning attacks on LLMs.

摘要: 大型语言模型（LLM）中的安全对齐是通过调节神经元激活以抑制有害内容的微调机制来实现的。在这项工作中，我们提出了一种新的方法，通过识别和修改负责安全约束的神经元来诱导失调。我们的方法包括三个关键步骤：神经元激活分析，在那里我们检查响应有害和无害提示的激活模式，以检测对区分有害和无害输入至关重要的神经元;基于相似性的神经元识别，系统地定位负责安全对齐的神经元;和Neuron Relearning for Safety Removal，我们对这些选定的神经元进行微调，以恢复模型生成先前受限响应的能力。实验结果表明，我们的方法可以通过最少的微调有效地消除安全约束，凸显了当前对齐技术中的一个关键漏洞。我们的研究结果强调了对LLM的对抗性微调攻击的强大防御的必要性。



## **46. Enhancing Leakage Attacks on Searchable Symmetric Encryption Using LLM-Based Synthetic Data Generation**

使用基于LLM的合成数据生成增强对可搜索对称加密的泄漏攻击 cs.CR

**SubmitDate**: 2025-04-29    [abs](http://arxiv.org/abs/2504.20414v1) [paper-pdf](http://arxiv.org/pdf/2504.20414v1)

**Authors**: Joshua Chiu, Partha Protim Paul, Zahin Wahab

**Abstract**: Searchable Symmetric Encryption (SSE) enables efficient search capabilities over encrypted data, allowing users to maintain privacy while utilizing cloud storage. However, SSE schemes are vulnerable to leakage attacks that exploit access patterns, search frequency, and volume information. Existing studies frequently assume that adversaries possess a substantial fraction of the encrypted dataset to mount effective inference attacks, implying there is a database leakage of such documents, thus, an assumption that may not hold in real-world scenarios. In this work, we investigate the feasibility of enhancing leakage attacks under a more realistic threat model in which adversaries have access to minimal leaked data. We propose a novel approach that leverages large language models (LLMs), specifically GPT-4 variants, to generate synthetic documents that statistically and semantically resemble the real-world dataset of Enron emails. Using the email corpus as a case study, we evaluate the effectiveness of synthetic data generated via random sampling and hierarchical clustering methods on the performance of the SAP (Search Access Pattern) keyword inference attack restricted to token volumes only. Our results demonstrate that, while the choice of LLM has limited effect, increasing dataset size and employing clustering-based generation significantly improve attack accuracy, achieving comparable performance to attacks using larger amounts of real data. We highlight the growing relevance of LLMs in adversarial contexts.

摘要: 可搜索对称加密（SSE）支持对加密数据进行高效搜索，使用户能够在利用云存储的同时维护隐私。然而，SSE方案很容易受到利用访问模式、搜索频率和量信息的泄露攻击。现有的研究经常假设对手拥有很大一部分加密数据集来发起有效的推理攻击，这意味着此类文档的数据库泄露，因此，这一假设在现实世界的场景中可能不成立。在这项工作中，我们研究了在更现实的威胁模型下增强泄露攻击的可行性，其中对手可以访问最少的泄露数据。我们提出了一种新颖的方法，利用大型语言模型（LLM），特别是GPT-4变体，来生成在统计和语义上与安然电子邮件的现实世界数据集相似的合成文档。使用电子邮件库作为案例研究，我们评估了通过随机抽样和分层集群方法生成的合成数据对仅限于令牌量的SAP（搜索访问模式）关键字推理攻击性能的有效性。我们的结果表明，虽然选择LLM的效果有限，但增加数据集大小和采用基于集群的生成可以显着提高攻击准确性，实现与使用大量真实数据的攻击相当的性能。我们强调法学硕士在对抗背景下日益增长的相关性。



## **47. The Automation Advantage in AI Red Teaming**

人工智能红色团队的自动化优势 cs.CR

15 pages, 6 figures

**SubmitDate**: 2025-04-29    [abs](http://arxiv.org/abs/2504.19855v2) [paper-pdf](http://arxiv.org/pdf/2504.19855v2)

**Authors**: Rob Mulla, Ads Dawson, Vincent Abruzzon, Brian Greunke, Nick Landers, Brad Palm, Will Pearce

**Abstract**: This paper analyzes Large Language Model (LLM) security vulnerabilities based on data from Crucible, encompassing 214,271 attack attempts by 1,674 users across 30 LLM challenges. Our findings reveal automated approaches significantly outperform manual techniques (69.5% vs 47.6% success rate), despite only 5.2% of users employing automation. We demonstrate that automated approaches excel in systematic exploration and pattern matching challenges, while manual approaches retain speed advantages in certain creative reasoning scenarios, often solving problems 5x faster when successful. Challenge categories requiring systematic exploration are most effectively targeted through automation, while intuitive challenges sometimes favor manual techniques for time-to-solve metrics. These results illuminate how algorithmic testing is transforming AI red-teaming practices, with implications for both offensive security research and defensive measures. Our analysis suggests optimal security testing combines human creativity for strategy development with programmatic execution for thorough exploration.

摘要: 本文基于Crucible的数据分析了大型语言模型（LLM）安全漏洞，涵盖1，674名用户在30个LLM挑战中进行的214，271次攻击尝试。我们的研究结果显示，尽管只有5.2%的用户采用自动化，但自动化方法的表现显着优于手动技术（成功率为69.5% vs 47.6%）。我们证明，自动化方法在系统探索和模式匹配挑战中表现出色，而手动方法在某些创造性推理场景中保留了速度优势，成功后解决问题的速度通常要快5倍。需要系统探索的挑战类别通过自动化最有效地针对，而直观的挑战有时更喜欢手动技术来衡量解决时间指标。这些结果阐明了算法测试如何改变人工智能红团队实践，并对进攻性安全研究和防御措施产生了影响。我们的分析表明，最佳安全测试将人类战略开发的创造力与彻底探索的程序执行相结合。



## **48. BadMoE: Backdooring Mixture-of-Experts LLMs via Optimizing Routing Triggers and Infecting Dormant Experts**

BadMoE：通过优化路由触发器和感染休眠专家来为混合专家LLM做后门 cs.CR

**SubmitDate**: 2025-04-29    [abs](http://arxiv.org/abs/2504.18598v2) [paper-pdf](http://arxiv.org/pdf/2504.18598v2)

**Authors**: Qingyue Wang, Qi Pang, Xixun Lin, Shuai Wang, Daoyuan Wu

**Abstract**: Mixture-of-Experts (MoE) have emerged as a powerful architecture for large language models (LLMs), enabling efficient scaling of model capacity while maintaining manageable computational costs. The key advantage lies in their ability to route different tokens to different ``expert'' networks within the model, enabling specialization and efficient handling of diverse input. However, the vulnerabilities of MoE-based LLMs still have barely been studied, and the potential for backdoor attacks in this context remains largely unexplored. This paper presents the first backdoor attack against MoE-based LLMs where the attackers poison ``dormant experts'' (i.e., underutilized experts) and activate them by optimizing routing triggers, thereby gaining control over the model's output. We first rigorously prove the existence of a few ``dominating experts'' in MoE models, whose outputs can determine the overall MoE's output. We also show that dormant experts can serve as dominating experts to manipulate model predictions. Accordingly, our attack, namely BadMoE, exploits the unique architecture of MoE models by 1) identifying dormant experts unrelated to the target task, 2) constructing a routing-aware loss to optimize the activation triggers of these experts, and 3) promoting dormant experts to dominating roles via poisoned training data. Extensive experiments show that BadMoE successfully enforces malicious prediction on attackers' target tasks while preserving overall model utility, making it a more potent and stealthy attack than existing methods.

摘要: 混合专家（Mixture-of-Experts，MoE）已经成为大型语言模型（LLM）的一个强大架构，能够有效扩展模型容量，同时保持可管理的计算成本。关键优势在于它们能够将不同的令牌路由到模型中的不同“专家”网络，从而实现专业化和有效处理不同的输入。然而，基于MoE的LLM的漏洞仍然很少被研究，在这种情况下后门攻击的可能性在很大程度上仍未被探索。本文介绍了第一个针对基于MoE的LLM的后门攻击，其中攻击者毒害"休眠专家“（即，未充分利用的专家）并通过优化路由触发器激活他们，从而获得对模型输出的控制。我们首先严格证明了存在一些"主导专家“的MoE模型，其输出可以确定整体MoE的输出。我们还表明，休眠专家可以充当主导专家来操纵模型预测。因此，我们的攻击（即BadMoE）利用MoE模型的独特架构，方法是1）识别与目标任务无关的休眠专家，2）构建路由感知损失来优化这些专家的激活触发器，以及3）通过有毒的训练数据将休眠专家提升为主导角色。大量实验表明，BadMoE成功地对攻击者的目标任务实施恶意预测，同时保留了整体模型效用，使其成为比现有方法更强大、更隐蔽的攻击。



## **49. Leveraging LLM to Strengthen ML-Based Cross-Site Scripting Detection**

利用LLM加强基于ML的跨站点脚本检测 cs.CR

This work has been accepted for presentation at the ACM Workshop on  Wireless Security and Machine Learning (WiseML 2025)

**SubmitDate**: 2025-04-28    [abs](http://arxiv.org/abs/2504.21045v1) [paper-pdf](http://arxiv.org/pdf/2504.21045v1)

**Authors**: Dennis Miczek, Divyesh Gabbireddy, Suman Saha

**Abstract**: According to the Open Web Application Security Project (OWASP), Cross-Site Scripting (XSS) is a critical security vulnerability. Despite decades of research, XSS remains among the top 10 security vulnerabilities. Researchers have proposed various techniques to protect systems from XSS attacks, with machine learning (ML) being one of the most widely used methods. An ML model is trained on a dataset to identify potential XSS threats, making its effectiveness highly dependent on the size and diversity of the training data. A variation of XSS is obfuscated XSS, where attackers apply obfuscation techniques to alter the code's structure, making it challenging for security systems to detect its malicious intent. Our study's random forest model was trained on traditional (non-obfuscated) XSS data achieved 99.8% accuracy. However, when tested against obfuscated XSS samples, accuracy dropped to 81.9%, underscoring the importance of training ML models with obfuscated data to improve their effectiveness in detecting XSS attacks. A significant challenge is to generate highly complex obfuscated code despite the availability of several public tools. These tools can only produce obfuscation up to certain levels of complexity.   In our proposed system, we fine-tune a Large Language Model (LLM) to generate complex obfuscated XSS payloads automatically. By transforming original XSS samples into diverse obfuscated variants, we create challenging training data for ML model evaluation. Our approach achieved a 99.5% accuracy rate with the obfuscated dataset. We also found that the obfuscated samples generated by the LLMs were 28.1% more complex than those created by other tools, significantly improving the model's ability to handle advanced XSS attacks and making it more effective for real-world application security.

摘要: 根据开放Web应用程序安全项目（OWISP），跨站点脚本（XSS）是一个严重的安全漏洞。尽管经过数十年的研究，XSS仍然是十大安全漏洞之一。研究人员提出了各种技术来保护系统免受XSS攻击，其中机器学习（ML）是最广泛使用的方法之一。ML模型在数据集上训练以识别潜在的XSS威胁，使其有效性高度依赖于训练数据的大小和多样性。XSS的一种变体是混淆XSS，攻击者应用混淆技术来改变代码的结构，这使得安全系统难以检测其恶意意图。我们研究的随机森林模型是在传统（非混淆）XSS数据上训练的，达到了99.8%的准确率。然而，当针对模糊的XSS样本进行测试时，准确率下降至81.9%，这凸显了使用模糊数据训练ML模型以提高其检测XSS攻击的有效性的重要性。一个重大挑战是，尽管有多种公共工具可用，但仍要生成高度复杂的混淆代码。这些工具只能产生达到一定复杂程度的混淆。   在我们提出的系统中，我们对大型语言模型（LLM）进行微调，以自动生成复杂的混淆XSS负载。通过将原始XSS样本转换为各种混淆变体，我们为ML模型评估创建具有挑战性的训练数据。我们的方法在模糊数据集中实现了99.5%的准确率。我们还发现，LLM生成的混淆样本比其他工具创建的样本复杂28.1%，显著提高了模型处理高级XSS攻击的能力，并使其对现实世界的应用程序安全更有效。



## **50. Exploring the Role of Large Language Models in Cybersecurity: A Systematic Survey**

探索大型语言模型在网络安全中的作用：系统性调查 cs.CR

20 pages, 3 figures

**SubmitDate**: 2025-04-28    [abs](http://arxiv.org/abs/2504.15622v2) [paper-pdf](http://arxiv.org/pdf/2504.15622v2)

**Authors**: Shuang Tian, Tao Zhang, Jiqiang Liu, Jiacheng Wang, Xuangou Wu, Xiaoqiang Zhu, Ruichen Zhang, Weiting Zhang, Zhenhui Yuan, Shiwen Mao, Dong In Kim

**Abstract**: With the rapid development of technology and the acceleration of digitalisation, the frequency and complexity of cyber security threats are increasing. Traditional cybersecurity approaches, often based on static rules and predefined scenarios, are struggling to adapt to the rapidly evolving nature of modern cyberattacks. There is an urgent need for more adaptive and intelligent defence strategies. The emergence of Large Language Model (LLM) provides an innovative solution to cope with the increasingly severe cyber threats, and its potential in analysing complex attack patterns, predicting threats and assisting real-time response has attracted a lot of attention in the field of cybersecurity, and exploring how to effectively use LLM to defend against cyberattacks has become a hot topic in the current research field. This survey examines the applications of LLM from the perspective of the cyber attack lifecycle, focusing on the three phases of defense reconnaissance, foothold establishment, and lateral movement, and it analyzes the potential of LLMs in Cyber Threat Intelligence (CTI) tasks. Meanwhile, we investigate how LLM-based security solutions are deployed and applied in different network scenarios. It also summarizes the internal and external risk issues faced by LLM during its application. Finally, this survey also points out the facing risk issues and possible future research directions in this domain.

摘要: 随着技术的快速发展和数字化进程的加快，网络安全威胁的频率和复杂性不断增加。传统的网络安全方法通常基于静态规则和预定义的场景，正在努力适应现代网络攻击快速变化的性质。迫切需要更具适应性和智能性的防御策略。大型语言模型（LLM）的出现为应对日益严重的网络威胁提供了创新解决方案，其在分析复杂攻击模式、预测威胁和辅助实时响应方面的潜力引起了网络安全领域的广泛关注，探索如何有效利用LLM防御网络攻击已成为当前研究领域的热门话题。本次调查从网络攻击生命周期的角度审视了LLM的应用，重点关注防御侦察、立足点建立和侧向移动三个阶段，并分析了LLM在网络威胁情报（RTI）任务中的潜力。同时，我们研究了基于LLM的安全解决方案如何在不同的网络场景中部署和应用。还总结了LLM在应用过程中面临的内部和外部风险问题。最后，本次调查还指出了该领域面临的风险问题以及未来可能的研究方向。



