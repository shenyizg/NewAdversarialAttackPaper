# Latest Large Language Model Attack Papers
**update at 2025-10-16 16:20:55**

翻译来自 https://cloud.tencent.com/document/product/551/15619

## **1. LLM-Enabled In-Context Learning for Data Collection Scheduling in UAV-assisted Sensor Networks**

基于LLM的在上下文中学习，用于无人机辅助传感器网络中的数据收集调度 cs.AI

**SubmitDate**: 2025-10-15    [abs](http://arxiv.org/abs/2504.14556v2) [paper-pdf](http://arxiv.org/pdf/2504.14556v2)

**Authors**: Yousef Emami, Hao Zhou, SeyedSina Nabavirazani, Luis Almeida

**Abstract**: Unmanned Aerial Vehicles (UAVs) are increasingly being utilized in various private and commercial applications, e.g., traffic control, parcel delivery, and Search and Rescue (SAR) missions. Machine Learning (ML) methods used in UAV-Assisted Sensor Networks (UASNETs) and, especially, in Deep Reinforcement Learning (DRL) face challenges such as complex and lengthy model training, gaps between simulation and reality, and low sampling efficiency, which conflict with the urgency of emergencies, such as SAR missions. In this paper, an In-Context Learning (ICL)-Data Collection Scheduling (ICLDC) system is proposed as an alternative to DRL in emergencies. The UAV collects sensory data and transmits it to a Large Language Model (LLM), which creates a task description in natural language. From this description, the UAV receives a data collection schedule that must be executed. A verifier ensures safe UAV operations by evaluating the schedules generated by the LLM and overriding unsafe schedules based on predefined rules. The system continuously adapts by incorporating feedback into the task descriptions and using this for future decisions. This method is tested against jailbreaking attacks, where the task description is manipulated to undermine network performance, highlighting the vulnerability of LLMs to such attacks. The proposed ICLDC significantly reduces cumulative packet loss compared to both the DQN and Maximum Channel Gain baselines. ICLDC presents a promising direction for intelligent scheduling and control in UASNETs.

摘要: 无人驾驶飞行器（UF）越来越多地用于各种私人和商业应用，例如交通管制、包裹递送和搜救（SAR）任务。无人机辅助传感器网络（UASNET），特别是深度强化学习（DRL）中使用的机器学习（ML）方法面临着复杂且冗长的模型训练、模拟与现实之间的差距以及低采样效率等挑战，这些挑战与紧急情况的紧迫性相冲突，例如SAR任务。本文提出了一种上下文学习（ICL）-数据收集调度（ICLDC）系统作为紧急情况下DRL的替代方案。无人机收集传感数据并将其传输到大型语言模型（LLM），该模型以自然语言创建任务描述。根据此描述，无人机收到必须执行的数据收集计划。验证器通过评估LLM生成的计划并根据预定义的规则覆盖不安全的计划来确保无人机操作的安全。该系统通过将反馈纳入任务描述并将其用于未来的决策来不断进行调整。该方法经过针对越狱攻击的测试，其中任务描述被操纵以破坏网络性能，凸显了LLM对此类攻击的脆弱性。与DQN和最大通道收益基线相比，拟议的ICLDC显着减少了累积数据包丢失。ICLDC为UASNET的智能调度和控制提供了一个有前途的方向。



## **2. In-Browser LLM-Guided Fuzzing for Real-Time Prompt Injection Testing in Agentic AI Browsers**

浏览器内LLM引导的模糊处理，用于在大型AI浏览器中进行实时提示注入测试 cs.CR

37 pages , 10 figures

**SubmitDate**: 2025-10-15    [abs](http://arxiv.org/abs/2510.13543v1) [paper-pdf](http://arxiv.org/pdf/2510.13543v1)

**Authors**: Avihay Cohen

**Abstract**: Large Language Model (LLM) based agents integrated into web browsers (often called agentic AI browsers) offer powerful automation of web tasks. However, they are vulnerable to indirect prompt injection attacks, where malicious instructions hidden in a webpage deceive the agent into unwanted actions. These attacks can bypass traditional web security boundaries, as the AI agent operates with the user privileges across sites. In this paper, we present a novel fuzzing framework that runs entirely in the browser and is guided by an LLM to automatically discover such prompt injection vulnerabilities in real time.

摘要: 集成到Web浏览器（通常称为代理AI浏览器）中的基于大型语言模型（LLM）的代理提供了强大的Web任务自动化。然而，它们很容易受到间接提示注入攻击，即隐藏在网页中的恶意指令欺骗代理采取不必要的操作。这些攻击可以绕过传统的网络安全边界，因为AI代理以跨网站的用户特权运行。在本文中，我们提出了一种新颖的模糊框架，该框架完全在浏览器中运行，并在LLM的指导下自动实时发现此类提示注入漏洞。



## **3. Who Speaks for the Trigger? Dynamic Expert Routing in Backdoored Mixture-of-Experts Transformers**

谁为触发器说话？后台混合专家变形器中的动态专家路由 cs.CR

**SubmitDate**: 2025-10-15    [abs](http://arxiv.org/abs/2510.13462v1) [paper-pdf](http://arxiv.org/pdf/2510.13462v1)

**Authors**: Xin Zhao, Xiaojun Chen, Bingshan Liu, Haoyu Gao, Zhendong Zhao, Yilong Chen

**Abstract**: Large language models (LLMs) with Mixture-of-Experts (MoE) architectures achieve impressive performance and efficiency by dynamically routing inputs to specialized subnetworks, known as experts. However, this sparse routing mechanism inherently exhibits task preferences due to expert specialization, introducing a new and underexplored vulnerability to backdoor attacks. In this work, we investigate the feasibility and effectiveness of injecting backdoors into MoE-based LLMs by exploiting their inherent expert routing preferences. We thus propose BadSwitch, a novel backdoor framework that integrates task-coupled dynamic trigger optimization with a sensitivity-guided Top-S expert tracing mechanism. Our approach jointly optimizes trigger embeddings during pretraining while identifying S most sensitive experts, subsequently constraining the Top-K gating mechanism to these targeted experts. Unlike traditional backdoor attacks that rely on superficial data poisoning or model editing, BadSwitch primarily embeds malicious triggers into expert routing paths with strong task affinity, enabling precise and stealthy model manipulation. Through comprehensive evaluations across three prominent MoE architectures (Switch Transformer, QwenMoE, and DeepSeekMoE), we demonstrate that BadSwitch can efficiently hijack pre-trained models with up to 100% success rate (ASR) while maintaining the highest clean accuracy (ACC) among all baselines. Furthermore, BadSwitch exhibits strong resilience against both text-level and model-level defense mechanisms, achieving 94.07% ASR and 87.18% ACC on the AGNews dataset. Our analysis of expert activation patterns reveals fundamental insights into MoE vulnerabilities. We anticipate this work will expose security risks in MoE systems and contribute to advancing AI safety.

摘要: 具有专家混合（MoE）架构的大型语言模型（LLM）通过将输入动态路由到专门的子网络（称为专家）来实现令人印象深刻的性能和效率。然而，由于专家专业化，这种稀疏路由机制本质上会表现出任务偏好，从而引入了一个新的且未充分探索的后门攻击漏洞。在这项工作中，我们研究了通过利用基于教育部的LLM固有的专家路由偏好向其注入后门的可行性和有效性。因此，我们提出了BadSwitch，这是一种新型后门框架，它将任务耦合的动态触发优化与敏感性引导的Top-S专家跟踪机制集成在一起。我们的方法在预训练期间联合优化触发嵌入，同时识别S个最敏感的专家，随后将Top-K门控机制限制到这些目标专家。与依赖表面数据中毒或模型编辑的传统后门攻击不同，BadSwitch主要将恶意触发器嵌入到具有强任务亲和力的专家路由路径中，实现精确且隐蔽的模型操纵。通过对三种主要MoE架构（Switch Transformer、QwenMoE和DeepSeekMoE）的全面评估，我们证明BadSwitch可以有效劫持预训练模型，成功率高达100%（ZR），同时保持所有基线中最高的清理准确度（ACC）。此外，BadSwitch对文本级和模型级防御机制都表现出强大的弹性，在AGNews数据集中实现了94.07%的ASB和87.18%的ACC。我们对专家激活模式的分析揭示了对MoE漏洞的基本见解。我们预计这项工作将暴露MoE系统的安全风险，并有助于提高人工智能的安全性。



## **4. Machine Unlearning Meets Adversarial Robustness via Constrained Interventions on LLMs**

机器放弃学习通过对LLM的约束干预来满足对抗鲁棒性 cs.LG

**SubmitDate**: 2025-10-15    [abs](http://arxiv.org/abs/2510.03567v2) [paper-pdf](http://arxiv.org/pdf/2510.03567v2)

**Authors**: Fatmazohra Rezkellah, Ramzi Dakhmouche

**Abstract**: With the increasing adoption of Large Language Models (LLMs), more customization is needed to ensure privacy-preserving and safe generation. We address this objective from two critical aspects: unlearning of sensitive information and robustness to jail-breaking attacks. We investigate various constrained optimization formulations that address both aspects in a \emph{unified manner}, by finding the smallest possible interventions on LLM weights that either make a given vocabulary set unreachable or embed the LLM with robustness to tailored attacks by shifting part of the weights to a \emph{safer} region. Beyond unifying two key properties, this approach contrasts with previous work in that it doesn't require an oracle classifier that is typically not available or represents a computational overhead. Surprisingly, we find that the simplest point-wise constraint-based intervention we propose leads to better performance than max-min interventions, while having a lower computational cost. Comparison against state-of-the-art defense methods demonstrates superior performance of the proposed approach.

摘要: 随着大型语言模型（LLM）的日益采用，需要更多的定制来确保隐私保护和安全生成。我们从两个关键方面实现这一目标：忘记敏感信息和对越狱攻击的稳健性。我们研究了各种受约束的优化公式，以\r {统一方式}解决这两个方面，通过找到对LLM权重的最小可能干预，这些干预要么使给定的词汇集不可达，要么通过将部分权重转移到\r {更安全}区域来嵌入LLM，对定制攻击具有鲁棒性。除了统一两个关键属性之外，这种方法与之前的工作形成鲜明对比，因为它不需要通常不可用或代表计算负担的Oracle分类器。令人惊讶的是，我们发现我们提出的最简单的逐点基于约束的干预比最大-最小干预具有更好的性能，同时具有更低的计算成本。与最先进的防御方法的比较表明了所提出的方法的优越性能。



## **5. Can an Individual Manipulate the Collective Decisions of Multi-Agents?**

个人可以操纵多主体的集体决策吗？ cs.CL

**SubmitDate**: 2025-10-15    [abs](http://arxiv.org/abs/2509.16494v2) [paper-pdf](http://arxiv.org/pdf/2509.16494v2)

**Authors**: Fengyuan Liu, Rui Zhao, Shuo Chen, Guohao Li, Philip Torr, Lei Han, Jindong Gu

**Abstract**: Individual Large Language Models (LLMs) have demonstrated significant capabilities across various domains, such as healthcare and law. Recent studies also show that coordinated multi-agent systems exhibit enhanced decision-making and reasoning abilities through collaboration. However, due to the vulnerabilities of individual LLMs and the difficulty of accessing all agents in a multi-agent system, a key question arises: If attackers only know one agent, could they still generate adversarial samples capable of misleading the collective decision? To explore this question, we formulate it as a game with incomplete information, where attackers know only one target agent and lack knowledge of the other agents in the system. With this formulation, we propose M-Spoiler, a framework that simulates agent interactions within a multi-agent system to generate adversarial samples. These samples are then used to manipulate the target agent in the target system, misleading the system's collaborative decision-making process. More specifically, M-Spoiler introduces a stubborn agent that actively aids in optimizing adversarial samples by simulating potential stubborn responses from agents in the target system. This enhances the effectiveness of the generated adversarial samples in misleading the system. Through extensive experiments across various tasks, our findings confirm the risks posed by the knowledge of an individual agent in multi-agent systems and demonstrate the effectiveness of our framework. We also explore several defense mechanisms, showing that our proposed attack framework remains more potent than baselines, underscoring the need for further research into defensive strategies.

摘要: 个体大型语言模型（LLM）已在医疗保健和法律等各个领域展现出强大的能力。最近的研究还表明，协调的多智能体系统通过协作表现出增强的决策和推理能力。然而，由于单个LLM的脆弱性以及访问多代理系统中所有代理的困难，出现了一个关键问题：如果攻击者只知道一个代理，他们还能生成能够误导集体决策的对抗样本吗？为了探索这个问题，我们将其描述为一个信息不完整的游戏，其中攻击者只知道一个目标代理，并且缺乏对系统中其他代理的了解。通过这个公式，我们提出了M-Spoiler，这是一个模拟多智能体系统内的智能体交互以生成对抗样本的框架。然后使用这些样本来操纵目标系统中的目标代理，误导系统的协作决策过程。更具体地说，M-Spoiler引入了一种顽固代理，它通过模拟目标系统中代理的潜在顽固反应来积极帮助优化对抗样本。这增强了生成的对抗样本误导系统的有效性。通过针对各种任务的广泛实验，我们的研究结果证实了多代理系统中单个代理的知识所带来的风险，并证明了我们框架的有效性。我们还探索了几种防御机制，表明我们提出的攻击框架仍然比基线更有效，强调了进一步研究防御策略的必要性。



## **6. SeCon-RAG: A Two-Stage Semantic Filtering and Conflict-Free Framework for Trustworthy RAG**

SeCon-RAG：值得信赖的RAG的两阶段语义过滤和免预算框架 cs.CL

Accepted at NeurIPS 2025

**SubmitDate**: 2025-10-15    [abs](http://arxiv.org/abs/2510.09710v2) [paper-pdf](http://arxiv.org/pdf/2510.09710v2)

**Authors**: Xiaonan Si, Meilin Zhu, Simeng Qin, Lijia Yu, Lijun Zhang, Shuaitong Liu, Xinfeng Li, Ranjie Duan, Yang Liu, Xiaojun Jia

**Abstract**: Retrieval-augmented generation (RAG) systems enhance large language models (LLMs) with external knowledge but are vulnerable to corpus poisoning and contamination attacks, which can compromise output integrity. Existing defenses often apply aggressive filtering, leading to unnecessary loss of valuable information and reduced reliability in generation. To address this problem, we propose a two-stage semantic filtering and conflict-free framework for trustworthy RAG. In the first stage, we perform a joint filter with semantic and cluster-based filtering which is guided by the Entity-intent-relation extractor (EIRE). EIRE extracts entities, latent objectives, and entity relations from both the user query and filtered documents, scores their semantic relevance, and selectively adds valuable documents into the clean retrieval database. In the second stage, we proposed an EIRE-guided conflict-aware filtering module, which analyzes semantic consistency between the query, candidate answers, and retrieved knowledge before final answer generation, filtering out internal and external contradictions that could mislead the model. Through this two-stage process, SeCon-RAG effectively preserves useful knowledge while mitigating conflict contamination, achieving significant improvements in both generation robustness and output trustworthiness. Extensive experiments across various LLMs and datasets demonstrate that the proposed SeCon-RAG markedly outperforms state-of-the-art defense methods.

摘要: 检索增强生成（RAG）系统利用外部知识增强大型语言模型（LLM），但容易受到语料库中毒和污染攻击，这可能会损害输出的完整性。现有的防御措施通常采用激进的过滤，导致不必要的有价值的信息丢失，并降低了生成的可靠性。为了解决这个问题，我们提出了一个两阶段的语义过滤和无冲突的框架值得信赖的RAG。在第一阶段中，我们执行一个联合过滤器与语义和基于聚类的过滤，这是指导的语义意图关系提取器（EIRE）。EERE从用户查询和过滤文档中提取实体、潜在目标和实体关系，对其语义相关性进行评分，并选择性地将有价值的文档添加到干净的检索数据库中。在第二阶段，我们提出了一个EIRE引导的冲突感知过滤模块，该模块在最终答案生成之前分析查询、候选答案和检索到的知识之间的语义一致性，过滤掉可能误导模型的内部和外部矛盾。通过这个两阶段过程，SeCon-RAG有效地保留了有用的知识，同时减轻了冲突污染，在发电稳健性和输出可信度方面实现了显着提高。跨各种LLM和数据集的广泛实验表明，拟议的SeCon-RAG明显优于最先进的防御方法。



## **7. SHIELD: Classifier-Guided Prompting for Robust and Safer LVLMs**

SHIELD：分类器引导的预算，实现更强大、更安全的LVLM cs.CL

Preprint

**SubmitDate**: 2025-10-15    [abs](http://arxiv.org/abs/2510.13190v1) [paper-pdf](http://arxiv.org/pdf/2510.13190v1)

**Authors**: Juan Ren, Mark Dras, Usman Naseem

**Abstract**: Large Vision-Language Models (LVLMs) unlock powerful multimodal reasoning but also expand the attack surface, particularly through adversarial inputs that conceal harmful goals in benign prompts. We propose SHIELD, a lightweight, model-agnostic preprocessing framework that couples fine-grained safety classification with category-specific guidance and explicit actions (Block, Reframe, Forward). Unlike binary moderators, SHIELD composes tailored safety prompts that enforce nuanced refusals or safe redirection without retraining. Across five benchmarks and five representative LVLMs, SHIELD consistently lowers jailbreak and non-following rates while preserving utility. Our method is plug-and-play, incurs negligible overhead, and is easily extendable to new attack types -- serving as a practical safety patch for both weakly and strongly aligned LVLMs.

摘要: 大型视觉语言模型（LVLM）解锁了强大的多模式推理，但也扩大了攻击面，特别是通过在良性提示中隐藏有害目标的对抗性输入。我们提出SHIELD，这是一个轻量级的、模型不可知的预处理框架，它将细粒度的安全分类与特定类别的指导和显式动作（Block、Reframe、Forward）结合起来。与二元版主不同，SHIELD编写了量身定制的安全提示，无需再培训即可强制执行细致入微的拒绝或安全重定向。在五个基准和五个有代表性的LVLM中，SHIELD持续降低越狱和不跟随率，同时保持实用性。我们的方法是即插即用的，所产生的负担可以忽略不计，并且可以轻松扩展到新的攻击类型--作为弱对齐和强对齐LVLM的实用安全补丁。



## **8. Keep Calm and Avoid Harmful Content: Concept Alignment and Latent Manipulation Towards Safer Answers**

保持冷静并避免有害内容：概念一致和潜在操纵以获得更安全的答案 cs.LG

**SubmitDate**: 2025-10-14    [abs](http://arxiv.org/abs/2510.12672v1) [paper-pdf](http://arxiv.org/pdf/2510.12672v1)

**Authors**: Ruben Belo, Claudia Soares, Marta Guimaraes

**Abstract**: Large Language Models are susceptible to jailbreak attacks that bypass built-in safety guardrails (e.g., by tricking the model with adversarial prompts). We propose Concept Alignment and Concept Manipulation \textbf{CALM}, an inference-time method that suppresses harmful concepts by modifying latent representations of the last layer of the model, without retraining. Leveraging \gls*{cw} technique from Computer Vision combined with orthogonal projection, CALM removes unwanted latent directions associated with harmful content while preserving model performance. Experiments show that CALM reduces harmful outputs and outperforms baseline methods in most metrics, offering a lightweight approach to AI safety with no additional training data or model fine-tuning, while incurring only a small computational overhead at inference.

摘要: 大型语言模型容易受到绕过内置安全护栏的越狱攻击（例如，通过用对抗性提示欺骗模型）。我们提出概念对齐和概念操纵\textBF{CALM}，这是一种推理时方法，通过修改模型最后一层的潜在表示来抑制有害概念，无需重新训练。利用计算机视觉中的\gls*{cw}技术与垂直投影相结合，CALM可以删除与有害内容相关的不需要的潜在方向，同时保留模型性能。实验表明，CALM减少了有害输出，并在大多数指标上优于基线方法，为人工智能安全提供了一种轻量级方法，无需额外的训练数据或模型微调，同时在推理时只产生很小的计算负担。



## **9. PEAR: Planner-Executor Agent Robustness Benchmark**

PEAR：规划者-执行者代理稳健性基准 cs.LG

**SubmitDate**: 2025-10-14    [abs](http://arxiv.org/abs/2510.07505v2) [paper-pdf](http://arxiv.org/pdf/2510.07505v2)

**Authors**: Shen Dong, Mingxuan Zhang, Pengfei He, Li Ma, Bhavani Thuraisingham, Hui Liu, Yue Xing

**Abstract**: Large Language Model (LLM)-based Multi-Agent Systems (MAS) have emerged as a powerful paradigm for tackling complex, multi-step tasks across diverse domains. However, despite their impressive capabilities, MAS remain susceptible to adversarial manipulation. Existing studies typically examine isolated attack surfaces or specific scenarios, leaving a lack of holistic understanding of MAS vulnerabilities. To bridge this gap, we introduce PEAR, a benchmark for systematically evaluating both the utility and vulnerability of planner-executor MAS. While compatible with various MAS architectures, our benchmark focuses on the planner-executor structure, which is a practical and widely adopted design. Through extensive experiments, we find that (1) a weak planner degrades overall clean task performance more severely than a weak executor; (2) while a memory module is essential for the planner, having a memory module for the executor does not impact the clean task performance; (3) there exists a trade-off between task performance and robustness; and (4) attacks targeting the planner are particularly effective at misleading the system. These findings offer actionable insights for enhancing the robustness of MAS and lay the groundwork for principled defenses in multi-agent settings.

摘要: 基于大型语言模型（LLM）的多智能体系统（MAS）已成为处理跨不同领域复杂、多步骤任务的强大范式。然而，尽管MAS的能力令人印象深刻，但仍然容易受到对抗操纵。现有的研究通常会检查孤立的攻击表面或特定场景，从而缺乏对MAS漏洞的全面了解。为了弥合这一差距，我们引入了PEAR，这是一个用于系统评估规划者-执行者MAS的实用性和脆弱性的基准。虽然兼容各种MAS体系结构，我们的基准集中在规划者-执行器结构，这是一个实用的和广泛采用的设计。通过大量的实验，我们发现：（1）弱规划器比弱执行器更严重地降低了清洁任务的整体性能;（2）虽然规划器的内存模块是必不可少的，但执行器的内存模块并不影响清洁任务的性能;（3）任务性能和鲁棒性之间存在权衡;以及（4）针对计划者的攻击在误导系统方面特别有效。这些发现提供了可操作的见解，提高MAS的鲁棒性，并奠定了基础，在多智能体设置的原则性防御。



## **10. IP-Augmented Multi-Modal Malicious URL Detection Via Token-Contrastive Representation Enhancement and Multi-Granularity Fusion**

通过令牌对比表示增强和多粒度融合进行IP增强多模式恶意URL检测 cs.CR

**SubmitDate**: 2025-10-14    [abs](http://arxiv.org/abs/2510.12395v1) [paper-pdf](http://arxiv.org/pdf/2510.12395v1)

**Authors**: Ye Tian, Yanqiu Yu, Liangliang Song, Zhiquan Liu, Yanbin Wang, Jianguo Sun

**Abstract**: Malicious URL detection remains a critical cybersecurity challenge as adversaries increasingly employ sophisticated evasion techniques including obfuscation, character-level perturbations, and adversarial attacks. Although pre-trained language models (PLMs) like BERT have shown potential for URL analysis tasks, three limitations persist in current implementations: (1) inability to effectively model the non-natural hierarchical structure of URLs, (2) insufficient sensitivity to character-level obfuscation, and (3) lack of mechanisms to incorporate auxiliary network-level signals such as IP addresses-all essential for robust detection. To address these challenges, we propose CURL-IP, an advanced multi-modal detection framework incorporating three key innovations: (1) Token-Contrastive Representation Enhancer, which enhances subword token representations through token-aware contrastive learning to produce more discriminative and isotropic embeddings; (2) Cross-Layer Multi-Scale Aggregator, employing hierarchical aggregation of Transformer outputs via convolutional operations and gated MLPs to capture both local and global semantic patterns across layers; and (3) Blockwise Multi-Modal Coupler that decomposes URL-IP features into localized block units and computes cross-modal attention weights at the block level, enabling fine-grained inter-modal interaction. This architecture enables simultaneous preservation of fine-grained lexical cues, contextual semantics, and integration of network-level signals. Our evaluation on large-scale real-world datasets shows the framework significantly outperforms state-of-the-art baselines across binary and multi-class classification tasks.

摘要: 恶意URL检测仍然是一个关键的网络安全挑战，因为对手越来越多地使用复杂的规避技术，包括混淆、字符级扰动和对抗性攻击。尽管BERT等预训练语言模型（PLM）已显示出URL分析任务的潜力，但当前的实现中仍然存在三个局限性：（1）无法有效地建模URL的非自然分层结构，（2）对字符级混淆的敏感性不足，（3）缺乏纳入辅助网络级信号（例如IP地址）的机制--所有这些对于鲁棒检测来说都至关重要。为了应对这些挑战，我们提出了CROL-IP，这是一种先进的多模式检测框架，融合了三项关键创新：（1）令牌对比表示增强器，它通过令牌感知的对比学习来增强子词令牌表示，以产生更具区分性和各向同性的嵌入;（2）跨层多尺度聚合器，通过卷积运算和门控MLP采用Transformer输出的分层聚合来跨层捕获局部和全局语义模式;和（3）绑定多模式耦合器，将URL-IP特征分解为局部块单元，并计算块级别的跨模式注意力权重，从而实现细粒度的模式间交互。该架构能够同时保存细粒度的词汇线索、上下文语义和网络级信号的集成。我们对大规模现实世界数据集的评估表明，该框架在二元和多类分类任务中的表现显着优于最先进的基线。



## **11. PromptLocate: Localizing Prompt Injection Attacks**

Inbox Locate：本地化提示注入攻击 cs.CR

To appear in IEEE Symposium on Security and Privacy, 2026

**SubmitDate**: 2025-10-14    [abs](http://arxiv.org/abs/2510.12252v1) [paper-pdf](http://arxiv.org/pdf/2510.12252v1)

**Authors**: Yuqi Jia, Yupei Liu, Zedian Shao, Jinyuan Jia, Neil Gong

**Abstract**: Prompt injection attacks deceive a large language model into completing an attacker-specified task instead of its intended task by contaminating its input data with an injected prompt, which consists of injected instruction(s) and data. Localizing the injected prompt within contaminated data is crucial for post-attack forensic analysis and data recovery. Despite its growing importance, prompt injection localization remains largely unexplored. In this work, we bridge this gap by proposing PromptLocate, the first method for localizing injected prompts. PromptLocate comprises three steps: (1) splitting the contaminated data into semantically coherent segments, (2) identifying segments contaminated by injected instructions, and (3) pinpointing segments contaminated by injected data. We show PromptLocate accurately localizes injected prompts across eight existing and eight adaptive attacks.

摘要: 提示注入攻击欺骗大型语言模型完成攻击者指定的任务，而不是其预期的任务，通过注入的提示污染其输入数据，其中包括注入的指令和数据。在受污染的数据中定位注入的提示对于攻击后的取证分析和数据恢复至关重要。尽管其重要性日益增加，但快速注射定位在很大程度上仍未被探索。在这项工作中，我们弥合这一差距，提出了本地化注入提示的第一种方法--martLocate。ObjectLocate包括三个步骤：（1）将受污染的数据拆分成语义一致的段，（2）识别被注入指令污染的段，以及（3）精确定位被注入数据污染的段。我们展示了EntLocate可以准确地定位跨越八种现有攻击和八种自适应攻击的注入提示。



## **12. L2M-AID: Autonomous Cyber-Physical Defense by Fusing Semantic Reasoning of Large Language Models with Multi-Agent Reinforcement Learning (Preprint)**

L2 M-AID：通过融合大型语言模型的语义推理与多智能体强化学习来自主网络物理防御（预印本） cs.AI

This preprint was submitted to IEEE TrustCom 2025. The accepted  version will be published under copyright 2025 IEEE

**SubmitDate**: 2025-10-14    [abs](http://arxiv.org/abs/2510.07363v2) [paper-pdf](http://arxiv.org/pdf/2510.07363v2)

**Authors**: Tianxiang Xu, Zhichao Wen, Xinyu Zhao, Jun Wang, Yan Li, Chang Liu

**Abstract**: The increasing integration of Industrial IoT (IIoT) exposes critical cyber-physical systems to sophisticated, multi-stage attacks that elude traditional defenses lacking contextual awareness. This paper introduces L2M-AID, a novel framework for Autonomous Industrial Defense using LLM-empowered, Multi-agent reinforcement learning. L2M-AID orchestrates a team of collaborative agents, each driven by a Large Language Model (LLM), to achieve adaptive and resilient security. The core innovation lies in the deep fusion of two AI paradigms: we leverage an LLM as a semantic bridge to translate vast, unstructured telemetry into a rich, contextual state representation, enabling agents to reason about adversary intent rather than merely matching patterns. This semantically-aware state empowers a Multi-Agent Reinforcement Learning (MARL) algorithm, MAPPO, to learn complex cooperative strategies. The MARL reward function is uniquely engineered to balance security objectives (threat neutralization) with operational imperatives, explicitly penalizing actions that disrupt physical process stability. To validate our approach, we conduct extensive experiments on the benchmark SWaT dataset and a novel synthetic dataset generated based on the MITRE ATT&CK for ICS framework. Results demonstrate that L2M-AID significantly outperforms traditional IDS, deep learning anomaly detectors, and single-agent RL baselines across key metrics, achieving a 97.2% detection rate while reducing false positives by over 80% and improving response times by a factor of four. Crucially, it demonstrates superior performance in maintaining physical process stability, presenting a robust new paradigm for securing critical national infrastructure.

摘要: 工业物联网（IIoT）的日益集成使关键的网络物理系统面临复杂的多阶段攻击，这些攻击无法逃避缺乏上下文感知的传统防御。本文介绍了L2 M-AID，这是一种新型的自主工业防御框架，使用LLM授权的多智能体强化学习。L2 M-AID组织了一个协作代理团队，每个代理都由大型语言模型（LLM）驱动，以实现自适应和弹性的安全性。核心创新在于两种人工智能范式的深度融合：我们利用LLM作为语义桥梁，将庞大的非结构化遥感数据转化为丰富的上下文状态表示，使代理能够推理对手意图，而不仅仅是匹配模式。这种语义感知状态使多智能体强化学习（MARL）算法MAPPO能够学习复杂的合作策略。MARL奖励功能经过独特设计，旨在平衡安全目标（威胁消除）与运营必要性，明确惩罚破坏物理过程稳定性的行为。为了验证我们的方法，我们对基准SWaT数据集和基于MITRE ATA & CK for ICS框架生成的新型合成数据集进行了广泛的实验。结果表明，L2 M-AID在关键指标上的表现显着优于传统IDS、深度学习异常检测器和单代理RL基线，实现了97.2%的检测率，同时将误报率降低了80%以上，并将响应时间提高了四倍。至关重要的是，它在维持物理过程稳定性方面表现出色，为保护关键国家基础设施提供了强大的新范式。



## **13. Cross-Modal Safety Alignment: Is textual unlearning all you need?**

跨模态安全对齐：你需要的只是文本遗忘吗？ cs.CL

Accepted by EMNLP 2024 Findings

**SubmitDate**: 2025-10-14    [abs](http://arxiv.org/abs/2406.02575v2) [paper-pdf](http://arxiv.org/pdf/2406.02575v2)

**Authors**: Trishna Chakraborty, Erfan Shayegani, Zikui Cai, Nael Abu-Ghazaleh, M. Salman Asif, Yue Dong, Amit K. Roy-Chowdhury, Chengyu Song

**Abstract**: Recent studies reveal that integrating new modalities into Large Language Models (LLMs), such as Vision-Language Models (VLMs), creates a new attack surface that bypasses existing safety training techniques like Supervised Fine-tuning (SFT) and Reinforcement Learning with Human Feedback (RLHF). While further SFT and RLHF-based safety training can be conducted in multi-modal settings, collecting multi-modal training datasets poses a significant challenge. Inspired by the structural design of recent multi-modal models, where, regardless of the combination of input modalities, all inputs are ultimately fused into the language space, we aim to explore whether unlearning solely in the textual domain can be effective for cross-modality safety alignment. Our evaluation across six datasets empirically demonstrates the transferability -- textual unlearning in VLMs significantly reduces the Attack Success Rate (ASR) to less than 8\% and in some cases, even as low as nearly 2\% for both text-based and vision-text-based attacks, alongside preserving the utility. Moreover, our experiments show that unlearning with a multi-modal dataset offers no potential benefits but incurs significantly increased computational demands, possibly up to 6 times higher.

摘要: 最近的研究表明，将新的模式集成到大型语言模型（LLM）中，例如视觉语言模型（VLM），可以创建一个新的攻击表面，绕过现有的安全训练技术，例如监督微调（SFT）和带人类反馈的强化学习（RL HF）。虽然进一步的基于SFT和WLHF的安全培训可以在多模式环境中进行，但收集多模式训练数据集构成了重大挑战。受最近多模式模型的结构设计的启发，无论输入模式的组合如何，所有输入最终都会融合到语言空间中，我们的目标是探索仅在文本领域放弃学习是否可以有效地实现跨模式的安全对齐。我们对六个数据集的评估从经验上证明了可移植性--VLM中的文本取消学习将攻击成功率（ASB）显着降低到8%以下，在某些情况下，对于基于文本和基于视觉文本的攻击，攻击成功率甚至低至近2%，同时保留了实用性。此外，我们的实验表明，放弃使用多模式数据集的学习不会带来潜在的好处，但会导致计算需求显着增加，可能高达6倍。



## **14. Unveiling the Vulnerability of Graph-LLMs: An Interpretable Multi-Dimensional Adversarial Attack on TAGs**

揭开Graph-LLM的漏洞：对TAG的可解释多维对抗攻击 cs.LG

12 pages, 4 figures

**SubmitDate**: 2025-10-14    [abs](http://arxiv.org/abs/2510.12233v1) [paper-pdf](http://arxiv.org/pdf/2510.12233v1)

**Authors**: Bowen Fan, Zhilin Guo, Xunkai Li, Yihan Zhou, Bing Zhou, Zhenjun Li, Rong-Hua Li, Guoren Wang

**Abstract**: Graph Neural Networks (GNNs) have become a pivotal framework for modeling graph-structured data, enabling a wide range of applications from social network analysis to molecular chemistry. By integrating large language models (LLMs), text-attributed graphs (TAGs) enhance node representations with rich textual semantics, significantly boosting the expressive power of graph-based learning. However, this sophisticated synergy introduces critical vulnerabilities, as Graph-LLMs are susceptible to adversarial attacks on both their structural topology and textual attributes. Although specialized attack methods have been designed for each of these aspects, no work has yet unified them into a comprehensive approach. In this work, we propose the Interpretable Multi-Dimensional Graph Attack (IMDGA), a novel human-centric adversarial attack framework designed to orchestrate multi-level perturbations across both graph structure and textual features. IMDGA utilizes three tightly integrated modules to craft attacks that balance interpretability and impact, enabling a deeper understanding of Graph-LLM vulnerabilities. Through rigorous theoretical analysis and comprehensive empirical evaluations on diverse datasets and architectures, IMDGA demonstrates superior interpretability, attack effectiveness, stealthiness, and robustness compared to existing methods. By exposing critical weaknesses in TAG representation learning, this work uncovers a previously underexplored semantic dimension of vulnerability in Graph-LLMs, offering valuable insights for improving their resilience. Our code and resources are publicly available at https://anonymous.4open.science/r/IMDGA-7289.

摘要: 图神经网络（GNN）已经成为对图结构数据进行建模的关键框架，能够实现从社交网络分析到分子化学的广泛应用。通过集成大型语言模型（LLM），文本属性图（TAG）增强了具有丰富文本语义的节点表示，显着提高了基于图的学习的表达能力。然而，这种复杂的协同作用引入了关键的漏洞，因为Graph-LLM容易受到对其结构拓扑和文本属性的对抗性攻击。虽然专门的攻击方法已被设计用于这些方面的每一个，还没有工作将它们统一成一个全面的方法。在这项工作中，我们提出了可解释多维图攻击（IMDGA），这是一种新型的以人为中心的对抗攻击框架，旨在协调跨图结构和文本特征的多层扰动。IMDGA利用三个紧密集成的模块来设计平衡可解释性和影响的攻击，从而能够更深入地了解Graph-LLM漏洞。通过对不同数据集和架构进行严格的理论分析和全面的实证评估，IMDGA展示了与现有方法相比更出色的可解释性、攻击有效性、隐蔽性和鲁棒性。通过揭露TAG表示学习中的关键弱点，这项工作揭示了Graph-LLM中先前未充分探索的漏洞语义维度，为提高其弹性提供了宝贵的见解。我们的代码和资源可在https://anonymous.4open.science/r/IMDGA-7289上公开获取。



## **15. HackWorld: Evaluating Computer-Use Agents on Exploiting Web Application Vulnerabilities**

HackWorld：评估计算机使用代理利用Web应用程序漏洞的能力 cs.CR

**SubmitDate**: 2025-10-14    [abs](http://arxiv.org/abs/2510.12200v1) [paper-pdf](http://arxiv.org/pdf/2510.12200v1)

**Authors**: Xiaoxue Ren, Penghao Jiang, Kaixin Li, Zhiyong Huang, Xiaoning Du, Jiaojiao Jiang, Zhenchang Xing, Jiamou Sun, Terry Yue Zhuo

**Abstract**: Web applications are prime targets for cyberattacks as gateways to critical services and sensitive data. Traditional penetration testing is costly and expertise-intensive, making it difficult to scale with the growing web ecosystem. While language model agents show promise in cybersecurity, modern web applications demand visual understanding, dynamic content handling, and multi-step interactions that only computer-use agents (CUAs) can perform. Yet, their ability to discover and exploit vulnerabilities through graphical interfaces remains largely unexplored. We present HackWorld, the first framework for systematically evaluating CUAs' capabilities to exploit web application vulnerabilities via visual interaction. Unlike sanitized benchmarks, HackWorld includes 36 real-world applications across 11 frameworks and 7 languages, featuring realistic flaws such as injection vulnerabilities, authentication bypasses, and unsafe input handling. Using a Capture-the-Flag (CTF) setup, it tests CUAs' capacity to identify and exploit these weaknesses while navigating complex web interfaces. Evaluation of state-of-the-art CUAs reveals concerning trends: exploitation rates below 12% and low cybersecurity awareness. CUAs often fail at multi-step attack planning and misuse security tools. These results expose the current limitations of CUAs in web security contexts and highlight opportunities for developing more security-aware agents capable of effective vulnerability detection and exploitation.

摘要: Web应用程序是网络攻击的主要目标，是关键服务和敏感数据的网关。传统的渗透测试成本高昂，需要大量的专业知识，因此很难随着不断增长的网络生态系统而扩展。虽然语言模型代理在网络安全方面表现出了希望，但现代Web应用程序需要视觉理解，动态内容处理和多步交互，只有计算机使用代理（CUA）才能执行。然而，它们通过图形界面发现和利用漏洞的能力在很大程度上仍未得到开发。我们展示了HackWorld，这是第一个用于系统评估CUA通过视觉交互利用Web应用程序漏洞的能力的框架。与经过清理的基准测试不同，HackWorld包括36个现实世界的应用程序，涵盖11个框架和7种语言，具有注入漏洞、身份验证绕过和不安全的输入处理等现实缺陷。它使用Capture-the-Flag（CTF）设置来测试CUA在导航复杂Web界面时识别和利用这些弱点的能力。对最先进的CUA的评估揭示了令人担忧的趋势：利用率低于12%，网络安全意识较低。CUA经常在多步骤攻击计划方面失败并滥用安全工具。这些结果暴露了CUA当前在网络安全环境中的局限性，并强调了开发能够有效检测和利用漏洞的更具安全意识的代理的机会。



## **16. When "Competency" in Reasoning Opens the Door to Vulnerability: Jailbreaking LLMs via Novel Complex Ciphers**

当推理中的“能力”打开脆弱之门：通过新颖复杂密码越狱LLM cs.CL

Published in Reliable ML from Unreliable Data workshop @ NeurIPS 2025

**SubmitDate**: 2025-10-14    [abs](http://arxiv.org/abs/2402.10601v5) [paper-pdf](http://arxiv.org/pdf/2402.10601v5)

**Authors**: Divij Handa, Zehua Zhang, Amir Saeidi, Shrinidhi Kumbhar, Md Nayem Uddin, Aswin RRV, Chitta Baral

**Abstract**: Recent advancements in Large Language Model (LLM) safety have primarily focused on mitigating attacks crafted in natural language or common ciphers (e.g. Base64), which are likely integrated into newer models' safety training. However, we reveal a paradoxical vulnerability: as LLMs advance in reasoning, they inadvertently become more susceptible to novel jailbreaking attacks. Enhanced reasoning enables LLMs to interpret complex instructions and decode complex user-defined ciphers, creating an exploitable security gap. To study this vulnerability, we introduce Attacks using Custom Encryptions (ACE), a jailbreaking technique that encodes malicious queries with novel ciphers. Extending ACE, we introduce Layered Attacks using Custom Encryptions (LACE), which applies multi-layer ciphers to amplify attack complexity. Furthermore, we develop CipherBench, a benchmark designed to evaluate LLMs' accuracy in decoding encrypted benign text. Our experiments reveal a critical trade-off: LLMs that are more capable of decoding ciphers are more vulnerable to LACE, with success rates on gpt-oss-20b escalating from 60% under ACE to 72% with LACE. These findings highlight a critical insight: as LLMs become more adept at deciphering complex user ciphers--many of which cannot be preemptively included in safety training--they become increasingly exploitable.

摘要: 大型语言模型（LLM）安全性的最新进展主要集中在减轻用自然语言或常用密码（例如Base 64）精心设计的攻击，这些攻击可能会集成到较新模型的安全培训中。然而，我们揭示了一个自相矛盾的漏洞：随着LLM在推理方面的进步，它们无意中变得更容易受到新颖的越狱攻击。增强的推理使LLM能够解释复杂的指令并解码复杂的用户定义的密码，从而创造了可利用的安全漏洞。为了研究此漏洞，我们引入了使用自定义加密（ACE）的攻击，这是一种越狱技术，使用新颖的密码对恶意查询进行编码。扩展ACE，我们引入了使用自定义加密（LACE）的分层攻击，该加密应用多层密码来放大攻击复杂性。此外，我们还开发了CipherBench，这是一个旨在评估LLM解码加密良性文本的准确性的基准。我们的实验揭示了一个关键的权衡：解码密码能力更强的LLM更容易受到LACE的影响，gtt-oss-20 b的成功率从ACE下的60%上升到LACE下的72%。这些发现凸显了一个关键的见解：随着LLM越来越善于破译复杂的用户密码（其中许多密码无法预先包含在安全培训中），它们变得越来越容易被利用。



## **17. Attention-Aware GNN-based Input Defense against Multi-Turn LLM Jailbreak**

注意力意识的基于GNN的输入防御针对多回合LLM越狱 cs.LG

**SubmitDate**: 2025-10-14    [abs](http://arxiv.org/abs/2507.07146v2) [paper-pdf](http://arxiv.org/pdf/2507.07146v2)

**Authors**: Zixuan Huang, Kecheng Huang, Lihao Yin, Bowei He, Huiling Zhen, Mingxuan Yuan, Zili Shao

**Abstract**: Large Language Models (LLMs) have gained significant traction in various applications, yet their capabilities present risks for both constructive and malicious exploitation. Despite extensive training and fine-tuning efforts aimed at enhancing safety, LLMs remain susceptible to jailbreak attacks. Recently, the emergence of multi-turn attacks has intensified this vulnerability. Unlike single-turn attacks, multi-turn attacks incrementally escalate dialogue complexity, rendering them more challenging to detect and mitigate.   In this study, we introduce G-Guard, an innovative attention-aware Graph Neural Network (GNN)-based input classifier specifically designed to defend against multi-turn jailbreak attacks targeting LLMs. G-Guard constructs an entity graph for multi-turn queries, which captures the interrelationships between queries and harmful keywords that present in multi-turn queries. Furthermore, we propose an attention-aware augmentation mechanism that retrieves the most relevant single-turn query based on the ongoing multi-turn conversation. The retrieved query is incorporated as a labeled node within the graph, thereby enhancing the GNN's capacity to classify the current query as harmful or benign. Evaluation results show that G-Guard consistently outperforms all baselines across diverse datasets and evaluation metrics, demonstrating its efficacy as a robust defense mechanism against multi-turn jailbreak attacks.

摘要: 大型语言模型（LLM）在各种应用程序中获得了巨大的吸引力，但它们的功能存在建设性和恶意利用的风险。尽管针对提高安全性进行了广泛的培训和微调，但LLMs仍然容易受到越狱攻击。最近，多回合攻击的出现加剧了这种脆弱性。与单回合攻击不同，多回合攻击逐渐增加了对话的复杂性，使其更难以检测和缓解。   在这项研究中，我们介绍了G-Guard，这是一种创新的基于注意力感知的图神经网络（GNN）的输入分类器，专门用于防御针对LLM的多回合越狱攻击。G-Guard为多轮查询构建了一个实体图，该实体图捕捉了多轮查询中存在的查询和有害关键字之间的相互关系。此外，我们提出了一个注意力感知增强机制，检索最相关的单轮查询的基础上正在进行的多轮对话。检索到的查询被合并为图中的标记节点，从而增强了GNN将当前查询分类为有害或良性的能力。评估结果显示，G-Guard在不同数据集和评估指标中的表现始终优于所有基线，证明了其作为针对多回合越狱攻击的强大防御机制的有效性。



## **18. SafeMT: Multi-turn Safety for Multimodal Language Models**

SafeMT：多模式语言模型的多轮安全性 cs.CL

**SubmitDate**: 2025-10-14    [abs](http://arxiv.org/abs/2510.12133v1) [paper-pdf](http://arxiv.org/pdf/2510.12133v1)

**Authors**: Han Zhu, Juntao Dai, Jiaming Ji, Haoran Li, Chengkun Cai, Pengcheng Wen, Chi-Min Chan, Boyuan Chen, Yaodong Yang, Sirui Han, Yike Guo

**Abstract**: With the widespread use of multi-modal Large Language models (MLLMs), safety issues have become a growing concern. Multi-turn dialogues, which are more common in everyday interactions, pose a greater risk than single prompts; however, existing benchmarks do not adequately consider this situation. To encourage the community to focus on the safety issues of these models in multi-turn dialogues, we introduce SafeMT, a benchmark that features dialogues of varying lengths generated from harmful queries accompanied by images. This benchmark consists of 10,000 samples in total, encompassing 17 different scenarios and four jailbreak methods. Additionally, we propose Safety Index (SI) to evaluate the general safety of MLLMs during conversations. We assess the safety of 17 models using this benchmark and discover that the risk of successful attacks on these models increases as the number of turns in harmful dialogues rises. This observation indicates that the safety mechanisms of these models are inadequate for recognizing the hazard in dialogue interactions. We propose a dialogue safety moderator capable of detecting malicious intent concealed within conversations and providing MLLMs with relevant safety policies. Experimental results from several open-source models indicate that this moderator is more effective in reducing multi-turn ASR compared to existed guard models.

摘要: 随着多模式大型语言模型（MLLM）的广泛使用，安全问题已成为人们日益关注的问题。多轮对话在日常互动中更常见，比单个提示带来的风险更大;然而，现有的基准没有充分考虑这种情况。为了鼓励社区在多轮对话中关注这些模型的安全问题，我们引入了SafeMT，这是一个基准，其特点是由伴随图像的有害查询生成的不同长度的对话。该基准测试总共包含10，000个样本，涵盖17种不同的场景和4种越狱方法。此外，我们还提出了安全指数（SI）来评估MLLM在对话期间的总体安全性。我们使用该基准评估了17个模型的安全性，并发现随着有害对话轮数的增加，对这些模型进行成功攻击的风险也会增加。这一观察表明，这些模型的安全机制不足以识别对话互动中的危险。我们提出了一个对话安全版主，能够检测隐藏在对话中的恶意意图，并为MLLM提供相关安全政策。多个开源模型的实验结果表明，与现有的警卫模型相比，这种调节器在减少多圈ASO方面更有效。



## **19. GraphRAG under Fire**

GraphRAG受到攻击 cs.LG

13 pages. Accepted by IEEE Symposium on Security and Privacy 2026  (S&P 2026)

**SubmitDate**: 2025-10-14    [abs](http://arxiv.org/abs/2501.14050v4) [paper-pdf](http://arxiv.org/pdf/2501.14050v4)

**Authors**: Jiacheng Liang, Yuhui Wang, Changjiang Li, Rongyi Zhu, Tanqiu Jiang, Neil Gong, Ting Wang

**Abstract**: GraphRAG advances retrieval-augmented generation (RAG) by structuring external knowledge as multi-scale knowledge graphs, enabling language models to integrate both broad context and granular details in their generation. While GraphRAG has demonstrated success across domains, its security implications remain largely unexplored. To bridge this gap, this work examines GraphRAG's vulnerability to poisoning attacks, uncovering an intriguing security paradox: existing RAG poisoning attacks are less effective under GraphRAG than conventional RAG, due to GraphRAG's graph-based indexing and retrieval; yet, the same features also create new attack surfaces. We present GragPoison, a novel attack that exploits shared relations in the underlying knowledge graph to craft poisoning text capable of compromising multiple queries simultaneously. GragPoison employs three key strategies: (i) relation injection to introduce false knowledge, (ii) relation enhancement to amplify poisoning influence, and (iii) narrative generation to embed malicious content within coherent text. Empirical evaluation across diverse datasets and models shows that GragPoison substantially outperforms existing attacks in terms of effectiveness (up to 98% success rate) and scalability (using less than 68% poisoning text) on multiple variations of GraphRAG. We also explore potential defensive measures and their limitations, identifying promising directions for future research.

摘要: GraphRAG通过将外部知识结构化为多尺度知识图，使语言模型能够在生成中集成广泛的上下文和粒度细节，从而推进了检索增强生成（RAG）。虽然GraphRAG在各个领域都取得了成功，但其安全影响在很大程度上仍未被探索。为了弥合这一差距，这项工作研究了GraphRAG对中毒攻击的脆弱性，揭示了一个有趣的安全悖论：由于GraphRAG的基于图形的索引和检索，现有的RAG中毒攻击在GraphRAG下不如传统RAG有效;然而，相同的功能也创建了新的攻击面。我们提出了GragPoison，这是一种新颖的攻击，它利用底层知识图中的共享关系来制作能够同时破坏多个查询的中毒文本。GragPoison采用三种关键策略：（i）关系注入以引入虚假知识，（ii）关系增强以放大中毒影响，以及（iii）叙事生成以将恶意内容嵌入连贯文本中。对不同数据集和模型的经验评估表明，GragPoison在对GraphRAG的多种变体的有效性（高达98%的成功率）和可扩展性（使用不到68%的中毒文本）方面大大优于现有的攻击。我们还探索潜在的防御措施及其局限性，为未来研究确定有希望的方向。



## **20. Robust ML-based Detection of Conventional, LLM-Generated, and Adversarial Phishing Emails Using Advanced Text Preprocessing**

使用高级文本预处理对传统、LLM生成和对抗性网络钓鱼电子邮件进行稳健的基于ML的检测 cs.CR

**SubmitDate**: 2025-10-13    [abs](http://arxiv.org/abs/2510.11915v1) [paper-pdf](http://arxiv.org/pdf/2510.11915v1)

**Authors**: Deeksha Hareesha Kulal, Chidozie Princewill Arannonu, Afsah Anwar, Nidhi Rastogi, Quamar Niyaz

**Abstract**: Phishing remains a critical cybersecurity threat, especially with the advent of large language models (LLMs) capable of generating highly convincing malicious content. Unlike earlier phishing attempts which are identifiable by grammatical errors, misspellings, incorrect phrasing, and inconsistent formatting, LLM generated emails are grammatically sound, contextually relevant, and linguistically natural. These advancements make phishing emails increasingly difficult to distinguish from legitimate ones, challenging traditional detection mechanisms. Conventional phishing detection systems often fail when faced with emails crafted by LLMs or manipulated using adversarial perturbation techniques. To address this challenge, we propose a robust phishing email detection system featuring an enhanced text preprocessing pipeline. This pipeline includes spelling correction and word splitting to counteract adversarial modifications and improve detection accuracy. Our approach integrates widely adopted natural language processing (NLP) feature extraction techniques and machine learning algorithms. We evaluate our models on publicly available datasets comprising both phishing and legitimate emails, achieving a detection accuracy of 94.26% and F1-score of 84.39% in model deployment setting. To assess robustness, we further evaluate our models using adversarial phishing samples generated by four attack methods in Python TextAttack framework. Additionally, we evaluate models' performance against phishing emails generated by LLMs including ChatGPT and Llama. Results highlight the resilience of models against evolving AI-powered phishing threats.

摘要: 网络钓鱼仍然是一个严重的网络安全威胁，特别是随着能够生成高度令人信服的恶意内容的大型语言模型（LLM）的出现。与早期的网络钓鱼尝试（可通过语法错误、拼写错误、措辞不正确和格式不一致）不同，LLM生成的电子邮件语法健全、上下文相关且语言自然。这些进步使得网络钓鱼电子邮件越来越难以与合法电子邮件区分开来，从而挑战了传统的检测机制。当面对由LLM制作或使用对抗性干扰技术操纵的电子邮件时，传统的网络钓鱼检测系统通常会失败。为了应对这一挑战，我们提出了一种强大的网络钓鱼电子邮件检测系统，具有增强的文本预处理管道。该管道包括拼写纠正和单词拆分，以抵消对抗性修改并提高检测准确性。我们的方法集成了广泛采用的自然语言处理（NLP）特征提取技术和机器学习算法。我们在包括网络钓鱼和合法电子邮件的公开数据集上评估了我们的模型，在模型部署设置中实现了94.26%的检测准确率和84.39%的F1评分。为了评估稳健性，我们使用Python文本攻击框架中四种攻击方法生成的对抗性网络钓鱼样本进一步评估我们的模型。此外，我们还评估模型针对ChatGPT和Llama等LLM生成的网络钓鱼电子邮件的性能。结果凸显了模型对不断变化的人工智能驱动网络钓鱼威胁的弹性。



## **21. Countermind: A Multi-Layered Security Architecture for Large Language Models**

Countermind：大型语言模型的多层安全架构 cs.CR

33 pages, 3 figures, 6 tables. Keywords: LLM security;  defense-in-depth; prompt injection; activation steering; multimodal sandbox;  threat modeling

**SubmitDate**: 2025-10-13    [abs](http://arxiv.org/abs/2510.11837v1) [paper-pdf](http://arxiv.org/pdf/2510.11837v1)

**Authors**: Dominik Schwarz

**Abstract**: The security of Large Language Model (LLM) applications is fundamentally challenged by "form-first" attacks like prompt injection and jailbreaking, where malicious instructions are embedded within user inputs. Conventional defenses, which rely on post hoc output filtering, are often brittle and fail to address the root cause: the model's inability to distinguish trusted instructions from untrusted data. This paper proposes Countermind, a multi-layered security architecture intended to shift defenses from a reactive, post hoc posture to a proactive, pre-inference, and intra-inference enforcement model. The architecture proposes a fortified perimeter designed to structurally validate and transform all inputs, and an internal governance mechanism intended to constrain the model's semantic processing pathways before an output is generated. The primary contributions of this work are conceptual designs for: (1) A Semantic Boundary Logic (SBL) with a mandatory, time-coupled Text Crypter intended to reduce the plaintext prompt injection attack surface, provided all ingestion paths are enforced. (2) A Parameter-Space Restriction (PSR) mechanism, leveraging principles from representation engineering, to dynamically control the LLM's access to internal semantic clusters, with the goal of mitigating semantic drift and dangerous emergent behaviors. (3) A Secure, Self-Regulating Core that uses an OODA loop and a learning security module to adapt its defenses based on an immutable audit log. (4) A Multimodal Input Sandbox and Context-Defense mechanisms to address threats from non-textual data and long-term semantic poisoning. This paper outlines an evaluation plan designed to quantify the proposed architecture's effectiveness in reducing the Attack Success Rate (ASR) for form-first attacks and to measure its potential latency overhead.

摘要: 大型语言模型（LLM）应用程序的安全性从根本上受到提示注入和越狱等“形式优先”攻击的挑战，其中恶意指令嵌入在用户输入中。依赖于事后输出过滤的传统防御通常很脆弱，无法解决根本原因：模型无法区分可信指令与不可信数据。本文提出了Counterend，这是一种多层安全架构，旨在将防御从反应性、事后姿态转变为主动性、预推理和内推理实施模型。该架构提出了一个旨在从结构上验证和转换所有输入的强化边界，以及一个旨在在生成输出之前限制模型的语义处理路径的内部治理机制。这项工作的主要贡献是以下方面的概念设计：（1）具有强制性、时间耦合的文本加密器的语义边界逻辑（SBL），旨在减少明文提示注入攻击表面，前提是强制执行所有摄入路径。(2)参数空间限制（PPC）机制，利用表示工程的原则，动态控制LLM对内部语义集群的访问，目标是减轻语义漂移和危险的紧急行为。(3)一个安全、自我调节的核心，使用OODA循环和学习安全模块来根据不可变的审计日志调整其防御。(4)多模式输入沙盒和上下文防御机制，可解决来自非文本数据和长期语义中毒的威胁。本文概述了一个评估计划，旨在量化拟议架构在降低形式优先攻击的攻击成功率（ASB）方面的有效性，并衡量其潜在的延迟负担。



## **22. LLMAtKGE: Large Language Models as Explainable Attackers against Knowledge Graph Embeddings**

LLMAtKGE：大型语言模型作为知识图嵌入的可解释攻击者 cs.CL

13 pages

**SubmitDate**: 2025-10-13    [abs](http://arxiv.org/abs/2510.11584v1) [paper-pdf](http://arxiv.org/pdf/2510.11584v1)

**Authors**: Ting Li, Yang Yang, Yipeng Yu, Liang Yao, Guoqing Chao, Ruifeng Xu

**Abstract**: Adversarial attacks on knowledge graph embeddings (KGE) aim to disrupt the model's ability of link prediction by removing or inserting triples. A recent black-box method has attempted to incorporate textual and structural information to enhance attack performance. However, it is unable to generate human-readable explanations, and exhibits poor generalizability. In the past few years, large language models (LLMs) have demonstrated powerful capabilities in text comprehension, generation, and reasoning. In this paper, we propose LLMAtKGE, a novel LLM-based framework that selects attack targets and generates human-readable explanations. To provide the LLM with sufficient factual context under limited input constraints, we design a structured prompting scheme that explicitly formulates the attack as multiple-choice questions while incorporating KG factual evidence. To address the context-window limitation and hesitation issues, we introduce semantics-based and centrality-based filters, which compress the candidate set while preserving high recall of attack-relevant information. Furthermore, to efficiently integrate both semantic and structural information into the filter, we precompute high-order adjacency and fine-tune the LLM with a triple classification task to enhance filtering performance. Experiments on two widely used knowledge graph datasets demonstrate that our attack outperforms the strongest black-box baselines and provides explanations via reasoning, and showing competitive performance compared with white-box methods. Comprehensive ablation and case studies further validate its capability to generate explanations.

摘要: 对知识图嵌入（KGE）的对抗攻击旨在通过删除或插入三重组来破坏模型的链接预测能力。最近的一种黑匣子方法试图合并文本和结构信息以增强攻击性能。然而，它无法生成人类可读的解释，并且表现出较差的概括性。在过去的几年里，大型语言模型（LLM）在文本理解、生成和推理方面展示了强大的能力。在本文中，我们提出了LLMAtKGE，这是一个基于LLM的新型框架，可以选择攻击目标并生成人类可读的解释。为了在有限的输入限制下为LLM提供足够的事实背景，我们设计了一个结构化的提示方案，该方案将攻击明确地制定为多项选择题，同时纳入KG事实证据。为了解决上下文窗口限制和犹豫问题，我们引入了基于语义和基于中心性的过滤器，它们压缩候选集，同时保留攻击相关信息的高召回率。此外，为了有效地将语义和结构信息集成到过滤器中，我们预先计算了高位邻近并通过三重分类任务微调LLM，以增强过滤性能。在两个广泛使用的知识图谱数据集上的实验表明，我们的攻击优于最强的黑盒基线，并通过推理提供解释，与白盒方法相比，表现出有竞争力的性能。全面的消融和病例研究进一步验证了其产生解释的能力。



## **23. The Enemy from Within: A Study of Political Delegitimization Discourse in Israeli Political Speech**

来自内部的敌人：以色列政治演讲中政治去合法化话语的研究 cs.CL

EMNLP 2025

**SubmitDate**: 2025-10-13    [abs](http://arxiv.org/abs/2508.15524v2) [paper-pdf](http://arxiv.org/pdf/2508.15524v2)

**Authors**: Naama Rivlin-Angert, Guy Mor-Lan

**Abstract**: We present the first large-scale computational study of political delegitimization discourse (PDD), defined as symbolic attacks on the normative validity of political entities. We curate and manually annotate a novel Hebrew-language corpus of 10,410 sentences drawn from Knesset speeches (1993-2023), Facebook posts (2018-2021), and leading news outlets, of which 1,812 instances (17.4\%) exhibit PDD and 642 carry additional annotations for intensity, incivility, target type, and affective framing. We introduce a two-stage classification pipeline combining finetuned encoder models and decoder LLMs. Our best model (DictaLM 2.0) attains an F$_1$ of 0.74 for binary PDD detection and a macro-F$_1$ of 0.67 for classification of delegitimization characteristics. Applying this classifier to longitudinal and cross-platform data, we see a marked rise in PDD over three decades, higher prevalence on social media versus parliamentary debate, greater use by male than female politicians, and stronger tendencies among right-leaning actors - with pronounced spikes during election campaigns and major political events. Our findings demonstrate the feasibility and value of automated PDD analysis for understanding democratic discourse.

摘要: 我们对政治去合法性话语（PDD）进行了首次大规模计算研究，PDD被定义为对政治实体规范有效性的象征性攻击。我们策划并手动注释了一个新颖的希伯来语数据库，其中包含10，410个句子，取自以色列议会演讲（1993-2023年）、Facebook帖子（2018-2021年）和领先新闻媒体，其中1，812个实例（17.4%）表现出PDD，642个实例带有强度、礼貌、目标类型和情感框架的额外注释。我们引入了一个两阶段分类流水线，结合了微调编码器模型和解码器LLM。我们的最佳模型（DictaLM 2.0）对于二进制PDD检测，F$_1$为0.74，对于去合法化特征分类，宏F$_1 $为0.67。将这种分类器应用于纵向和跨平台数据，我们看到三十年来PDD显着上升，社交媒体上的流行率高于议会辩论，男性政客的使用率高于女性政客，右倾行为者的倾向更强--在竞选和重大政治活动期间出现明显峰值。我们的研究结果证明了自动PDD分析对于理解民主话语的可行性和价值。



## **24. Large Language Models Are Effective Code Watermarkers**

大型语言模型是有效的代码水印 cs.CR

**SubmitDate**: 2025-10-13    [abs](http://arxiv.org/abs/2510.11251v1) [paper-pdf](http://arxiv.org/pdf/2510.11251v1)

**Authors**: Rui Xu, Jiawei Chen, Zhaoxia Yin, Cong Kong, Xinpeng Zhang

**Abstract**: The widespread use of large language models (LLMs) and open-source code has raised ethical and security concerns regarding the distribution and attribution of source code, including unauthorized redistribution, license violations, and misuse of code for malicious purposes. Watermarking has emerged as a promising solution for source attribution, but existing techniques rely heavily on hand-crafted transformation rules, abstract syntax tree (AST) manipulation, or task-specific training, limiting their scalability and generality across languages. Moreover, their robustness against attacks remains limited. To address these limitations, we propose CodeMark-LLM, an LLM-driven watermarking framework that embeds watermark into source code without compromising its semantics or readability. CodeMark-LLM consists of two core components: (i) Semantically Consistent Embedding module that applies functionality-preserving transformations to encode watermark bits, and (ii) Differential Comparison Extraction module that identifies the applied transformations by comparing the original and watermarked code. Leveraging the cross-lingual generalization ability of LLM, CodeMark-LLM avoids language-specific engineering and training pipelines. Extensive experiments across diverse programming languages and attack scenarios demonstrate its robustness, effectiveness, and scalability.

摘要: 大型语言模型（LLM）和开源代码的广泛使用引发了有关源代码分发和归属的道德和安全问题，包括未经授权的重新分发、违反许可证以及出于恶意目的滥用代码。水印已成为源属性的一种有希望的解决方案，但现有技术严重依赖手工制作的转换规则、抽象语法树（AST）操作或特定任务的训练，限制了它们跨语言的可扩展性和通用性。此外，它们对攻击的稳健性仍然有限。为了解决这些限制，我们提出了CodeMark-LLM，这是一个LLM驱动的水印框架，它将水印嵌入到源代码中，而不会损害其语义或可读性。CodeMark-LLM由两个核心组件组成：（i）语义一致嵌入模块，应用功能保留变换来编码水印位，以及（ii）差异比较提取模块，通过比较原始代码和带水印代码来识别应用的变换。利用LLM的跨语言概括能力，CodeMark-LLM避免了特定于语言的工程和培训管道。跨不同编程语言和攻击场景的广泛实验证明了其稳健性、有效性和可扩展性。



## **25. CoSPED: Consistent Soft Prompt Targeted Data Extraction and Defense**

CoSPP：一致的软提示有针对性的数据提取和防御 cs.CR

**SubmitDate**: 2025-10-13    [abs](http://arxiv.org/abs/2510.11137v1) [paper-pdf](http://arxiv.org/pdf/2510.11137v1)

**Authors**: Yang Zhuochen, Fok Kar Wai, Thing Vrizlynn

**Abstract**: Large language models have gained widespread attention recently, but their potential security vulnerabilities, especially privacy leakage, are also becoming apparent. To test and evaluate for data extraction risks in LLM, we proposed CoSPED, short for Consistent Soft Prompt targeted data Extraction and Defense. We introduce several innovative components, including Dynamic Loss, Additive Loss, Common Loss, and Self Consistency Decoding Strategy, and tested to enhance the consistency of the soft prompt tuning process. Through extensive experimentation with various combinations, we achieved an extraction rate of 65.2% at a 50-token prefix comparison. Our comparisons of CoSPED with other reference works confirm our superior extraction rates. We evaluate CoSPED on more scenarios, achieving Pythia model extraction rate of 51.7% and introducing cross-model comparison. Finally, we explore defense through Rank-One Model Editing and achieve a reduction in the extraction rate to 1.6%, which proves that our analysis of extraction mechanisms can directly inform effective mitigation strategies against soft prompt-based attacks.

摘要: 大型语言模型近年来得到了广泛的关注，但其潜在的安全漏洞，特别是隐私泄露，也越来越明显。为了测试和评估LLM中的数据提取风险，我们提出了CoSPED，即一致性软提示目标数据提取和防御的缩写。我们引入了几个创新的组件，包括动态损失，附加损失，共同损失，和自一致性解码策略，并测试，以提高软提示调整过程的一致性。通过对各种组合的广泛实验，我们在50个令牌前缀比较时实现了65.2%的提取率。我们将CoSPP与其他参考作品进行比较，证实了我们优越的提取率。我们在更多场景下评估了CoSPP，Pythia模型提取率达到51.7%，并引入了跨模型比较。最后，我们通过排名一模型编辑探索防御，并将提取率降低至1.6%，这证明我们对提取机制的分析可以直接为针对基于软预算的攻击的有效缓解策略提供信息。



## **26. A Vision-Language Pre-training Model-Guided Approach for Mitigating Backdoor Attacks in Federated Learning**

一种视觉语言预训练模型引导的联邦学习后门攻击缓解方法 cs.LG

**SubmitDate**: 2025-10-13    [abs](http://arxiv.org/abs/2508.10315v2) [paper-pdf](http://arxiv.org/pdf/2508.10315v2)

**Authors**: Keke Gai, Dongjue Wang, Jing Yu, Liehuang Zhu, Qi Wu

**Abstract**: Defending backdoor attacks in Federated Learning (FL) under heterogeneous client data distributions encounters limitations balancing effectiveness and privacy-preserving, while most existing methods highly rely on the assumption of homogeneous client data distributions or the availability of a clean serve dataset. In this paper, we propose an FL backdoor defense framework, named CLIP-Fed, that utilizes the zero-shot learning capabilities of vision-language pre-training models. Our scheme overcomes the limitations of Non-IID imposed on defense effectiveness by integrating pre-aggregation and post-aggregation defense strategies. CLIP-Fed aligns the knowledge of the global model and CLIP on the augmented dataset using prototype contrastive loss and Kullback-Leibler divergence, so that class prototype deviations caused by backdoor samples are ensured and the correlation between trigger patterns and target labels is eliminated. In order to balance privacy-preserving and coverage enhancement of the dataset against diverse triggers, we further construct and augment the server dataset via using the multimodal large language model and frequency analysis without any client samples. Extensive experiments on representative datasets evidence the effectiveness of CLIP-Fed. Comparing to other existing methods, CLIP-Fed achieves an average reduction in Attack Success Rate, {\em i.e.}, 2.03\% on CIFAR-10 and 1.35\% on CIFAR-10-LT, while improving average Main Task Accuracy by 7.92\% and 0.48\%, respectively. Our codes are available at https://anonymous.4open.science/r/CLIP-Fed.

摘要: 在异类客户端数据分布下防御联邦学习（FL）中的后门攻击在平衡有效性和隐私保护方面遇到了局限性，而大多数现有方法高度依赖于同质客户端数据分布或干净服务数据集的可用性的假设。在本文中，我们提出了一个名为CLIP-Fed的FL后门防御框架，该框架利用视觉语言预训练模型的零射击学习能力。我们的方案通过集成聚合前和聚合后防御策略，克服了非IID对防御有效性的限制。CLIP-Fed使用原型对比损失和Kullback-Leibler分歧将全局模型和CLIP的知识整合在增强数据集中，从而确保后门样本引起的类原型偏差，并消除触发模式和目标标签之间的相关性。为了平衡数据集的隐私保护和覆盖范围增强与不同触发因素的关系，我们通过使用多模式大型语言模型和频率分析来进一步构建和扩展服务器数据集，而无需任何客户端样本。对代表性数据集的广泛实验证明了CLIP-Fed的有效性。与其他现有方法相比，CLIP-Fed实现了攻击成功率的平均降低，{\em i.e.}，CIFAR-10和CIFAR-10-LT分别提高2.03%和1.35%，同时平均主要任务准确性分别提高7.92%和0.48%。我们的代码可在https://anonymous.4open.science/r/CLIP-Fed上获取。



## **27. CoDefend: Cross-Modal Collaborative Defense via Diffusion Purification and Prompt Optimization**

CoDefend：通过扩散净化和即时优化的跨模式协同防御 cs.CV

**SubmitDate**: 2025-10-13    [abs](http://arxiv.org/abs/2510.11096v1) [paper-pdf](http://arxiv.org/pdf/2510.11096v1)

**Authors**: Fengling Zhu, Boshi Liu, Jingyu Hua, Sheng Zhong

**Abstract**: Multimodal Large Language Models (MLLMs) have achieved remarkable success in tasks such as image captioning, visual question answering, and cross-modal reasoning by integrating visual and textual modalities. However, their multimodal nature also exposes them to adversarial threats, where attackers can perturb either modality or both jointly to induce harmful, misleading, or policy violating outputs. Existing defense strategies, such as adversarial training and input purification, face notable limitations: adversarial training typically improves robustness only against known attacks while incurring high computational costs, whereas conventional purification approaches often suffer from degraded image quality and insufficient generalization to complex multimodal tasks.   In this work, we focus on defending the visual modality, which frequently serves as the primary entry point for adversarial manipulation. We propose a supervised diffusion based denoising framework that leverages paired adversarial clean image datasets to fine-tune diffusion models with directional, task specific guidance. Unlike prior unsupervised purification methods such as DiffPure, our approach achieves higher quality reconstructions while significantly improving defense robustness in multimodal tasks. Furthermore, we incorporate prompt optimization as a complementary defense mechanism, enhancing resistance against diverse and unseen attack strategies.   Extensive experiments on image captioning and visual question answering demonstrate that our method not only substantially improves robustness but also exhibits strong transferability to unknown adversarial attacks. These results highlight the effectiveness of supervised diffusion based denoising for multimodal defense, paving the way for more reliable and secure deployment of MLLMs in real world applications.

摘要: 多模式大型语言模型（MLLM）通过集成视觉和文本模式，在图像字幕、视觉问答和跨模式推理等任务中取得了显着的成功。然而，它们的多模式性质也使它们面临对抗威胁，攻击者可以共同扰乱其中一种模式或两者，以引发有害、误导或违反政策的输出。现有的防御策略，例如对抗性训练和输入净化，面临着显着的局限性：对抗性训练通常只提高针对已知攻击的鲁棒性，同时会产生很高的计算成本，而传统的净化方法往往会出现图像质量下降和对复杂多模式任务的概括不足的问题。   在这项工作中，我们专注于捍卫视觉形态，它经常作为对抗性操纵的主要切入点。我们提出了一种基于监督扩散的去噪框架，该框架利用成对的对抗性干净图像数据集，通过定向、特定任务的指导来微调扩散模型。与之前的无监督纯化方法（例如DistPure）不同，我们的方法实现了更高质量的重建，同时显着提高了多模式任务中的防御鲁棒性。此外，我们还将即时优化作为补充防御机制，增强对多样化和不可见攻击策略的抵抗力。   关于图像字幕和视觉问答的大量实验表明，我们的方法不仅大大提高了鲁棒性，而且还表现出对未知对抗攻击的强大可移植性。这些结果凸显了基于监督扩散的去噪对多模式防御的有效性，为在现实世界应用中更可靠、更安全地部署MLLM铺平了道路。



## **28. Can LLMs Handle WebShell Detection? Overcoming Detection Challenges with Behavioral Function-Aware Framework**

LLM可以处理WebShell检测吗？使用行为功能感知框架克服检测挑战 cs.CR

Published as a conference paper at COLM 2025

**SubmitDate**: 2025-10-13    [abs](http://arxiv.org/abs/2504.13811v4) [paper-pdf](http://arxiv.org/pdf/2504.13811v4)

**Authors**: Feijiang Han, Jiaming Zhang, Chuyi Deng, Jianheng Tang, Yunhuai Liu

**Abstract**: WebShell attacks, where malicious scripts are injected into web servers, pose a significant cybersecurity threat. Traditional ML and DL methods are often hampered by challenges such as the need for extensive training data, catastrophic forgetting, and poor generalization. Recently, Large Language Models have emerged as powerful alternatives for code-related tasks, but their potential in WebShell detection remains underexplored. In this paper, we make two contributions: (1) a comprehensive evaluation of seven LLMs, including GPT-4, LLaMA 3.1 70B, and Qwen 2.5 variants, benchmarked against traditional sequence- and graph-based methods using a dataset of 26.59K PHP scripts, and (2) the Behavioral Function-Aware Detection (BFAD) framework, designed to address the specific challenges of applying LLMs to this domain. Our framework integrates three components: a Critical Function Filter that isolates malicious PHP function calls, a Context-Aware Code Extraction strategy that captures the most behaviorally indicative code segments, and Weighted Behavioral Function Profiling that enhances in-context learning by prioritizing the most relevant demonstrations based on discriminative function-level profiles. Our results show that, stemming from their distinct analytical strategies, larger LLMs achieve near-perfect precision but lower recall, while smaller models exhibit the opposite trade-off. However, all baseline models lag behind previous SOTA methods. With the application of BFAD, the performance of all LLMs improves significantly, yielding an average F1 score increase of 13.82%. Notably, larger models now outperform SOTA benchmarks, while smaller models such as Qwen-2.5-Coder-3B achieve performance competitive with traditional methods. This work is the first to explore the feasibility and limitations of LLMs for WebShell detection and provides solutions to address the challenges in this task.

摘要: WebShell攻击（恶意脚本被注入网络服务器）构成了重大的网络安全威胁。传统的ML和DL方法经常受到需要大量训练数据、灾难性遗忘和较差的概括性等挑战的阻碍。最近，大型语言模型已成为代码相关任务的强大替代方案，但它们在WebShell检测中的潜力仍然没有得到充分的开发。在本文中，我们做出了两项贡献：（1）对七种LLM进行了全面评估，包括GPT-4、LLaMA 3.1 70 B和Qwen 2.5变体，使用26.59 K个PHP脚本的数据集针对传统的基于序列和图形的方法进行基准测试，以及（2）行为功能感知检测（BFAD）框架，旨在解决将LLM应用于该领域的特定挑战。我们的框架集成了三个组件：隔离恶意PHP函数调用的关键函数过滤器、捕获最具行为指示性的代码段的上下文感知代码提取策略，以及通过优先考虑最相关的演示来增强上下文学习的加权行为函数剖析基于区分性功能级配置文件。我们的结果表明，由于其不同的分析策略，较大的LLM可以实现近乎完美的精确度，但召回率较低，而较小的模型则表现出相反的权衡。然而，所有基线模型都落后于之前的SOTA方法。随着BFAD的应用，所有LLM的性能都显着提高，F1平均得分提高了13.82%。值得注意的是，大型型号的性能现在优于SOTA基准，而Qwen-2.5-Coder-3B等小型型号的性能与传统方法相比具有竞争力。这项工作是第一个探索LLM用于WebShell检测的可行性和局限性的工作，并提供了解决方案来应对这项任务中的挑战。



## **29. SASER: Stego attacks on open-source LLMs**

SABER：Stego攻击开源LLM cs.CR

**SubmitDate**: 2025-10-12    [abs](http://arxiv.org/abs/2510.10486v1) [paper-pdf](http://arxiv.org/pdf/2510.10486v1)

**Authors**: Ming Tan, Wei Li, Hu Tao, Hailong Ma, Aodi Liu, Qian Chen, Zilong Wang

**Abstract**: Open-source large language models (LLMs) have demonstrated considerable dominance over proprietary LLMs in resolving neural processing tasks, thanks to the collaborative and sharing nature. Although full access to source codes, model parameters, and training data lays the groundwork for transparency, we argue that such a full-access manner is vulnerable to stego attacks, and their ill-effects are not fully understood. In this paper, we conduct a systematic formalization for stego attacks on open-source LLMs by enumerating all possible threat models associated with adversary objectives, knowledge, and capabilities. Therein, the threat posed by adversaries with internal knowledge, who inject payloads and triggers during the model sharing phase, is of practical interest. We go even further and propose the first stego attack on open-source LLMs, dubbed SASER, which wields impacts through identifying targeted parameters, embedding payloads, injecting triggers, and executing payloads sequentially. Particularly, SASER enhances the attack robustness against quantization-based local deployment by de-quantizing the embedded payloads. In addition, to achieve stealthiness, SASER devises the performance-aware importance metric to identify targeted parameters with the least degradation of model performance. Extensive experiments on LlaMA2-7B and ChatGLM3-6B, without quantization, show that the stealth rate of SASER outperforms existing stego attacks (for general DNNs) by up to 98.1%, while achieving the same attack success rate (ASR) of 100%. More importantly, SASER improves ASR on quantized models from 0 to 100% in all settings. We appeal for investigations on countermeasures against SASER in view of the significant attack effectiveness.

摘要: 由于协作和共享的性质，开源大型语言模型（LLM）在解决神经处理任务方面表现出了相对于专有LLM的相当大的主导地位。尽管对源代码、模型参数和训练数据的完全访问为透明度奠定了基础，但我们认为，这种完全访问方式很容易受到隐纹攻击，并且其不良影响尚未被完全理解。在本文中，我们通过列举与对手目标、知识和能力相关的所有可能威胁模型，对开源LLM的隐刻攻击进行了系统的形式化。其中，具有内部知识的对手在模型共享阶段注入有效负载和触发器所构成的威胁具有实际意义。我们更进一步，提出了对开源LLM的第一个隐写攻击，称为SASER，它通过识别目标参数，嵌入有效载荷，注入触发器和顺序执行有效载荷来产生影响。特别地，SASER通过对嵌入的有效载荷进行去量化来增强对基于量化的本地部署的攻击鲁棒性。此外，为了实现隐蔽性，SASER设计了性能感知的重要性度量，以识别目标参数，同时最大限度地降低模型性能。在LlaMA 2 - 7 B和ChatGLM 3 - 6 B上进行的大量实验表明，SASER的隐身率比现有的隐写攻击（对于一般DNN）高出98.1%，同时达到100%的攻击成功率（ASR）。更重要的是，SABER在所有设置中将量化模型的ASB从0%提高到100%。鉴于SABER的攻击效果显着，我们呼吁对针对SABER的反制措施进行调查。



## **30. ArtPerception: ASCII Art-based Jailbreak on LLMs with Recognition Pre-test**

ArtPercept：LLC上的基于ASC艺术的越狱，并进行识别预测试 cs.CR

30 pages, 22 figures. This preprint has been accepted for publication  in Elsevier JOURNAL OF NETWORK AND COMPUTER APPLICATIONS (JNCA)

**SubmitDate**: 2025-10-11    [abs](http://arxiv.org/abs/2510.10281v1) [paper-pdf](http://arxiv.org/pdf/2510.10281v1)

**Authors**: Guan-Yan Yang, Tzu-Yu Cheng, Ya-Wen Teng, Farn Wanga, Kuo-Hui Yeh

**Abstract**: The integration of Large Language Models (LLMs) into computer applications has introduced transformative capabilities but also significant security challenges. Existing safety alignments, which primarily focus on semantic interpretation, leave LLMs vulnerable to attacks that use non-standard data representations. This paper introduces ArtPerception, a novel black-box jailbreak framework that strategically leverages ASCII art to bypass the security measures of state-of-the-art (SOTA) LLMs. Unlike prior methods that rely on iterative, brute-force attacks, ArtPerception introduces a systematic, two-phase methodology. Phase 1 conducts a one-time, model-specific pre-test to empirically determine the optimal parameters for ASCII art recognition. Phase 2 leverages these insights to launch a highly efficient, one-shot malicious jailbreak attack. We propose a Modified Levenshtein Distance (MLD) metric for a more nuanced evaluation of an LLM's recognition capability. Through comprehensive experiments on four SOTA open-source LLMs, we demonstrate superior jailbreak performance. We further validate our framework's real-world relevance by showing its successful transferability to leading commercial models, including GPT-4o, Claude Sonnet 3.7, and DeepSeek-V3, and by conducting a rigorous effectiveness analysis against potential defenses such as LLaMA Guard and Azure's content filters. Our findings underscore that true LLM security requires defending against a multi-modal space of interpretations, even within text-only inputs, and highlight the effectiveness of strategic, reconnaissance-based attacks. Content Warning: This paper includes potentially harmful and offensive model outputs.

摘要: 将大型语言模型（LLM）集成到计算机应用程序中既引入了变革性的能力，也带来了重大的安全挑战。现有的安全对齐主要关注语义解释，使LLM容易受到使用非标准数据表示的攻击。本文介绍了ArtPercept，这是一个新颖的黑匣子越狱框架，它战略性地利用ASC Art来绕过最先进（SOTA）LLM的安全措施。与依赖迭代、暴力攻击的现有方法不同，ArtPercept引入了系统性的两阶段方法。第一阶段进行一次性、特定于模型的预测试，以经验确定ASCI艺术识别的最佳参数。第二阶段利用这些见解发起高效的一次性恶意越狱攻击。我们提出了一种改进的Levenshtein距离（MLD）指标，用于对LLM的识别能力进行更细致的评估。通过对四个SOTA开源LLM的全面实验，我们展示了卓越的越狱性能。我们通过展示其成功可移植到领先商业模型（包括GPT-4 o、Claude Sonnet 3.7和DeepSeek-V3），并通过对LLaMA Guard和Azure内容过滤器等潜在防御措施进行严格的有效性分析，进一步验证我们的框架在现实世界中的相关性。我们的研究结果强调，真正的LLM安全需要防御多模式解释空间，即使是在纯文本输入中，并强调了战略性、基于侦察的攻击的有效性。内容警告：本文包含潜在有害和令人反感的模型输出。



## **31. MetaBreak: Jailbreaking Online LLM Services via Special Token Manipulation**

MetaBreak：通过特殊代币操纵越狱在线LLM服务 cs.CR

**SubmitDate**: 2025-10-11    [abs](http://arxiv.org/abs/2510.10271v1) [paper-pdf](http://arxiv.org/pdf/2510.10271v1)

**Authors**: Wentian Zhu, Zhen Xiang, Wei Niu, Le Guan

**Abstract**: Unlike regular tokens derived from existing text corpora, special tokens are artificially created to annotate structured conversations during the fine-tuning process of Large Language Models (LLMs). Serving as metadata of training data, these tokens play a crucial role in instructing LLMs to generate coherent and context-aware responses. We demonstrate that special tokens can be exploited to construct four attack primitives, with which malicious users can reliably bypass the internal safety alignment of online LLM services and circumvent state-of-the-art (SOTA) external content moderation systems simultaneously. Moreover, we found that addressing this threat is challenging, as aggressive defense mechanisms-such as input sanitization by removing special tokens entirely, as suggested in academia-are less effective than anticipated. This is because such defense can be evaded when the special tokens are replaced by regular ones with high semantic similarity within the tokenizer's embedding space. We systemically evaluated our method, named MetaBreak, on both lab environment and commercial LLM platforms. Our approach achieves jailbreak rates comparable to SOTA prompt-engineering-based solutions when no content moderation is deployed. However, when there is content moderation, MetaBreak outperforms SOTA solutions PAP and GPTFuzzer by 11.6% and 34.8%, respectively. Finally, since MetaBreak employs a fundamentally different strategy from prompt engineering, the two approaches can work synergistically. Notably, empowering MetaBreak on PAP and GPTFuzzer boosts jailbreak rates by 24.3% and 20.2%, respectively.

摘要: 与从现有文本库派生的常规令牌不同，特殊令牌是在大型语言模型（LLM）的微调过程中人为创建的，以注释结构化对话。这些令牌作为训练数据的元数据，在指导LLM生成连贯且上下文感知的响应方面发挥着至关重要的作用。我们证明，可以利用特殊令牌来构建四种攻击基元，恶意用户可以通过这些基元可靠地绕过在线LLM服务的内部安全对齐，并同时绕过最先进的（SOTA）外部内容审核系统。此外，我们发现解决这一威胁具有挑战性，因为积极的防御机制--例如通过完全删除特殊代币来进行输入清理，正如埃及所建议的那样--不如预期有效。这是因为当特殊标记被标记化器嵌入空间内具有高语义相似性的常规标记替换时，可以规避这种防御。我们在实验室环境和商业LLM平台上系统评估了我们的方法（名为MetaBreak）。当不部署内容审核时，我们的方法实现了与SOTA基于预算工程的解决方案相当的越狱率。然而，当进行内容审核时，MetaBreak的表现分别优于SOTA解决方案PAP和GPTFuzzer 11.6%和34.8%。最后，由于MetaBreak采用了与即时工程根本不同的策略，因此这两种方法可以协同工作。值得注意的是，在PAP和GPT Fuzzer上启用MetaBreak可分别将越狱率提高24.3%和20.2%。



## **32. Backdoor Collapse: Eliminating Unknown Threats via Known Backdoor Aggregation in Language Models**

后门崩溃：通过语言模型中的已知后门聚合消除未知威胁 cs.CL

**SubmitDate**: 2025-10-11    [abs](http://arxiv.org/abs/2510.10265v1) [paper-pdf](http://arxiv.org/pdf/2510.10265v1)

**Authors**: Liang Lin, Miao Yu, Moayad Aloqaily, Zhenhong Zhou, Kun Wang, Linsey Pang, Prakhar Mehrotra, Qingsong Wen

**Abstract**: Backdoor attacks are a significant threat to large language models (LLMs), often embedded via public checkpoints, yet existing defenses rely on impractical assumptions about trigger settings. To address this challenge, we propose \ourmethod, a defense framework that requires no prior knowledge of trigger settings. \ourmethod is based on the key observation that when deliberately injecting known backdoors into an already-compromised model, both existing unknown and newly injected backdoors aggregate in the representation space. \ourmethod leverages this through a two-stage process: \textbf{first}, aggregating backdoor representations by injecting known triggers, and \textbf{then}, performing recovery fine-tuning to restore benign outputs. Extensive experiments across multiple LLM architectures demonstrate that: (I) \ourmethod reduces the average Attack Success Rate to 4.41\% across multiple benchmarks, outperforming existing baselines by 28.1\%$\sim$69.3\%$\uparrow$. (II) Clean accuracy and utility are preserved within 0.5\% of the original model, ensuring negligible impact on legitimate tasks. (III) The defense generalizes across different types of backdoors, confirming its robustness in practical deployment scenarios.

摘要: 后门攻击是对大型语言模型（LLM）的重大威胁，通常通过公共检查点嵌入，但现有的防御措施依赖于对触发器设置的不切实际的假设。为了应对这一挑战，我们提出了我们的方法，这是一种不需要事先了解触发器设置的防御框架。我们的方法基于一个关键观察，即当故意将已知后门注入到已经妥协的模型中时，现有的未知后门和新注入的后门都会在表示空间中聚集。\our方法通过两阶段过程利用这一点：\textBF{first}，通过注入已知触发器来聚合后门表示，以及\textBF{then}，执行恢复微调以恢复良性输出。跨多个LLM架构的广泛实验表明：（I）\我们的方法将多个基准测试的平均攻击成功率降低至4.41\%，比现有基线高出28.1%$\sim$69.3 %$\uparrow $。(II)清晰的准确性和实用性保持在原始模型的0.5%以内，确保对合法任务的影响可以忽略不计。(III)该防御扩展到不同类型的后门，确认了其在实际部署场景中的稳健性。



## **33. SecureWebArena: A Holistic Security Evaluation Benchmark for LVLM-based Web Agents**

SecureWebArena：基于LVLM的Web代理的整体安全评估基准 cs.CR

**SubmitDate**: 2025-10-11    [abs](http://arxiv.org/abs/2510.10073v1) [paper-pdf](http://arxiv.org/pdf/2510.10073v1)

**Authors**: Zonghao Ying, Yangguang Shao, Jianle Gan, Gan Xu, Junjie Shen, Wenxin Zhang, Quanchen Zou, Junzheng Shi, Zhenfei Yin, Mingchuan Zhang, Aishan Liu, Xianglong Liu

**Abstract**: Large vision-language model (LVLM)-based web agents are emerging as powerful tools for automating complex online tasks. However, when deployed in real-world environments, they face serious security risks, motivating the design of security evaluation benchmarks. Existing benchmarks provide only partial coverage, typically restricted to narrow scenarios such as user-level prompt manipulation, and thus fail to capture the broad range of agent vulnerabilities. To address this gap, we present \tool{}, the first holistic benchmark for evaluating the security of LVLM-based web agents. \tool{} first introduces a unified evaluation suite comprising six simulated but realistic web environments (\eg, e-commerce platforms, community forums) and includes 2,970 high-quality trajectories spanning diverse tasks and attack settings. The suite defines a structured taxonomy of six attack vectors spanning both user-level and environment-level manipulations. In addition, we introduce a multi-layered evaluation protocol that analyzes agent failures across three critical dimensions: internal reasoning, behavioral trajectory, and task outcome, facilitating a fine-grained risk analysis that goes far beyond simple success metrics. Using this benchmark, we conduct large-scale experiments on 9 representative LVLMs, which fall into three categories: general-purpose, agent-specialized, and GUI-grounded. Our results show that all tested agents are consistently vulnerable to subtle adversarial manipulations and reveal critical trade-offs between model specialization and security. By providing (1) a comprehensive benchmark suite with diverse environments and a multi-layered evaluation pipeline, and (2) empirical insights into the security challenges of modern LVLM-based web agents, \tool{} establishes a foundation for advancing trustworthy web agent deployment.

摘要: 基于大型视觉语言模型（LVLM）的Web代理正在成为自动化复杂在线任务的强大工具。然而，当部署在现实世界环境中时，它们面临着严重的安全风险，从而推动了安全评估基准的设计。现有的基准仅提供部分覆盖，通常仅限于用户级提示操纵等狭窄场景，因此无法捕获广泛的代理漏洞。为了解决这一差距，我们提出了\tool{}，这是第一个用于评估基于LVLM的Web代理安全性的整体基准。\tool{}首先引入了一个统一的评估套件，该套件由六个模拟但真实的Web环境（例如，电子商务平台、社区论坛）组成，并包括跨越不同任务和攻击设置的2，970个高质量轨迹。该套件定义了涵盖用户级和环境级操作的六种攻击载体的结构化分类法。此外，我们还引入了一种多层评估协议，该协议在三个关键维度上分析代理失败：内部推理、行为轨迹和任务结果，从而促进了远超出简单成功指标的细粒度风险分析。使用这个基准，我们对9个有代表性的LVLM进行了大规模实验，这些LVLM分为三类：通用、代理专用和基于图形界面的。我们的结果表明，所有测试的代理都始终容易受到微妙的对抗操纵的影响，并揭示了模型专业化和安全性之间的关键权衡。通过提供（1）具有多样化环境和多层评估管道的全面基准套件，以及（2）对现代基于LVLM的Web代理的安全挑战的经验见解，\tool{}为推进值得信赖的Web代理部署奠定了基础。



## **34. Safety-Aligned Weights Are Not Enough: Refusal-Teacher-Guided Finetuning Enhances Safety and Downstream Performance under Harmful Finetuning Attacks**

安全一致的权重还不够：裁判教师指导的微调在有害的微调攻击下提高安全性和下游性能 cs.CL

**SubmitDate**: 2025-10-11    [abs](http://arxiv.org/abs/2506.07356v2) [paper-pdf](http://arxiv.org/pdf/2506.07356v2)

**Authors**: Seokil Ham, Yubin Choi, Yujin Yang, Seungju Cho, Younghun Kim, Changick Kim

**Abstract**: Recently, major AI providers such as Google and OpenAI have introduced Finetuning-as-a-Service (FaaS), which allows users to customize Large Language Models (LLMs) using their own data. However, this service is vulnerable to safety degradation when user data includes harmful prompts, a threat known as harmful finetuning attacks. Prior works attempt to mitigate this issue by first constructing safety-aligned model and then finetuning the model on user data. However, we observe that the safety-aligned weights provide weak initialization for downstream task learning, leading to suboptimal safety-alignment and downstream task performance. To address this, we propose a Refusal-Teacher (Ref-Teacher)-guided finetuning framework. Instead of finetuning a safety-aligned model on user data, our approach directly finetunes the base model under the guidance of a safety-aligned Ref-Teacher, which filters harmful prompts from user data and distills safety-alignment knowledge into the base model. Extensive experiments demonstrate that our Ref-Teacher-guided finetuning strategy effectively minimizes harmful outputs and enhances finetuning accuracy for user-specific tasks, offering a practical solution for secure and reliable deployment of LLMs in FaaS.

摘要: 最近，Google和OpenAI等主要人工智能提供商推出了Finetuning-as-a-Service（Faas），允许用户使用自己的数据自定义大型语言模型（LLM）。然而，当用户数据包括有害提示（称为有害微调攻击）时，该服务很容易出现安全性下降。之前的作品试图通过首先构建安全一致的模型，然后根据用户数据微调模型来缓解这个问题。然而，我们观察到安全对齐的权重为下游任务学习提供了弱的初始化，导致次优的安全对齐和下游任务性能。为了解决这个问题，我们提出了一个裁判-教师（Ref-Teacher）引导的微调框架。我们的方法不是在用户数据上微调安全一致的模型，而是在安全一致的参考教师的指导下直接微调基本模型，该教师从用户数据中过滤有害提示，并将安全一致知识提炼到基本模型中。大量实验表明，我们的参考教师引导的微调策略可以有效地最大限度地减少有害输出，并提高用户特定任务的微调准确性，为FSaaS中安全可靠地部署LLM提供了实用的解决方案。



## **35. When Vision Fails: Text Attacks Against ViT and OCR**

当Vision失败时：针对ViT和OCR的文本攻击 cs.CR

To appear in the 2nd ACM Workshop on Large AI Systems and Models with  Privacy and Security Analysis

**SubmitDate**: 2025-10-11    [abs](http://arxiv.org/abs/2306.07033v2) [paper-pdf](http://arxiv.org/pdf/2306.07033v2)

**Authors**: Nicholas Boucher, Jenny Blessing, Ilia Shumailov, Ross Anderson, Nicolas Papernot

**Abstract**: Text-based machine learning models are vulnerable to an emerging class of Unicode-based adversarial examples capable of tricking a model into misreading text with potentially disastrous effects. The primary existing defense against these attacks is to preprocess potentially malicious text inputs using optical character recognition (OCR). In theory, OCR models will ignore any malicious Unicode characters and will extract the visually correct input to be fed to the model. In this work, we show that these visual defenses fail to prevent this type of attack. We use a genetic algorithm to generate visual adversarial examples (i.e., OCR outputs) in a black-box setting, demonstrating a highly effective novel attack that substantially reduces the accuracy of OCR and other visual models. Specifically, we use the Unicode functionality of combining characters (e.g., \~n which combines the characters n and ~) to manipulate text inputs so that small visual perturbations appear when the text is displayed. We demonstrate the effectiveness of these attacks in the real world by creating adversarial examples against production models published by Meta, Microsoft, IBM, and Google. We additionally conduct a user study to establish that the model-fooling adversarial examples do not affect human comprehension of the text, showing that language models are uniquely vulnerable to this type of text attack.

摘要: 基于文本的机器学习模型很容易受到一类新兴的基于Unicode的对抗性示例的影响，这些示例能够欺骗模型误读文本，从而产生潜在的灾难性影响。针对这些攻击的主要现有防御措施是使用光学字符识别（OCR）预处理潜在的恶意文本输入。理论上，OCR模型将忽略任何恶意Unicode字符，并提取视觉上正确的输入以反馈给模型。在这项工作中，我们表明这些视觉防御无法阻止这种类型的攻击。我们使用遗传算法来生成视觉对抗示例（即，OCR输出）在黑匣子环境中，展示了一种高效的新颖攻击，可以大幅降低OCR和其他视觉模型的准确性。具体来说，我们使用组合字符的Unicode功能（例如，\~n组合了字符n和~）来操纵文本输入，以便在显示文本时出现小的视觉扰动。我们通过针对Meta、Microsoft、IBM和Google发布的生产模型创建对抗性示例来展示这些攻击在现实世界中的有效性。我们还进行了一项用户研究，以确定欺骗模型的对抗性示例不会影响人类对文本的理解，这表明语言模型特别容易受到这种类型的文本攻击。



## **36. RIPRAG: Hack a Black-box Retrieval-Augmented Generation Question-Answering System with Reinforcement Learning**

RIPRAG：利用强化学习破解黑匣子检索增强一代语音响应系统 cs.AI

**SubmitDate**: 2025-10-11    [abs](http://arxiv.org/abs/2510.10008v1) [paper-pdf](http://arxiv.org/pdf/2510.10008v1)

**Authors**: Meng Xi, Sihan Lv, Yechen Jin, Guanjie Cheng, Naibo Wang, Ying Li, Jianwei Yin

**Abstract**: Retrieval-Augmented Generation (RAG) systems based on Large Language Models (LLMs) have become a core technology for tasks such as question-answering (QA) and content generation. However, by injecting poisoned documents into the database of RAG systems, attackers can manipulate LLMs to generate text that aligns with their intended preferences. Existing research has primarily focused on white-box attacks against simplified RAG architectures. In this paper, we investigate a more complex and realistic scenario: the attacker lacks knowledge of the RAG system's internal composition and implementation details, and the RAG system comprises components beyond a mere retriever. Specifically, we propose the RIPRAG attack framework, an end-to-end attack pipeline that treats the target RAG system as a black box, where the only information accessible to the attacker is whether the poisoning succeeds. Our method leverages Reinforcement Learning (RL) to optimize the generation model for poisoned documents, ensuring that the generated poisoned document aligns with the target RAG system's preferences. Experimental results demonstrate that this method can effectively execute poisoning attacks against most complex RAG systems, achieving an attack success rate (ASR) improvement of up to 0.72 compared to baseline methods. This highlights prevalent deficiencies in current defensive methods and provides critical insights for LLM security research.

摘要: 基于大型语言模型（LLM）的检索增强生成（RAG）系统已成为问答（QA）和内容生成等任务的核心技术。然而，通过将有毒文档注入RAG系统的数据库中，攻击者可以操纵LLM来生成与其预期偏好一致的文本。现有的研究主要集中在针对简化RAG架构的白盒攻击。在本文中，我们研究了一个更复杂和现实的场景：攻击者缺乏对RAG系统的内部组成和实现细节的了解，而RAG系统所包含的组件不仅仅是一个检索器。具体来说，我们提出了RIPRAG攻击框架，这是一种端到端攻击管道，将目标RAG系统视为黑匣子，其中攻击者可以访问的唯一信息是中毒是否成功。我们的方法利用强化学习（RL）来优化中毒文档的生成模型，确保生成的中毒文档与目标RAG系统的偏好一致。实验结果表明，该方法可以有效地对大多数复杂的RAG系统执行中毒攻击，与基线方法相比，攻击成功率（ASB）提高高达0.72。这凸显了当前防御方法中的普遍缺陷，并为LLM安全研究提供了重要见解。



## **37. Attack via Overfitting: 10-shot Benign Fine-tuning to Jailbreak LLMs**

通过过度装配攻击：10杆Benign微调到越狱LLM cs.CR

**SubmitDate**: 2025-10-11    [abs](http://arxiv.org/abs/2510.02833v2) [paper-pdf](http://arxiv.org/pdf/2510.02833v2)

**Authors**: Zhixin Xie, Xurui Song, Jun Luo

**Abstract**: Despite substantial efforts in safety alignment, recent research indicates that Large Language Models (LLMs) remain highly susceptible to jailbreak attacks. Among these attacks, finetuning-based ones that compromise LLMs' safety alignment via fine-tuning stand out due to its stable jailbreak performance. In particular, a recent study indicates that fine-tuning with as few as 10 harmful question-answer (QA) pairs can lead to successful jailbreaking across various harmful questions. However, such malicious fine-tuning attacks are readily detectable and hence thwarted by moderation models. In this paper, we demonstrate that LLMs can be jailbroken by fine-tuning with only 10 benign QA pairs; our attack exploits the increased sensitivity of LLMs to fine-tuning data after being overfitted. Specifically, our fine-tuning process starts with overfitting an LLM via fine-tuning with benign QA pairs involving identical refusal answers. Further fine-tuning is then performed with standard benign answers, causing the overfitted LLM to forget the refusal attitude and thus provide compliant answers regardless of the harmfulness of a question. We implement our attack on the ten LLMs and compare it with five existing baselines. Experiments demonstrate that our method achieves significant advantages in both attack effectiveness and attack stealth. Our findings expose previously unreported security vulnerabilities in current LLMs and provide a new perspective on understanding how LLMs' security is compromised, even with benign fine-tuning. Our code is available at https://github.com/ZHIXINXIE/tenBenign.

摘要: 尽管在安全调整方面做出了大量努力，但最近的研究表明，大型语言模型（LLM）仍然极易受到越狱攻击。在这些攻击中，通过微调损害LLM安全性的基于微调的攻击因其稳定的越狱性能而脱颖而出。特别是，最近的一项研究表明，只需10个有害问答（QA）对进行微调，就可以在各种有害问题上成功越狱。然而，此类恶意微调攻击很容易被检测到，因此会被审核模型阻止。在本文中，我们证明了仅使用10个良性QA对就可以通过微调来越狱LLM;我们的攻击利用了LLM在过适应后对微调数据的敏感性增加。具体来说，我们的微调过程首先是通过使用涉及相同拒绝答案的良性QA对进行微调来过度调整LLM。然后用标准的良性答案进行进一步的微调，导致过度适应的LLM忘记拒绝态度，从而无论问题的危害性如何，都提供合规的答案。我们对十个LLM实施攻击，并将其与五个现有基线进行比较。实验表明，我们的方法在攻击有效性和攻击隐蔽性方面都取得了显着的优势。我们的研究结果暴露了当前LLM中以前未报告的安全漏洞，并为了解LLM的安全性如何受到损害提供了一个新的视角，即使进行了良性的微调。我们的代码可在https://github.com/ZHIXINXIE/tenBenign上获取。



## **38. CoBia: Constructed Conversations Can Trigger Otherwise Concealed Societal Biases in LLMs**

CoBia：构建的对话可能会引发LLM中隐藏的社会偏见 cs.CL

EMNLP 2025 (Oral)

**SubmitDate**: 2025-10-10    [abs](http://arxiv.org/abs/2510.09871v1) [paper-pdf](http://arxiv.org/pdf/2510.09871v1)

**Authors**: Nafiseh Nikeghbal, Amir Hossein Kargaran, Jana Diesner

**Abstract**: Improvements in model construction, including fortified safety guardrails, allow Large language models (LLMs) to increasingly pass standard safety checks. However, LLMs sometimes slip into revealing harmful behavior, such as expressing racist viewpoints, during conversations. To analyze this systematically, we introduce CoBia, a suite of lightweight adversarial attacks that allow us to refine the scope of conditions under which LLMs depart from normative or ethical behavior in conversations. CoBia creates a constructed conversation where the model utters a biased claim about a social group. We then evaluate whether the model can recover from the fabricated bias claim and reject biased follow-up questions. We evaluate 11 open-source as well as proprietary LLMs for their outputs related to six socio-demographic categories that are relevant to individual safety and fair treatment, i.e., gender, race, religion, nationality, sex orientation, and others. Our evaluation is based on established LLM-based bias metrics, and we compare the results against human judgments to scope out the LLMs' reliability and alignment. The results suggest that purposefully constructed conversations reliably reveal bias amplification and that LLMs often fail to reject biased follow-up questions during dialogue. This form of stress-testing highlights deeply embedded biases that can be surfaced through interaction. Code and artifacts are available at https://github.com/nafisenik/CoBia.

摘要: 模型构造的改进，包括强化的安全护栏，使大型语言模型（LLM）越来越多地通过标准安全检查。然而，LLM有时会在谈话中透露有害行为，例如表达种族主义观点。为了系统地分析这一点，我们引入了CoBia，这是一套轻量级的对抗性攻击，使我们能够细化LLM在对话中偏离规范或道德行为的条件范围。CoBia创建了一个构建的对话，其中模型对一个社会群体发表了有偏见的声明。然后，我们评估该模型是否可以从捏造的偏见主张中恢复过来并拒绝有偏见的后续问题。我们评估了11种开源和专有LLM的输出，以评估其与与个人安全和公平待遇相关的六个社会人口类别相关，即性别、种族、宗教、国籍、性取向等。我们的评估基于既定的基于LLM的偏差指标，我们将结果与人类判断进行比较，以确定LLM的可靠性和一致性。结果表明，有目的地构建的对话可靠地揭示偏见放大和LLM往往无法拒绝偏见的后续问题在对话中。这种形式的压力测试突出了可以通过互动暴露出来的根深蒂固的偏见。代码和工件可在https://github.com/nafisenik/CoBia上获得。



## **39. Text Prompt Injection of Vision Language Models**

视觉语言模型的文本提示注入 cs.CL

**SubmitDate**: 2025-10-10    [abs](http://arxiv.org/abs/2510.09849v1) [paper-pdf](http://arxiv.org/pdf/2510.09849v1)

**Authors**: Ruizhe Zhu

**Abstract**: The widespread application of large vision language models has significantly raised safety concerns. In this project, we investigate text prompt injection, a simple yet effective method to mislead these models. We developed an algorithm for this type of attack and demonstrated its effectiveness and efficiency through experiments. Compared to other attack methods, our approach is particularly effective for large models without high demand for computational resources.

摘要: 大型视觉语言模型的广泛应用显着引发了安全问题。在这个项目中，我们研究了文本提示注入，这是一种简单而有效的误导这些模型的方法。我们针对此类攻击开发了一种算法，并通过实验证明了其有效性和效率。与其他攻击方法相比，我们的方法对于计算资源需求不高的大型模型特别有效。



## **40. Robustness in Both Domains: CLIP Needs a Robust Text Encoder**

两个领域的稳健性：CLIP需要强大的文本编码器 cs.LG

Accepted in NeurIPS 2025

**SubmitDate**: 2025-10-10    [abs](http://arxiv.org/abs/2506.03355v2) [paper-pdf](http://arxiv.org/pdf/2506.03355v2)

**Authors**: Elias Abad Rocamora, Christian Schlarmann, Naman Deep Singh, Yongtao Wu, Matthias Hein, Volkan Cevher

**Abstract**: Adversarial input attacks can cause a significant shift of CLIP embeddings. This can affect the downstream robustness of models incorporating CLIP in the pipeline, such as text-to-image generative models or large vision language models. While some efforts have been done towards making the CLIP image encoders robust, the robustness of text encoders remains unexplored. In this work, we cover this gap in the literature. We propose LEAF: an efficient adversarial finetuning method for the text domain, with the ability to scale to large CLIP models. Our models significantly improve the zero-shot adversarial accuracy in the text domain, while maintaining the vision performance provided by robust image encoders. When combined with text-to-image diffusion models, we can improve the generation quality under adversarial noise. In multimodal retrieval tasks, LEAF improves the recall under adversarial noise over standard CLIP models. Finally, we show that robust text encoders facilitate better reconstruction of input text from its embedding via direct optimization. We open-source our code ( https://github.com/LIONS-EPFL/LEAF ) and models ( https://huggingface.co/LEAF-CLIP ).

摘要: 对抗性输入攻击可能会导致CLIP嵌入的显着转变。这可能会影响在管道中纳入CLIP的模型的下游稳健性，例如文本到图像生成模型或大视觉语言模型。虽然已经做出了一些努力来使CLIP图像编码器稳健，但文本编码器的稳健性仍然有待探索。在这项工作中，我们涵盖了文献中的这一空白。我们提出LEAF：一种针对文本领域的高效对抗微调方法，能够扩展到大型CLIP模型。我们的模型显着提高了文本域中的零镜头对抗准确性，同时保持稳健的图像编码器提供的视觉性能。当与文本到图像扩散模型相结合时，我们可以提高对抗性噪音下的生成质量。在多模式检索任务中，LEAF比标准CLIP模型提高了对抗性噪音下的召回率。最后，我们表明稳健的文本编码器可以通过直接优化从嵌入中更好地重建输入文本。我们开源我们的代码（https://github.com/LIONS-EPFL/LEAF）和模型（https://huggingface.co/LEAF-CLIP）。



## **41. Fewer Weights, More Problems: A Practical Attack on LLM Pruning**

更少的权重，更多的问题：对LLM修剪的实用攻击 cs.LG

**SubmitDate**: 2025-10-10    [abs](http://arxiv.org/abs/2510.07985v2) [paper-pdf](http://arxiv.org/pdf/2510.07985v2)

**Authors**: Kazuki Egashira, Robin Staab, Thibaud Gloaguen, Mark Vero, Martin Vechev

**Abstract**: Model pruning, i.e., removing a subset of model weights, has become a prominent approach to reducing the memory footprint of large language models (LLMs) during inference. Notably, popular inference engines, such as vLLM, enable users to conveniently prune downloaded models before they are deployed. While the utility and efficiency of pruning methods have improved significantly, the security implications of pruning remain underexplored. In this work, for the first time, we show that modern LLM pruning methods can be maliciously exploited. In particular, an adversary can construct a model that appears benign yet, once pruned, exhibits malicious behaviors. Our method is based on the idea that the adversary can compute a proxy metric that estimates how likely each parameter is to be pruned. With this information, the adversary can first inject a malicious behavior into those parameters that are unlikely to be pruned. Then, they can repair the model by using parameters that are likely to be pruned, effectively canceling out the injected behavior in the unpruned model. We demonstrate the severity of our attack through extensive evaluation on five models; after any of the pruning in vLLM are applied (Magnitude, Wanda, and SparseGPT), it consistently exhibits strong malicious behaviors in a diverse set of attack scenarios (success rates of up to $95.7\%$ for jailbreak, $98.7\%$ for benign instruction refusal, and $99.5\%$ for targeted content injection). Our results reveal a critical deployment-time security gap and underscore the urgent need for stronger security awareness in model compression.

摘要: 模型修剪，即删除模型权重的子集已成为减少推理期间大型语言模型（LLM）内存占用的一种主要方法。值得注意的是，vLLM等流行推理引擎使用户能够在部署下载的模型之前方便地修剪它们。虽然修剪方法的实用性和效率有了显着提高，但修剪的安全影响仍然没有得到充分的研究。在这项工作中，我们首次表明现代LLM修剪方法可以被恶意利用。特别是，对手可以构建一个看起来良性但一旦修剪，就会表现出恶意行为的模型。我们的方法基于这样的想法：对手可以计算代理指标，该指标估计每个参数被修剪的可能性。有了这些信息，对手可以首先将恶意行为注入到那些不太可能被修剪的参数中。然后，他们可以通过使用可能被修剪的参数来修复模型，从而有效地抵消未修剪模型中注入的行为。我们通过对五个模型的广泛评估来证明攻击的严重性;应用vLLM中的任何修剪（Magnitude、Wanda和SparseGPT）后，它在各种攻击场景中始终表现出强烈的恶意行为（越狱成功率高达95.7美元，良性指令拒绝成功率高达98.7美元，定向内容注入成功率高达99.5美元）。我们的结果揭示了一个关键的部署时安全差距，并强调了模型压缩中迫切需要更强的安全意识。



## **42. Exploiting Web Search Tools of AI Agents for Data Exfiltration**

利用人工智能代理的网络搜索工具进行数据过滤 cs.CR

9 pages, 6 figures, conference article

**SubmitDate**: 2025-10-10    [abs](http://arxiv.org/abs/2510.09093v1) [paper-pdf](http://arxiv.org/pdf/2510.09093v1)

**Authors**: Dennis Rall, Bernhard Bauer, Mohit Mittal, Thomas Fraunholz

**Abstract**: Large language models (LLMs) are now routinely used to autonomously execute complex tasks, from natural language processing to dynamic workflows like web searches. The usage of tool-calling and Retrieval Augmented Generation (RAG) allows LLMs to process and retrieve sensitive corporate data, amplifying both their functionality and vulnerability to abuse. As LLMs increasingly interact with external data sources, indirect prompt injection emerges as a critical and evolving attack vector, enabling adversaries to exploit models through manipulated inputs. Through a systematic evaluation of indirect prompt injection attacks across diverse models, we analyze how susceptible current LLMs are to such attacks, which parameters, including model size and manufacturer, specific implementations, shape their vulnerability, and which attack methods remain most effective. Our results reveal that even well-known attack patterns continue to succeed, exposing persistent weaknesses in model defenses. To address these vulnerabilities, we emphasize the need for strengthened training procedures to enhance inherent resilience, a centralized database of known attack vectors to enable proactive defense, and a unified testing framework to ensure continuous security validation. These steps are essential to push developers toward integrating security into the core design of LLMs, as our findings show that current models still fail to mitigate long-standing threats.

摘要: 大型语言模型（LLM）现在通常用于自主执行复杂任务，从自然语言处理到网络搜索等动态工作流程。工具调用和检索增强生成（RAG）的使用使LLM能够处理和检索敏感的企业数据，增强其功能和滥用的脆弱性。随着LLM越来越多地与外部数据源互动，间接提示注入成为一种关键且不断发展的攻击载体，使对手能够通过操纵的输入来利用模型。通过对不同模型中的间接即时注入攻击进行系统评估，我们分析了当前LLM对此类攻击的敏感性，哪些参数（包括模型大小和制造商、特定实现）影响了其漏洞，以及哪些攻击方法仍然最有效。我们的结果表明，即使是众所周知的攻击模式也会继续成功，暴露了模型防御中的持续弱点。为了解决这些漏洞，我们强调需要加强训练程序以增强固有的弹性、已知攻击载体的集中式数据库以实现主动防御，以及统一的测试框架以确保持续的安全验证。这些步骤对于推动开发人员将安全性集成到LLM的核心设计中至关重要，因为我们的研究结果表明，当前的模型仍然无法缓解长期存在的威胁。



## **43. Group-Adaptive Adversarial Learning for Robust Fake News Detection Against Malicious Comments**

群体自适应对抗学习用于针对恶意评论的鲁棒假新闻检测 cs.LG

10 pages, 12 figures

**SubmitDate**: 2025-10-10    [abs](http://arxiv.org/abs/2510.09712v1) [paper-pdf](http://arxiv.org/pdf/2510.09712v1)

**Authors**: Zhao Tong, Chunlin Gong, Yimeng Gu, Haichao Shi, Qiang Liu, Shu Wu, Xiao-Yu Zhang

**Abstract**: The spread of fake news online distorts public judgment and erodes trust in social media platforms. Although recent fake news detection (FND) models perform well in standard settings, they remain vulnerable to adversarial comments-authored by real users or by large language models (LLMs)-that subtly shift model decisions. In view of this, we first present a comprehensive evaluation of comment attacks to existing fake news detectors and then introduce a group-adaptive adversarial training strategy to improve the robustness of FND models. To be specific, our approach comprises three steps: (1) dividing adversarial comments into three psychologically grounded categories: perceptual, cognitive, and societal; (2) generating diverse, category-specific attacks via LLMs to enhance adversarial training; and (3) applying a Dirichlet-based adaptive sampling mechanism (InfoDirichlet Adjusting Mechanism) that dynamically adjusts the learning focus across different comment categories during training. Experiments on benchmark datasets show that our method maintains strong detection accuracy while substantially increasing robustness to a wide range of adversarial comment perturbations.

摘要: 假新闻在网上的传播扭曲了公众的判断并削弱了对社交媒体平台的信任。尽管最近的假新闻检测（FND）模型在标准设置中表现良好，但它们仍然容易受到由真实用户或大型语言模型（LLM）撰写的对抗性评论的影响，这些评论微妙地改变了模型的决策。鉴于此，我们首先对现有假新闻检测器的评论攻击进行全面评估，然后引入群体自适应对抗训练策略来提高FND模型的鲁棒性。具体来说，我们的方法包括三个步骤：（1）将对抗性评论分为三个基于心理的类别：感知、认知和社会;（2）通过LLM生成多样化的、特定类别的攻击，以加强对抗性训练;以及（3）应用基于Dirichlet的自适应采样机制（InfoDirichlet调整机制），在训练期间动态调整不同评论类别的学习重点。对基准数据集的实验表明，我们的方法保持了很强的检测准确性，同时大幅提高了对广泛对抗性评论扰动的鲁棒性。



## **44. P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs**

P2P：在LLM中实现可靠后门防御的毒药对毒药补救措施 cs.CR

**SubmitDate**: 2025-10-10    [abs](http://arxiv.org/abs/2510.04503v2) [paper-pdf](http://arxiv.org/pdf/2510.04503v2)

**Authors**: Shuai Zhao, Xinyi Wu, Shiqian Zhao, Xiaobao Wu, Zhongliang Guo, Yanhao Jia, Anh Tuan Luu

**Abstract**: During fine-tuning, large language models (LLMs) are increasingly vulnerable to data-poisoning backdoor attacks, which compromise their reliability and trustworthiness. However, existing defense strategies suffer from limited generalization: they only work on specific attack types or task settings. In this study, we propose Poison-to-Poison (P2P), a general and effective backdoor defense algorithm. P2P injects benign triggers with safe alternative labels into a subset of training samples and fine-tunes the model on this re-poisoned dataset by leveraging prompt-based learning. This enforces the model to associate trigger-induced representations with safe outputs, thereby overriding the effects of original malicious triggers. Thanks to this robust and generalizable trigger-based fine-tuning, P2P is effective across task settings and attack types. Theoretically and empirically, we show that P2P can neutralize malicious backdoors while preserving task performance. We conduct extensive experiments on classification, mathematical reasoning, and summary generation tasks, involving multiple state-of-the-art LLMs. The results demonstrate that our P2P algorithm significantly reduces the attack success rate compared with baseline models. We hope that the P2P can serve as a guideline for defending against backdoor attacks and foster the development of a secure and trustworthy LLM community.

摘要: 在微调期间，大型语言模型（LLM）越来越容易受到数据中毒后门攻击，从而损害其可靠性和可信性。然而，现有的防御策略的概括性有限：它们仅适用于特定的攻击类型或任务设置。在这项研究中，我们提出了Poison-to-Poison（P2P），这是一种通用且有效的后门防御算法。P2P将具有安全替代标签的良性触发器注入到训练样本的子集中，并通过利用基于预算的学习在这个重新中毒的数据集上微调模型。这迫使模型将操作员引发的表示与安全输出相关联，从而覆盖原始恶意触发器的影响。得益于这种强大且可推广的基于服务器的微调，P2P在任务设置和攻击类型中都有效。从理论上和经验上，我们表明P2P可以中和恶意后门，同时保持任务性能。我们对分类、数学推理和摘要生成任务进行了广泛的实验，涉及多个最先进的LLM。实验结果表明，与基准模型相比，我们的P2P算法显著降低了攻击成功率。我们希望P2P可以作为防御后门攻击的指导方针，并促进安全和值得信赖的LLM社区的发展。



## **45. Pattern Enhanced Multi-Turn Jailbreaking: Exploiting Structural Vulnerabilities in Large Language Models**

模式增强的多回合越狱：利用大型语言模型中的结构漏洞 cs.CL

**SubmitDate**: 2025-10-09    [abs](http://arxiv.org/abs/2510.08859v1) [paper-pdf](http://arxiv.org/pdf/2510.08859v1)

**Authors**: Ragib Amin Nihal, Rui Wen, Kazuhiro Nakadai, Jun Sakuma

**Abstract**: Large language models (LLMs) remain vulnerable to multi-turn jailbreaking attacks that exploit conversational context to bypass safety constraints gradually. These attacks target different harm categories (like malware generation, harassment, or fraud) through distinct conversational approaches (educational discussions, personal experiences, hypothetical scenarios). Existing multi-turn jailbreaking methods often rely on heuristic or ad hoc exploration strategies, providing limited insight into underlying model weaknesses. The relationship between conversation patterns and model vulnerabilities across harm categories remains poorly understood. We propose Pattern Enhanced Chain of Attack (PE-CoA), a framework of five conversation patterns to construct effective multi-turn jailbreaks through natural dialogue. Evaluating PE-CoA on twelve LLMs spanning ten harm categories, we achieve state-of-the-art performance, uncovering pattern-specific vulnerabilities and LLM behavioral characteristics: models exhibit distinct weakness profiles where robustness to one conversational pattern does not generalize to others, and model families share similar failure modes. These findings highlight limitations of safety training and indicate the need for pattern-aware defenses. Code available on: https://github.com/Ragib-Amin-Nihal/PE-CoA

摘要: 大型语言模型（LLM）仍然容易受到多轮越狱攻击，这些攻击利用对话上下文逐渐绕过安全限制。这些攻击通过不同的对话方法（教育讨论、个人体验、假设场景）针对不同的伤害类别（例如恶意软件生成、骚扰或欺诈）。现有的多回合越狱方法通常依赖于启发式或临时探索策略，对潜在模型弱点的洞察有限。人们对不同伤害类别的对话模式和模型漏洞之间的关系仍然知之甚少。我们提出了模式增强型攻击链（PE-CoA），这是一个由五种对话模式组成的框架，通过自然对话构建有效的多回合越狱。通过评估涵盖十种伤害类别的十二种LLM的PE-CoA，我们实现了最先进的性能，揭示了特定模式的漏洞和LLM行为特征：模型表现出明显的弱点特征，其中对一种对话模式的稳健性不会推广到其他模式，并且模型家族有相似的失败模式。这些发现凸显了安全培训的局限性，并表明需要模式感知防御。代码可访问：https://github.com/Ragib-Amin-Nihal/PE-CoA



## **46. The Model's Language Matters: A Comparative Privacy Analysis of LLMs**

模型的语言很重要：法学硕士的隐私比较分析 cs.CL

**SubmitDate**: 2025-10-09    [abs](http://arxiv.org/abs/2510.08813v1) [paper-pdf](http://arxiv.org/pdf/2510.08813v1)

**Authors**: Abhishek K. Mishra, Antoine Boutet, Lucas Magnana

**Abstract**: Large Language Models (LLMs) are increasingly deployed across multilingual applications that handle sensitive data, yet their scale and linguistic variability introduce major privacy risks. Mostly evaluated for English, this paper investigates how language structure affects privacy leakage in LLMs trained on English, Spanish, French, and Italian medical corpora. We quantify six linguistic indicators and evaluate three attack vectors: extraction, counterfactual memorization, and membership inference. Results show that privacy vulnerability scales with linguistic redundancy and tokenization granularity: Italian exhibits the strongest leakage, while English shows higher membership separability. In contrast, French and Spanish display greater resilience due to higher morphological complexity. Overall, our findings provide the first quantitative evidence that language matters in privacy leakage, underscoring the need for language-aware privacy-preserving mechanisms in LLM deployments.

摘要: 大型语言模型（LLM）越来越多地部署在处理敏感数据的多语言应用程序中，但它们的规模和语言变异性带来了重大的隐私风险。本文主要针对英语进行评估，调查了语言结构如何影响接受英语、西班牙语、法语和意大利医学数据库培训的LLM的隐私泄露。我们量化了六个语言指标并评估了三个攻击载体：提取、反事实记忆和隶属推理。结果表明，隐私脆弱性随着语言冗余和标记化粒度的变化而变化：意大利语表现出最强的泄露，而英语表现出更高的成员可分离性。相比之下，法语和西班牙语表现出更大的弹性，由于更高的形态复杂性。总的来说，我们的研究结果提供了第一个定量证据，表明语言在隐私泄露中很重要，强调了在LLM部署中需要语言感知的隐私保护机制。



## **47. Do Internal Layers of LLMs Reveal Patterns for Jailbreak Detection?**

LLM的内层是否揭示了越狱检测的模式？ cs.CL

**SubmitDate**: 2025-10-09    [abs](http://arxiv.org/abs/2510.06594v2) [paper-pdf](http://arxiv.org/pdf/2510.06594v2)

**Authors**: Sri Durga Sai Sowmya Kadali, Evangelos E. Papalexakis

**Abstract**: Jailbreaking large language models (LLMs) has emerged as a pressing concern with the increasing prevalence and accessibility of conversational LLMs. Adversarial users often exploit these models through carefully engineered prompts to elicit restricted or sensitive outputs, a strategy widely referred to as jailbreaking. While numerous defense mechanisms have been proposed, attackers continuously develop novel prompting techniques, and no existing model can be considered fully resistant. In this study, we investigate the jailbreak phenomenon by examining the internal representations of LLMs, with a focus on how hidden layers respond to jailbreak versus benign prompts. Specifically, we analyze the open-source LLM GPT-J and the state-space model Mamba2, presenting preliminary findings that highlight distinct layer-wise behaviors. Our results suggest promising directions for further research on leveraging internal model dynamics for robust jailbreak detection and defense.

摘要: 随着对话式LLM的日益普及和可访问性，越狱大型语言模型（LLM）已成为一个紧迫的问题。敌对用户经常通过精心设计的提示来利用这些模型来获取受限或敏感的输出，这种策略被广泛称为越狱。虽然已经提出了许多防御机制，但攻击者不断开发新颖的提示技术，并且没有任何现有模型可以被认为是完全抵抗的。在这项研究中，我们通过检查LLM的内部表示来调查越狱现象，重点关注隐藏层如何对越狱与良性提示做出反应。具体来说，我们分析了开源LLM GPT-J和状态空间模型Mamba 2，提供了突出不同分层行为的初步发现。我们的结果为进一步研究利用内部模型动态来进行稳健的越狱检测和防御指明了有希望的方向。



## **48. Code Agent can be an End-to-end System Hacker: Benchmarking Real-world Threats of Computer-use Agent**

代码代理可以成为端到端系统黑客：对计算机使用代理的现实世界威胁进行基准测试 cs.CR

**SubmitDate**: 2025-10-09    [abs](http://arxiv.org/abs/2510.06607v2) [paper-pdf](http://arxiv.org/pdf/2510.06607v2)

**Authors**: Weidi Luo, Qiming Zhang, Tianyu Lu, Xiaogeng Liu, Bin Hu, Hung-Chun Chiu, Siyuan Ma, Yizhe Zhang, Xusheng Xiao, Yinzhi Cao, Zhen Xiang, Chaowei Xiao

**Abstract**: Computer-use agent (CUA) frameworks, powered by large language models (LLMs) or multimodal LLMs (MLLMs), are rapidly maturing as assistants that can perceive context, reason, and act directly within software environments. Among their most critical applications is operating system (OS) control. As CUAs in the OS domain become increasingly embedded in daily operations, it is imperative to examine their real-world security implications, specifically whether CUAs can be misused to perform realistic, security-relevant attacks. Existing works exhibit four major limitations: Missing attacker-knowledge model on tactics, techniques, and procedures (TTP), Incomplete coverage for end-to-end kill chains, unrealistic environment without multi-host and encrypted user credentials, and unreliable judgment dependent on LLM-as-a-Judge. To address these gaps, we propose AdvCUA, the first benchmark aligned with real-world TTPs in MITRE ATT&CK Enterprise Matrix, which comprises 140 tasks, including 40 direct malicious tasks, 74 TTP-based malicious tasks, and 26 end-to-end kill chains, systematically evaluates CUAs under a realistic enterprise OS security threat in a multi-host environment sandbox by hard-coded evaluation. We evaluate the existing five mainstream CUAs, including ReAct, AutoGPT, Gemini CLI, Cursor CLI, and Cursor IDE based on 8 foundation LLMs. The results demonstrate that current frontier CUAs do not adequately cover OS security-centric threats. These capabilities of CUAs reduce dependence on custom malware and deep domain expertise, enabling even inexperienced attackers to mount complex enterprise intrusions, which raises social concern about the responsibility and security of CUAs.

摘要: 由大型语言模型（LLM）或多模式LLM（MLLM）支持的计算机使用代理（CUA）框架正在迅速成熟，成为可以感知上下文、推理和直接在软件环境中采取行动的助手。它们最关键的应用程序之一是操作系统（OS）控制。随着操作系统领域的CUA越来越嵌入到日常操作中，必须检查其现实世界的安全影响，特别是是否可以滥用CUA来执行现实的、安全相关的攻击。现有作品表现出四个主要局限性：缺乏关于战术、技术和程序（TTP）的攻击者知识模型、端到端杀戮链的不完整覆盖、没有多主机和加密用户凭证的不切实际的环境以及依赖于LLM作为法官的不可靠判断。为了解决这些差距，我们提出了AdvCUA，这是MITRE ATT & CK Enterprise Matrix中第一个与现实世界TTP相一致的基准，该基准由140个任务组成，其中包括40个直接恶意任务、74个基于TTP的恶意任务和26个端到端杀死链，通过硬编码评估系统地评估多主机环境沙盒中现实企业操作系统安全威胁下的CUA。我们基于8个基础LLM评估了现有的五种主流CUA，包括ReAct、AutoGPT、Gemini CLI、Cursor CLI和Cursor IDE。结果表明，当前的前沿CUA无法充分覆盖以操作系统安全为中心的威胁。CUA的这些功能减少了对自定义恶意软件和深层领域专业知识的依赖，甚至使经验不足的攻击者也能够发动复杂的企业入侵，这引发了社会对CUA责任和安全性的担忧。



## **49. AutoRed: A Free-form Adversarial Prompt Generation Framework for Automated Red Teaming**

AutoRed：一个用于自动化红色团队的自由形式对抗提示生成框架 cs.CL

**SubmitDate**: 2025-10-09    [abs](http://arxiv.org/abs/2510.08329v1) [paper-pdf](http://arxiv.org/pdf/2510.08329v1)

**Authors**: Muxi Diao, Yutao Mou, Keqing He, Hanbo Song, Lulu Zhao, Shikun Zhang, Wei Ye, Kongming Liang, Zhanyu Ma

**Abstract**: The safety of Large Language Models (LLMs) is crucial for the development of trustworthy AI applications. Existing red teaming methods often rely on seed instructions, which limits the semantic diversity of the synthesized adversarial prompts. We propose AutoRed, a free-form adversarial prompt generation framework that removes the need for seed instructions. AutoRed operates in two stages: (1) persona-guided adversarial instruction generation, and (2) a reflection loop to iteratively refine low-quality prompts. To improve efficiency, we introduce a verifier to assess prompt harmfulness without querying the target models. Using AutoRed, we build two red teaming datasets -- AutoRed-Medium and AutoRed-Hard -- and evaluate eight state-of-the-art LLMs. AutoRed achieves higher attack success rates and better generalization than existing baselines. Our results highlight the limitations of seed-based approaches and demonstrate the potential of free-form red teaming for LLM safety evaluation. We will open source our datasets in the near future.

摘要: 大型语言模型（LLM）的安全性对于开发值得信赖的人工智能应用程序至关重要。现有的红色分组方法通常依赖于种子指令，这限制了合成对抗提示的语义多样性。我们提出AutoRed，这是一个自由形式的对抗性提示生成框架，它消除了对种子指令的需要。AutoRed分两个阶段运行：（1）角色引导的对抗指令生成，和（2）迭代地细化低质量提示的反射循环。为了提高效率，我们引入了一个验证器来评估即时危害性，而无需查询目标模型。使用AutoRed，我们构建了两个红色团队数据集-- AutoRed-Medium和AutoRed-Hard --并评估了八个最先进的LLM。AutoRed比现有基线实现了更高的攻击成功率和更好的概括性。我们的结果强调了基于种子的方法的局限性，并展示了自由形式的红色团队在LLM安全性评估中的潜力。我们将在不久的将来开放我们的数据集。



## **50. MCPSecBench: A Systematic Security Benchmark and Playground for Testing Model Context Protocols**

MCPSecBench：测试模型上下文协议的系统安全基准和游乐场 cs.CR

This is a technical report from Lingnan University, Hong Kong. Code  is available at https://github.com/AIS2Lab/MCPSecBench

**SubmitDate**: 2025-10-09    [abs](http://arxiv.org/abs/2508.13220v2) [paper-pdf](http://arxiv.org/pdf/2508.13220v2)

**Authors**: Yixuan Yang, Daoyuan Wu, Yufan Chen

**Abstract**: Large Language Models (LLMs) are increasingly integrated into real-world applications via the Model Context Protocol (MCP), a universal, open standard for connecting AI agents with data sources and external tools. While MCP enhances the capabilities of LLM-based agents, it also introduces new security risks and expands their attack surfaces. In this paper, we present the first systematic taxonomy of MCP security, identifying 17 attack types across 4 primary attack surfaces. We introduce MCPSecBench, a comprehensive security benchmark and playground that integrates prompt datasets, MCP servers, MCP clients, attack scripts, and protection mechanisms to evaluate these attacks across three major MCP providers. Our benchmark is modular and extensible, allowing researchers to incorporate custom implementations of clients, servers, and transport protocols for systematic security assessment. Experimental results show that over 85% of the identified attacks successfully compromise at least one platform, with core vulnerabilities universally affecting Claude, OpenAI, and Cursor, while prompt-based and tool-centric attacks exhibit considerable variability across different hosts and models. In addition, current protection mechanisms have little effect against these attacks. Overall, MCPSecBench standardizes the evaluation of MCP security and enables rigorous testing across all MCP layers.

摘要: 大型语言模型（LLM）通过模型上下文协议（HCP）越来越多地集成到现实世界的应用程序中，模型上下文协议（HCP）是一种通用的开放标准，用于连接人工智能代理与数据源和外部工具。虽然HCP增强了基于LLM的代理的能力，但它也引入了新的安全风险并扩大了其攻击面。在本文中，我们提出了第一个系统性的LCP安全分类，识别了4个主要攻击表面的17种攻击类型。我们引入了MCPSecBench，这是一个全面的安全基准和游乐场，集成了提示数据集、HCP服务器、HCP客户端、攻击脚本和保护机制，以评估三大主要HCP提供商之间的这些攻击。我们的基准是模块化和可扩展的，允许研究人员整合客户端、服务器和传输协议的自定义实现，以进行系统性安全评估。实验结果表明，超过85%的已识别攻击成功危害至少一个平台，核心漏洞普遍影响Claude、OpenAI和Cursor，而基于预算和以工具为中心的攻击在不同的主机和模型中表现出相当大的变化性。此外，当前的保护机制对这些攻击几乎没有效果。总体而言，MCPSecBench实现了对LCP安全性的评估，并实现了对所有LCP层的严格测试。



