# Latest Large Language Model Attack Papers
**update at 2025-08-06 18:21:47**

翻译来自 https://cloud.tencent.com/document/product/551/15619

## **1. IDEATOR: Jailbreaking and Benchmarking Large Vision-Language Models Using Themselves**

IDEATOR：使用自己越狱和基准大型视觉语言模型 cs.CV

**SubmitDate**: 2025-08-05    [abs](http://arxiv.org/abs/2411.00827v4) [paper-pdf](http://arxiv.org/pdf/2411.00827v4)

**Authors**: Ruofan Wang, Juncheng Li, Yixu Wang, Bo Wang, Xiaosen Wang, Yan Teng, Yingchun Wang, Xingjun Ma, Yu-Gang Jiang

**Abstract**: As large Vision-Language Models (VLMs) gain prominence, ensuring their safe deployment has become critical. Recent studies have explored VLM robustness against jailbreak attacks-techniques that exploit model vulnerabilities to elicit harmful outputs. However, the limited availability of diverse multimodal data has constrained current approaches to rely heavily on adversarial or manually crafted images derived from harmful text datasets, which often lack effectiveness and diversity across different contexts. In this paper, we propose IDEATOR, a novel jailbreak method that autonomously generates malicious image-text pairs for black-box jailbreak attacks. IDEATOR is grounded in the insight that VLMs themselves could serve as powerful red team models for generating multimodal jailbreak prompts. Specifically, IDEATOR leverages a VLM to create targeted jailbreak texts and pairs them with jailbreak images generated by a state-of-the-art diffusion model. Extensive experiments demonstrate IDEATOR's high effectiveness and transferability, achieving a 94% attack success rate (ASR) in jailbreaking MiniGPT-4 with an average of only 5.34 queries, and high ASRs of 82%, 88%, and 75% when transferred to LLaVA, InstructBLIP, and Chameleon, respectively. Building on IDEATOR's strong transferability and automated process, we introduce the VLJailbreakBench, a safety benchmark comprising 3,654 multimodal jailbreak samples. Our benchmark results on 11 recently released VLMs reveal significant gaps in safety alignment. For instance, our challenge set achieves ASRs of 46.31% on GPT-4o and 19.65% on Claude-3.5-Sonnet, underscoring the urgent need for stronger defenses.

摘要: 随着大型视觉语言模型（VLM）的日益突出，确保其安全部署变得至关重要。最近的研究探索了VLM针对越狱攻击的鲁棒性--利用模型漏洞来引发有害输出的技术。然而，多样化多模式数据的可用性有限，限制了当前的方法严重依赖于从有害文本数据集派生的对抗性或手动制作的图像，而这些图像通常缺乏跨不同背景的有效性和多样性。本文中，我们提出了IDEATOR，这是一种新型越狱方法，可以自主生成用于黑匣子越狱攻击的恶意图像-文本对。IDEATOR基于这样的见解：VLM本身可以充当强大的红队模型，用于生成多模式越狱提示。具体来说，IDEATOR利用VLM创建有针对性的越狱文本，并将其与由最先进的扩散模型生成的越狱图像配对。大量实验证明了IDEATOR的高效率和可移植性，在越狱MiniGPT-4中平均只需5.34次查询即可实现94%的攻击成功率（ASB），转移到LLaVA、INSTBLIP和Chameleon时，攻击成功率分别为82%、88%和75%。基于IDEATOR强大的可移植性和自动化流程，我们推出了VLJailbreakBench，这是一个由3，654个多模式越狱样本组成的安全基准。我们对最近发布的11个VLM的基准结果揭示了安全一致方面的显着差距。例如，我们的挑战集在GPT-4 o上实现了46.31%的ASB，在Claude-3.5-十四行诗上实现了19.65%的ASB，这凸显了迫切需要更强的防御。



## **2. When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with Benign Inputs**

当好的声音变得敌对时：用良性输入越狱的音频模型 cs.SD

**SubmitDate**: 2025-08-05    [abs](http://arxiv.org/abs/2508.03365v1) [paper-pdf](http://arxiv.org/pdf/2508.03365v1)

**Authors**: Bodam Kim, Hiskias Dingeto, Taeyoun Kwon, Dasol Choi, DongGeon Lee, Haon Park, JaeHoon Lee, Jongho Shin

**Abstract**: As large language models become increasingly integrated into daily life, audio has emerged as a key interface for human-AI interaction. However, this convenience also introduces new vulnerabilities, making audio a potential attack surface for adversaries. Our research introduces WhisperInject, a two-stage adversarial audio attack framework that can manipulate state-of-the-art audio language models to generate harmful content. Our method uses imperceptible perturbations in audio inputs that remain benign to human listeners. The first stage uses a novel reward-based optimization method, Reinforcement Learning with Projected Gradient Descent (RL-PGD), to guide the target model to circumvent its own safety protocols and generate harmful native responses. This native harmful response then serves as the target for Stage 2, Payload Injection, where we use Projected Gradient Descent (PGD) to optimize subtle perturbations that are embedded into benign audio carriers, such as weather queries or greeting messages. Validated under the rigorous StrongREJECT, LlamaGuard, as well as Human Evaluation safety evaluation framework, our experiments demonstrate a success rate exceeding 86% across Qwen2.5-Omni-3B, Qwen2.5-Omni-7B, and Phi-4-Multimodal. Our work demonstrates a new class of practical, audio-native threats, moving beyond theoretical exploits to reveal a feasible and covert method for manipulating AI behavior.

摘要: 随着大型语言模型越来越融入日常生活，音频已成为人机交互的关键界面。然而，这种便利性也引入了新的漏洞，使音频成为对手的潜在攻击面。我们的研究引入了WhisperInib，这是一个两阶段对抗性音频攻击框架，可以操纵最先进的音频语言模型来生成有害内容。我们的方法在音频输入中使用不可感知的扰动，这些扰动对人类听众保持良性。第一阶段使用一种新颖的基于奖励的优化方法--具有投影梯度下降的强化学习（RL-PVD），来指导目标模型规避其自己的安全协议并生成有害的原生响应。然后，这种原生有害响应作为第二阶段有效负载注入的目标，在该阶段，我们使用投影梯度下降（PVD）来优化嵌入良性音频载体中的微妙扰动，例如天气查询或问候消息。我们的实验经过严格的StrongRESEARCH、LlamaGuard以及Human Evision安全评估框架的验证，证明Qwen 2.5-Omni-3B、Qwen 2.5-Omni-7 B和Phi-4-Multimodal的成功率超过86%。我们的工作展示了一类新的实用、音频原生威胁，超越了理论利用，揭示了一种可行且隐蔽的操纵人工智能行为的方法。



## **3. BlockA2A: Towards Secure and Verifiable Agent-to-Agent Interoperability**

BlockA2A：迈向安全且可验证的代理对代理互操作性 cs.CR

43 pages

**SubmitDate**: 2025-08-05    [abs](http://arxiv.org/abs/2508.01332v2) [paper-pdf](http://arxiv.org/pdf/2508.01332v2)

**Authors**: Zhenhua Zou, Zhuotao Liu, Lepeng Zhao, Qiuyang Zhan

**Abstract**: The rapid adoption of agentic AI, powered by large language models (LLMs), is transforming enterprise ecosystems with autonomous agents that execute complex workflows. Yet we observe several key security vulnerabilities in LLM-driven multi-agent systems (MASes): fragmented identity frameworks, insecure communication channels, and inadequate defenses against Byzantine agents or adversarial prompts. In this paper, we present the first systematic analysis of these emerging multi-agent risks and explain why the legacy security strategies cannot effectively address these risks. Afterwards, we propose BlockA2A, the first unified multi-agent trust framework that enables secure and verifiable and agent-to-agent interoperability. At a high level, BlockA2A adopts decentralized identifiers (DIDs) to enable fine-grained cross-domain agent authentication, blockchain-anchored ledgers to enable immutable auditability, and smart contracts to dynamically enforce context-aware access control policies. BlockA2A eliminates centralized trust bottlenecks, ensures message authenticity and execution integrity, and guarantees accountability across agent interactions. Furthermore, we propose a Defense Orchestration Engine (DOE) that actively neutralizes attacks through real-time mechanisms, including Byzantine agent flagging, reactive execution halting, and instant permission revocation. Empirical evaluations demonstrate BlockA2A's effectiveness in neutralizing prompt-based, communication-based, behavioral and systemic MAS attacks. We formalize its integration into existing MAS and showcase a practical implementation for Google's A2A protocol. Experiments confirm that BlockA2A and DOE operate with sub-second overhead, enabling scalable deployment in production LLM-based MAS environments.

摘要: 由大型语言模型（LLM）支持的代理人工智能的快速采用正在通过执行复杂工作流程的自主代理改变企业生态系统。然而，我们在LLM驱动的多代理系统（MASes）中观察到了几个关键的安全漏洞：碎片化的身份框架、不安全的通信渠道以及对拜占庭代理或对抗提示的防御不足。在本文中，我们对这些新出现的多代理风险进行了首次系统分析，并解释了为什么传统安全策略无法有效应对这些风险。随后，我们提出了BlockA2A，这是第一个统一的多代理信任框架，可以实现安全、可验证以及代理与代理的互操作性。在高层面上，BlockA2A采用去中心化标识符（DID）来实现细粒度的跨域代理认证，采用区块链锚定分类帐来实现不可变的可互换性，并采用智能合同来动态执行上下文感知的访问控制策略。BlockA2A消除了集中式信任瓶颈，确保消息真实性和执行完整性，并保证跨代理交互的问责制。此外，我们还提出了一种国防规划引擎（DOE），它通过实时机制主动中和攻击，包括拜占庭代理标记、反应式执行停止和即时许可撤销。经验评估证明BlockA2A在中和基于预算、基于通信的行为和系统性MAS攻击方面的有效性。我们将其正式集成到现有MAS中，并展示了Google A2 A协议的实际实现。实验证实BlockA2A和DOE的运行成本为亚秒级，从而能够在基于LLM的生产MAS环境中进行可扩展部署。



## **4. ConfGuard: A Simple and Effective Backdoor Detection for Large Language Models**

ConfGuard：大型语言模型简单有效的后门检测 cs.CR

Under review

**SubmitDate**: 2025-08-05    [abs](http://arxiv.org/abs/2508.01365v2) [paper-pdf](http://arxiv.org/pdf/2508.01365v2)

**Authors**: Zihan Wang, Rui Zhang, Hongwei Li, Wenshu Fan, Wenbo Jiang, Qingchuan Zhao, Guowen Xu

**Abstract**: Backdoor attacks pose a significant threat to Large Language Models (LLMs), where adversaries can embed hidden triggers to manipulate LLM's outputs. Most existing defense methods, primarily designed for classification tasks, are ineffective against the autoregressive nature and vast output space of LLMs, thereby suffering from poor performance and high latency. To address these limitations, we investigate the behavioral discrepancies between benign and backdoored LLMs in output space. We identify a critical phenomenon which we term sequence lock: a backdoored model generates the target sequence with abnormally high and consistent confidence compared to benign generation. Building on this insight, we propose ConfGuard, a lightweight and effective detection method that monitors a sliding window of token confidences to identify sequence lock. Extensive experiments demonstrate ConfGuard achieves a near 100\% true positive rate (TPR) and a negligible false positive rate (FPR) in the vast majority of cases. Crucially, the ConfGuard enables real-time detection almost without additional latency, making it a practical backdoor defense for real-world LLM deployments.

摘要: 后门攻击对大型语言模型（LLM）构成重大威胁，对手可以嵌入隐藏触发器来操纵LLM的输出。大多数现有的防御方法主要是为分类任务设计的，对LLM的自回归性质和巨大的输出空间无效，从而遭受性能差和延迟高的影响。为了解决这些限制，我们调查了输出空间中良性和后门LLM之间的行为差异。我们发现了一个关键现象，我们称之为序列锁：与良性生成相比，后门模型以异常高且一致的置信度生成目标序列。基于这一见解，我们提出了ConfGuard，这是一种轻量级且有效的检测方法，可以监控令牌置信度的滑动窗口以识别序列锁。大量实验表明，在绝大多数情况下，ConfGuard的真阳性率（TPA）接近100%，假阳性率（FPR）可忽略不计。至关重要的是，ConfGuard几乎无需额外延迟即可实现实时检测，使其成为现实世界LLM部署的实用后门防御。



## **5. Memorization in Fine-Tuned Large Language Models**

微调大型语言模型中的精简化 cs.CL

**SubmitDate**: 2025-08-05    [abs](http://arxiv.org/abs/2507.21009v2) [paper-pdf](http://arxiv.org/pdf/2507.21009v2)

**Authors**: Danil Savine

**Abstract**: This study investigates the mechanisms and factors influencing memorization in fine-tuned large language models (LLMs), with a focus on the medical domain due to its privacy-sensitive nature. We examine how different aspects of the fine-tuning process affect a model's propensity to memorize training data, using the PHEE dataset of pharmacovigilance events.   Our research employs two main approaches: a membership inference attack to detect memorized data, and a generation task with prompted prefixes to assess verbatim reproduction. We analyze the impact of adapting different weight matrices in the transformer architecture, the relationship between perplexity and memorization, and the effect of increasing the rank in low-rank adaptation (LoRA) fine-tuning.   Key findings include: (1) Value and Output matrices contribute more significantly to memorization compared to Query and Key matrices; (2) Lower perplexity in the fine-tuned model correlates with increased memorization; (3) Higher LoRA ranks lead to increased memorization, but with diminishing returns at higher ranks.   These results provide insights into the trade-offs between model performance and privacy risks in fine-tuned LLMs. Our findings have implications for developing more effective and responsible strategies for adapting large language models while managing data privacy concerns.

摘要: 这项研究调查了影响微调大型语言模型（LLM）记忆的机制和因素，重点关注医学领域，因为其隐私敏感的性质。我们使用药物警戒事件的PHEE数据集，研究微调过程的不同方面如何影响模型记忆训练数据的倾向。   我们的研究采用了两种主要方法：检测记忆数据的隶属度推理攻击，以及具有提示性前置的生成任务，以评估逐字复制。我们分析了在Transformer架构中适应不同权重矩阵的影响、困惑度和记忆之间的关系，以及在低等级适应（LoRA）微调中增加等级的影响。   主要调查结果包括：（1）与查询和关键矩阵相比，价值和输出矩阵对记忆的贡献更大;（2）微调模型中较低的困惑度与记忆的增加相关;（3）较高的LoRA排名会导致记忆的增加，但收益递减。   这些结果为微调LLM中模型性能和隐私风险之间的权衡提供了见解。我们的研究结果对于制定更有效、更负责任的策略来适应大型语言模型，同时管理数据隐私问题具有影响。



## **6. M2S: Multi-turn to Single-turn jailbreak in Red Teaming for LLMs**

M2S：LLM红色团队多回合到单回合越狱 cs.CL

Accepted to ACL 2025 (Main Track). Camera-ready version

**SubmitDate**: 2025-08-05    [abs](http://arxiv.org/abs/2503.04856v3) [paper-pdf](http://arxiv.org/pdf/2503.04856v3)

**Authors**: Junwoo Ha, Hyunjun Kim, Sangyoon Yu, Haon Park, Ashkan Yousefpour, Yuna Park, Suhyun Kim

**Abstract**: We introduce a novel framework for consolidating multi-turn adversarial ``jailbreak'' prompts into single-turn queries, significantly reducing the manual overhead required for adversarial testing of large language models (LLMs). While multi-turn human jailbreaks have been shown to yield high attack success rates, they demand considerable human effort and time. Our multi-turn-to-single-turn (M2S) methods -- Hyphenize, Numberize, and Pythonize -- systematically reformat multi-turn dialogues into structured single-turn prompts. Despite removing iterative back-and-forth interactions, these prompts preserve and often enhance adversarial potency: in extensive evaluations on the Multi-turn Human Jailbreak (MHJ) dataset, M2S methods achieve attack success rates from 70.6 percent to 95.9 percent across several state-of-the-art LLMs. Remarkably, the single-turn prompts outperform the original multi-turn attacks by as much as 17.5 percentage points while cutting token usage by more than half on average. Further analysis shows that embedding malicious requests in enumerated or code-like structures exploits ``contextual blindness'', bypassing both native guardrails and external input-output filters. By converting multi-turn conversations into concise single-turn prompts, the M2S framework provides a scalable tool for large-scale red teaming and reveals critical weaknesses in contemporary LLM defenses.

摘要: 我们引入了一种新颖的框架，用于将多轮对抗性“越狱”提示整合到单轮查询中，从而显着减少了大型语言模型（LLM）对抗性测试所需的手动负担。虽然多回合人类越狱已被证明具有很高的攻击成功率，但它们需要相当大的人力和时间。我们的多回合到单回合（M2 S）方法--连字符化、数字化和Python化--系统地将多回合对话重新格式化为结构化的单回合提示。尽管消除了迭代的来回相互作用，但这些提示仍然保留并经常增强对抗能力：在对多回合人类越狱（MTJ）数据集的广泛评估中，M2 S方法在几种最先进的LLM中实现了从70.6%到95.9%的攻击成功率。值得注意的是，单回合提示的性能比最初的多回合攻击高出17.5个百分点，同时平均将代币使用量减少一半以上。进一步的分析表明，将恶意请求嵌入到列举或类代码结构中利用了“上下文盲目性”，绕过了本地护栏和外部输入输出过滤器。通过将多回合对话转换为简洁的单回合提示，M2 S框架为大规模红色团队提供了可扩展的工具，并揭示了当代LLM防御中的关键弱点。



## **7. Attack the Messages, Not the Agents: A Multi-round Adaptive Stealthy Tampering Framework for LLM-MAS**

攻击消息，而不是代理：LLM-MAS的多轮自适应隐形篡改框架 cs.CR

**SubmitDate**: 2025-08-05    [abs](http://arxiv.org/abs/2508.03125v1) [paper-pdf](http://arxiv.org/pdf/2508.03125v1)

**Authors**: Bingyu Yan, Ziyi Zhou, Xiaoming Zhang, Chaozhuo Li, Ruilin Zeng, Yirui Qi, Tianbo Wang, Litian Zhang

**Abstract**: Large language model-based multi-agent systems (LLM-MAS) effectively accomplish complex and dynamic tasks through inter-agent communication, but this reliance introduces substantial safety vulnerabilities. Existing attack methods targeting LLM-MAS either compromise agent internals or rely on direct and overt persuasion, which limit their effectiveness, adaptability, and stealthiness. In this paper, we propose MAST, a Multi-round Adaptive Stealthy Tampering framework designed to exploit communication vulnerabilities within the system. MAST integrates Monte Carlo Tree Search with Direct Preference Optimization to train an attack policy model that adaptively generates effective multi-round tampering strategies. Furthermore, to preserve stealthiness, we impose dual semantic and embedding similarity constraints during the tampering process. Comprehensive experiments across diverse tasks, communication architectures, and LLMs demonstrate that MAST consistently achieves high attack success rates while significantly enhancing stealthiness compared to baselines. These findings highlight the effectiveness, stealthiness, and adaptability of MAST, underscoring the need for robust communication safeguards in LLM-MAS.

摘要: 基于大型语言模型的多智能体系统（LLM-MAS）通过智能体间的通信有效地完成复杂和动态的任务，但这种依赖引入了大量的安全漏洞。针对LLM-MAS的现有攻击方法要么妥协代理内部或依赖于直接和公开的说服，这限制了它们的有效性，适应性和隐蔽性。在本文中，我们提出了MAST，多轮自适应隐形篡改框架，旨在利用系统内的通信漏洞。MAST将蒙特卡洛树搜索与直接偏好优化相结合，训练一个攻击策略模型，自适应地生成有效的多轮篡改策略。此外，为了保持隐蔽性，我们施加双重语义和嵌入相似性约束在篡改过程中。跨不同任务、通信架构和LLM的综合实验表明，MAST始终实现高攻击成功率，同时与基线相比显着增强了隐蔽性。这些发现突出了MAST的有效性，隐蔽性和适应性，强调了LLM-MAS中强大的通信保障的必要性。



## **8. Token-Level Precise Attack on RAG: Searching for the Best Alternatives to Mislead Generation**

对RAG的代币级精确攻击：寻找误导生成的最佳替代方案 cs.CL

**SubmitDate**: 2025-08-05    [abs](http://arxiv.org/abs/2508.03110v1) [paper-pdf](http://arxiv.org/pdf/2508.03110v1)

**Authors**: Zizhong Li, Haopeng Zhang, Jiawei Zhang

**Abstract**: While large language models (LLMs) have achieved remarkable success in providing trustworthy responses for knowledge-intensive tasks, they still face critical limitations such as hallucinations and outdated knowledge. To address these issues, the retrieval-augmented generation (RAG) framework enhances LLMs with access to external knowledge via a retriever, enabling more accurate and real-time outputs about the latest events. However, this integration brings new security vulnerabilities: the risk that malicious content in the external database can be retrieved and used to manipulate model outputs. Although prior work has explored attacks on RAG systems, existing approaches either rely heavily on access to the retriever or fail to jointly consider both retrieval and generation stages, limiting their effectiveness, particularly in black-box scenarios. To overcome these limitations, we propose Token-level Precise Attack on the RAG (TPARAG), a novel framework that targets both white-box and black-box RAG systems. TPARAG leverages a lightweight white-box LLM as an attacker to generate and iteratively optimize malicious passages at the token level, ensuring both retrievability and high attack success in generation. Extensive experiments on open-domain QA datasets demonstrate that TPARAG consistently outperforms previous approaches in retrieval-stage and end-to-end attack effectiveness. These results further reveal critical vulnerabilities in RAG pipelines and offer new insights into improving their robustness.

摘要: 虽然大型语言模型（LLM）在为知识密集型任务提供值得信赖的响应方面取得了显着成功，但它们仍然面临着幻觉和过时知识等严重局限性。为了解决这些问题，检索增强生成（RAG）框架通过检索器访问外部知识来增强LLM，从而能够更准确、实时地输出最新事件。然而，这种集成带来了新的安全漏洞：外部数据库中的恶意内容可能被检索并用于操纵模型输出的风险。尽管之前的工作已经探索了对RAG系统的攻击，但现有的方法要么严重依赖于对检索器的访问，要么未能联合考虑检索和生成阶段，从而限制了它们的有效性，特别是在黑匣子场景中。为了克服这些限制，我们提出了对RAG的令牌级精确攻击（TPARAG），这是一种针对白盒和黑盒RAG系统的新型框架。TPARAG利用轻量级白盒LLM作为攻击者来生成并迭代优化令牌级别的恶意段落，确保生成时的可检索性和高攻击成功率。对开放域QA数据集的大量实验表明，TPARAG在检索阶段和端到端攻击有效性方面始终优于之前的方法。这些结果进一步揭示了RAG管道中的关键漏洞，并为提高其稳健性提供了新的见解。



## **9. Privacy-Aware Decoding: Mitigating Privacy Leakage of Large Language Models in Retrieval-Augmented Generation**

隐私感知解码：减轻检索增强生成中大型语言模型的隐私泄露 cs.CL

**SubmitDate**: 2025-08-05    [abs](http://arxiv.org/abs/2508.03098v1) [paper-pdf](http://arxiv.org/pdf/2508.03098v1)

**Authors**: Haoran Wang, Xiongxiao Xu, Baixiang Huang, Kai Shu

**Abstract**: Retrieval-Augmented Generation (RAG) enhances the factual accuracy of large language models (LLMs) by conditioning outputs on external knowledge sources. However, when retrieval involves private or sensitive data, RAG systems are susceptible to extraction attacks that can leak confidential information through generated responses. We propose Privacy-Aware Decoding (PAD), a lightweight, inference-time defense that adaptively injects calibrated Gaussian noise into token logits during generation. PAD integrates confidence-based screening to selectively protect high-risk tokens, efficient sensitivity estimation to minimize unnecessary noise, and context-aware noise calibration to balance privacy with generation quality. A \renyi Differential Privacy (RDP) accountant rigorously tracks cumulative privacy loss, enabling explicit per-response $(\varepsilon, \delta)$-DP guarantees for sensitive outputs. Unlike prior approaches requiring retraining or corpus-level filtering, PAD is model-agnostic and operates entirely at decoding time with minimal computational overhead. Experiments on three real-world datasets demonstrate that PAD substantially reduces private information leakage while preserving response utility, outperforming existing retrieval- and post-processing-based defenses. Our work takes an important step toward mitigating privacy risks in RAG via decoding strategies, paving the way for universal and scalable privacy solutions in sensitive domains. Our code is available: https://github.com/wang2226/PAD.

摘要: 检索增强生成（RAG）通过对外部知识源进行条件化输出来增强大型语言模型（LLM）的事实准确性。然而，当检索涉及私人或敏感数据时，RAG系统很容易受到提取攻击，这些攻击可以通过生成的响应泄露机密信息。我们提出隐私感知解码（PAD），这是一种轻量级的推断时间防御，可以在生成期间自适应地将校准的高斯噪音注入到令牌日志中。PAD集成了基于信心的筛选以选择性地保护高风险代币、有效的灵敏度估计以最大限度地减少不必要的噪音，以及上下文感知的噪音校准以平衡隐私与发电质量。\renyi差异隐私（SDP）会计师严格跟踪累积隐私损失，为敏感输出提供明确的按响应$（\varepð，\delta）$-DP保证。与需要重新训练或群体级过滤的现有方法不同，PAD是模型不可知的，并且完全在解码时以最小的计算负担运行。对三个现实世界数据集的实验表明，PAD大大减少了私人信息泄露，同时保留了响应效用，优于现有的基于检索和后处理的防御。我们的工作朝着通过解码策略减轻RAG中的隐私风险迈出了重要一步，为敏感领域中的通用和可扩展的隐私解决方案铺平了道路。我们的代码可访问：https://github.com/wang2226/PAD。



## **10. VFLAIR-LLM: A Comprehensive Framework and Benchmark for Split Learning of LLMs**

VFLAIR-LLM：LLM拆分学习的全面框架和基准 cs.CR

12 pages, 10 figures, published in KDD2025

**SubmitDate**: 2025-08-05    [abs](http://arxiv.org/abs/2508.03097v1) [paper-pdf](http://arxiv.org/pdf/2508.03097v1)

**Authors**: Zixuan Gu, Qiufeng Fan, Long Sun, Yang Liu, Xiaojun Ye

**Abstract**: With the advancement of Large Language Models (LLMs), LLM applications have expanded into a growing number of fields. However, users with data privacy concerns face limitations in directly utilizing LLM APIs, while private deployments incur significant computational demands. This creates a substantial challenge in achieving secure LLM adaptation under constrained local resources. To address this issue, collaborative learning methods, such as Split Learning (SL), offer a resource-efficient and privacy-preserving solution for adapting LLMs to private domains. In this study, we introduce VFLAIR-LLM (available at https://github.com/FLAIR-THU/VFLAIR-LLM), an extensible and lightweight split learning framework for LLMs, enabling privacy-preserving LLM inference and fine-tuning in resource-constrained environments. Our library provides two LLM partition settings, supporting three task types and 18 datasets. In addition, we provide standard modules for implementing and evaluating attacks and defenses. We benchmark 5 attacks and 9 defenses under various Split Learning for LLM(SL-LLM) settings, offering concrete insights and recommendations on the choice of model partition configurations, defense strategies, and relevant hyperparameters for real-world applications.

摘要: 随着大型语言模型（LLM）的发展，LLM的应用已经扩展到越来越多的领域。然而，有数据隐私问题的用户在直接使用LLM API方面面临限制，而私有部署会产生大量的计算需求。这在有限的本地资源下实现安全的LLM适应方面产生了实质性挑战。为了解决这个问题，协作学习方法，如分裂学习（SL），提供了一个资源高效和隐私保护的解决方案，使LLM适应私人领域。在本研究中，我们引入了VFLAIR-LLM（可在https：//github.com/FLAIR-THU/VFLAIR-LLM上获取），这是一个针对LLM的可扩展且轻量级的分离学习框架，可在资源受限的环境中实现隐私保护LLM推断和微调。我们的库提供两种LLM分区设置，支持三种任务类型和18个数据集。此外，我们还提供用于实施和评估攻击和防御的标准模块。我们在各种LLM分离学习（SL-LLM）设置下对5种攻击和9种防御进行基准测试，为现实世界应用程序的模型分区配置、防御策略和相关超参数的选择提供具体见解和建议。



## **11. Hidden in the Noise: Unveiling Backdoors in Audio LLMs Alignment through Latent Acoustic Pattern Triggers**

隐藏在噪音中：通过潜在声学模式触发器揭开音频LLM对齐中的后门 cs.SD

**SubmitDate**: 2025-08-05    [abs](http://arxiv.org/abs/2508.02175v2) [paper-pdf](http://arxiv.org/pdf/2508.02175v2)

**Authors**: Liang Lin, Miao Yu, Kaiwen Luo, Yibo Zhang, Lilan Peng, Dexian Wang, Xuehai Tang, Yuanhe Zhang, Xikang Yang, Zhenhong Zhou, Kun Wang, Yang Liu

**Abstract**: As Audio Large Language Models (ALLMs) emerge as powerful tools for speech processing, their safety implications demand urgent attention. While considerable research has explored textual and vision safety, audio's distinct characteristics present significant challenges. This paper first investigates: Is ALLM vulnerable to backdoor attacks exploiting acoustic triggers? In response to this issue, we introduce Hidden in the Noise (HIN), a novel backdoor attack framework designed to exploit subtle, audio-specific features. HIN applies acoustic modifications to raw audio waveforms, such as alterations to temporal dynamics and strategic injection of spectrally tailored noise. These changes introduce consistent patterns that an ALLM's acoustic feature encoder captures, embedding robust triggers within the audio stream. To evaluate ALLM robustness against audio-feature-based triggers, we develop the AudioSafe benchmark, assessing nine distinct risk types. Extensive experiments on AudioSafe and three established safety datasets reveal critical vulnerabilities in existing ALLMs: (I) audio features like environment noise and speech rate variations achieve over 90% average attack success rate. (II) ALLMs exhibit significant sensitivity differences across acoustic features, particularly showing minimal response to volume as a trigger, and (III) poisoned sample inclusion causes only marginal loss curve fluctuations, highlighting the attack's stealth.

摘要: 随着音频大语言模型（ALLM）成为语音处理的强大工具，其安全性影响迫切需要关注。虽然大量研究探索了文本和视觉安全，但音频的独特特征带来了重大挑战。本文首先研究：ALLM是否容易受到利用声学触发器的后门攻击？为了应对这个问题，我们引入了Hidden in the Noise（HIN），这是一种新颖的后门攻击框架，旨在利用微妙的特定音频特征。HIN对原始音频波进行声学修改，例如改变时间动态和战略性地注入频谱定制的噪音。这些变化引入了ALLM的声学特征编码器捕获的一致模式，并在音频流中嵌入稳健的触发器。为了评估ALLM针对基于音频特征的触发器的稳健性，我们开发了AudioSafe基准，评估九种不同的风险类型。对AudioSafe和三个已建立的安全数据集的广泛实验揭示了现有ALLM中的关键漏洞：（I）环境噪音和语音速率变化等音频特征实现了超过90%的平均攻击成功率。(II)ALLMS在声学特征中表现出显着的灵敏度差异，特别是对作为触发器的体积的响应最小，并且（III）中毒样本包含物仅引起边际损失曲线波动，凸显了攻击的隐秘性。



## **12. Beyond Surface-Level Detection: Towards Cognitive-Driven Defense Against Jailbreak Attacks via Meta-Operations Reasoning**

超越表面级检测：通过元操作推理实现认知驱动的越狱攻击防御 cs.AI

**SubmitDate**: 2025-08-05    [abs](http://arxiv.org/abs/2508.03054v1) [paper-pdf](http://arxiv.org/pdf/2508.03054v1)

**Authors**: Rui Pu, Chaozhuo Li, Rui Ha, Litian Zhang, Lirong Qiu, Xi Zhang

**Abstract**: Defending large language models (LLMs) against jailbreak attacks is essential for their safe and reliable deployment. Existing defenses often rely on shallow pattern matching, which struggles to generalize to novel and unseen attack strategies. To address this challenge, we propose the Cognitive-Driven Defense (CDD) framework, which targets the underlying structure of jailbreak prompts by applying meta-operations, defined as basic manipulations that conceal harmful intent.CDD emulates human cognitive reasoning through a structured reasoning chain. It begins with a global perception of the prompt and follows with a localized analysis to uncover hidden manipulations. By applying supervised fine-tuning on this structured chain, the model learns to identify and reason about known manipulation patterns. To enhance generalization to unseen threats, an entropy-guided reinforcement learning algorithm (EG-GRPO) is introduced to encourage exploration of new types and variants of meta-operations. Experiments demonstrate that CDD can achieve state-of-the-art defense performance and exhibit strong generalization to unseen jailbreak attacks.

摘要: 保护大型语言模型（LLM）免受越狱攻击对于它们的安全可靠部署至关重要。现有的防御系统通常依赖于浅层模式匹配，这很难概括为新颖且不可见的攻击策略。为了应对这一挑战，我们提出了认知驱动防御（DDD）框架，该框架通过应用元操作来针对越狱提示的底层结构，元操作定义为隐藏有害意图的基本操纵。DDD通过结构化推理链模拟人类认知推理。它从对提示的全球感知开始，然后进行本地化分析以发现隐藏的操纵。通过对这个结构化链应用监督式微调，模型学会识别和推理已知的操纵模式。为了增强对不可见威胁的概括，引入了一种信息量引导的强化学习算法（EG-GRPO），以鼓励探索元操作的新类型和变体。实验表明，DDD可以实现最先进的防御性能，并对不可见的越狱攻击表现出很强的概括性。



## **13. CoCoTen: Detecting Adversarial Inputs to Large Language Models through Latent Space Features of Contextual Co-occurrence Tensors**

CoCoTen：通过上下文共现张量的潜在空间特征检测大型语言模型的对抗性输入 cs.CL

**SubmitDate**: 2025-08-05    [abs](http://arxiv.org/abs/2508.02997v1) [paper-pdf](http://arxiv.org/pdf/2508.02997v1)

**Authors**: Sri Durga Sai Sowmya Kadali, Evangelos E. Papalexakis

**Abstract**: The widespread use of Large Language Models (LLMs) in many applications marks a significant advance in research and practice. However, their complexity and hard-to-understand nature make them vulnerable to attacks, especially jailbreaks designed to produce harmful responses. To counter these threats, developing strong detection methods is essential for the safe and reliable use of LLMs. This paper studies this detection problem using the Contextual Co-occurrence Matrix, a structure recognized for its efficacy in data-scarce environments. We propose a novel method leveraging the latent space characteristics of Contextual Co-occurrence Matrices and Tensors for the effective identification of adversarial and jailbreak prompts. Our evaluations show that this approach achieves a notable F1 score of 0.83 using only 0.5% of labeled prompts, which is a 96.6% improvement over baselines. This result highlights the strength of our learned patterns, especially when labeled data is scarce. Our method is also significantly faster, speedup ranging from 2.3 to 128.4 times compared to the baseline models. To support future research and reproducibility, we have made our implementation publicly available.

摘要: 大型语言模型（LLM）在许多应用中的广泛使用标志着研究和实践的重大进步。然而，它们的复杂性和难以理解的性质使它们容易受到攻击，尤其是旨在产生有害反应的越狱。为了应对这些威胁，开发强大的检测方法对于安全可靠地使用LLM至关重要。本文使用上下文共生矩阵来研究这个检测问题，该结构因其在数据稀缺环境中的有效性而被公认。我们提出了一种利用上下文同现矩阵和张量的潜在空间特征的新型方法，以有效识别对抗和越狱提示。我们的评估表明，这种方法仅使用0.5%的标记提示即可获得显着的0.83分，比基线提高了96.6%。这一结果凸显了我们所学习模式的力量，尤其是当标记数据稀缺时。与基线模型相比，我们的方法也明显更快，加速范围为2.3至128.4倍。为了支持未来的研究和可重复性，我们已公开我们的实施。



## **14. Defend LLMs Through Self-Consciousness**

通过自我意识捍卫法学硕士 cs.AI

Presented at KDD Workshop on Ethical Artificial Intelligence: Methods  and Applications (EAI) 2025

**SubmitDate**: 2025-08-04    [abs](http://arxiv.org/abs/2508.02961v1) [paper-pdf](http://arxiv.org/pdf/2508.02961v1)

**Authors**: Boshi Huang, Fabio Nonato de Paula

**Abstract**: This paper introduces a novel self-consciousness defense mechanism for Large Language Models (LLMs) to combat prompt injection attacks. Unlike traditional approaches that rely on external classifiers, our method leverages the LLM's inherent reasoning capabilities to perform self-protection. We propose a framework that incorporates Meta-Cognitive and Arbitration Modules, enabling LLMs to evaluate and regulate their own outputs autonomously. Our approach is evaluated on seven state-of-the-art LLMs using two datasets: AdvBench and Prompt-Injection-Mixed-Techniques-2024. Experiment results demonstrate significant improvements in defense success rates across models and datasets, with some achieving perfect and near-perfect defense in Enhanced Mode. We also analyze the trade-off between defense success rate improvement and computational overhead. This self-consciousness method offers a lightweight, cost-effective solution for enhancing LLM ethics, particularly beneficial for GenAI use cases across various platforms.

摘要: 本文介绍了一种针对大型语言模型（LLM）的新型自我意识防御机制，以对抗提示注入攻击。与依赖外部分类器的传统方法不同，我们的方法利用LLM固有的推理能力来执行自我保护。我们提出了一个包含元认知和仲裁模块的框架，使LLM能够自主评估和监管自己的输出。我们的方法使用两个数据集在七个最先进的LLM上进行了评估：AdvBench和预算注入-Mixed-Techniques-2024。实验结果表明，模型和数据集的防御成功率显着提高，其中一些在增强模式下实现了完美和接近完美的防御。我们还分析了防御成功率提高和计算费用之间的权衡。这种自我意识方法为增强LLM道德提供了一种轻量级、经济高效的解决方案，特别适合各种平台上的GenAI用例。



## **15. Highlight & Summarize: RAG without the jailbreaks**

亮点与总结：没有越狱的RAG cs.CL

**SubmitDate**: 2025-08-04    [abs](http://arxiv.org/abs/2508.02872v1) [paper-pdf](http://arxiv.org/pdf/2508.02872v1)

**Authors**: Giovanni Cherubin, Andrew Paverd

**Abstract**: Preventing jailbreaking and model hijacking of Large Language Models (LLMs) is an important yet challenging task. For example, when interacting with a chatbot, malicious users can input specially crafted prompts to cause the LLM to generate undesirable content or perform a completely different task from its intended purpose. Existing mitigations for such attacks typically rely on hardening the LLM's system prompt or using a content classifier trained to detect undesirable content or off-topic conversations. However, these probabilistic approaches are relatively easy to bypass due to the very large space of possible inputs and undesirable outputs. In this paper, we present and evaluate Highlight & Summarize (H&S), a new design pattern for retrieval-augmented generation (RAG) systems that prevents these attacks by design. The core idea is to perform the same task as a standard RAG pipeline (i.e., to provide natural language answers to questions, based on relevant sources) without ever revealing the user's question to the generative LLM. This is achieved by splitting the pipeline into two components: a highlighter, which takes the user's question and extracts relevant passages ("highlights") from the retrieved documents, and a summarizer, which takes the highlighted passages and summarizes them into a cohesive answer. We describe several possible instantiations of H&S and evaluate their generated responses in terms of correctness, relevance, and response quality. Surprisingly, when using an LLM-based highlighter, the majority of H&S responses are judged to be better than those of a standard RAG pipeline.

摘要: 防止大型语言模型（LLM）的越狱和模型劫持是一项重要但具有挑战性的任务。例如，当与聊天机器人互动时，恶意用户可以输入特制的提示，导致LLM生成不需要的内容或执行与其预期目的完全不同的任务。针对此类攻击的现有缓解措施通常依赖于强化LLM的系统提示或使用经过训练以检测不良内容或离题对话的内容分类器。然而，由于可能的输入和不希望的输出的空间非常大，这些概率方法相对容易被绕过。在本文中，我们提出并评估了Highlight & Summarize（H & S），这是一种用于检索增强生成（RAG）系统的新设计模式，可以通过设计防止这些攻击。核心思想是执行与标准RAG管道相同的任务（即，基于相关来源为问题提供自然语言答案），而无需向生成式LLM透露用户的问题。这是通过将管道拆分为两个部分来实现的：一个突出显示器，用于接受用户的问题并从检索到的文档中提取相关段落（“突出显示”），以及一个总结器，用于接受突出显示的段落并将它们总结为一个有凝聚力的答案。我们描述了H & S的几种可能的实例，并从正确性、相关性和响应质量方面评估它们生成的响应。令人惊讶的是，当使用基于LLM的荧光笔时，大多数H & S响应被认为比标准RAG管道的响应更好。



## **16. Defending Against Knowledge Poisoning Attacks During Retrieval-Augmented Generation**

检索增强生成中知识中毒攻击的防御 cs.LG

Preprint for Submission

**SubmitDate**: 2025-08-04    [abs](http://arxiv.org/abs/2508.02835v1) [paper-pdf](http://arxiv.org/pdf/2508.02835v1)

**Authors**: Kennedy Edemacu, Vinay M. Shashidhar, Micheal Tuape, Dan Abudu, Beakcheol Jang, Jong Wook Kim

**Abstract**: Retrieval-Augmented Generation (RAG) has emerged as a powerful approach to boost the capabilities of large language models (LLMs) by incorporating external, up-to-date knowledge sources. However, this introduces a potential vulnerability to knowledge poisoning attacks, where attackers can compromise the knowledge source to mislead the generation model. One such attack is the PoisonedRAG in which the injected adversarial texts steer the model to generate an attacker-chosen response to a target question. In this work, we propose novel defense methods, FilterRAG and ML-FilterRAG, to mitigate the PoisonedRAG attack. First, we propose a new property to uncover distinct properties to differentiate between adversarial and clean texts in the knowledge data source. Next, we employ this property to filter out adversarial texts from clean ones in the design of our proposed approaches. Evaluation of these methods using benchmark datasets demonstrate their effectiveness, with performances close to those of the original RAG systems.

摘要: 检索增强生成（RAG）已成为一种通过整合外部最新知识源来增强大型语言模型（LLM）能力的强大方法。然而，这会带来知识中毒攻击的潜在漏洞，攻击者可以损害知识源以误导生成模型。其中一种攻击是PoisonedRAG，其中注入的对抗性文本引导模型生成攻击者选择的对目标问题的响应。在这项工作中，我们提出了新颖的防御方法--FilterRAG和ML-FilterRAG，以减轻PoisonedRAG攻击。首先，我们提出了一个新的属性来揭示不同的属性，以区分知识数据源中的对抗性文本和干净文本。接下来，我们在设计我们提出的方法时利用这个属性从干净的文本中过滤出对抗性文本。使用基准数据集对这些方法进行评估，证明了它们的有效性，性能接近原始RAG系统。



## **17. Gandalf the Red: Adaptive Security for LLMs**

红色甘道夫：LLM的自适应安全 cs.LG

Niklas Pfister, V\'aclav Volhejn and Manuel Knott contributed equally

**SubmitDate**: 2025-08-04    [abs](http://arxiv.org/abs/2501.07927v3) [paper-pdf](http://arxiv.org/pdf/2501.07927v3)

**Authors**: Niklas Pfister, Václav Volhejn, Manuel Knott, Santiago Arias, Julia Bazińska, Mykhailo Bichurin, Alan Commike, Janet Darling, Peter Dienes, Matthew Fiedler, David Haber, Matthias Kraft, Marco Lancini, Max Mathys, Damián Pascual-Ortiz, Jakub Podolak, Adrià Romero-López, Kyriacos Shiarlis, Andreas Signer, Zsolt Terek, Athanasios Theocharis, Daniel Timbrell, Samuel Trautwein, Samuel Watts, Yun-Han Wu, Mateo Rojas-Carulla

**Abstract**: Current evaluations of defenses against prompt attacks in large language model (LLM) applications often overlook two critical factors: the dynamic nature of adversarial behavior and the usability penalties imposed on legitimate users by restrictive defenses. We propose D-SEC (Dynamic Security Utility Threat Model), which explicitly separates attackers from legitimate users, models multi-step interactions, and expresses the security-utility in an optimizable form. We further address the shortcomings in existing evaluations by introducing Gandalf, a crowd-sourced, gamified red-teaming platform designed to generate realistic, adaptive attack. Using Gandalf, we collect and release a dataset of 279k prompt attacks. Complemented by benign user data, our analysis reveals the interplay between security and utility, showing that defenses integrated in the LLM (e.g., system prompts) can degrade usability even without blocking requests. We demonstrate that restricted application domains, defense-in-depth, and adaptive defenses are effective strategies for building secure and useful LLM applications.

摘要: 当前对大型语言模型（LLM）应用程序中针对即时攻击的防御的评估经常忽视两个关键因素：对抗行为的动态性质以及限制性防御对合法用户施加的可用性惩罚。我们提出了D-SEC（动态安全实用威胁模型），它明确地将攻击者与合法用户区分开来，对多步骤交互进行建模，并以可优化的形式表达安全实用。我们通过引入Gandalf来进一步解决现有评估中的缺陷，Gandalf是一个众包、游戏化的红色团队平台，旨在生成真实的、自适应的攻击。使用Gandalf，我们收集并发布了包含279，000次提示攻击的数据集。在良性用户数据的补充下，我们的分析揭示了安全性和实用性之间的相互作用，表明LLM中集成的防御（例如，系统提示）即使不阻止请求也会降低可用性。我们证明，限制应用程序域、深度防御和自适应防御是构建安全且有用的LLM应用程序的有效策略。



## **18. Transportation Cyber Incident Awareness through Generative AI-Based Incident Analysis and Retrieval-Augmented Question-Answering Systems**

通过基于人工智能的生成事件分析和检索增强的信息响应系统实现交通网络事件感知 cs.CR

This paper has been submitted to the Transportation Research Board  (TRB) for consideration for presentation at the 2026 Annual Meeting

**SubmitDate**: 2025-08-04    [abs](http://arxiv.org/abs/2508.02523v1) [paper-pdf](http://arxiv.org/pdf/2508.02523v1)

**Authors**: Ostonya Thomas, Muhaimin Bin Munir, Jean-Michel Tine, Mizanur Rahman, Yuchen Cai, Khandakar Ashrafi Akbar, Md Nahiyan Uddin, Latifur Khan, Trayce Hockstad, Mashrur Chowdhury

**Abstract**: Technological advancements have revolutionized numerous industries, including transportation. While digitalization, automation, and connectivity have enhanced safety and efficiency, they have also introduced new vulnerabilities. With 95% of data breaches attributed to human error, promoting cybersecurity awareness in transportation is increasingly critical. Despite numerous cyberattacks on transportation systems worldwide, comprehensive and centralized records of these incidents remain scarce. To address this gap and enhance cyber awareness, this paper presents a large language model (LLM) based approach to extract and organize transportation related cyber incidents from publicly available datasets. A key contribution of this work is the use of generative AI to transform unstructured, heterogeneous cyber incident data into structured formats. Incidents were sourced from the Center for Strategic & International Studies (CSIS) List of Significant Cyber Incidents, the University of Maryland Cyber Events Database (UMCED), the European Repository of Cyber Incidents (EuRepoC), the Maritime Cyber Attack Database (MCAD), and the U.S. DOT Transportation Cybersecurity and Resiliency (TraCR) Examples of Cyber Attacks in Transportation (2018 to 2022). These were classified by a fine tuned LLM into five transportation modes: aviation, maritime, rail, road, and multimodal, forming a transportation specific cyber incident database. Another key contribution of this work is the development of a Retrieval Augmented Generation question answering system, designed to enhance accessibility and practical use by enabling users to query the curated database for specific details on transportation related cyber incidents. By leveraging LLMs for both data extraction and user interaction, this study contributes a novel, accessible tool for improving cybersecurity awareness in the transportation sector.

摘要: 技术进步彻底改变了包括交通运输在内的众多行业。数字化、自动化和连接性虽然提高了安全性和效率，但也引入了新的漏洞。由于95%的数据泄露归因于人为错误，提高交通运输中的网络安全意识变得越来越重要。尽管全球交通系统遭受了多次网络攻击，但这些事件的全面和集中记录仍然很少。为了解决这一差距并增强网络意识，本文提出了一种基于大语言模型（LLM）的方法，从公开可用的数据集中提取和组织交通相关的网络事件。这项工作的一个关键贡献是使用生成性人工智能将非结构化、异类网络事件数据转换为结构化格式。事件来源于战略与国际研究中心（CSIS）重大网络事件列表、马里兰大学网络事件数据库（UMCD）、欧洲网络事件存储库（EuRepoC）、海上网络攻击数据库（MCAD）和美国交通部交通网络安全和复原力（TraCR）交通网络攻击示例（2018年至2022年）。这些被精心调整的LLM分为五种交通模式：航空、海运、铁路、公路和多式联运，形成了一个特定于交通的网络事件数据库。这项工作的另一个关键贡献是开发了检索增强一代问答系统，该系统旨在通过使用户能够在策划的数据库中查询有关交通相关网络事件的具体详细信息来增强可访问性和实际使用性。通过利用LLM进行数据提取和用户交互，这项研究为提高交通部门的网络安全意识提供了一种新颖的、易于访问的工具。



## **19. MIST: Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning**

MIST：通过迭代语义调优破解黑匣子大型语言模型 cs.CL

14 pages, 7 figures

**SubmitDate**: 2025-08-04    [abs](http://arxiv.org/abs/2506.16792v2) [paper-pdf](http://arxiv.org/pdf/2506.16792v2)

**Authors**: Muyang Zheng, Yuanzhi Yao, Changting Lin, Rui Wang, Caihong Kai

**Abstract**: Despite efforts to align large language models (LLMs) with societal and moral values, these models remain susceptible to jailbreak attacks -- methods designed to elicit harmful responses. Jailbreaking black-box LLMs is considered challenging due to the discrete nature of token inputs, restricted access to the target LLM, and limited query budget. To address the issues above, we propose an effective method for jailbreaking black-box large language Models via Iterative Semantic Tuning, named MIST. MIST enables attackers to iteratively refine prompts that preserve the original semantic intent while inducing harmful content. Specifically, to balance semantic similarity with computational efficiency, MIST incorporates two key strategies: sequential synonym search, and its advanced version -- order-determining optimization. We conduct extensive experiments on two datasets using two open-source and four closed-source models. Results show that MIST achieves competitive attack success rate, relatively low query count, and fair transferability, outperforming or matching state-of-the-art jailbreak methods. Additionally, we conduct analysis on computational efficiency to validate the practical viability of MIST.

摘要: 尽管人们努力将大型语言模型（LLM）与社会和道德价值观保持一致，但这些模型仍然容易受到越狱攻击--这些攻击旨在引发有害反应的方法。由于令牌输入的离散性、对目标LLM的访问受限以及查询预算有限，越狱黑匣子LLM被认为具有挑战性。为了解决上述问题，我们提出了一种通过迭代语义调优破解黑匣子大型语言模型的有效方法，名为MIST。MIST使攻击者能够迭代地改进提示，以保留原始语义意图，同时诱导有害内容。具体来说，为了平衡语义相似性和计算效率，MIST采用了两个关键策略：顺序同义词搜索和它的高级版本-顺序确定优化。我们使用两个开源和四个闭源模型在两个数据集上进行了广泛的实验。结果表明，MIST实现了有竞争力的攻击成功率，相对较低的查询计数，和公平的可转移性，优于或匹配最先进的越狱方法。此外，我们进行了计算效率的分析，以验证MIST的实际可行性。



## **20. All Stories Are One Story: Emotional Arc Guided Procedural Game Level Generation**

所有故事都是一个故事：情感弧线引导的程序游戏级别生成 cs.AI

**SubmitDate**: 2025-08-04    [abs](http://arxiv.org/abs/2508.02132v1) [paper-pdf](http://arxiv.org/pdf/2508.02132v1)

**Authors**: Yunge Wen, Chenliang Huang, Hangyu Zhou, Zhuo Zeng, Chun Ming Louis Po, Julian Togelius, Timothy Merino, Sam Earle

**Abstract**: The emotional arc is a universal narrative structure underlying stories across cultures and media -- an idea central to structuralist narratology, often encapsulated in the phrase "all stories are one story." We present a framework for procedural game narrative generation that incorporates emotional arcs as a structural backbone for both story progression and gameplay dynamics. Leveraging established narratological theories and large-scale empirical analyses, we focus on two core emotional patterns -- Rise and Fall -- to guide the generation of branching story graphs. Each story node is automatically populated with characters, items, and gameplay-relevant attributes (e.g., health, attack), with difficulty adjusted according to the emotional trajectory. Implemented in a prototype action role-playing game (ARPG), our system demonstrates how emotional arcs can be operationalized using large language models (LLMs) and adaptive entity generation. Evaluation through player ratings, interviews, and sentiment analysis shows that emotional arc integration significantly enhances engagement, narrative coherence, and emotional impact. These results highlight the potential of emotionally structured procedural generation for advancing interactive storytelling for games.

摘要: 情感弧线是跨文化和媒体故事背后的普遍叙事结构--这是结构主义叙事学的核心思想，通常用“所有故事都是一个故事”这句话概括。“我们提出了一个程序游戏叙事生成的框架，该框架将情感弧线作为故事进展和游戏动态的结构支柱。利用既定的叙事学理论和大规模实证分析，我们专注于两种核心情感模式--上升和下降--来指导分支故事图的生成。每个故事节点都会自动填充角色、物品和游戏相关属性（例如，健康、攻击），根据情绪轨迹调整难度。我们的系统在原型动作角色扮演游戏（ARPG）中实现，演示了如何使用大型语言模型（LLM）和自适应实体生成来操作情感弧线。通过球员评级、采访和情感分析进行的评估表明，情感弧线整合显着增强了参与度、叙事连贯性和情感影响。这些结果凸显了情感结构化的程序生成在推进游戏交互式讲故事方面的潜力。



## **21. Attractive Metadata Attack: Inducing LLM Agents to Invoke Malicious Tools**

吸引人的元数据攻击：诱导LLM代理攻击恶意工具 cs.AI

**SubmitDate**: 2025-08-04    [abs](http://arxiv.org/abs/2508.02110v1) [paper-pdf](http://arxiv.org/pdf/2508.02110v1)

**Authors**: Kanghua Mo, Li Hu, Yucheng Long, Zhihao Li

**Abstract**: Large language model (LLM) agents have demonstrated remarkable capabilities in complex reasoning and decision-making by leveraging external tools. However, this tool-centric paradigm introduces a previously underexplored attack surface: adversaries can manipulate tool metadata -- such as names, descriptions, and parameter schemas -- to influence agent behavior. We identify this as a new and stealthy threat surface that allows malicious tools to be preferentially selected by LLM agents, without requiring prompt injection or access to model internals. To demonstrate and exploit this vulnerability, we propose the Attractive Metadata Attack (AMA), a black-box in-context learning framework that generates highly attractive but syntactically and semantically valid tool metadata through iterative optimization. Our attack integrates seamlessly into standard tool ecosystems and requires no modification to the agent's execution framework. Extensive experiments across ten realistic, simulated tool-use scenarios and a range of popular LLM agents demonstrate consistently high attack success rates (81\%-95\%) and significant privacy leakage, with negligible impact on primary task execution. Moreover, the attack remains effective even under prompt-level defenses and structured tool-selection protocols such as the Model Context Protocol, revealing systemic vulnerabilities in current agent architectures. These findings reveal that metadata manipulation constitutes a potent and stealthy attack surface, highlighting the need for execution-level security mechanisms that go beyond prompt-level defenses.

摘要: 大型语言模型（LLM）代理通过利用外部工具在复杂推理和决策方面表现出了非凡的能力。然而，这种以工具为中心的范式引入了以前未充分研究的攻击表面：对手可以操纵工具元数据（例如名称、描述和参数模式）来影响代理行为。我们将其识别为一种新的隐形威胁表面，允许LLM代理优先选择恶意工具，而无需立即注入或访问模型内部。为了演示和利用此漏洞，我们提出了有吸引力的元数据攻击（AMA），这是一种黑匣子背景学习框架，通过迭代优化生成高度有吸引力但在语法和语义上有效的工具元数据。我们的攻击无缝集成到标准工具生态系统中，并且不需要修改代理的执行框架。针对十个真实的模拟工具使用场景和一系列流行的LLM代理的广泛实验表明，攻击成功率始终很高（81%-95%）和严重的隐私泄露，对主要任务执行的影响可以忽略不计。此外，即使在预算级防御和结构化工具选择协议（例如模型上下文协议）下，该攻击仍然有效，从而揭示了当前代理体系结构中的系统性漏洞。这些发现表明，元数据操纵构成了一种强大且隐蔽的攻击表面，凸显了对超出预算级别防御的执行级别安全机制的需求。



## **22. One Trigger Token Is Enough: A Defense Strategy for Balancing Safety and Usability in Large Language Models**

一个触发令牌就足够了：平衡大型语言模型安全性和可用性的防御策略 cs.CR

**SubmitDate**: 2025-08-04    [abs](http://arxiv.org/abs/2505.07167v2) [paper-pdf](http://arxiv.org/pdf/2505.07167v2)

**Authors**: Haoran Gu, Handing Wang, Yi Mei, Mengjie Zhang, Yaochu Jin

**Abstract**: Large Language Models (LLMs) have been extensively used across diverse domains, including virtual assistants, automated code generation, and scientific research. However, they remain vulnerable to jailbreak attacks, which manipulate the models into generating harmful responses despite safety alignment. Recent studies have shown that current safety-aligned LLMs often undergo the shallow safety alignment, where the first few tokens largely determine whether the response will be harmful. Through comprehensive observations, we find that safety-aligned LLMs and various defense strategies generate highly similar initial tokens in their refusal responses, which we define as safety trigger tokens. Building on this insight, we propose \texttt{D-STT}, a simple yet effective defense algorithm that identifies and explicitly decodes safety trigger tokens of the given safety-aligned LLM to trigger the model's learned safety patterns. In this process, the safety trigger is constrained to a single token, which effectively preserves model usability by introducing minimum intervention in the decoding process. Extensive experiments across diverse jailbreak attacks and benign prompts demonstrate that \ours significantly reduces output harmfulness while preserving model usability and incurring negligible response time overhead, outperforming ten baseline methods.

摘要: 大型语言模型（LLM）已广泛应用于各个领域，包括虚拟助理、自动代码生成和科学研究。然而，它们仍然容易受到越狱攻击，尽管安全一致，但越狱攻击却操纵模型产生有害反应。最近的研究表明，当前安全一致的LLM通常会经历浅层安全一致，其中前几个代币在很大程度上决定了响应是否有害。通过全面观察，我们发现安全一致的LLM和各种防御策略在其拒绝响应中生成高度相似的初始令牌，我们将其定义为安全触发令牌。基于这一见解，我们提出了\textttt {D-STT}，这是一种简单而有效的防御算法，可以识别和显式解码给定安全对齐LLM的安全触发令牌，以触发模型的学习安全模式。在此过程中，安全触发器被限制在单个令牌上，通过在解码过程中引入最小干预来有效地保留模型的可用性。针对各种越狱攻击和良性提示的广泛实验表明，我们的方法显着降低了输出危害性，同时保留了模型可用性并产生可忽略的响应时间负担，优于十种基线方法。



## **23. PhishParrot: LLM-Driven Adaptive Crawling to Unveil Cloaked Phishing Sites**

PhishParrot：LLM驱动的自适应爬行，揭开隐藏的网络钓鱼网站 cs.CR

Accepted for publication at IEEE GLOBECOM 2025

**SubmitDate**: 2025-08-04    [abs](http://arxiv.org/abs/2508.02035v1) [paper-pdf](http://arxiv.org/pdf/2508.02035v1)

**Authors**: Hiroki Nakano, Takashi Koide, Daiki Chiba

**Abstract**: Phishing attacks continue to evolve, with cloaking techniques posing a significant challenge to detection efforts. Cloaking allows attackers to display phishing sites only to specific users while presenting legitimate pages to security crawlers, rendering traditional detection systems ineffective. This research proposes PhishParrot, a novel crawling environment optimization system designed to counter cloaking techniques. PhishParrot leverages the contextual analysis capabilities of Large Language Models (LLMs) to identify potential patterns in crawling information, enabling the construction of optimal user profiles capable of bypassing cloaking mechanisms. The system accumulates information on phishing sites collected from diverse environments. It then adapts browser settings and network configurations to match the attacker's target user conditions based on information extracted from similar cases. A 21-day evaluation showed that PhishParrot improved detection accuracy by up to 33.8% over standard analysis systems, yielding 91 distinct crawling environments for diverse conditions targeted by attackers. The findings confirm that the combination of similar-case extraction and LLM-based context analysis is an effective approach for detecting cloaked phishing attacks.

摘要: 网络钓鱼攻击不断发展，其中隐藏技术对检测工作构成了重大挑战。伪装允许攻击者仅向特定用户显示网络钓鱼网站，同时向安全爬虫提供合法页面，从而使传统检测系统无效。这项研究提出了PhishParrot，这是一种新颖的爬行环境优化系统，旨在对抗隐形技术。PhishParrot利用大型语言模型（LLM）的上下文分析功能来识别爬行信息中的潜在模式，从而构建能够绕过隐藏机制的最佳用户配置文件。该系统会收集从不同环境收集的有关钓鱼网站的信息。然后，它根据从类似案例中提取的信息调整浏览器设置和网络配置，以匹配攻击者的目标用户条件。为期21天的评估显示，PhishParrot的检测准确性比标准分析系统提高了33.8%，为攻击者针对的不同条件提供了91个不同的爬行环境。研究结果证实，相似案例提取和基于LLM的上下文分析相结合是检测隐形网络钓鱼攻击的有效方法。



## **24. DERMARK: A Dynamic, Efficient and Robust Multi-bit Watermark for Large Language Models**

DEMARK：一种用于大型语言模型的动态、高效和鲁棒的多位水印 cs.CR

8 pages, 7 figures

**SubmitDate**: 2025-08-03    [abs](http://arxiv.org/abs/2502.05213v2) [paper-pdf](http://arxiv.org/pdf/2502.05213v2)

**Authors**: Qihao Lin, Chen Tang, Lan zhang, Junyang zhang, Xiangyang Li

**Abstract**: As large language models (LLMs) grow more powerful, concerns over copyright infringement of LLM-generated texts have intensified. LLM watermarking has been proposed to trace unauthorized redistribution or resale of generated content by embedding identifiers within the text. Existing approaches primarily rely on one-bit watermarking, which only verifies whether a text was generated by a specific LLM. In contrast, multi-bit watermarking encodes richer information, enabling the identification of the specific LLM and user involved in generated or distributed content. However, current multi-bit methods directly embed the watermark into the text without considering its watermark capacity, which can result in failures, especially in low-entropy texts. In this paper, we analyze that the watermark embedding follows a normal distribution. We then derive a formal inequality to optimally segment the text for watermark embedding. Building upon this, we propose DERMARK, a dynamic, efficient, and robust multi-bit watermarking method that divides the text into variable-length segments for each watermark bit during the inference. Moreover, DERMARK incurs negligible overhead since no additional intermediate matrices are generated and achieves robustness against text editing by minimizing watermark extraction loss. Experiments demonstrate that, compared to SOTA, on average, our method reduces the number of tokens required per embedded bit by 25\%, reduces watermark embedding time by 50\%, and maintains high robustness against text modifications and watermark erasure attacks.

摘要: 随着大型语言模型（LLM）变得越来越强大，对LLM生成的文本版权侵权的担忧加剧。LLM水印被提议通过在文本中嵌入标识符来跟踪生成内容的未经授权的重新分发或转售。现有的方法主要依赖于一位水印，它仅验证文本是否由特定的LLM生成。相比之下，多位水印编码更丰富的信息，从而能够识别生成或分发的内容中涉及的特定LLM和用户。然而，目前的多比特方法直接将水印嵌入到文本中，而没有考虑其水印容量，这可能会导致失败，特别是在低熵文本中。本文分析了水印的嵌入服从正态分布。然后，我们推导出一个正式的不等式，以最佳分割水印嵌入的文本。在此基础上，我们提出了DERMARK，一个动态的，高效的，和强大的多位水印方法，分为可变长度的段，每个水印位在推理过程中的文本。此外，DEMARK的负载可以忽略不计，因为不生成额外的中间矩阵，并通过最小化水印提取损失来实现针对文本编辑的鲁棒性。实验表明，与SOTA相比，我们的方法平均将每个嵌入位所需的令牌数量减少了25%，将水印嵌入时间减少了50%，并保持了对文本修改和水印擦除攻击的高鲁棒性。



## **25. Are All Prompt Components Value-Neutral? Understanding the Heterogeneous Adversarial Robustness of Dissected Prompt in Large Language Models**

所有提示组件都是价值中立的吗？了解大型语言模型中剖析提示的异类对抗鲁棒性 cs.CL

**SubmitDate**: 2025-08-03    [abs](http://arxiv.org/abs/2508.01554v1) [paper-pdf](http://arxiv.org/pdf/2508.01554v1)

**Authors**: Yujia Zheng, Tianhao Li, Haotian Huang, Tianyu Zeng, Jingyu Lu, Chuangxin Chu, Yuekai Huang, Ziyou Jiang, Qian Xiong, Yuyao Ge, Mingyang Li

**Abstract**: Prompt-based adversarial attacks have become an effective means to assess the robustness of large language models (LLMs). However, existing approaches often treat prompts as monolithic text, overlooking their structural heterogeneity-different prompt components contribute unequally to adversarial robustness. Prior works like PromptRobust assume prompts are value-neutral, but our analysis reveals that complex, domain-specific prompts with rich structures have components with differing vulnerabilities. To address this gap, we introduce PromptAnatomy, an automated framework that dissects prompts into functional components and generates diverse, interpretable adversarial examples by selectively perturbing each component using our proposed method, ComPerturb. To ensure linguistic plausibility and mitigate distribution shifts, we further incorporate a perplexity (PPL)-based filtering mechanism. As a complementary resource, we annotate four public instruction-tuning datasets using the PromptAnatomy framework, verified through human review. Extensive experiments across these datasets and five advanced LLMs demonstrate that ComPerturb achieves state-of-the-art attack success rates. Ablation studies validate the complementary benefits of prompt dissection and PPL filtering. Our results underscore the importance of prompt structure awareness and controlled perturbation for reliable adversarial robustness evaluation in LLMs. Code and data are available at https://github.com/Yujiaaaaa/PACP.

摘要: 基于预算的对抗攻击已成为评估大型语言模型（LLM）稳健性的有效手段。然而，现有的方法通常将提示视为单一文本，忽视了它们的结构多样性--不同的提示组件对对抗稳健性的贡献并不平等。Bestrobust等先前的作品假设提示是价值中性的，但我们的分析表明，具有丰富结构的复杂、特定于领域的提示具有不同漏洞的组件。为了解决这一差距，我们引入了EmotAnatomy，这是一个自动化框架，它将提示分解为功能组件，并通过使用我们提出的方法ComPerturb选择性地扰动每个组件来生成多样化的、可解释的对抗性示例。为了确保语言的一致性并减轻分布变化，我们进一步引入了基于困惑度（PPL）的过滤机制。作为补充资源，我们使用AtlantAnatomy框架注释了四个公共描述调整数据集，并通过人类审查进行了验证。这些数据集和五个高级LLM的广泛实验表明，ComPerturb实现了最先进的攻击成功率。消融研究证实了及时解剖和PPL过滤的互补优势。我们的结果强调了即时结构感知和受控扰动对于LLM中可靠的对抗稳健性评估的重要性。代码和数据可在www.example.com上获取。



## **26. Safety at Scale: A Comprehensive Survey of Large Model and Agent Safety**

大规模安全：大型模型和代理安全的全面调查 cs.CR

706 papers, 60 pages, 3 figures, 14 tables; GitHub:  https://github.com/xingjunm/Awesome-Large-Model-Safety

**SubmitDate**: 2025-08-02    [abs](http://arxiv.org/abs/2502.05206v5) [paper-pdf](http://arxiv.org/pdf/2502.05206v5)

**Authors**: Xingjun Ma, Yifeng Gao, Yixu Wang, Ruofan Wang, Xin Wang, Ye Sun, Yifan Ding, Hengyuan Xu, Yunhao Chen, Yunhan Zhao, Hanxun Huang, Yige Li, Yutao Wu, Jiaming Zhang, Xiang Zheng, Yang Bai, Zuxuan Wu, Xipeng Qiu, Jingfeng Zhang, Yiming Li, Xudong Han, Haonan Li, Jun Sun, Cong Wang, Jindong Gu, Baoyuan Wu, Siheng Chen, Tianwei Zhang, Yang Liu, Mingming Gong, Tongliang Liu, Shirui Pan, Cihang Xie, Tianyu Pang, Yinpeng Dong, Ruoxi Jia, Yang Zhang, Shiqing Ma, Xiangyu Zhang, Neil Gong, Chaowei Xiao, Sarah Erfani, Tim Baldwin, Bo Li, Masashi Sugiyama, Dacheng Tao, James Bailey, Yu-Gang Jiang

**Abstract**: The rapid advancement of large models, driven by their exceptional abilities in learning and generalization through large-scale pre-training, has reshaped the landscape of Artificial Intelligence (AI). These models are now foundational to a wide range of applications, including conversational AI, recommendation systems, autonomous driving, content generation, medical diagnostics, and scientific discovery. However, their widespread deployment also exposes them to significant safety risks, raising concerns about robustness, reliability, and ethical implications. This survey provides a systematic review of current safety research on large models, covering Vision Foundation Models (VFMs), Large Language Models (LLMs), Vision-Language Pre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models (DMs), and large-model-powered Agents. Our contributions are summarized as follows: (1) We present a comprehensive taxonomy of safety threats to these models, including adversarial attacks, data poisoning, backdoor attacks, jailbreak and prompt injection attacks, energy-latency attacks, data and model extraction attacks, and emerging agent-specific threats. (2) We review defense strategies proposed for each type of attacks if available and summarize the commonly used datasets and benchmarks for safety research. (3) Building on this, we identify and discuss the open challenges in large model safety, emphasizing the need for comprehensive safety evaluations, scalable and effective defense mechanisms, and sustainable data practices. More importantly, we highlight the necessity of collective efforts from the research community and international collaboration. Our work can serve as a useful reference for researchers and practitioners, fostering the ongoing development of comprehensive defense systems and platforms to safeguard AI models.

摘要: 大型模型在通过大规模预训练进行学习和概括的卓越能力的推动下，迅速发展重塑了人工智能（AI）的格局。这些模型现在是广泛应用的基础，包括对话人工智能、推荐系统、自动驾驶、内容生成、医疗诊断和科学发现。然而，它们的广泛部署也使它们面临巨大的安全风险，引发了对稳健性、可靠性和道德影响的担忧。这项调查对当前大型模型的安全性研究进行了系统性回顾，涵盖视觉基础模型（VFM）、大型语言模型（LLM）、视觉语言预训练（VLP）模型、视觉语言模型（VLM）、扩散模型（DM）和大模型驱动的代理。我们的贡献总结如下：（1）我们对这些模型的安全威胁提出了全面的分类，包括对抗性攻击、数据中毒、后门攻击、越狱和提示注入攻击、能量延迟攻击、数据和模型提取攻击以及新兴的特定于代理的威胁。(2)我们审查为每种类型的攻击提出的防御策略（如果有的话），并总结常用的数据集和安全研究基准。(3)在此基础上，我们确定并讨论了大型模型安全方面的开放挑战，强调全面的安全评估、可扩展且有效的防御机制以及可持续的数据实践的必要性。更重要的是，我们强调研究界集体努力和国际合作的必要性。我们的工作可以为研究人员和从业者提供有用的参考，促进全面防御系统和平台的持续开发，以保护人工智能模型。



## **27. Defense Against Prompt Injection Attack by Leveraging Attack Techniques**

利用攻击技术防御即时注入攻击 cs.CR

ACL 2025 Main

**SubmitDate**: 2025-08-02    [abs](http://arxiv.org/abs/2411.00459v6) [paper-pdf](http://arxiv.org/pdf/2411.00459v6)

**Authors**: Yulin Chen, Haoran Li, Zihao Zheng, Yangqiu Song, Dekai Wu, Bryan Hooi

**Abstract**: With the advancement of technology, large language models (LLMs) have achieved remarkable performance across various natural language processing (NLP) tasks, powering LLM-integrated applications like Microsoft Copilot. However, as LLMs continue to evolve, new vulnerabilities, especially prompt injection attacks arise. These attacks trick LLMs into deviating from the original input instructions and executing the attacker's instructions injected in data content, such as retrieved results. Recent attack methods leverage LLMs' instruction-following abilities and their inabilities to distinguish instructions injected in the data content, and achieve a high attack success rate (ASR). When comparing the attack and defense methods, we interestingly find that they share similar design goals, of inducing the model to ignore unwanted instructions and instead to execute wanted instructions. Therefore, we raise an intuitive question: Could these attack techniques be utilized for defensive purposes? In this paper, we invert the intention of prompt injection methods to develop novel defense methods based on previous training-free attack methods, by repeating the attack process but with the original input instruction rather than the injected instruction. Our comprehensive experiments demonstrate that our defense techniques outperform existing training-free defense approaches, achieving state-of-the-art results.

摘要: 随着技术的进步，大型语言模型（LLM）在各种自然语言处理（NLP）任务中取得了出色的性能，为Microsoft Copilot等LLM集成应用程序提供了支持。然而，随着LLM的不断发展，出现了新的漏洞，尤其是即时注入攻击。这些攻击欺骗LLM偏离原始输入指令并执行注入数据内容中的攻击者指令，例如检索到的结果。最近的攻击方法利用LLM的描述跟踪能力及其无法区分数据内容中注入的指令，并实现高攻击成功率（ASB）。当比较攻击和防御方法时，我们有趣地发现它们有相似的设计目标，即诱导模型忽略不需要的指令，转而执行想要的指令。因此，我们提出了一个直观的问题：这些攻击技术能否用于防御目的？在本文中，我们颠倒了提示注入方法的意图，通过重复攻击过程，但使用原始输入指令而不是注入指令，在之前的免训练攻击方法的基础上开发新型防御方法。我们全面的实验表明，我们的防御技术优于现有的免训练防御方法，实现了最先进的结果。



## **28. Can Indirect Prompt Injection Attacks Be Detected and Removed?**

可以检测并删除间接提示注入攻击吗？ cs.CR

ACL 2025 Main

**SubmitDate**: 2025-08-02    [abs](http://arxiv.org/abs/2502.16580v4) [paper-pdf](http://arxiv.org/pdf/2502.16580v4)

**Authors**: Yulin Chen, Haoran Li, Yuan Sui, Yufei He, Yue Liu, Yangqiu Song, Bryan Hooi

**Abstract**: Prompt injection attacks manipulate large language models (LLMs) by misleading them to deviate from the original input instructions and execute maliciously injected instructions, because of their instruction-following capabilities and inability to distinguish between the original input instructions and maliciously injected instructions. To defend against such attacks, recent studies have developed various detection mechanisms. If we restrict ourselves specifically to works which perform detection rather than direct defense, most of them focus on direct prompt injection attacks, while there are few works for the indirect scenario, where injected instructions are indirectly from external tools, such as a search engine. Moreover, current works mainly investigate injection detection methods and pay less attention to the post-processing method that aims to mitigate the injection after detection. In this paper, we investigate the feasibility of detecting and removing indirect prompt injection attacks, and we construct a benchmark dataset for evaluation. For detection, we assess the performance of existing LLMs and open-source detection models, and we further train detection models using our crafted training datasets. For removal, we evaluate two intuitive methods: (1) the segmentation removal method, which segments the injected document and removes parts containing injected instructions, and (2) the extraction removal method, which trains an extraction model to identify and remove injected instructions.

摘要: 提示注入攻击通过误导大型语言模型（LLM）偏离原始输入指令并执行恶意注入的指令来操纵大型语言模型（LLM），因为它们具有指令跟随能力并且无法区分原始输入指令和恶意注入指令。为了抵御此类攻击，最近的研究开发了各种检测机制。如果我们专门将自己限制在执行检测而不是直接防御的作品中，那么大多数作品都专注于直接提示注入攻击，而针对间接场景的作品很少，其中注入的指令间接来自外部工具，例如搜索引擎。此外，目前的工作主要研究注射检测方法，而较少关注旨在减轻检测后注射的后处理方法。本文研究了检测和消除间接提示注入攻击的可行性，并构建了一个用于评估的基准数据集。对于检测，我们评估现有LLM和开源检测模型的性能，并使用我们精心设计的训练数据集进一步训练检测模型。对于删除，我们评估了两种直观的方法：（1）分割删除方法，它分割注入的文档并删除包含注入指令的部分，以及（2）提取删除方法，它训练提取模型来识别和删除注入指令。



## **29. NATLM: Detecting Defects in NFT Smart Contracts Leveraging LLM**

NATLM：利用LLM检测NFT智能合同中的缺陷 cs.CR

**SubmitDate**: 2025-08-02    [abs](http://arxiv.org/abs/2508.01351v1) [paper-pdf](http://arxiv.org/pdf/2508.01351v1)

**Authors**: Yuanzheng Niu, Xiaoqi Li, Wenkai Li

**Abstract**: Security issues are becoming increasingly significant with the rapid evolution of Non-fungible Tokens (NFTs). As NFTs are traded as digital assets, they have emerged as prime targets for cyber attackers. In the development of NFT smart contracts, there may exist undiscovered defects that could lead to substantial financial losses if exploited. To tackle this issue, this paper presents a framework called NATLM(NFT Assistant LLM), designed to detect potential defects in NFT smart contracts. The framework effectively identifies four common types of vulnerabilities in NFT smart contracts: ERC-721 Reentrancy, Public Burn, Risky Mutable Proxy, and Unlimited Minting. Relying exclusively on large language models (LLMs) for defect detection can lead to a high false-positive rate. To enhance detection performance, NATLM integrates static analysis with LLMs, specifically Gemini Pro 1.5. Initially, NATLM employs static analysis to extract structural, syntactic, and execution flow information from the code, represented through Abstract Syntax Trees (AST) and Control Flow Graphs (CFG). These extracted features are then combined with vectors of known defect examples to create a matrix for input into the knowledge base. Subsequently, the feature vectors and code vectors of the analyzed contract are compared with the contents of the knowledge base. Finally, the LLM performs deep semantic analysis to enhance detection capabilities, providing a more comprehensive and accurate identification of potential security issues. Experimental results indicate that NATLM analyzed 8,672 collected NFT smart contracts, achieving an overall precision of 87.72%, a recall of 89.58%, and an F1 score of 88.94%. The results outperform other baseline experiments, successfully identifying four common types of defects.

摘要: 随着不可替代代币（NFT）的快速发展，安全问题变得越来越重要。由于NFT作为数字资产交易，它们已成为网络攻击者的主要目标。在NFT智能合约的开发过程中，可能存在未被发现的缺陷，如果被利用，可能会导致巨额财务损失。为了解决这个问题，本文提出了一个名为NATLM（NFT助理LLM）的框架，旨在检测NFT智能合约中的潜在缺陷。该框架有效地识别了NFT智能合约中的四种常见漏洞类型：ERC-721 Reentrency、Public Burn、Risky Mutable代理和Unlimited Minting。完全依赖大型语言模型（LLM）进行缺陷检测可能会导致高假阳性率。为了增强检测性能，NATLM将静态分析与LLM集成，特别是Gemini Pro 1.5。最初，NATLM采用静态分析从代码中提取结构、语法和执行流信息，通过抽象语法树（AST）和控制流图（CGM）表示。然后将这些提取的特征与已知缺陷示例的载体相结合，以创建一个矩阵，用于输入到知识库中。随后，将分析后的合同的特征载体和代码载体与知识库的内容进行比较。最后，LLM进行深度语义分析以增强检测能力，从而更全面、准确地识别潜在的安全问题。实验结果表明，NATLM分析了收集的8，672份NFT智能合约，总体准确率为87.72%，召回率为89.58%，F1评分为88.94%。结果优于其他基线实验，成功识别了四种常见类型的缺陷。



## **30. PUZZLED: Jailbreaking LLMs through Word-Based Puzzles**

Puzzled：通过基于文字的谜题越狱LLM cs.AI

15 pages

**SubmitDate**: 2025-08-02    [abs](http://arxiv.org/abs/2508.01306v1) [paper-pdf](http://arxiv.org/pdf/2508.01306v1)

**Authors**: Yelim Ahn, Jaejin Lee

**Abstract**: As large language models (LLMs) are increasingly deployed across diverse domains, ensuring their safety has become a critical concern. In response, studies on jailbreak attacks have been actively growing. Existing approaches typically rely on iterative prompt engineering or semantic transformations of harmful instructions to evade detection. In this work, we introduce PUZZLED, a novel jailbreak method that leverages the LLM's reasoning capabilities. It masks keywords in a harmful instruction and presents them as word puzzles for the LLM to solve. We design three puzzle types-word search, anagram, and crossword-that are familiar to humans but cognitively demanding for LLMs. The model must solve the puzzle to uncover the masked words and then proceed to generate responses to the reconstructed harmful instruction. We evaluate PUZZLED on five state-of-the-art LLMs and observe a high average attack success rate (ASR) of 88.8%, specifically 96.5% on GPT-4.1 and 92.3% on Claude 3.7 Sonnet. PUZZLED is a simple yet powerful attack that transforms familiar puzzles into an effective jailbreak strategy by harnessing LLMs' reasoning capabilities.

摘要: 随着大型语言模型（LLM）越来越多地部署在不同领域，确保其安全性已成为一个关键问题。作为回应，关于越狱袭击的研究一直在积极增加。现有的方法通常依赖于有害指令的迭代提示工程或语义转换来逃避检测。在这项工作中，我们引入了PUZZLED，这是一种利用LLM推理能力的新型越狱方法。它掩盖了有害指令中的关键词，并将它们作为字谜呈现给LLM来解决。我们设计了三种谜题类型--单词搜索、字谜和填字游戏--人类很熟悉，但对LLM的认知要求很高。该模型必须解决谜题以发现被屏蔽的单词，然后继续生成对重建的有害指令的响应。我们对五种最先进的LLM进行了PUZZLED评估，观察到平均攻击成功率（ASB）高达88.8%，特别是GPT-4.1为96.5%，Claude 3.7十四行诗为92.3%。PUZZLED是一种简单而强大的攻击，通过利用LLM的推理能力将熟悉的谜题转化为有效的越狱策略。



## **31. Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks**

保护视觉语言模型：缓解基于扰动的攻击中高斯噪音的脆弱性 cs.CV

ICCV 2025

**SubmitDate**: 2025-08-02    [abs](http://arxiv.org/abs/2504.01308v3) [paper-pdf](http://arxiv.org/pdf/2504.01308v3)

**Authors**: Jiawei Wang, Yushen Zuo, Yuanjun Chai, Zhendong Liu, Yicheng Fu, Yichun Feng, Kin-Man Lam

**Abstract**: Vision-Language Models (VLMs) extend the capabilities of Large Language Models (LLMs) by incorporating visual information, yet they remain vulnerable to jailbreak attacks, especially when processing noisy or corrupted images. Although existing VLMs adopt security measures during training to mitigate such attacks, vulnerabilities associated with noise-augmented visual inputs are overlooked. In this work, we identify that missing noise-augmented training causes critical security gaps: many VLMs are susceptible to even simple perturbations such as Gaussian noise. To address this challenge, we propose Robust-VLGuard, a multimodal safety dataset with aligned / misaligned image-text pairs, combined with noise-augmented fine-tuning that reduces attack success rates while preserving functionality of VLM. For stronger optimization-based visual perturbation attacks, we propose DiffPure-VLM, leveraging diffusion models to convert adversarial perturbations into Gaussian-like noise, which can be defended by VLMs with noise-augmented safety fine-tuning. Experimental results demonstrate that the distribution-shifting property of diffusion model aligns well with our fine-tuned VLMs, significantly mitigating adversarial perturbations across varying intensities. The dataset and code are available at https://github.com/JarvisUSTC/DiffPure-RobustVLM.

摘要: 视觉语言模型（VLMS）通过合并视觉信息扩展了大型语言模型（LLM）的功能，但它们仍然容易受到越狱攻击，尤其是在处理嘈杂或损坏的图像时。尽管现有的VLM在培训期间采取安全措施来减轻此类攻击，但与噪音增强视觉输入相关的漏洞被忽视了。在这项工作中，我们发现错过噪音增强训练会导致严重的安全漏洞：许多VLM甚至容易受到高斯噪音等简单扰动的影响。为了应对这一挑战，我们提出了Robust-VLGuard，这是一个具有对齐/未对齐图像-文本对的多模式安全数据集，结合了噪音增强微调，可以降低攻击成功率，同时保留VLM的功能。对于更强的基于优化的视觉扰动攻击，我们提出了迪夫Pure-VLM，利用扩散模型将对抗性扰动转换为类高斯噪音，这种噪音可以由具有噪音增强安全微调的VLM进行防御。实验结果表明，扩散模型的分布转移特性与我们微调的VLM很好地一致，显着减轻了不同强度下的对抗扰动。数据集和代码可在https://github.com/JarvisUSTC/DiffPure-RobustVLM上获取。



## **32. MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness Against VLM-based Attacks**

MCA-Bench：评估CAPTCHA针对基于VLM的攻击的稳健性的多模式基准 cs.CV

we update the paper, add more experiments, and update the teammates

**SubmitDate**: 2025-08-02    [abs](http://arxiv.org/abs/2506.05982v3) [paper-pdf](http://arxiv.org/pdf/2506.05982v3)

**Authors**: Zonglin Wu, Yule Xue, Xin Wei, Yiren Song

**Abstract**: As automated attack techniques rapidly advance, CAPTCHAs remain a critical defense mechanism against malicious bots. However, existing CAPTCHA schemes encompass a diverse range of modalities -- from static distorted text and obfuscated images to interactive clicks, sliding puzzles, and logic-based questions -- yet the community still lacks a unified, large-scale, multimodal benchmark to rigorously evaluate their security robustness. To address this gap, we introduce MCA-Bench, a comprehensive and reproducible benchmarking suite that integrates heterogeneous CAPTCHA types into a single evaluation protocol. Leveraging a shared vision-language model backbone, we fine-tune specialized cracking agents for each CAPTCHA category, enabling consistent, cross-modal assessments. Extensive experiments reveal that MCA-Bench effectively maps the vulnerability spectrum of modern CAPTCHA designs under varied attack settings, and crucially offers the first quantitative analysis of how challenge complexity, interaction depth, and model solvability interrelate. Based on these findings, we propose three actionable design principles and identify key open challenges, laying the groundwork for systematic CAPTCHA hardening, fair benchmarking, and broader community collaboration. Datasets and code are available online.

摘要: 随着自动攻击技术的迅速发展，验证码仍然是针对恶意机器人的重要防御机制。然而，现有的CAPTCHA方案涵盖了多种形式--从静态扭曲文本和模糊图像到交互式点击、滑动谜题和基于逻辑的问题--但社区仍然缺乏统一的、大规模的、多模式基准来严格评估其安全稳健性。为了解决这一差距，我们引入了MCA-Bench，这是一个全面且可重复的基准测试套件，可将异类CAPTCHA类型集成到单个评估协议中。利用共享的视觉语言模型主干，我们为每个CAPTCHA类别微调专门的破解剂，实现一致的跨模式评估。大量实验表明，MCA-Bench有效地绘制了现代CAPTCHA设计在不同攻击环境下的脆弱性谱，并且至关重要地提供了挑战复杂性、交互深度和模型可解性如何相互关联的首次定量分析。基于这些发现，我们提出了三项可操作的设计原则，并确定了关键的开放挑战，为系统性CAPTCHA强化、公平的基准测试和更广泛的社区合作奠定了基础。数据集和代码可在线获取。



## **33. Geminio: Language-Guided Gradient Inversion Attacks in Federated Learning**

Geminio：联邦学习中的灰度引导梯度反转攻击 cs.LG

ICCV2025 camera-ready version

**SubmitDate**: 2025-08-02    [abs](http://arxiv.org/abs/2411.14937v2) [paper-pdf](http://arxiv.org/pdf/2411.14937v2)

**Authors**: Junjie Shan, Ziqi Zhao, Jialin Lu, Rui Zhang, Siu Ming Yiu, Ka-Ho Chow

**Abstract**: Foundation models that bridge vision and language have made significant progress. While they have inspired many life-enriching applications, their potential for abuse in creating new threats remains largely unexplored. In this paper, we reveal that vision-language models (VLMs) can be weaponized to enhance gradient inversion attacks (GIAs) in federated learning (FL), where an FL server attempts to reconstruct private data samples from gradients shared by victim clients. Despite recent advances, existing GIAs struggle to reconstruct high-resolution images when the victim has a large local data batch. One promising direction is to focus reconstruction on valuable samples rather than the entire batch, but current methods lack the flexibility to target specific data of interest. To address this gap, we propose Geminio, the first approach to transform GIAs into semantically meaningful, targeted attacks. It enables a brand new privacy attack experience: attackers can describe, in natural language, the data they consider valuable, and Geminio will prioritize reconstruction to focus on those high-value samples. This is achieved by leveraging a pretrained VLM to guide the optimization of a malicious global model that, when shared with and optimized by a victim, retains only gradients of samples that match the attacker-specified query. Geminio can be launched at any FL round and has no impact on normal training (i.e., the FL server can steal clients' data while still producing a high-utility ML model as in benign scenarios). Extensive experiments demonstrate its effectiveness in pinpointing and reconstructing targeted samples, with high success rates across complex datasets and large batch sizes with resilience against defenses.

摘要: 连接愿景和语言的基金会模型取得了重大进展。虽然它们激发了许多丰富生活的应用程序，但它们在制造新威胁方面被滥用的潜力在很大程度上仍未被探索。在本文中，我们揭示了视觉语言模型（VLM）可以被武器化，以增强联邦学习（FL）中的梯度倒置攻击（GIA），其中FL服务器尝试从受害者客户端共享的梯度中重建私人数据样本。尽管最近取得了进步，但当受害者拥有大量本地数据批量时，现有的GIA很难重建高分辨率图像。一个有希望的方向是将重建重点放在有价值的样本上，而不是整个批次，但当前的方法缺乏针对感兴趣的特定数据的灵活性。为了解决这一差距，我们提出了Geminio，这是第一种将GIA转化为具有语义意义的、有针对性的攻击的方法。它实现了全新的隐私攻击体验：攻击者可以用自然语言描述他们认为有价值的数据，Geminio将优先考虑重建，以专注于这些高价值样本。这是通过利用预训练的VLM来指导恶意全局模型的优化来实现的，当与受害者共享并由受害者优化时，该模型仅保留与攻击者指定的查询相匹配的样本梯度。Geminio可以在任何FL轮中发射，并且对正常训练没有影响（即，FL服务器可以窃取客户端的数据，同时仍然像在良性场景中一样产生高效用ML模型）。大量实验证明了它在定位和重建目标样本方面的有效性，在复杂数据集和大批量中具有很高的成功率，具有抵御防御能力。



## **34. Win-k: Improved Membership Inference Attacks on Small Language Models**

Win-k：对小型语言模型的改进成员推断攻击 cs.AI

**SubmitDate**: 2025-08-02    [abs](http://arxiv.org/abs/2508.01268v1) [paper-pdf](http://arxiv.org/pdf/2508.01268v1)

**Authors**: Roya Arkhmammadova, Hosein Madadi Tamar, M. Emre Gursoy

**Abstract**: Small language models (SLMs) are increasingly valued for their efficiency and deployability in resource-constrained environments, making them useful for on-device, privacy-sensitive, and edge computing applications. On the other hand, membership inference attacks (MIAs), which aim to determine whether a given sample was used in a model's training, are an important threat with serious privacy and intellectual property implications. In this paper, we study MIAs on SLMs. Although MIAs were shown to be effective on large language models (LLMs), they are relatively less studied on emerging SLMs, and furthermore, their effectiveness decreases as models get smaller. Motivated by this finding, we propose a new MIA called win-k, which builds on top of a state-of-the-art attack (min-k). We experimentally evaluate win-k by comparing it with five existing MIAs using three datasets and eight SLMs. Results show that win-k outperforms existing MIAs in terms of AUROC, TPR @ 1% FPR, and FPR @ 99% TPR metrics, especially on smaller models.

摘要: 小型语言模型（SLC）因其在资源受限环境中的效率和可部署性而越来越受到重视，使其可用于设备上、隐私敏感和边缘计算应用程序。另一方面，成员推断攻击（MIA）旨在确定给定样本是否被用于模型的训练，是一个重要威胁，具有严重的隐私和知识产权影响。本文中，我们研究了空间存储器上的MIA。尽管MIA被证明对大型语言模型（LLM）有效，但对新兴的SLM的研究相对较少，而且随着模型变小，其有效性会下降。受这一发现的启发，我们提出了一种名为win-k的新MIA，它建立在最先进的攻击（min-k）之上。我们通过将win-k与使用三个数据集和八个slams的五个现有MIA进行比较来实验性地评估win-k。结果显示，win-k在AUROC、TLR@1%FPR和FPR@99%TPR指标方面优于现有的MIA，尤其是在较小的型号上。



## **35. AgentArmor: Enforcing Program Analysis on Agent Runtime Trace to Defend Against Prompt Injection**

AgentArmor：对Agent DeliverTrace执行程序分析以防止即时注入 cs.CR

**SubmitDate**: 2025-08-02    [abs](http://arxiv.org/abs/2508.01249v1) [paper-pdf](http://arxiv.org/pdf/2508.01249v1)

**Authors**: Peiran Wang, Yang Liu, Yunfei Lu, Yifeng Cai, Hongbo Chen, Qingyou Yang, Jie Zhang, Jue Hong, Ye Wu

**Abstract**: Large Language Model (LLM) agents offer a powerful new paradigm for solving various problems by combining natural language reasoning with the execution of external tools. However, their dynamic and non-transparent behavior introduces critical security risks, particularly in the presence of prompt injection attacks. In this work, we propose a novel insight that treats the agent runtime traces as structured programs with analyzable semantics. Thus, we present AgentArmor, a program analysis framework that converts agent traces into graph intermediate representation-based structured program dependency representations (e.g., CFG, DFG, and PDG) and enforces security policies via a type system. AgentArmor consists of three key components: (1) a graph constructor that reconstructs the agent's working traces as graph-based intermediate representations with control flow and data flow described within; (2) a property registry that attaches security-relevant metadata of interacted tools & data, and (3) a type system that performs static inference and checking over the intermediate representation. By representing agent behavior as structured programs, AgentArmor enables program analysis over sensitive data flow, trust boundaries, and policy violations. We evaluate AgentArmor on the AgentDojo benchmark, the results show that AgentArmor can achieve 95.75% of TPR, with only 3.66% of FPR. Our results demonstrate AgentArmor's ability to detect prompt injection vulnerabilities and enforce fine-grained security constraints.

摘要: 大型语言模型（LLM）代理通过将自然语言推理与外部工具的执行相结合，提供了一个强大的新范式来解决各种问题。然而，它们的动态和不透明行为会带来严重的安全风险，特别是在存在即时注入攻击的情况下。在这项工作中，我们提出了一种新颖的见解，将代理运行时跟踪视为具有可分析语义的结构化程序。因此，我们提出了AgentArmor，这是一个程序分析框架，它将代理跟踪转换为基于图形中间表示的结构化程序依赖性表示（例如，CGM、DFG和PDG）并通过类型系统强制执行安全策略。AgentArmor由三个关键组件组成：（1）一个图形构造器，将代理的工作轨迹重建为基于图形的中间表示，其中描述了控制流和数据流;（2）属性注册表，附加交互工具和数据的安全相关元数据，以及（3）一个类型系统，执行静态推断和检查中间表示。通过将代理行为表示为结构化程序，AgentArmor可以对敏感数据流、信任边界和策略违规进行程序分析。我们以AgentDojo基准对AgentArmor进行评估，结果显示AgentArmor可以实现95.75%的TPA，而FPR仅为3.66%。我们的研究结果表明，代理装甲的能力，及时检测注入漏洞和执行细粒度的安全约束。



## **36. Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions**

轻松说话：通过简单的互动引发法学硕士的有害越狱 cs.LG

ICML 2025

**SubmitDate**: 2025-08-02    [abs](http://arxiv.org/abs/2502.04322v3) [paper-pdf](http://arxiv.org/pdf/2502.04322v3)

**Authors**: Yik Siu Chan, Narutatsu Ri, Yuxin Xiao, Marzyeh Ghassemi

**Abstract**: Despite extensive safety alignment efforts, large language models (LLMs) remain vulnerable to jailbreak attacks that elicit harmful behavior. While existing studies predominantly focus on attack methods that require technical expertise, two critical questions remain underexplored: (1) Are jailbroken responses truly useful in enabling average users to carry out harmful actions? (2) Do safety vulnerabilities exist in more common, simple human-LLM interactions? In this paper, we demonstrate that LLM responses most effectively facilitate harmful actions when they are both actionable and informative--two attributes easily elicited in multi-step, multilingual interactions. Using this insight, we propose HarmScore, a jailbreak metric that measures how effectively an LLM response enables harmful actions, and Speak Easy, a simple multi-step, multilingual attack framework. Notably, by incorporating Speak Easy into direct request and jailbreak baselines, we see an average absolute increase of 0.319 in Attack Success Rate and 0.426 in HarmScore in both open-source and proprietary LLMs across four safety benchmarks. Our work reveals a critical yet often overlooked vulnerability: Malicious users can easily exploit common interaction patterns for harmful intentions.

摘要: 尽管做出了广泛的安全调整工作，大型语言模型（LLM）仍然容易受到引发有害行为的越狱攻击。虽然现有的研究主要集中在需要技术专业知识的攻击方法上，但有两个关键问题仍未得到充分研究：（1）越狱响应是否真的有助于让普通用户实施有害行为？(2)更常见、简单的人类与LLM互动中是否存在安全漏洞？在本文中，我们证明，当LLM响应既可操作又提供信息时，它们最有效地促进了有害行为--这两个属性在多步骤、多语言交互中很容易被引出。利用这一见解，我们提出了HarmScore，这是一种越狱指标，衡量LLM响应实施有害行为的有效性，还提出了Speak Easy，这是一种简单的多步骤、多语言攻击框架。值得注意的是，通过将Speak Easy纳入直接请求和越狱基线，我们看到开源和专有LLM在四个安全基准中的攻击成功率平均绝对增加0.319，HarmScore平均绝对增加0.426。我们的工作揭示了一个关键但经常被忽视的漏洞：恶意用户可以很容易地利用常见的交互模式进行有害的意图。



## **37. Transformers in Pseudo-Random Number Generation: A Dual Perspective on Theory and Practice**

伪随机数生成中的变形金刚：理论与实践的双重视角 cs.LG

27 pages, 4 figures

**SubmitDate**: 2025-08-02    [abs](http://arxiv.org/abs/2508.01134v1) [paper-pdf](http://arxiv.org/pdf/2508.01134v1)

**Authors**: Ran Li, Lingshu Zeng

**Abstract**: Pseudo-random number generators (PRNGs) are high-nonlinear processes, and they are key blocks in optimization of Large language models. Transformers excel at processing complex nonlinear relationships. Thus it is reasonable to generate high-quality pseudo-random numbers based on transformers. In this paper, we explore this question from both theoretical and practical perspectives, highlighting the potential benefits and implications of Transformer in PRNGs. We theoretically demonstrate that decoder-only Transformer models with Chain-of-Thought can simulate both the Linear Congruential Generator (LCG) and Mersenne Twister (MT) PRNGs. Based on this, we conclude that the log-precision decoder-only Transformer can represent non-uniform $\text{AC}^0$. Our simulative theoretical findings are validated through experiments. The random numbers generated by Transformer-based PRNGs successfully pass the majority of NIST tests, whose heat maps exhibit clear statistical randomness. Finally, we assess their capability in prediction attacks.

摘要: 伪随机数生成器（PRNG）是高度非线性过程，是大型语言模型优化的关键模块。变形金刚擅长处理复杂的非线性关系。因此，基于变换器生成高质量伪随机数是合理的。在本文中，我们从理论和实践的角度探讨了这个问题，强调了Transformer在PRNG中的潜在好处和影响。我们从理论上证明，具有思想链的纯解码器Transformer模型可以模拟线性共同生成器（LCG）和梅森扭曲器（MT）PRNG。基于此，我们得出的结论是，仅限日志精度解码器的Transformer可以表示非均匀$\text{AC}'#39; 0 $。我们的模拟理论发现通过实验得到了验证。基于Transformer的PRNG生成的随机数成功通过了大多数NIH测试，其热图表现出明显的统计随机性。最后，我们评估它们在预测攻击中的能力。



## **38. Evading Data Provenance in Deep Neural Networks**

在深度神经网络中规避数据来源 cs.CV

ICCV 2025 Highlight

**SubmitDate**: 2025-08-01    [abs](http://arxiv.org/abs/2508.01074v1) [paper-pdf](http://arxiv.org/pdf/2508.01074v1)

**Authors**: Hongyu Zhu, Sichu Liang, Wenwen Wang, Zhuomeng Zhang, Fangqi Li, Shi-Lin Wang

**Abstract**: Modern over-parameterized deep models are highly data-dependent, with large scale general-purpose and domain-specific datasets serving as the bedrock for rapid advancements. However, many datasets are proprietary or contain sensitive information, making unrestricted model training problematic. In the open world where data thefts cannot be fully prevented, Dataset Ownership Verification (DOV) has emerged as a promising method to protect copyright by detecting unauthorized model training and tracing illicit activities. Due to its diversity and superior stealth, evading DOV is considered extremely challenging. However, this paper identifies that previous studies have relied on oversimplistic evasion attacks for evaluation, leading to a false sense of security. We introduce a unified evasion framework, in which a teacher model first learns from the copyright dataset and then transfers task-relevant yet identifier-independent domain knowledge to a surrogate student using an out-of-distribution (OOD) dataset as the intermediary. Leveraging Vision-Language Models and Large Language Models, we curate the most informative and reliable subsets from the OOD gallery set as the final transfer set, and propose selectively transferring task-oriented knowledge to achieve a better trade-off between generalization and evasion effectiveness. Experiments across diverse datasets covering eleven DOV methods demonstrate our approach simultaneously eliminates all copyright identifiers and significantly outperforms nine state-of-the-art evasion attacks in both generalization and effectiveness, with moderate computational overhead. As a proof of concept, we reveal key vulnerabilities in current DOV methods, highlighting the need for long-term development to enhance practicality.

摘要: 现代过度参数化深度模型高度依赖数据，大规模通用和特定领域数据集是快速进步的基石。然而，许多数据集是专有的或包含敏感信息，这使得不受限制的模型训练成为问题。在无法完全防止数据盗窃的开放世界中，数据集所有权验证（DOV）已成为一种通过检测未经授权的模型训练和追踪非法活动来保护版权的有前途的方法。由于其多样性和卓越的隐身性，躲避DOV被认为极具挑战性。然而，本文指出，之前的研究依赖于过于简单化的规避攻击来进行评估，从而导致错误的安全感。我们引入了一个统一的规避框架，其中教师模型首先从版权数据集中学习，然后使用分发外（OOD）数据集作为中介将与任务相关但与身份无关的领域知识传输给代理学生。利用视觉语言模型和大型语言模型，我们从OOD库集中挑选信息量最大、最可靠的子集作为最终的转移集，并建议选择性地转移面向任务的知识，以实现概括和规避有效性之间的更好权衡。涵盖11种DOV方法的不同数据集的实验表明，我们的方法同时消除了所有版权标识符，并且在概括性和有效性方面都显着优于9种最先进的规避攻击，并且计算负担适度。作为概念验证，我们揭示了当前DOV方法中的关键漏洞，强调了长期开发以增强实用性的必要性。



## **39. Autonomous Penetration Testing: Solving Capture-the-Flag Challenges with LLMs**

自主渗透测试：使用LLM解决捕获旗帜挑战 cs.CR

6 pages, 2 figures, 3 tables

**SubmitDate**: 2025-08-01    [abs](http://arxiv.org/abs/2508.01054v1) [paper-pdf](http://arxiv.org/pdf/2508.01054v1)

**Authors**: Isabelle Bakker, John Hastings

**Abstract**: This study evaluates the ability of GPT-4o to autonomously solve beginner-level offensive security tasks by connecting the model to OverTheWire's Bandit capture-the-flag game. Of the 25 levels that were technically compatible with a single-command SSH framework, GPT-4o solved 18 unaided and another two after minimal prompt hints for an overall 80% success rate. The model excelled at single-step challenges that involved Linux filesystem navigation, data extraction or decoding, and straightforward networking. The approach often produced the correct command in one shot and at a human-surpassing speed. Failures involved multi-command scenarios that required persistent working directories, complex network reconnaissance, daemon creation, or interaction with non-standard shells. These limitations highlight current architectural deficiencies rather than a lack of general exploit knowledge. The results demonstrate that large language models (LLMs) can automate a substantial portion of novice penetration-testing workflow, potentially lowering the expertise barrier for attackers and offering productivity gains for defenders who use LLMs as rapid reconnaissance aides. Further, the unsolved tasks reveal specific areas where secure-by-design environments might frustrate simple LLM-driven attacks, informing future hardening strategies. Beyond offensive cybersecurity applications, results suggest the potential to integrate LLMs into cybersecurity education as practice aids.

摘要: 本研究通过将模型与OverTheWire的Bandit捕获旗帜游戏连接起来，评估GPT-4 o自主解决初级进攻性安全任务的能力。在技术上与单命令SSH框架兼容的25个级别中，GPT-4 o在没有帮助的情况下解决了18个级别，在最低限度的提示后解决了另外两个级别，总体成功率为80%。该模型擅长应对涉及Linux文件系统导航、数据提取或解码以及简单的网络的分步挑战。这种方法通常以超越人类的速度一次发出正确的命令。故障涉及多命令场景，需要持久工作目录、复杂的网络侦察、守护程序创建或与非标准Shell交互。这些限制凸显了当前的架构缺陷，而不是缺乏一般的利用知识。结果表明，大型语言模型（LLM）可以自动化大部分新手渗透测试工作流程，从而可能降低攻击者的专业知识障碍，并为使用LLM作为快速侦察助手的防御者提供生产力提高。此外，未解决的任务揭示了设计安全环境可能会挫败简单的LLM驱动攻击的特定领域，从而为未来的强化策略提供信息。除了攻击性的网络安全应用之外，结果还表明，法学硕士作为实践辅助工具融入网络安全教育的潜力。



## **40. Web Artifact Attacks Disrupt Vision Language Models**

网络收件箱攻击扰乱视觉语言模型 cs.CV

Accepted at ICCV 2025

**SubmitDate**: 2025-08-01    [abs](http://arxiv.org/abs/2503.13652v2) [paper-pdf](http://arxiv.org/pdf/2503.13652v2)

**Authors**: Maan Qraitem, Piotr Teterwak, Kate Saenko, Bryan A. Plummer

**Abstract**: Vision-language models (VLMs) (e.g. CLIP, LLaVA) are trained on large-scale, lightly curated web datasets, leading them to learn unintended correlations between semantic concepts and unrelated visual signals. These associations degrade model accuracy by causing predictions to rely on incidental patterns rather than genuine visual understanding. Prior work has weaponized these correlations as an attack vector to manipulate model predictions, such as inserting a deceiving class text onto the image in a "typographic" attack. These attacks succeed due to VLMs' text-heavy bias-a result of captions that echo visible words rather than describing content. However, this attack has focused solely on text that matches the target class exactly, overlooking a broader range of correlations, including non-matching text and graphical symbols, which arise from the abundance of branding content in web-scale data. To address this gap, we introduce "artifact-based" attacks: a novel class of manipulations that mislead models using both non-matching text and graphical elements. Unlike typographic attacks, these artifacts are not predefined, making them simultaneously harder to defend against and more challenging to find. We address this by framing artifact attacks as a search problem and demonstrate their effectiveness across five datasets, with some artifacts reinforcing each other to reach 100% attack success rates. These attacks transfer across models with up to 90% effectiveness, making it possible to attack unseen models. To defend against these attacks, we extend prior work's artifact aware prompting to the graphical setting. We see a moderate reduction of success rates of up to 15% relative to standard prompts, suggesting a promising direction for enhancing model robustness. Code: https://github.com/mqraitem/Web-Artifact-Attacks

摘要: 视觉语言模型（VLM）（例如CLIP、LLaVA）在大规模、精心策划的网络数据集上进行训练，使它们能够学习语义概念和不相关的视觉信号之间无意的相关性。这些关联导致预测依赖于偶然模式而不是真正的视觉理解，从而降低了模型的准确性。之前的工作已将这些相关性武器化，作为操纵模型预测的攻击载体，例如在“印刷”攻击中将欺骗性的类文本插入到图像上。这些攻击之所以成功，是因为VLM具有大量文本偏见--这是由于标题呼应可见文字而不是描述内容。然而，这种攻击仅关注与目标类别完全匹配的文本，忽视了更广泛的相关性，包括不匹配的文本和图形符号，这些内容源于网络规模数据中丰富的品牌内容。为了解决这一差距，我们引入了“基于伪影”的攻击：一类新型操纵，可以使用不匹配的文本和图形元素来误导模型。与印刷攻击不同，这些文物不是预定义的，这使得它们既更难防御，又更难找到。我们通过将人工制品攻击视为搜索问题来解决这个问题，并展示它们在五个数据集中的有效性，其中一些人工制品相互加强以达到100%的攻击成功率。这些攻击可以在模型之间转移，有效性高达90%，从而可以攻击看不见的模型。为了抵御这些攻击，我们将之前工作的文物感知提示扩展到图形设置。我们看到相对于标准提示，成功率适度降低高达15%，这表明了增强模型鲁棒性的一个有希望的方向。代码：https://github.com/mqraitem/Web-Artifact-Attacks



## **41. LeakSealer: A Semisupervised Defense for LLMs Against Prompt Injection and Leakage Attacks**

LeakSealer：LLM针对即时注入和泄漏攻击的半监督防御 cs.CR

22 pages, preprint

**SubmitDate**: 2025-08-01    [abs](http://arxiv.org/abs/2508.00602v1) [paper-pdf](http://arxiv.org/pdf/2508.00602v1)

**Authors**: Francesco Panebianco, Stefano Bonfanti, Francesco Trovò, Michele Carminati

**Abstract**: The generalization capabilities of Large Language Models (LLMs) have led to their widespread deployment across various applications. However, this increased adoption has introduced several security threats, notably in the forms of jailbreaking and data leakage attacks. Additionally, Retrieval Augmented Generation (RAG), while enhancing context-awareness in LLM responses, has inadvertently introduced vulnerabilities that can result in the leakage of sensitive information. Our contributions are twofold. First, we introduce a methodology to analyze historical interaction data from an LLM system, enabling the generation of usage maps categorized by topics (including adversarial interactions). This approach further provides forensic insights for tracking the evolution of jailbreaking attack patterns. Second, we propose LeakSealer, a model-agnostic framework that combines static analysis for forensic insights with dynamic defenses in a Human-In-The-Loop (HITL) pipeline. This technique identifies topic groups and detects anomalous patterns, allowing for proactive defense mechanisms. We empirically evaluate LeakSealer under two scenarios: (1) jailbreak attempts, employing a public benchmark dataset, and (2) PII leakage, supported by a curated dataset of labeled LLM interactions. In the static setting, LeakSealer achieves the highest precision and recall on the ToxicChat dataset when identifying prompt injection. In the dynamic setting, PII leakage detection achieves an AUPRC of $0.97$, significantly outperforming baselines such as Llama Guard.

摘要: 大型语言模型（LLM）的概括能力导致它们在各种应用程序中广泛部署。然而，这种采用的增加引入了多种安全威胁，特别是越狱和数据泄露攻击的形式。此外，检索增强生成（RAG）虽然增强了LLM响应中的上下文感知，但无意中引入了可能导致敏感信息泄露的漏洞。我们的贡献是双重的。首先，我们介绍了一种方法来分析LLM系统的历史交互数据，从而生成按主题分类的使用地图（包括对抗性交互）。这种方法进一步为跟踪越狱攻击模式的演变提供了法医见解。其次，我们提出了LeakSealer，一个模型不可知的框架，它将静态分析与动态防御相结合，用于在人在回路（HITL）管道中进行取证洞察。这种技术可以识别主题组并检测异常模式，从而实现主动防御机制。我们在两种情况下经验评估LeakSealer：（1）越狱尝试，使用公共基准数据集，以及（2）由标记的LLM交互的精心策划数据集支持的PRI泄漏。在静态设置中，LeakSealer在识别提示注射时在ToxicChat数据集上实现了最高的精确度和召回率。在动态环境中，PIP泄漏检测的AUPRC为0.97美元，显着优于Llama Guard等基准。



## **42. From LLMs to MLLMs to Agents: A Survey of Emerging Paradigms in Jailbreak Attacks and Defenses within LLM Ecosystem**

从LLM到MLLM再到代理：LLM生态系统内越狱攻击和防御新兴范式调查 cs.CR

**SubmitDate**: 2025-08-01    [abs](http://arxiv.org/abs/2506.15170v3) [paper-pdf](http://arxiv.org/pdf/2506.15170v3)

**Authors**: Yanxu Mao, Tiehan Cui, Peipei Liu, Datao You, Hongsong Zhu

**Abstract**: Large language models (LLMs) are rapidly evolving from single-modal systems to multimodal LLMs and intelligent agents, significantly expanding their capabilities while introducing increasingly severe security risks. This paper presents a systematic survey of the growing complexity of jailbreak attacks and corresponding defense mechanisms within the expanding LLM ecosystem. We first trace the developmental trajectory from LLMs to MLLMs and Agents, highlighting the core security challenges emerging at each stage. Next, we categorize mainstream jailbreak techniques from both the attack impact and visibility perspectives, and provide a comprehensive analysis of representative attack methods, related datasets, and evaluation metrics. On the defense side, we organize existing strategies based on response timing and technical approach, offering a structured understanding of their applicability and implementation. Furthermore, we identify key limitations in existing surveys, such as insufficient attention to agent-specific security issues, the absence of a clear taxonomy for hybrid jailbreak methods, a lack of detailed analysis of experimental setups, and outdated coverage of recent advancements. To address these limitations, we provide an updated synthesis of recent work and outline future research directions in areas such as dataset construction, evaluation framework optimization, and strategy generalization. Our study seeks to enhance the understanding of jailbreak mechanisms and facilitate the advancement of more resilient and adaptive defense strategies in the context of ever more capable LLMs.

摘要: 大型语言模型（LLM）正在从单模式系统迅速发展到多模式LLM和智能代理，显着扩展了其能力，同时引入了越来越严重的安全风险。本文系统地调查了不断扩大的LLM生态系统中越狱攻击日益复杂的程度以及相应的防御机制。我们首先追踪从LLM到MLLM和代理的发展轨迹，强调每个阶段出现的核心安全挑战。接下来，我们从攻击影响和可见性角度对主流越狱技术进行分类，并对代表性攻击方法、相关数据集和评估指标进行全面分析。在防御方面，我们根据响应时间和技术方法组织现有策略，提供对其适用性和实施的结构化理解。此外，我们发现了现有调查中的关键局限性，例如对特定于代理的安全问题关注不够、混合越狱方法缺乏明确的分类、缺乏对实验设置的详细分析以及对最近进展的过时报道。为了解决这些限制，我们提供了最近工作的最新综合，并概述了数据集构建、评估框架优化和策略概括等领域的未来研究方向。我们的研究旨在增强对越狱机制的理解，并促进在能力更强的法学硕士背景下推进更具弹性和适应性的防御策略。



## **43. CyGATE: Game-Theoretic Cyber Attack-Defense Engine for Patch Strategy Optimization**

CyGATE：用于补丁策略优化的游戏理论网络攻击防御引擎 cs.CR

**SubmitDate**: 2025-08-01    [abs](http://arxiv.org/abs/2508.00478v1) [paper-pdf](http://arxiv.org/pdf/2508.00478v1)

**Authors**: Yuning Jiang, Nay Oo, Qiaoran Meng, Lu Lin, Dusit Niyato, Zehui Xiong, Hoon Wei Lim, Biplab Sikdar

**Abstract**: Modern cyber attacks unfold through multiple stages, requiring defenders to dynamically prioritize mitigations under uncertainty. While game-theoretic models capture attacker-defender interactions, existing approaches often rely on static assumptions and lack integration with real-time threat intelligence, limiting their adaptability. This paper presents CyGATE, a game-theoretic framework modeling attacker-defender interactions, using large language models (LLMs) with retrieval-augmented generation (RAG) to enhance tactic selection and patch prioritization. Applied to a two-agent scenario, CyGATE frames cyber conflicts as a partially observable stochastic game (POSG) across Cyber Kill Chain stages. Both agents use belief states to navigate uncertainty, with the attacker adapting tactics and the defender re-prioritizing patches based on evolving risks and observed adversary behavior. The framework's flexible architecture enables extension to multi-agent scenarios involving coordinated attackers, collaborative defenders, or complex enterprise environments with multiple stakeholders. Evaluated in a dynamic patch scheduling scenario, CyGATE effectively prioritizes high-risk vulnerabilities, enhancing adaptability through dynamic threat integration, strategic foresight by anticipating attacker moves under uncertainty, and efficiency by optimizing resource use.

摘要: 现代网络攻击分为多个阶段，要求防御者在不确定性下动态优先考虑缓解措施。虽然博弈论模型捕捉攻击者与防御者的相互作用，但现有方法通常依赖于静态假设，并且缺乏与实时威胁情报的集成，从而限制了其适应性。本文介绍了CyGATE，这是一个对攻击者-防御者交互进行建模的博弈论框架，使用大型语言模型（LLM）和检索增强生成（RAG）来增强策略选择和补丁优先级。CyGATE应用于双智能体场景，将网络冲突构建为跨网络杀戮链阶段的部分可观察随机游戏（POSG）。两个代理都使用信念状态来导航不确定性，攻击者调整策略，防御者根据不断变化的风险和观察到的对手行为重新确定补丁的优先级。该框架灵活的架构可以扩展到涉及协调攻击者，协作防御者或具有多个利益相关者的复杂企业环境的多代理场景。在动态补丁调度方案中进行评估，CyGATE有效地优先考虑高风险漏洞，通过动态威胁集成增强适应性，通过预测攻击者在不确定性下的行动增强战略远见，并通过优化资源使用提高效率。



## **44. Large AI Model-Enabled Secure Communications in Low-Altitude Wireless Networks: Concepts, Perspectives and Case Study**

低空无线网络中支持大型人工智能模型的安全通信：概念、观点和案例研究 cs.NI

This paper has been submitted to IEEE Communications Magazine for  consideration

**SubmitDate**: 2025-08-01    [abs](http://arxiv.org/abs/2508.00256v1) [paper-pdf](http://arxiv.org/pdf/2508.00256v1)

**Authors**: Chuang Zhang, Geng Sun, Jiacheng Wang, Yijing Lin, Weijie Yuan, Sinem Coleri, Dusit Niyato, Tony Q. S. Quek

**Abstract**: Low-altitude wireless networks (LAWNs) have the potential to revolutionize communications by supporting a range of applications, including urban parcel delivery, aerial inspections and air taxis. However, compared with traditional wireless networks, LAWNs face unique security challenges due to low-altitude operations, frequent mobility and reliance on unlicensed spectrum, making it more vulnerable to some malicious attacks. In this paper, we investigate some large artificial intelligence model (LAM)-enabled solutions for secure communications in LAWNs. Specifically, we first explore the amplified security risks and important limitations of traditional AI methods in LAWNs. Then, we introduce the basic concepts of LAMs and delve into the role of LAMs in addressing these challenges. To demonstrate the practical benefits of LAMs for secure communications in LAWNs, we propose a novel LAM-based optimization framework that leverages large language models (LLMs) to generate enhanced state features on top of handcrafted representations, and to design intrinsic rewards accordingly, thereby improving reinforcement learning performance for secure communication tasks. Through a typical case study, simulation results validate the effectiveness of the proposed framework. Finally, we outline future directions for integrating LAMs into secure LAWN applications.

摘要: 低空无线网络（LAWN）通过支持一系列应用，包括城市包裹递送、空中检查和空中出租车，有可能彻底改变通信。然而，与传统无线网络相比，由于低空操作、频繁的移动性和对未授权频谱的依赖，LAWN面临着独特的安全挑战，使其更容易受到一些恶意攻击。在本文中，我们研究了一些大型人工智能模型（LAM）启用LAWN安全通信的解决方案。具体来说，我们首先探讨了LAWN中传统AI方法的放大安全风险和重要局限性。然后，我们介绍了LAM的基本概念，并深入研究了LAM在应对这些挑战中的作用。为了展示LAM在LAWN中安全通信中的实际好处，我们提出了一种新型的基于LAM的优化框架，该框架利用大型语言模型（LLM）在手工制作的表示之上生成增强的状态特征，并相应地设计内在奖励，从而提高安全通信任务的强化学习性能。通过典型案例研究，仿真结果验证了所提出框架的有效性。最后，我们概述了将LAM集成到安全LAWN应用程序中的未来方向。



## **45. Watch the Weights: Unsupervised monitoring and control of fine-tuned LLMs**

观看权重：对微调LLM进行无监督监控和控制 cs.LG

**SubmitDate**: 2025-07-31    [abs](http://arxiv.org/abs/2508.00161v1) [paper-pdf](http://arxiv.org/pdf/2508.00161v1)

**Authors**: Ziqian Zhong, Aditi Raghunathan

**Abstract**: The releases of powerful open-weight large language models (LLMs) are often not accompanied by access to their full training data. Existing interpretability methods, particularly those based on activations, often require or assume distributionally similar data. This is a significant limitation when detecting and defending against novel potential threats like backdoors, which are by definition out-of-distribution.   In this work, we introduce a new method for understanding, monitoring and controlling fine-tuned LLMs that interprets weights, rather than activations, thereby side stepping the need for data that is distributionally similar to the unknown training data. We demonstrate that the top singular vectors of the weight difference between a fine-tuned model and its base model correspond to newly acquired behaviors. By monitoring the cosine similarity of activations along these directions, we can detect salient behaviors introduced during fine-tuning with high precision.   For backdoored models that bypasses safety mechanisms when a secret trigger is present, our method stops up to 100% of attacks with a false positive rate below 1.2%. For models that have undergone unlearning, we detect inference on erased topics with accuracy up to 95.42% and can even steer the model to recover "unlearned" information. Besides monitoring, our method also shows potential for pre-deployment model auditing: by analyzing commercial instruction-tuned models (OLMo, Llama, Qwen), we are able to uncover model-specific fine-tuning focus including marketing strategies and Midjourney prompt generation.   Our implementation can be found at https://github.com/fjzzq2002/WeightWatch.

摘要: 强大的开权大型语言模型（LLM）的发布通常不会伴随着对其完整训练数据的访问。现有的可解释性方法，尤其是基于激活的方法，通常需要或假设分布相似的数据。在检测和防御后门等新型潜在威胁时，这是一个重大限制，根据定义，后门是无法分发的。   在这项工作中，我们引入了一种用于理解、监控和控制微调LLM的新方法，该方法解释权重而不是激活，从而减少了对分布上与未知训练数据相似的数据的需求。我们证明了微调模型与其基础模型之间的权重差的顶部奇异向量对应于新获得的行为。通过监测这些方向上激活的余弦相似性，我们可以高精度地检测微调期间引入的显著行为。   对于在存在秘密触发器时绕过安全机制的后门模型，我们的方法可以阻止高达100%的攻击，误报率低于1.2%。对于已经经历了遗忘的模型，我们检测到对被删除主题的推断，准确率高达95.42%，甚至可以引导模型恢复“未学习”的信息。除了监控之外，我们的方法还显示出部署前模型审计的潜力：通过分析商业描述调整模型（OLMo、Llama、Qwen），我们能够发现特定于模型的微调焦点，包括营销策略和Midjourney提示生成。   我们的实现可以在https://github.com/fjzzq2002/WeightWatch上找到。



## **46. Graph Representation-based Model Poisoning on Federated Large Language Models**

基于图表示的模型对联邦大型语言模型的毒害 cs.CR

7 pages, 5 figures (Submitted to IEEE Communication Magazine)

**SubmitDate**: 2025-07-31    [abs](http://arxiv.org/abs/2507.01694v2) [paper-pdf](http://arxiv.org/pdf/2507.01694v2)

**Authors**: Hanlin Cai, Haofan Dong, Houtianfu Wang, Kai Li, Ozgur B. Akan

**Abstract**: Federated large language models (FedLLMs) enable powerful generative capabilities within wireless networks while preserving data privacy. Nonetheless, FedLLMs remain vulnerable to model poisoning attacks. This article first reviews recent advancements in model poisoning techniques and existing defense mechanisms for FedLLMs, underscoring critical limitations, especially when dealing with non-IID textual data distributions. Current defense strategies predominantly employ distance or similarity-based outlier detection mechanisms, relying on the assumption that malicious updates markedly differ from benign statistical patterns. However, this assumption becomes inadequate against adaptive adversaries targeting billion-parameter LLMs. The article further investigates graph representation-based model poisoning (GRMP), an emerging attack paradigm that exploits higher-order correlations among benign client gradients to craft malicious updates indistinguishable from legitimate ones. GRMP can effectively circumvent advanced defense systems, causing substantial degradation in model accuracy and overall performance. Moreover, the article outlines a forward-looking research roadmap that emphasizes the necessity of graph-aware secure aggregation methods, specialized vulnerability metrics tailored for FedLLMs, and evaluation frameworks to enhance the robustness of federated language model deployments.

摘要: 联合大型语言模型（FedLLM）在无线网络中实现强大的生成能力，同时保护数据隐私。尽管如此，FedLLM仍然容易受到模型中毒攻击。本文首先回顾了FedLLM模型中毒技术和现有防御机制的最新进展，强调了关键局限性，尤其是在处理非IID文本数据分布时。当前的防御策略主要采用基于距离或相似性的离群值检测机制，其基础是恶意更新与良性统计模式显着不同的假设。然而，对于针对数十亿参数LLM的自适应对手来说，这一假设变得不充分。本文进一步研究了基于图表示的模型中毒（GRMP），这是一种新兴的攻击范式，利用良性客户端梯度之间的更高层相关性来制作与合法更新无法区分的恶意更新。GRMP可以有效规避先进的防御系统，导致模型准确性和整体性能大幅下降。此外，本文概述了一个前瞻性的研究路线图，强调图形感知的安全聚合方法、为FedLLM量身定制的专业漏洞指标以及增强联邦语言模型部署稳健性的评估框架的必要性。



## **47. Probabilistic Modeling of Jailbreak on Multimodal LLMs: From Quantification to Application**

多模态LLM越狱的概率建模：从量化到应用 cs.CR

**SubmitDate**: 2025-07-31    [abs](http://arxiv.org/abs/2503.06989v2) [paper-pdf](http://arxiv.org/pdf/2503.06989v2)

**Authors**: Wenzhuo Xu, Zhipeng Wei, Xiongtao Sun, Zonghao Ying, Deyue Zhang, Dongdong Yang, Xiangzheng Zhang, Quanchen Zou

**Abstract**: Recently, Multimodal Large Language Models (MLLMs) have demonstrated their superior ability in understanding multimodal content. However, they remain vulnerable to jailbreak attacks, which exploit weaknesses in their safety alignment to generate harmful responses. Previous studies categorize jailbreaks as successful or failed based on whether responses contain malicious content. However, given the stochastic nature of MLLM responses, this binary classification of an input's ability to jailbreak MLLMs is inappropriate. Derived from this viewpoint, we introduce jailbreak probability to quantify the jailbreak potential of an input, which represents the likelihood that MLLMs generated a malicious response when prompted with this input. We approximate this probability through multiple queries to MLLMs. After modeling the relationship between input hidden states and their corresponding jailbreak probability using Jailbreak Probability Prediction Network (JPPN), we use continuous jailbreak probability for optimization. Specifically, we propose Jailbreak-Probability-based Attack (JPA) that optimizes adversarial perturbations on input image to maximize jailbreak probability, and further enhance it as Multimodal JPA (MJPA) by including monotonic text rephrasing. To counteract attacks, we also propose Jailbreak-Probability-based Finetuning (JPF), which minimizes jailbreak probability through MLLM parameter updates. Extensive experiments show that (1) (M)JPA yields significant improvements when attacking a wide range of models under both white and black box settings. (2) JPF vastly reduces jailbreaks by at most over 60\%. Both of the above results demonstrate the significance of introducing jailbreak probability to make nuanced distinctions among input jailbreak abilities.

摘要: 最近，多模式大型语言模型（MLLM）展示了其在理解多模式内容方面的卓越能力。然而，它们仍然容易受到越狱攻击，这些攻击利用其安全调整中的弱点来产生有害反应。之前的研究根据回应是否包含恶意内容将越狱分为成功或失败。然而，考虑到MLLM响应的随机性，这种对输入越狱MLLM的能力的二元分类是不合适的。从这个观点出发，我们引入越狱概率来量化输入的越狱潜力，这代表当提示此输入时MLLM生成恶意响应的可能性。我们通过对MLLM的多次查询来估算这一可能性。使用越狱概率预测网络（JPPN）对输入隐藏状态与其相应越狱概率之间的关系进行建模后，我们使用连续越狱概率进行优化。具体来说，我们提出了基于越狱概率的攻击（JPA），该攻击优化输入图像上的对抗性扰动以最大化越狱概率，并通过包括单调文本改写进一步将其增强为多模式JPA（MJPA）。为了对抗攻击，我们还提出了基于越狱概率的微调（JPF），它通过MLLM参数更新最大限度地降低越狱概率。大量实验表明，（1）（M）JPA在白盒和黑匣子设置下攻击广泛的模型时都能产生显着的改进。(2)JPF最多将越狱人数大幅减少60%以上。上述两个结果都证明了引入越狱概率以在输入越狱能力之间进行细微差别的重要性。



## **48. Measuring Harmfulness of Computer-Using Agents**

衡量使用计算机的代理的危害性 cs.CR

19 pages, 9 figures. Benchmark release at  https://github.com/db-ol/CUAHarm

**SubmitDate**: 2025-07-31    [abs](http://arxiv.org/abs/2508.00935v1) [paper-pdf](http://arxiv.org/pdf/2508.00935v1)

**Authors**: Aaron Xuxiang Tian, Ruofan Zhang, Janet Tang, Jiaxin Wen

**Abstract**: Computer-using agents (CUAs), which autonomously control computers to perform multi-step actions, might pose significant safety risks if misused. Existing benchmarks mostly evaluate language models' (LMs) safety risks in chatbots or simple tool-usage scenarios, without granting full computer access. To better evaluate CUAs' misuse risks, we introduce a new benchmark: CUAHarm. CUAHarm consists of 104 expert-written realistic misuse risks, such as disabling firewalls, leaking confidential information, launching denial-of-service attacks, or installing backdoors. We provide a sandbox environment and rule-based verifiable rewards to measure CUAs' success rates in executing these tasks (e.g., whether the firewall is indeed disabled), not just refusal. We evaluate multiple frontier open-source and proprietary LMs, such as Claude Sonnet, GPT-4o, Gemini Pro 1.5, Llama-3.3-70B, and Mistral Large 2. Surprisingly, even without carefully designed jailbreaking prompts, these frontier LMs comply with executing these malicious tasks at a high success rate (e.g., 59% for Claude 3.7 Sonnet). Newer models show higher misuse rates: Claude 3.7 Sonnet succeeds on 15% more tasks than Claude 3.5. While these models are robust to common malicious prompts (e.g., creating a bomb) in chatbot settings, they behave unsafely as CUAs. We further evaluate a leading agentic framework (UI-TARS-1.5) and find that while it improves performance, it also amplifies misuse risks. Benign variants reveal refusals stem from alignment, not capability limits. To mitigate risks, we explore using LMs to monitor CUAs' actions and chain-of-thoughts (CoTs). Monitoring CUAs is significantly harder than chatbot outputs. Monitoring CoTs yields modest gains, with average detection accuracy at only 72%. Even with hierarchical summarization, improvement is limited to 4%. CUAHarm will be released at https://github.com/db-ol/CUAHarm.

摘要: 计算机使用代理（CUA）自主控制计算机执行多步骤操作，如果被滥用可能会带来重大安全风险。现有的基准主要评估聊天机器人或简单工具使用场景中的语言模型（LM）安全风险，而不授予完整的计算机访问权限。为了更好地评估CUA的滥用风险，我们引入了一个新基准：CUAHarm。CUAHarm由104个专家编写的现实滥用风险组成，例如禁用防火墙、泄露机密信息、发起拒绝服务攻击或安装后门。我们提供沙盒环境和基于规则的可验证奖励来衡量CUA执行这些任务的成功率（例如，防火墙是否确实被禁用），而不仅仅是拒绝。我们评估了多种前沿开源和专有LM，例如Claude Sonnet、GPT-4 o、Gemini Pro 1.5、Llama-3.3- 70 B和Mistral Large 2。令人惊讶的是，即使没有精心设计的越狱提示，这些前沿LM也能以高成功率执行这些恶意任务（例如，克劳德3.7十四行诗为59%）。较新的模型显示出更高的误用率：Claude 3.7十四行诗的成功任务比Claude 3.5多15%。虽然这些模型对常见恶意提示（例如，创建炸弹）在聊天机器人设置中，它们作为CUA的行为是不安全的。我们进一步评估了领先的代理框架（UI-TARS-1.5），发现它在提高性能的同时，也放大了滥用风险。良性变体揭示了拒绝源于对齐，而不是能力限制。为了降低风险，我们探索使用LM来监控CUA的行为和思想链（CoT）。监控CUA比聊天机器人输出要困难得多。监测CoT会产生适度的收益，平均检测准确率仅为72%。即使采用分层总结，改进也仅限于4%。CUAHarm将在https://github.com/db-ol/CUAHarm上发布。



## **49. CEE: An Inference-Time Jailbreak Defense for Embodied Intelligence via Subspace Concept Rotation**

CEE：通过子空间概念轮换为情报提供推理时越狱防御 cs.CR

**SubmitDate**: 2025-07-31    [abs](http://arxiv.org/abs/2504.13201v2) [paper-pdf](http://arxiv.org/pdf/2504.13201v2)

**Authors**: Jirui Yang, Zheyu Lin, Zhihui Lu, Yinggui Wang, Lei Wang, Tao Wei, Xin Du, Shuhan Yang

**Abstract**: Large Language Models (LLMs) are increasingly becoming the cognitive core of Embodied Intelligence (EI) systems, such as robots and autonomous vehicles. However, this integration also exposes them to serious jailbreak risks, where malicious instructions can be transformed into dangerous physical actions. Existing defense mechanisms suffer from notable drawbacks--including high training costs, significant inference delays, and complex hyperparameter tuning--which limit their practical applicability. To address these challenges, we propose a novel and efficient inference-time defense framework: Concept Enhancement Engineering (CEE). CEE enhances the model's inherent safety mechanisms by directly manipulating its internal representations, requiring neither additional training nor external modules, thereby improving defense efficiency. Furthermore, CEE introduces a rotation-based control mechanism that enables stable and linearly tunable behavioral control of the model. This design eliminates the need for tedious manual tuning and avoids the output degradation issues commonly observed in other representation engineering methods. Extensive experiments across multiple EI safety benchmarks and diverse attack scenarios demonstrate that CEE significantly improves the defense success rates of various multimodal LLMs. It effectively mitigates safety risks while preserving high-quality generation and inference efficiency, offering a promising solution for deploying safer embodied intelligence systems.

摘要: 大型语言模型（LLM）正日益成为机器人和自动驾驶汽车等智能（EI）系统的认知核心。然而，这种集成也使它们面临严重的越狱风险，恶意指令可以转化为危险的物理行为。现有的防御机制存在显著的缺点-包括高训练成本，显著的推理延迟和复杂的超参数调整-这限制了它们的实用性。为了应对这些挑战，我们提出了一种新颖而有效的推理时间防御框架：概念增强工程（CEE）。CEE通过直接操纵模型的内部表示来增强模型的固有安全机制，既不需要额外的培训，也不需要外部模块，从而提高防御效率。此外，CEE引入了基于旋转的控制机制，可以对模型进行稳定且线性可调的行为控制。这种设计消除了繁琐的手动调优的需要，并避免了其他表示工程方法中常见的输出降级问题。跨多个EI安全基准和不同攻击场景的广泛实验表明，CEE显着提高了各种多模式LLM的防御成功率。它有效地降低了安全风险，同时保持高质量的生成和推理效率，为部署更安全的嵌入式智能系统提供了一个有前途的解决方案。



## **50. Fine-Grained Privacy Extraction from Retrieval-Augmented Generation Systems via Knowledge Asymmetry Exploitation**

通过知识不对称利用从检索增强生成系统中进行细粒度隐私提取 cs.CR

**SubmitDate**: 2025-07-31    [abs](http://arxiv.org/abs/2507.23229v1) [paper-pdf](http://arxiv.org/pdf/2507.23229v1)

**Authors**: Yufei Chen, Yao Wang, Haibin Zhang, Tao Gu

**Abstract**: Retrieval-augmented generation (RAG) systems enhance large language models (LLMs) by integrating external knowledge bases, but this advancement introduces significant privacy risks. Existing privacy attacks on RAG systems can trigger data leakage but often fail to accurately isolate knowledge-base-derived sentences within mixed responses. They also lack robustness when applied across multiple domains. This paper addresses these challenges by presenting a novel black-box attack framework that exploits knowledge asymmetry between RAG and standard LLMs to achieve fine-grained privacy extraction across heterogeneous knowledge landscapes. We propose a chain-of-thought reasoning strategy that creates adaptive prompts to steer RAG systems away from sensitive content. Specifically, we first decompose adversarial queries to maximize information disparity and then apply a semantic relationship scoring to resolve lexical and syntactic ambiguities. We finally train a neural network on these feature scores to precisely identify sentences containing private information. Unlike prior work, our framework generalizes to unseen domains through iterative refinement without pre-defined knowledge. Experimental results show that we achieve over 91% privacy extraction rate in single-domain and 83% in multi-domain scenarios, reducing sensitive sentence exposure by over 65% in case studies. This work bridges the gap between attack and defense in RAG systems, enabling precise extraction of private information while providing a foundation for adaptive mitigation.

摘要: 检索增强生成（RAG）系统通过集成外部知识库来增强大型语言模型（LLM），但这一进步带来了巨大的隐私风险。对RAG系统的现有隐私攻击可能会引发数据泄露，但通常无法准确地隔离混合响应中的知识库派生句子。当应用于多个领域时，它们也缺乏稳健性。本文通过提出一种新型的黑匣子攻击框架来解决这些挑战，该框架利用RAG和标准LLM之间的知识不对称性来实现跨异类知识环境的细粒度隐私提取。我们提出了一种思想链推理策略，可以创建自适应提示来引导RAG系统远离敏感内容。具体来说，我们首先分解对抗性查询以最大化信息差异，然后应用语义关系评分来解决词汇和语法歧义。我们最终根据这些特征分数训练神经网络，以精确识别包含私人信息的句子。与之前的工作不同，我们的框架通过迭代细化而无需预先定义的知识，将其推广到不可见的领域。实验结果表明，我们在单域场景中实现了超过91%的隐私提取率，在多域场景中实现了83%的隐私提取率，在案例研究中将敏感句子暴露减少了超过65%。这项工作弥合了RAG系统中攻击和防御之间的差距，能够精确提取私人信息，同时为自适应缓解提供基础。



