# Latest Large Language Model Attack Papers
**update at 2025-06-05 10:57:24**

翻译来自 https://cloud.tencent.com/document/product/551/15619

## **1. TracLLM: A Generic Framework for Attributing Long Context LLMs**

TracLLM：用于赋予长上下文LLM属性的通用框架 cs.CR

To appear in USENIX Security Symposium 2025. The code and data are  at: https://github.com/Wang-Yanting/TracLLM

**SubmitDate**: 2025-06-04    [abs](http://arxiv.org/abs/2506.04202v1) [paper-pdf](http://arxiv.org/pdf/2506.04202v1)

**Authors**: Yanting Wang, Wei Zou, Runpeng Geng, Jinyuan Jia

**Abstract**: Long context large language models (LLMs) are deployed in many real-world applications such as RAG, agent, and broad LLM-integrated applications. Given an instruction and a long context (e.g., documents, PDF files, webpages), a long context LLM can generate an output grounded in the provided context, aiming to provide more accurate, up-to-date, and verifiable outputs while reducing hallucinations and unsupported claims. This raises a research question: how to pinpoint the texts (e.g., sentences, passages, or paragraphs) in the context that contribute most to or are responsible for the generated output by an LLM? This process, which we call context traceback, has various real-world applications, such as 1) debugging LLM-based systems, 2) conducting post-attack forensic analysis for attacks (e.g., prompt injection attack, knowledge corruption attacks) to an LLM, and 3) highlighting knowledge sources to enhance the trust of users towards outputs generated by LLMs. When applied to context traceback for long context LLMs, existing feature attribution methods such as Shapley have sub-optimal performance and/or incur a large computational cost. In this work, we develop TracLLM, the first generic context traceback framework tailored to long context LLMs. Our framework can improve the effectiveness and efficiency of existing feature attribution methods. To improve the efficiency, we develop an informed search based algorithm in TracLLM. We also develop contribution score ensemble/denoising techniques to improve the accuracy of TracLLM. Our evaluation results show TracLLM can effectively identify texts in a long context that lead to the output of an LLM. Our code and data are at: https://github.com/Wang-Yanting/TracLLM.

摘要: 长上下文大型语言模型（LLM）部署在许多现实世界的应用程序中，例如RAG、代理和广泛的LLM集成应用程序。给定指令和长上下文（例如，文档、PDF文件、网页）、长上下文LLM可以基于所提供的上下文生成输出，旨在提供更准确、最新和可验证的输出，同时减少幻觉和不支持的主张。这提出了一个研究问题：如何确定文本（例如，句子、段落或段落）在对LLM生成的输出做出最大贡献或负责的上下文中？这个过程（我们称之为上下文追溯）具有各种现实世界的应用程序，例如1）调试基于LLM的系统，2）对攻击进行攻击后取证分析（例如，即时注入攻击、知识腐败攻击）对LLM，以及3）强调知识源以增强用户对LLM生成的输出的信任。当应用于长上下文LLM的上下文追溯时，现有的特征属性方法（例如Shapley）的性能不佳和/或会产生很大的计算成本。在这项工作中，我们开发了TracLLM，这是第一个针对长上下文LLM量身定制的通用上下文追溯框架。我们的框架可以提高现有特征归因方法的有效性和效率。为了提高效率，我们在TracLLM中开发了一种基于明智搜索的算法。我们还开发贡献分数集成/去噪技术来提高TracLLM的准确性。我们的评估结果表明TracLLM可以有效地识别导致LLM输出的长期上下文中的文本。我们的代码和数据位于：https://github.com/Wang-Yanting/TracLLM。



## **2. Privacy and Security Threat for OpenAI GPTs**

OpenAI GPT的隐私和安全威胁 cs.CR

**SubmitDate**: 2025-06-04    [abs](http://arxiv.org/abs/2506.04036v1) [paper-pdf](http://arxiv.org/pdf/2506.04036v1)

**Authors**: Wei Wenying, Zhao Kaifa, Xue Lei, Fan Ming

**Abstract**: Large language models (LLMs) demonstrate powerful information handling capabilities and are widely integrated into chatbot applications. OpenAI provides a platform for developers to construct custom GPTs, extending ChatGPT's functions and integrating external services. Since its release in November 2023, over 3 million custom GPTs have been created. However, such a vast ecosystem also conceals security and privacy threats. For developers, instruction leaking attacks threaten the intellectual property of instructions in custom GPTs through carefully crafted adversarial prompts. For users, unwanted data access behavior by custom GPTs or integrated third-party services raises significant privacy concerns. To systematically evaluate the scope of threats in real-world LLM applications, we develop three phases instruction leaking attacks target GPTs with different defense level. Our widespread experiments on 10,000 real-world custom GPTs reveal that over 98.8% of GPTs are vulnerable to instruction leaking attacks via one or more adversarial prompts, and half of the remaining GPTs can also be attacked through multiround conversations. We also developed a framework to assess the effectiveness of defensive strategies and identify unwanted behaviors in custom GPTs. Our findings show that 77.5% of custom GPTs with defense strategies are vulnerable to basic instruction leaking attacks. Additionally, we reveal that 738 custom GPTs collect user conversational information, and identified 8 GPTs exhibiting data access behaviors that are unnecessary for their intended functionalities. Our findings raise awareness among GPT developers about the importance of integrating specific defensive strategies in their instructions and highlight users' concerns about data privacy when using LLM-based applications.

摘要: 大型语言模型（LLM）展示了强大的信息处理能力，并被广泛集成到聊天机器人应用程序中。OpenAI为开发人员提供了一个构建自定义GPT的平台，扩展了ChatGPT的功能并集成了外部服务。自2023年11月发布以来，已创建了超过300万个自定义GPT。然而，如此庞大的生态系统也隐藏着安全和隐私威胁。对于开发人员来说，指令泄露攻击通过精心设计的对抗性提示威胁自定义GPT中指令的知识产权。对于用户来说，自定义GPT或集成第三方服务的不想要的数据访问行为会引发严重的隐私问题。为了系统地评估现实LLM应用中的威胁范围，我们开发了三个阶段的指令泄露攻击，目标是不同防御级别的GPT。我们对10，000个现实世界的自定义GPT进行的广泛实验表明，超过98.8%的GPT容易受到通过一个或多个对抗提示的指令泄露攻击，其余一半的GPT也可以通过多轮对话受到攻击。我们还开发了一个框架来评估防御策略的有效性并识别自定义GPT中不想要的行为。我们的研究结果表明，77.5%的具有防御策略的自定义GPT容易受到基本指令泄露攻击。此外，我们还透露，738个自定义GPT收集用户对话信息，并识别出8个GPT表现出其预期功能不必要的数据访问行为。我们的研究结果提高了GPT开发人员对在其指令中集成特定防御策略重要性的认识，并强调了用户在使用基于LLM的应用程序时对数据隐私的担忧。



## **3. Evaluating Apple Intelligence's Writing Tools for Privacy Against Large Language Model-Based Inference Attacks: Insights from Early Datasets**

评估Apple Intelligence的隐私撰写工具免受基于大型语言模型的推理攻击：来自早期数据集的见解 cs.LG

**SubmitDate**: 2025-06-04    [abs](http://arxiv.org/abs/2506.03870v1) [paper-pdf](http://arxiv.org/pdf/2506.03870v1)

**Authors**: Mohd. Farhan Israk Soumik, Syed Mhamudul Hasan, Abdur R. Shahid

**Abstract**: The misuse of Large Language Models (LLMs) to infer emotions from text for malicious purposes, known as emotion inference attacks, poses a significant threat to user privacy. In this paper, we investigate the potential of Apple Intelligence's writing tools, integrated across iPhone, iPad, and MacBook, to mitigate these risks through text modifications such as rewriting and tone adjustment. By developing early novel datasets specifically for this purpose, we empirically assess how different text modifications influence LLM-based detection. This capability suggests strong potential for Apple Intelligence's writing tools as privacy-preserving mechanisms. Our findings lay the groundwork for future adaptive rewriting systems capable of dynamically neutralizing sensitive emotional content to enhance user privacy. To the best of our knowledge, this research provides the first empirical analysis of Apple Intelligence's text-modification tools within a privacy-preservation context with the broader goal of developing on-device, user-centric privacy-preserving mechanisms to protect against LLMs-based advanced inference attacks on deployed systems.

摘要: 滥用大型语言模型（LLM）从文本中推断情感以用于恶意目的，称为情感推理攻击，对用户隐私构成了重大威胁。在本文中，我们调查了Apple Intelligence的写作工具的潜力，这些工具集成在iPhone，iPad和MacBook上，通过重写和音调调整等文本修改来减轻这些风险。通过专门为此目的开发早期的新数据集，我们经验性地评估了不同的文本修改如何影响基于LLM的检测。这种能力表明Apple Intelligence的写作工具作为隐私保护机制的强大潜力。我们的发现为未来能够动态中和敏感情感内容以增强用户隐私的自适应重写系统奠定了基础。据我们所知，这项研究对隐私保护环境下的Apple Intelligence文本修改工具进行了首次实证分析，其更广泛的目标是开发设备上、以用户为中心的隐私保护机制，以防止针对部署系统的基于LLM的高级推理攻击。



## **4. Client-Side Zero-Shot LLM Inference for Comprehensive In-Browser URL Analysis**

客户端零镜头LLM推理，用于全面的浏览器内URL分析 cs.CR

46 pages , 5 figures

**SubmitDate**: 2025-06-04    [abs](http://arxiv.org/abs/2506.03656v1) [paper-pdf](http://arxiv.org/pdf/2506.03656v1)

**Authors**: Avihay Cohen

**Abstract**: Malicious websites and phishing URLs pose an ever-increasing cybersecurity risk, with phishing attacks growing by 40% in a single year. Traditional detection approaches rely on machine learning classifiers or rule-based scanners operating in the cloud, but these face significant challenges in generalization, privacy, and evasion by sophisticated threats. In this paper, we propose a novel client-side framework for comprehensive URL analysis that leverages zero-shot inference by a local large language model (LLM) running entirely in-browser. Our system uses a compact LLM (e.g., 3B/8B parameters) via WebLLM to perform reasoning over rich context collected from the target webpage, including static code analysis (JavaScript abstract syntax trees, structure, and code patterns), dynamic sandbox execution results (DOM changes, API calls, and network requests),and visible content. We detail the architecture and methodology of the system, which combines a real browser sandbox (using iframes) resistant to common anti-analysis techniques, with an LLM-based analyzer that assesses potential vulnerabilities and malicious behaviors without any task-specific training (zero-shot). The LLM aggregates evidence from multiple sources (code, execution trace, page content) to classify the URL as benign or malicious and to provide an explanation of the threats or security issues identified. We evaluate our approach on a diverse set of benign and malicious URLs, demonstrating that even a compact client-side model can achieve high detection accuracy and insightful explanations comparable to cloud-based solutions, while operating privately on end-user devices. The results show that client-side LLM inference is a feasible and effective solution to web threat analysis, eliminating the need to send potentially sensitive data to cloud services.

摘要: 恶意网站和网络钓鱼网址构成了不断增加的网络安全风险，网络钓鱼攻击一年内增长了40%。传统的检测方法依赖于在云中运行的机器学习分类器或基于规则的扫描仪，但这些方法在概括、隐私和复杂威胁的规避方面面临着重大挑战。在本文中，我们提出了一种新颖的客户端框架，用于全面的URL分析，该框架利用完全在浏览器中运行的本地大型语言模型（LLM）的零触发推理。我们的系统使用紧凑的LLM（例如，3B/8B参数）通过WebLLM对从目标网页收集的丰富上下文执行推理，包括静态代码分析（JavaScript抽象语法树、结构和代码模式）、动态沙箱执行结果（多姆更改、API调用和网络请求）和可见内容。我们详细介绍了该系统的架构和方法，该系统将抵抗常见反分析技术的真正浏览器沙箱（使用iframe）与基于LLM的分析器结合起来，该分析器在无需任何特定任务的训练（零射击）的情况下评估潜在的漏洞和恶意行为。LLM汇总来自多个来源（代码、执行跟踪、页面内容）的证据，将URL归类为良性或恶意，并提供所识别的威胁或安全问题的解释。我们评估了我们对一组不同的良性和恶意URL的方法，证明即使是紧凑的客户端模型也可以实现与基于云的解决方案相媲美的高检测准确性和富有洞察力的解释，同时在最终用户设备上进行私下操作。结果表明，客户端LLM推理是一种可行且有效的Web威胁分析解决方案，无需将潜在敏感数据发送到云服务。



## **5. Robustness of Prompting: Enhancing Robustness of Large Language Models Against Prompting Attacks**

预算处理的鲁棒性：增强大型语言模型针对预算处理攻击的鲁棒性 cs.CL

13pages

**SubmitDate**: 2025-06-04    [abs](http://arxiv.org/abs/2506.03627v1) [paper-pdf](http://arxiv.org/pdf/2506.03627v1)

**Authors**: Lin Mu, Guowei Chu, Li Ni, Lei Sang, Zhize Wu, Peiquan Jin, Yiwen Zhang

**Abstract**: Large Language Models (LLMs) have demonstrated remarkable performance across various tasks by effectively utilizing a prompting strategy. However, they are highly sensitive to input perturbations, such as typographical errors or slight character order errors, which can substantially degrade their performance. Despite advances in prompting techniques, developing a prompting strategy that explicitly mitigates the negative impact of such perturbations remains an open challenge. To bridge this gap, we propose Robustness of Prompting (RoP), a novel prompting strategy specifically designed to enhance the robustness of LLMs. RoP consists of two stages: Error Correction and Guidance. In the Error Correction stage, RoP applies diverse perturbation methods to generate adversarial examples, which are then used to construct prompts that automatically correct input errors. In the Guidance stage, RoP generates an optimal guidance prompting based on the corrected input, steering the model toward more robust and accurate inferences. Through comprehensive experiments spanning arithmetic, commonsense, and logical reasoning tasks, we demonstrate that RoP significantly improves LLMs' robustness against adversarial perturbations. Notably, it maintains model accuracy with only minimal degradation compared to clean input scenarios, thereby establishing RoP as a practical and effective approach for enhancing LLM robustness in real-world applications.

摘要: 通过有效利用提示策略，大型语言模型（LLM）在各种任务中表现出了出色的性能。然而，它们对输入扰动高度敏感，例如印刷错误或轻微的字符顺序错误，这可能会大幅降低它们的性能。尽管提示技术取得了进步，但开发一种明确减轻此类扰动负面影响的提示策略仍然是一个悬而未决的挑战。为了弥合这一差距，我们提出了激励稳健性（RoP），这是一种专门设计用于增强LLM稳健性的新型激励策略。规则包括两个阶段：错误纠正和指导。在纠错阶段，RoP应用各种扰动方法来生成对抗性示例，然后使用这些示例来构造自动纠正输入错误的提示。在指导阶段，RoP基于校正的输入生成最佳指导提示，将模型转向更鲁棒和准确的推断。通过跨越算术，常识和逻辑推理任务的综合实验，我们证明了RoP显着提高了LLM对对抗性扰动的鲁棒性。值得注意的是，与干净的输入场景相比，它保持了模型准确性，仅降低了最低限度，从而将RoP确立为在现实世界应用中增强LLM稳健性的实用有效方法。



## **6. Should LLM Safety Be More Than Refusing Harmful Instructions?**

LLM的安全是否应该不仅仅是拒绝有害的指示？ cs.CL

Preprint

**SubmitDate**: 2025-06-04    [abs](http://arxiv.org/abs/2506.02442v2) [paper-pdf](http://arxiv.org/pdf/2506.02442v2)

**Authors**: Utsav Maskey, Mark Dras, Usman Naseem

**Abstract**: This paper presents a systematic evaluation of Large Language Models' (LLMs) behavior on long-tail distributed (encrypted) texts and their safety implications. We introduce a two-dimensional framework for assessing LLM safety: (1) instruction refusal-the ability to reject harmful obfuscated instructions, and (2) generation safety-the suppression of generating harmful responses. Through comprehensive experiments, we demonstrate that models that possess capabilities to decrypt ciphers may be susceptible to mismatched-generalization attacks: their safety mechanisms fail on at least one safety dimension, leading to unsafe responses or over-refusal. Based on these findings, we evaluate a number of pre-LLM and post-LLM safeguards and discuss their strengths and limitations. This work contributes to understanding the safety of LLM in long-tail text scenarios and provides directions for developing robust safety mechanisms.

摘要: 本文对长尾分布式（加密）文本上的大型语言模型（LLM）行为及其安全影响进行了系统评估。我们引入了一个评估LLM安全性的二维框架：（1）指令反思-拒绝有害混淆指令的能力，和（2）生成安全-抑制生成有害响应。通过全面的实验，我们证明，拥有解密密码能力的模型可能容易受到不匹配概括攻击：它们的安全机制至少在一个安全维度上失败，导致不安全的响应或过度拒绝。基于这些调查结果，我们评估了LLM前和LLM后的一些保障措施，并讨论了它们的优点和局限性。这项工作有助于了解LLM在长尾文本场景中的安全性，并为开发稳健的安全机制提供了方向。



## **7. PC-MoE: Memory-Efficient and Privacy-Preserving Collaborative Training for Mixture-of-Experts LLMs**

PC-MoE：针对混合型专家法学硕士的内存高效且隐私保护的协作培训 cs.LG

20 pages, 4 figures

**SubmitDate**: 2025-06-04    [abs](http://arxiv.org/abs/2506.02965v2) [paper-pdf](http://arxiv.org/pdf/2506.02965v2)

**Authors**: Ze Yu Zhang, Bolin Ding, Bryan Kian Hsiang Low

**Abstract**: Mixture-of-Experts (MoE) has been gaining popularity due to its successful adaptation to large language models (LLMs). In this work, we introduce Privacy-preserving Collaborative Mixture-of-Experts (PC-MoE), which leverages the sparsity of the MoE architecture for memory-efficient decentralized collaborative LLM training, enabling multiple parties with limited GPU-memory and data resources to collectively train more capable LLMs than they could achieve individually. At the same time, this approach protects training data privacy of each participant by keeping training data, as well as parts of the forward pass signal and gradients locally within each party. By design, PC-MoE synergistically combines the strengths of distributed computation with strong confidentiality assurances. Unlike most privacy-preserving schemes, which pay for confidentiality with lower task accuracy, our framework breaks that trade-off: across seven popular LLM benchmarks, it almost matches (and sometimes exceeds) the performance and convergence rate of a fully centralized model, enjoys near 70% peak GPU RAM reduction, while being fully robust against reconstruction attacks.

摘要: 混合专家（MoE）由于其成功地适应大型语言模型（LLM）而越来越受欢迎。在这项工作中，我们引入了隐私保护协作混合专家（PC-MoE），它利用MoE架构的稀疏性进行内存高效的分散式协作LLM训练，使具有有限GPU内存和数据资源的多方能够共同训练比他们单独实现的更有能力的LLM。与此同时，这种方法通过将训练数据以及部分前向传递信号和梯度保存在各方内部来保护每个参与者的训练数据隐私。通过设计，PC-MoE将分布式计算的优势与强大的机密性保证协同结合起来。与大多数隐私保护方案（以较低的任务准确性来支付机密性费用）不同，我们的框架打破了这种权衡：在七个流行的LLM基准测试中，它几乎与（有时甚至超过）完全集中化模型的性能和收敛速度相匹配，享受近70%的峰值图形内存减少，同时完全稳健地抵御重建攻击。



## **8. Across Programming Language Silos: A Study on Cross-Lingual Retrieval-augmented Code Generation**

跨编程语言筒仓：跨语言检索增强代码生成研究 cs.SE

**SubmitDate**: 2025-06-04    [abs](http://arxiv.org/abs/2506.03535v1) [paper-pdf](http://arxiv.org/pdf/2506.03535v1)

**Authors**: Qiming Zhu, Jialun Cao, Xuanang Chen, Yaojie Lu, Hongyu Lin, Xianpei Han, Le Sun, Shing-Chi Cheung

**Abstract**: Current research on large language models (LLMs) with retrieval-augmented code generation (RACG) mainly focuses on single-language settings, leaving cross-lingual effectiveness and security unexplored. Multi-lingual RACG systems are valuable for migrating code-bases across programming languages (PLs), yet face risks from error (e.g. adversarial data corruption) propagation in cross-lingual transfer. We construct a dataset spanning 13 PLs with nearly 14k instances to explore utility and robustness of multi-lingual RACG systems. Our investigation reveals four key insights: (1) Effectiveness: multi-lingual RACG significantly enhances multi-lingual code LLMs generation; (2) Inequality: Java demonstrate superior cross-lingual utility over Python in RACG; (3) Robustness: Adversarial attacks degrade performance significantly in mono-lingual RACG but show mitigated impacts in cross-lingual scenarios; Counterintuitively, perturbed code may improve RACG in cross-lingual scenarios; (4) Specialization: Domain-specific code retrievers outperform significantly general text retrievers. These findings establish foundation for developing effective and secure multi-lingual code assistants.

摘要: 目前对具有检索增强代码生成（RACG）的大型语言模型（LLM）的研究主要集中在单语言设置上，而跨语言有效性和安全性尚未得到探索。多语言RACG系统对于跨编程语言（PL）迁移代码库很有价值，但在跨语言传输中面临错误传播（例如对抗性数据损坏）的风险。我们构建了一个跨越13个PL和近14，000个实例的数据集，以探索多语言RACG系统的实用性和稳健性。我们的调查揭示了四个关键见解：（1）有效性：多语言RACG显着增强了多语言代码LLM的生成;（2）不平等性：Java在RACG中表现出优于Python的跨语言实用性;（3）鲁棒性：对抗性攻击在单语言RACG中显着降低性能，但在跨语言场景中显示出减轻的影响;与直觉相反，受干扰的代码可能会在跨语言场景中改进RACG;（4）专业化：领域特定代码检索器的性能明显优于一般文本检索器。这些发现为开发有效且安全的多语言代码助手奠定了基础。



## **9. Defensive Prompt Patch: A Robust and Interpretable Defense of LLMs against Jailbreak Attacks**

防御提示补丁：LLM针对越狱攻击的强大且可解释的防御 cs.CR

**SubmitDate**: 2025-06-04    [abs](http://arxiv.org/abs/2405.20099v2) [paper-pdf](http://arxiv.org/pdf/2405.20099v2)

**Authors**: Chen Xiong, Xiangyu Qi, Pin-Yu Chen, Tsung-Yi Ho

**Abstract**: Safety, security, and compliance are essential requirements when aligning large language models (LLMs). However, many seemingly aligned LLMs are soon shown to be susceptible to jailbreak attacks. These attacks aim to circumvent the models' safety guardrails and security mechanisms by introducing jailbreak prompts into malicious queries. In response to these challenges, this paper introduces Defensive Prompt Patch (DPP), a novel prompt-based defense mechanism specifically designed to protect LLMs against such sophisticated jailbreak strategies. Unlike previous approaches, which have often compromised the utility of the model for the sake of safety, DPP is designed to achieve a minimal Attack Success Rate (ASR) while preserving the high utility of LLMs. Our method uses strategically designed interpretable suffix prompts that effectively thwart a wide range of standard and adaptive jailbreak techniques. Empirical results conducted on LLAMA-2-7B-Chat and Mistral-7B-Instruct-v0.2 models demonstrate the robustness and adaptability of DPP, showing significant reductions in ASR with negligible impact on utility. Our approach not only outperforms existing defense strategies in balancing safety and functionality, but also provides a scalable and interpretable solution applicable to various LLM platforms.

摘要: 安全性、安全性和合规性是协调大型语言模型（LLM）时的基本要求。然而，许多看似一致的LLM很快就被证明容易受到越狱攻击。这些攻击旨在通过在恶意查询中引入越狱提示来绕过模型的安全护栏和安全机制。为了应对这些挑战，本文引入了防御提示补丁（DPP），这是一种新型的基于预算的防御机制，专门设计用于保护LLM免受此类复杂越狱策略的侵害。与以前的方法不同，为了安全起见，这些方法通常会损害模型的实用性，DPP旨在实现最小的攻击成功率（ASR），同时保留LLM的高实用性。我们的方法使用策略性设计的可解释后缀提示，有效地挫败了广泛的标准和自适应越狱技术。在LLAMA-2- 7 B-Chat和Mistral-7 B-Instruct-v0.2模型上进行的实证结果证明了DPP的鲁棒性和适应性，表明ASR显著降低，对效用的影响可以忽略不计。我们的方法不仅在平衡安全性和功能性方面优于现有的防御策略，而且还提供了适用于各种LLM平台的可扩展和可解释的解决方案。



## **10. DyePack: Provably Flagging Test Set Contamination in LLMs Using Backdoors**

DyePack：使用后门可证明标记LLM中的测试集污染 cs.CL

**SubmitDate**: 2025-06-04    [abs](http://arxiv.org/abs/2505.23001v3) [paper-pdf](http://arxiv.org/pdf/2505.23001v3)

**Authors**: Yize Cheng, Wenxiao Wang, Mazda Moayeri, Soheil Feizi

**Abstract**: Open benchmarks are essential for evaluating and advancing large language models, offering reproducibility and transparency. However, their accessibility makes them likely targets of test set contamination. In this work, we introduce DyePack, a framework that leverages backdoor attacks to identify models that used benchmark test sets during training, without requiring access to the loss, logits, or any internal details of the model. Like how banks mix dye packs with their money to mark robbers, DyePack mixes backdoor samples with the test data to flag models that trained on it. We propose a principled design incorporating multiple backdoors with stochastic targets, enabling exact false positive rate (FPR) computation when flagging every model. This provably prevents false accusations while providing strong evidence for every detected case of contamination. We evaluate DyePack on five models across three datasets, covering both multiple-choice and open-ended generation tasks. For multiple-choice questions, it successfully detects all contaminated models with guaranteed FPRs as low as 0.000073% on MMLU-Pro and 0.000017% on Big-Bench-Hard using eight backdoors. For open-ended generation tasks, it generalizes well and identifies all contaminated models on Alpaca with a guaranteed false positive rate of just 0.127% using six backdoors.

摘要: 开放基准对于评估和推进大型语言模型、提供可重复性和透明度至关重要。然而，它们的可及性使它们可能成为测试集污染的目标。在这项工作中，我们引入了DyePack，这是一个利用后门攻击来识别在训练期间使用基准测试集的模型的框架，而不需要访问模型的损失、日志或任何内部细节。就像银行将染料包与钱混合来标记劫匪一样，DyePack将后门样本与测试数据混合起来，以标记对其进行训练的模型。我们提出了一种原则性设计，将多个后门与随机目标结合在一起，在标记每个模型时实现精确的假阳性率（FPR）计算。事实证明，这可以防止虚假指控，同时为每一个检测到的污染案例提供强有力的证据。我们在三个数据集的五个模型上评估了DyePack，涵盖多项选择和开放式生成任务。对于多项选择题，它使用八个后门成功检测到所有受污染的型号，保证FPR在MMLU-Pro上低至0.000073%，在Big-Bench-Hard上低至0.00017%。对于开放式生成任务，它可以很好地推广，并使用六个后门识别羊驼上所有受污染的模型，保证假阳性率仅为0.127%。



## **11. Robustness in Both Domains: CLIP Needs a Robust Text Encoder**

两个领域的稳健性：CLIP需要强大的文本编码器 cs.LG

**SubmitDate**: 2025-06-03    [abs](http://arxiv.org/abs/2506.03355v1) [paper-pdf](http://arxiv.org/pdf/2506.03355v1)

**Authors**: Elias Abad Rocamora, Christian Schlarmann, Naman Deep Singh, Yongtao Wu, Matthias Hein, Volkan Cevher

**Abstract**: Adversarial input attacks can cause a significant shift of CLIP embeddings. This can affect the downstream robustness of models incorporating CLIP in the pipeline, such as text-to-image generative models or large vision language models. While some efforts have been done towards making the CLIP image encoders robust, the robustness of text encoders remains unexplored. In this work, we cover this gap in the literature. We propose LEAF: an efficient adversarial finetuning method for the text domain, with the ability to scale to large CLIP models. Our models significantly improve the zero-shot adversarial accuracy in the text domain, while maintaining the vision performance provided by robust image encoders. When combined with text-to-image diffusion models, we can improve the generation quality under adversarial noise. When employing our robust CLIP encoders in multimodal retrieval tasks, we improve the recall under adversarial noise over standard CLIP models. Finally, we show that robust text encoders facilitate better reconstruction of input text from its embedding via direct optimization.

摘要: 对抗性输入攻击可能会导致CLIP嵌入的显着转变。这可能会影响在管道中纳入CLIP的模型的下游稳健性，例如文本到图像生成模型或大视觉语言模型。虽然已经做出了一些努力来使CLIP图像编码器稳健，但文本编码器的稳健性仍然有待探索。在这项工作中，我们涵盖了文献中的这一空白。我们提出LEAF：一种针对文本领域的高效对抗微调方法，能够扩展到大型CLIP模型。我们的模型显着提高了文本域中的零镜头对抗准确性，同时保持稳健的图像编码器提供的视觉性能。当与文本到图像扩散模型相结合时，我们可以提高对抗性噪音下的生成质量。当在多模式检索任务中使用我们稳健的CLIP编码器时，我们在对抗性噪音下比标准CLIP模型提高了召回率。最后，我们表明稳健的文本编码器可以通过直接优化从嵌入中更好地重建输入文本。



## **12. Adversarial Attacks on Robotic Vision Language Action Models**

对机器人视觉语言动作模型的对抗攻击 cs.RO

**SubmitDate**: 2025-06-03    [abs](http://arxiv.org/abs/2506.03350v1) [paper-pdf](http://arxiv.org/pdf/2506.03350v1)

**Authors**: Eliot Krzysztof Jones, Alexander Robey, Andy Zou, Zachary Ravichandran, George J. Pappas, Hamed Hassani, Matt Fredrikson, J. Zico Kolter

**Abstract**: The emergence of vision-language-action models (VLAs) for end-to-end control is reshaping the field of robotics by enabling the fusion of multimodal sensory inputs at the billion-parameter scale. The capabilities of VLAs stem primarily from their architectures, which are often based on frontier large language models (LLMs). However, LLMs are known to be susceptible to adversarial misuse, and given the significant physical risks inherent to robotics, questions remain regarding the extent to which VLAs inherit these vulnerabilities. Motivated by these concerns, in this work we initiate the study of adversarial attacks on VLA-controlled robots. Our main algorithmic contribution is the adaptation and application of LLM jailbreaking attacks to obtain complete control authority over VLAs. We find that textual attacks, which are applied once at the beginning of a rollout, facilitate full reachability of the action space of commonly used VLAs and often persist over longer horizons. This differs significantly from LLM jailbreaking literature, as attacks in the real world do not have to be semantically linked to notions of harm. We make all code available at https://github.com/eliotjones1/robogcg .

摘要: 用于端到端控制的视觉-语言-动作模型（VLA）的出现正在重塑机器人领域，它能够在十亿参数规模上融合多模态感官输入。VLA的能力主要来自其架构，这些架构通常基于前沿大型语言模型（LLM）。然而，众所周知，LLM容易受到对抗性滥用的影响，并且考虑到机器人固有的重大物理风险，关于VLA继承这些漏洞的程度仍然存在问题。出于这些担忧，在这项工作中，我们开始研究对VLA控制的机器人的对抗性攻击。我们的主要算法的贡献是适应和应用LLM越狱攻击，以获得完全控制权的VLA。我们发现，文本攻击，这是适用于一次在一开始的推出，促进充分的可达性的常用的VLA的动作空间，并经常持续较长的视野。这与LLM越狱文学有很大的不同，因为现实世界中的攻击不必与伤害的概念在语义上联系起来。我们在https://github.com/eliotjones1/robogcg上提供所有代码。



## **13. Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region**

为什么受保障的船只会搁浅？对齐的大型语言模型的安全机制倾向于锚定在模板区域 cs.CL

ACL 2025 Main

**SubmitDate**: 2025-06-03    [abs](http://arxiv.org/abs/2502.13946v2) [paper-pdf](http://arxiv.org/pdf/2502.13946v2)

**Authors**: Chak Tou Leong, Qingyu Yin, Jian Wang, Wenjie Li

**Abstract**: The safety alignment of large language models (LLMs) remains vulnerable, as their initial behavior can be easily jailbroken by even relatively simple attacks. Since infilling a fixed template between the input instruction and initial model output is a common practice for existing LLMs, we hypothesize that this template is a key factor behind their vulnerabilities: LLMs' safety-related decision-making overly relies on the aggregated information from the template region, which largely influences these models' safety behavior. We refer to this issue as template-anchored safety alignment. In this paper, we conduct extensive experiments and verify that template-anchored safety alignment is widespread across various aligned LLMs. Our mechanistic analyses demonstrate how it leads to models' susceptibility when encountering inference-time jailbreak attacks. Furthermore, we show that detaching safety mechanisms from the template region is promising in mitigating vulnerabilities to jailbreak attacks. We encourage future research to develop more robust safety alignment techniques that reduce reliance on the template region.

摘要: 大型语言模型（LLM）的安全一致仍然很脆弱，因为即使是相对简单的攻击也可以轻松破解它们的初始行为。由于在输入指令和初始模型输出之间填充固定模板是现有LLM的常见做法，因此我们假设该模板是其漏洞背后的关键因素：LLM的安全相关决策过度依赖来自模板区域的聚合信息，这在很大程度上影响了这些模型的安全行为。我们将这个问题称为模板锚定安全对齐。在本文中，我们进行了广泛的实验并验证模板锚定安全对齐在各种对齐的LLM中广泛存在。我们的机制分析展示了它如何导致模型在遇到推理时越狱攻击时的易感性。此外，我们表明，将安全机制与模板区域分离在减轻越狱攻击的脆弱性方面有希望。我们鼓励未来的研究开发更强大的安全对齐技术，以减少对模板区域的依赖。



## **14. Unveiling Privacy Risks in LLM Agent Memory**

揭露LLM代理内存中的隐私风险 cs.CR

ACL 2025 (Main Conference)

**SubmitDate**: 2025-06-03    [abs](http://arxiv.org/abs/2502.13172v2) [paper-pdf](http://arxiv.org/pdf/2502.13172v2)

**Authors**: Bo Wang, Weiyi He, Shenglai Zeng, Zhen Xiang, Yue Xing, Jiliang Tang, Pengfei He

**Abstract**: Large Language Model (LLM) agents have become increasingly prevalent across various real-world applications. They enhance decision-making by storing private user-agent interactions in the memory module for demonstrations, introducing new privacy risks for LLM agents. In this work, we systematically investigate the vulnerability of LLM agents to our proposed Memory EXTRaction Attack (MEXTRA) under a black-box setting. To extract private information from memory, we propose an effective attacking prompt design and an automated prompt generation method based on different levels of knowledge about the LLM agent. Experiments on two representative agents demonstrate the effectiveness of MEXTRA. Moreover, we explore key factors influencing memory leakage from both the agent designer's and the attacker's perspectives. Our findings highlight the urgent need for effective memory safeguards in LLM agent design and deployment.

摘要: 大型语言模型（LLM）代理在各种现实世界的应用程序中变得越来越普遍。它们通过将私人用户-代理交互存储在内存模块中以进行演示来增强决策，从而为LLM代理带来新的隐私风险。在这项工作中，我们系统地研究了LLM代理在黑匣子环境下对我们提出的内存EXTRaction攻击（MEXTRA）的脆弱性。为了从内存中提取私人信息，我们提出了一种有效的攻击提示设计和基于LLM代理不同知识水平的自动提示生成方法。对两个代表性代理的实验证明了MEXTRA的有效性。此外，我们还从代理设计者和攻击者的角度探讨了影响内存泄漏的关键因素。我们的研究结果凸显了LLM代理设计和部署中迫切需要有效的内存保护措施。



## **15. Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs**

只见树木不见森林：利用启发式和偏见来激发对LLM的非理性选择 cs.CL

**SubmitDate**: 2025-06-03    [abs](http://arxiv.org/abs/2505.02862v2) [paper-pdf](http://arxiv.org/pdf/2505.02862v2)

**Authors**: Haoming Yang, Ke Ma, Xiaojun Jia, Yingfei Sun, Qianqian Xu, Qingming Huang

**Abstract**: Despite the remarkable performance of Large Language Models (LLMs), they remain vulnerable to jailbreak attacks, which can compromise their safety mechanisms. Existing studies often rely on brute-force optimization or manual design, failing to uncover potential risks in real-world scenarios. To address this, we propose a novel jailbreak attack framework, ICRT, inspired by heuristics and biases in human cognition. Leveraging the simplicity effect, we employ cognitive decomposition to reduce the complexity of malicious prompts. Simultaneously, relevance bias is utilized to reorganize prompts, enhancing semantic alignment and inducing harmful outputs effectively. Furthermore, we introduce a ranking-based harmfulness evaluation metric that surpasses the traditional binary success-or-failure paradigm by employing ranking aggregation methods such as Elo, HodgeRank, and Rank Centrality to comprehensively quantify the harmfulness of generated content. Experimental results show that our approach consistently bypasses mainstream LLMs' safety mechanisms and generates high-risk content, providing insights into jailbreak attack risks and contributing to stronger defense strategies.

摘要: 尽管大型语言模型（LLM）性能出色，但它们仍然容易受到越狱攻击，这可能会损害其安全机制。现有的研究通常依赖于暴力优化或手动设计，未能发现现实世界场景中的潜在风险。为了解决这个问题，我们提出了一种新颖的越狱攻击框架ICRT，其灵感来自人类认知中的启发和偏见。利用简单性效应，我们采用认知分解来降低恶意提示的复杂性。同时，利用相关性偏差来重组提示，增强语义对齐并有效地诱导有害输出。此外，我们引入了一种基于排名的危害性评估指标，通过采用Elo、HodgeRank和Rank Centrality等排名聚合方法来全面量化生成内容的危害性，超越了传统的二元成败范式。实验结果表明，我们的方法始终绕过主流LLM的安全机制并生成高风险内容，提供了对越狱攻击风险的见解，并有助于制定更强有力的防御策略。



## **16. A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of Iterative Chaos**

捕鼠器：用迭代混乱链欺骗越狱的大型推理模型 cs.CR

**SubmitDate**: 2025-06-03    [abs](http://arxiv.org/abs/2502.15806v2) [paper-pdf](http://arxiv.org/pdf/2502.15806v2)

**Authors**: Yang Yao, Xuan Tong, Ruofan Wang, Yixu Wang, Lujundong Li, Liang Liu, Yan Teng, Yingchun Wang

**Abstract**: Large Reasoning Models (LRMs) have significantly advanced beyond traditional Large Language Models (LLMs) with their exceptional logical reasoning capabilities, yet these improvements introduce heightened safety risks. When subjected to jailbreak attacks, their ability to generate more targeted and organized content can lead to greater harm. Although some studies claim that reasoning enables safer LRMs against existing LLM attacks, they overlook the inherent flaws within the reasoning process itself. To address this gap, we propose the first jailbreak attack targeting LRMs, exploiting their unique vulnerabilities stemming from the advanced reasoning capabilities. Specifically, we introduce a Chaos Machine, a novel component to transform attack prompts with diverse one-to-one mappings. The chaos mappings iteratively generated by the machine are embedded into the reasoning chain, which strengthens the variability and complexity and also promotes a more robust attack. Based on this, we construct the Mousetrap framework, which makes attacks projected into nonlinear-like low sample spaces with mismatched generalization enhanced. Also, due to the more competing objectives, LRMs gradually maintain the inertia of unpredictable iterative reasoning and fall into our trap. Success rates of the Mousetrap attacking o1-mini, Claude-Sonnet and Gemini-Thinking are as high as 96%, 86% and 98% respectively on our toxic dataset Trotter. On benchmarks such as AdvBench, StrongREJECT, and HarmBench, attacking Claude-Sonnet, well-known for its safety, Mousetrap can astonishingly achieve success rates of 87.5%, 86.58% and 93.13% respectively. Attention: This paper contains inappropriate, offensive and harmful content.

摘要: 大型推理模型（LRM）凭借其卓越的逻辑推理能力，已大大超越传统的大型语言模型（LLM），但这些改进带来了更高的安全风险。当遭受越狱攻击时，他们生成更具针对性和组织性内容的能力可能会导致更大的伤害。尽管一些研究声称推理可以使LRM更安全地对抗现有的LLM攻击，但他们忽视了推理过程本身的固有缺陷。为了解决这一差距，我们提出了针对LRM的第一次越狱攻击，利用其源自高级推理能力的独特漏洞。具体来说，我们引入了一个Chaos Machine，这是一种新颖的组件，可以通过各种一对一映射来转换攻击提示。机器迭代生成的混乱映射被嵌入到推理链中，这增强了可变性和复杂性，同时也促进了更稳健的攻击。在此基础上，我们构建了Mousetrap框架，使攻击投射到类非线性的低样本空间，并增强了不匹配的概括性。此外，由于目标的竞争性更大，LRM逐渐保持着不可预测的迭代推理的惯性并落入我们的陷阱。在我们的有毒数据集Trotter上，捕鼠器攻击o 1-mini、Claude-Sonnet和Gemini-Thinking的成功率分别高达96%、86%和98%。在AdvBench、StrongRESEARCH和HarmBench等基准测试上，攻击以安全性而闻名的Claude-Sonnet，Mousetrap的成功率分别达到87.5%、86.58%和93.13%。注意：本文包含不恰当、冒犯性和有害内容。



## **17. PoisonArena: Uncovering Competing Poisoning Attacks in Retrieval-Augmented Generation**

PoisonArena：揭露检索增强一代中的竞争中毒攻击 cs.IR

Project page: https://poison-arena.github.io/

**SubmitDate**: 2025-06-03    [abs](http://arxiv.org/abs/2505.12574v4) [paper-pdf](http://arxiv.org/pdf/2505.12574v4)

**Authors**: Liuji Chen, Xiaofang Yang, Yuanzhuo Lu, Jinghao Zhang, Xin Sun, Qiang Liu, Shu Wu, Jing Dong, Liang Wang

**Abstract**: Retrieval-Augmented Generation (RAG) systems, widely used to improve the factual grounding of large language models (LLMs), are increasingly vulnerable to poisoning attacks, where adversaries inject manipulated content into the retriever's corpus. While prior research has predominantly focused on single-attacker settings, real-world scenarios often involve multiple, competing attackers with conflicting objectives. In this work, we introduce PoisonArena, the first benchmark to systematically study and evaluate competing poisoning attacks in RAG. We formalize the multi-attacker threat model, where attackers vie to control the answer to the same query using mutually exclusive misinformation. PoisonArena leverages the Bradley-Terry model to quantify each method's competitive effectiveness in such adversarial environments. Through extensive experiments on the Natural Questions and MS MARCO datasets, we demonstrate that many attack strategies successful in isolation fail under competitive pressure. Our findings highlight the limitations of conventional evaluation metrics like Attack Success Rate (ASR) and F1 score and underscore the need for competitive evaluation to assess real-world attack robustness. PoisonArena provides a standardized framework to benchmark and develop future attack and defense strategies under more realistic, multi-adversary conditions.

摘要: 检索增强生成（RAG）系统，广泛用于改善大型语言模型（LLM）的事实基础，越来越容易受到中毒攻击，其中对手将操纵的内容注入检索器的语料库。虽然以前的研究主要集中在单个攻击者的设置，但现实世界的场景往往涉及多个相互竞争的攻击者，这些攻击者的目标相互冲突。在这项工作中，我们介绍PoisonArena，第一个基准系统地研究和评估竞争中毒攻击在RAG。我们形式化的多攻击者威胁模型，攻击者争夺控制答案相同的查询使用互斥的错误信息。PoisonArena利用Bradley-Terry模型来量化每种方法在此类对抗环境中的竞争有效性。通过对Natural Questions和MS MARCO数据集的广泛实验，我们证明了许多孤立成功的攻击策略在竞争压力下失败。我们的研究结果强调了攻击成功率（SVR）和F1评分等传统评估指标的局限性，并强调了竞争性评估来评估现实世界攻击稳健性的必要性。PoisonArena提供了一个标准化的框架，可以在更现实的多对手条件下基准和开发未来的攻击和防御策略。



## **18. SATA: A Paradigm for LLM Jailbreak via Simple Assistive Task Linkage**

SATA：通过简单辅助任务链接实现LLM越狱的范例 cs.CR

**SubmitDate**: 2025-06-03    [abs](http://arxiv.org/abs/2412.15289v3) [paper-pdf](http://arxiv.org/pdf/2412.15289v3)

**Authors**: Xiaoning Dong, Wenbo Hu, Wei Xu, Tianxing He

**Abstract**: Large language models (LLMs) have made significant advancements across various tasks, but their safety alignment remain a major concern. Exploring jailbreak prompts can expose LLMs' vulnerabilities and guide efforts to secure them. Existing methods primarily design sophisticated instructions for the LLM to follow, or rely on multiple iterations, which could hinder the performance and efficiency of jailbreaks. In this work, we propose a novel jailbreak paradigm, Simple Assistive Task Linkage (SATA), which can effectively circumvent LLM safeguards and elicit harmful responses. Specifically, SATA first masks harmful keywords within a malicious query to generate a relatively benign query containing one or multiple [MASK] special tokens. It then employs a simple assistive task such as a masked language model task or an element lookup by position task to encode the semantics of the masked keywords. Finally, SATA links the assistive task with the masked query to jointly perform the jailbreak. Extensive experiments show that SATA achieves state-of-the-art performance and outperforms baselines by a large margin. Specifically, on AdvBench dataset, with mask language model (MLM) assistive task, SATA achieves an overall attack success rate (ASR) of 85% and harmful score (HS) of 4.57, and with element lookup by position (ELP) assistive task, SATA attains an overall ASR of 76% and HS of 4.43.

摘要: 大型语言模型（LLM）在各种任务中取得了重大进展，但它们的安全性一致仍然是一个主要问题。探索越狱提示可以暴露LLM的漏洞并指导保护它们的工作。现有的方法主要设计复杂的指令供LLM遵循，或者依赖于多次迭代，这可能会阻碍越狱的性能和效率。在这项工作中，我们提出了一种新颖的越狱范式--简单辅助任务链接（ATA），它可以有效地规避LLM保障措施并引发有害反应。具体来说，ATA首先屏蔽恶意查询中的有害关键词，以生成包含一个或多个[MASK]特殊令牌的相对良性的查询。然后，它采用简单的辅助任务，例如掩蔽语言模型任务或按位置查找元素任务来编码掩蔽关键词的语义。最后，ATA将辅助任务与屏蔽查询链接起来，共同执行越狱。大量实验表明，ATA实现了最先进的性能，并且大幅优于基线。具体来说，在AdvBench数据集上，通过屏蔽语言模型（MLM）辅助任务，ATA的总体攻击成功率（ASB）达到85%，有害评分（HS）达到4.57，通过按位置查找元素（ELP）辅助任务，ATA的总体攻击成功率（ASB）达到76%，HS达到4.43。



## **19. ATAG: AI-Agent Application Threat Assessment with Attack Graphs**

ATAG：使用攻击图进行AI代理应用威胁评估 cs.CR

**SubmitDate**: 2025-06-03    [abs](http://arxiv.org/abs/2506.02859v1) [paper-pdf](http://arxiv.org/pdf/2506.02859v1)

**Authors**: Parth Atulbhai Gandhi, Akansha Shukla, David Tayouri, Beni Ifland, Yuval Elovici, Rami Puzis, Asaf Shabtai

**Abstract**: Evaluating the security of multi-agent systems (MASs) powered by large language models (LLMs) is challenging, primarily because of the systems' complex internal dynamics and the evolving nature of LLM vulnerabilities. Traditional attack graph (AG) methods often lack the specific capabilities to model attacks on LLMs. This paper introduces AI-agent application Threat assessment with Attack Graphs (ATAG), a novel framework designed to systematically analyze the security risks associated with AI-agent applications. ATAG extends the MulVAL logic-based AG generation tool with custom facts and interaction rules to accurately represent AI-agent topologies, vulnerabilities, and attack scenarios. As part of this research, we also created the LLM vulnerability database (LVD) to initiate the process of standardizing LLM vulnerabilities documentation. To demonstrate ATAG's efficacy, we applied it to two multi-agent applications. Our case studies demonstrated the framework's ability to model and generate AGs for sophisticated, multi-step attack scenarios exploiting vulnerabilities such as prompt injection, excessive agency, sensitive information disclosure, and insecure output handling across interconnected agents. ATAG is an important step toward a robust methodology and toolset to help understand, visualize, and prioritize complex attack paths in multi-agent AI systems (MAASs). It facilitates proactive identification and mitigation of AI-agent threats in multi-agent applications.

摘要: 评估由大型语言模型（LLM）驱动的多智能体系统（MAS）的安全性具有挑战性，主要是因为系统复杂的内部动态和LLM漏洞的演变性质。传统的攻击图（AG）方法往往缺乏特定的能力来建模对LLM的攻击。本文介绍了AI代理应用程序的威胁评估与攻击图（ATAG），一个新的框架，旨在系统地分析与AI代理应用程序相关的安全风险。ATAG通过自定义事实和交互规则扩展了基于MulVAL逻辑的AG生成工具，以准确表示AI代理拓扑、漏洞和攻击场景。作为这项研究的一部分，我们还创建了LLM漏洞数据库（LVD），以启动标准化LLM漏洞文档的过程。为了证明ATAG的有效性，我们将其应用于两个多智能体应用程序。我们的案例研究表明，该框架的建模和生成复杂的，多步骤的攻击方案，利用漏洞，如即时注入，过度代理，敏感信息泄露，以及不安全的输出处理跨互联代理的AG的能力。ATAG是迈向强大方法和工具集的重要一步，可以帮助理解、可视化多智能体人工智能系统（MAAS）中复杂的攻击路径并对其进行优先级排序。它有助于主动识别和缓解多代理应用程序中的AI代理威胁。



## **20. Towards the Worst-case Robustness of Large Language Models**

走向大型语言模型的最坏情况稳健性 cs.LG

**SubmitDate**: 2025-06-03    [abs](http://arxiv.org/abs/2501.19040v3) [paper-pdf](http://arxiv.org/pdf/2501.19040v3)

**Authors**: Huanran Chen, Yinpeng Dong, Zeming Wei, Hang Su, Jun Zhu

**Abstract**: Recent studies have revealed the vulnerability of large language models to adversarial attacks, where adversaries craft specific input sequences to induce harmful, violent, private, or incorrect outputs. In this work, we study their worst-case robustness, i.e., whether an adversarial example exists that leads to such undesirable outputs. We upper bound the worst-case robustness using stronger white-box attacks, indicating that most current deterministic defenses achieve nearly 0\% worst-case robustness. We propose a general tight lower bound for randomized smoothing using fractional knapsack solvers or 0-1 knapsack solvers, and using them to bound the worst-case robustness of all stochastic defenses. Based on these solvers, we provide theoretical lower bounds for several previous empirical defenses. For example, we certify the robustness of a specific case, smoothing using a uniform kernel, against \textit{any possible attack} with an average $\ell_0$ perturbation of 2.02 or an average suffix length of 6.41.

摘要: 最近的研究揭示了大型语言模型容易受到对抗攻击，对手会精心设计特定的输入序列来引发有害、暴力、私密或错误的输出。在这项工作中，我们研究了它们的最坏情况稳健性，即是否存在导致此类不良结果的对抗性例子。我们使用更强的白盒攻击来对最坏情况的稳健性进行上限，这表明当前大多数确定性防御实现了近0%的最坏情况的稳健性。我们提出了使用分数背包求解器或0-1背包求解器的随机平滑的一般紧下界，并使用它们来限制所有随机防御的最坏情况稳健性。基于这些求解器，我们为之前的几个经验防御提供了理论下限。例如，我们证明了特定情况的稳健性，使用统一核进行平滑，针对\texttit {任何可能的攻击}，平均$\ell_0 $扰动为2.02或平均后缀长度为6.41。



## **21. BitBypass: A New Direction in Jailbreaking Aligned Large Language Models with Bitstream Camouflage**

BitBypass：通过Bitstream伪装越狱对齐大型语言模型的新方向 cs.CR

24 pages, 24 figures, and 7 tables

**SubmitDate**: 2025-06-03    [abs](http://arxiv.org/abs/2506.02479v1) [paper-pdf](http://arxiv.org/pdf/2506.02479v1)

**Authors**: Kalyan Nakka, Nitesh Saxena

**Abstract**: The inherent risk of generating harmful and unsafe content by Large Language Models (LLMs), has highlighted the need for their safety alignment. Various techniques like supervised fine-tuning, reinforcement learning from human feedback, and red-teaming were developed for ensuring the safety alignment of LLMs. However, the robustness of these aligned LLMs is always challenged by adversarial attacks that exploit unexplored and underlying vulnerabilities of the safety alignment. In this paper, we develop a novel black-box jailbreak attack, called BitBypass, that leverages hyphen-separated bitstream camouflage for jailbreaking aligned LLMs. This represents a new direction in jailbreaking by exploiting fundamental information representation of data as continuous bits, rather than leveraging prompt engineering or adversarial manipulations. Our evaluation of five state-of-the-art LLMs, namely GPT-4o, Gemini 1.5, Claude 3.5, Llama 3.1, and Mixtral, in adversarial perspective, revealed the capabilities of BitBypass in bypassing their safety alignment and tricking them into generating harmful and unsafe content. Further, we observed that BitBypass outperforms several state-of-the-art jailbreak attacks in terms of stealthiness and attack success. Overall, these results highlights the effectiveness and efficiency of BitBypass in jailbreaking these state-of-the-art LLMs.

摘要: 大型语言模型（LLM）生成有害和不安全内容的固有风险凸显了对其安全性进行调整的必要性。开发了监督式微调、来自人类反馈的强化学习和红色团队等各种技术，以确保LLM的安全对齐。然而，这些对齐的LLM的稳健性总是受到利用安全对齐未探索的潜在漏洞的对抗攻击的挑战。在本文中，我们开发了一种新型的黑匣子越狱攻击，称为BitBypass，它利用连字符分离的比特流伪装来越狱对齐的LLM。这代表了越狱的一个新方向，它利用数据的基本信息表示为连续比特，而不是利用即时工程或对抗性操纵。我们从对抗的角度对五种最先进的LLM（即GPT-4 o、Gemini 1.5、Claude 3.5、Llama 3.1和Mixtral）进行了评估，揭示了BitBypass绕过其安全对齐并诱骗其生成有害和不安全内容的能力。此外，我们观察到BitBypass在隐蔽性和攻击成功率方面优于几种最先进的越狱攻击。总体而言，这些结果凸显了BitBypass在越狱这些最先进的LLM方面的有效性和效率。



## **22. Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language Models**

揭示一致大型语言模型内在的道德脆弱性 cs.CL

**SubmitDate**: 2025-06-03    [abs](http://arxiv.org/abs/2504.05050v4) [paper-pdf](http://arxiv.org/pdf/2504.05050v4)

**Authors**: Jiawei Lian, Jianhong Pan, Lefan Wang, Yi Wang, Shaohui Mei, Lap-Pui Chau

**Abstract**: Large language models (LLMs) are foundational explorations to artificial general intelligence, yet their alignment with human values via instruction tuning and preference learning achieves only superficial compliance. Here, we demonstrate that harmful knowledge embedded during pretraining persists as indelible "dark patterns" in LLMs' parametric memory, evading alignment safeguards and resurfacing under adversarial inducement at distributional shifts. In this study, we first theoretically analyze the intrinsic ethical vulnerability of aligned LLMs by proving that current alignment methods yield only local "safety regions" in the knowledge manifold. In contrast, pretrained knowledge remains globally connected to harmful concepts via high-likelihood adversarial trajectories. Building on this theoretical insight, we empirically validate our findings by employing semantic coherence inducement under distributional shifts--a method that systematically bypasses alignment constraints through optimized adversarial prompts. This combined theoretical and empirical approach achieves a 100% attack success rate across 19 out of 23 state-of-the-art aligned LLMs, including DeepSeek-R1 and LLaMA-3, revealing their universal vulnerabilities.

摘要: 大型语言模型（LLM）是人工通用智能的基础探索，但它们通过指令调整和偏好学习与人类价值观的一致只能实现表面的合规性。在这里，我们证明，预训练期间嵌入的有害知识在LLM参数记忆中作为不可磨灭的“黑暗模式”持续存在，逃避对齐保障措施，并在分布变化时的对抗诱导下重新浮出水面。在这项研究中，我们首先通过证明当前的对齐方法只产生知识集合中的局部“安全区域”来从理论上分析对齐LLM的内在道德脆弱性。相比之下，预先训练的知识仍然通过高可能性的对抗轨迹与有害概念保持全球联系。基于这一理论见解，我们通过在分布转移下采用语义一致诱导来从经验上验证我们的发现--一种通过优化的对抗提示系统性地绕过对齐约束的方法。这种理论和经验相结合的方法在23个最先进的对齐LLM中的19个（包括DeepSeek-R1和LLaMA-3）上实现了100%的攻击成功率，揭示了它们的普遍漏洞。



## **23. SEA: Low-Resource Safety Alignment for Multimodal Large Language Models via Synthetic Embeddings**

SEA：通过合成嵌入实现多模式大型语言模型的低资源安全性对齐 cs.CL

Accepted in ACL 2025 Main Track

**SubmitDate**: 2025-06-02    [abs](http://arxiv.org/abs/2502.12562v3) [paper-pdf](http://arxiv.org/pdf/2502.12562v3)

**Authors**: Weikai Lu, Hao Peng, Huiping Zhuang, Cen Chen, Ziqian Zeng

**Abstract**: Multimodal Large Language Models (MLLMs) have serious security vulnerabilities.While safety alignment using multimodal datasets consisting of text and data of additional modalities can effectively enhance MLLM's security, it is costly to construct these datasets. Existing low-resource security alignment methods, including textual alignment, have been found to struggle with the security risks posed by additional modalities. To address this, we propose Synthetic Embedding augmented safety Alignment (SEA), which optimizes embeddings of additional modality through gradient updates to expand textual datasets. This enables multimodal safety alignment training even when only textual data is available. Extensive experiments on image, video, and audio-based MLLMs demonstrate that SEA can synthesize a high-quality embedding on a single RTX3090 GPU within 24 seconds. SEA significantly improves the security of MLLMs when faced with threats from additional modalities. To assess the security risks introduced by video and audio, we also introduced a new benchmark called VA-SafetyBench. High attack success rates across multiple MLLMs validate its challenge. Our code and data will be available at https://github.com/ZeroNLP/SEA.

摘要: 多模式大型语言模型（MLLM）存在严重的安全漏洞。虽然使用由文本和其他模式数据组成的多模式数据集进行安全对齐可以有效增强MLLM的安全性，但构建这些数据集的成本很高。现有的低资源安全对齐方法（包括文本对齐）被发现难以应对额外模式带来的安全风险。为了解决这个问题，我们提出了合成嵌入增强安全对齐（SEA），它通过梯度更新来优化额外模式的嵌入以扩展文本数据集。即使只有文本数据可用，这也可以实现多模式安全对齐训练。基于图像、视频和音频的MLLM的广泛实验表明，SEA可以在24秒内在单个RTX 3090图形处理器上合成高质量嵌入。SEA在面临来自其他模式的威胁时显着提高了MLLM的安全性。为了评估视频和音频带来的安全风险，我们还引入了名为VA-SafetyBench的新基准。多个MLLM的高攻击成功率证实了其挑战。我们的代码和数据可在https://github.com/ZeroNLP/SEA上获取。



## **24. AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs**

顾问：LLM的快速自适应对抗预算 cs.CR

Accepted to ICML 2025. Code is available at  http://github.com/facebookresearch/advprompter

**SubmitDate**: 2025-06-02    [abs](http://arxiv.org/abs/2404.16873v2) [paper-pdf](http://arxiv.org/pdf/2404.16873v2)

**Authors**: Anselm Paulus, Arman Zharmagambetov, Chuan Guo, Brandon Amos, Yuandong Tian

**Abstract**: Large Language Models (LLMs) are vulnerable to jailbreaking attacks that lead to generation of inappropriate or harmful content. Manual red-teaming requires a time-consuming search for adversarial prompts, whereas automatic adversarial prompt generation often leads to semantically meaningless attacks that do not scale well. In this paper, we present a novel method that uses another LLM, called AdvPrompter, to generate human-readable adversarial prompts in seconds. AdvPrompter, which is trained using an alternating optimization algorithm, generates suffixes that veil the input instruction without changing its meaning, such that the TargetLLM is lured to give a harmful response. Experimental results on popular open source TargetLLMs show highly competitive results on the AdvBench and HarmBench datasets, that also transfer to closed-source black-box LLMs. We also show that training on adversarial suffixes generated by AdvPrompter is a promising strategy for improving the robustness of LLMs to jailbreaking attacks.

摘要: 大型语言模型（LLM）很容易受到越狱攻击，从而导致生成不适当或有害的内容。手动红组需要对对抗提示进行耗时的搜索，而自动对抗提示生成通常会导致语义上毫无意义的攻击，并且无法很好地扩展。在本文中，我们提出了一种新颖的方法，该方法使用另一种名为Advancer的LLM来在几秒钟内生成人类可读的对抗提示。Advancer使用交替优化算法进行训练，它会生成掩盖输入指令而不改变其含义的后缀，从而引诱Target LLM给出有害响应。流行的开源Target LLM上的实验结果显示，AdvBench和HarmBench数据集具有高度竞争力的结果，这些结果也转移到封闭源黑匣子LLM。我们还表明，对Advencer生成的敌对后缀进行训练是一种有希望的策略，可以提高LLM对越狱攻击的稳健性。



## **25. INVARLLM: LLM-assisted Physical Invariant Extraction for Cyber-Physical Systems Anomaly Detection**

INVAR LLM：LLM辅助的物理不变量提取，用于网络物理系统异常检测 cs.CR

**SubmitDate**: 2025-06-02    [abs](http://arxiv.org/abs/2411.10918v2) [paper-pdf](http://arxiv.org/pdf/2411.10918v2)

**Authors**: Danial Abshari, Peiran Shi, Chenglong Fu, Meera Sridhar, Xiaojiang Du

**Abstract**: Cyber-Physical Systems (CPS) are vulnerable to cyber-physical attacks that violate physical laws. While invariant-based anomaly detection is effective, existing methods are limited: data-driven approaches lack semantic context, and physics-based models require extensive manual work. We propose INVARLLM, a hybrid framework that uses large language models (LLMs) to extract semantic information from CPS documentation and generate physical invariants, then validates these against real system data using a PCMCI+-inspired K-means method. This approach combines LLM semantic understanding with empirical validation to ensure both interpretability and reliability. We evaluate INVARLLM on SWaT and WADI datasets, achieving 100% precision in anomaly detection with no false alarms, outperforming all existing methods. Our results demonstrate that integrating LLM-derived semantics with statistical validation provides a scalable and dependable solution for CPS security.

摘要: 网络物理系统（CPS）容易受到违反物理定律的网络物理攻击。虽然基于不变量的异常检测是有效的，但现有方法是有限的：数据驱动的方法缺乏语义上下文，而基于物理的模型需要大量的手工工作。我们提出了INVAR LLM，这是一个混合框架，它使用大型语言模型（LLM）从CPS文档中提取语义信息并生成物理不变量，然后使用受PCICI+启发的K-means方法针对真实系统数据进行验证。这种方法将LLM语义理解与经验验证相结合，以确保可解释性和可靠性。我们在SWaT和WADI数据集上评估INVAR LLM，在异常检测方面实现了100%的精确度，没有误报，优于所有现有方法。我们的结果表明，将LLM衍生的语义与统计验证集成为CPS安全性提供了可扩展且可靠的解决方案。



## **26. FaceCoT: A Benchmark Dataset for Face Anti-Spoofing with Chain-of-Thought Reasoning**

FaceCoT：一个基于思想链推理的人脸反欺骗基准数据集 cs.CV

**SubmitDate**: 2025-06-02    [abs](http://arxiv.org/abs/2506.01783v1) [paper-pdf](http://arxiv.org/pdf/2506.01783v1)

**Authors**: Honglu Zhang, Zhiqin Fang, Ningning Zhao, Saihui Hou, Long Ma, Renwang Pei, Zhaofeng He

**Abstract**: Face Anti-Spoofing (FAS) typically depends on a single visual modality when defending against presentation attacks such as print attacks, screen replays, and 3D masks, resulting in limited generalization across devices, environments, and attack types. Meanwhile, Multimodal Large Language Models (MLLMs) have recently achieved breakthroughs in image-text understanding and semantic reasoning, suggesting that integrating visual and linguistic co-inference into FAS can substantially improve both robustness and interpretability. However, the lack of a high-quality vision-language multimodal dataset has been a critical bottleneck. To address this, we introduce FaceCoT (Face Chain-of-Thought), the first large-scale Visual Question Answering (VQA) dataset tailored for FAS. FaceCoT covers 14 spoofing attack types and enriches model learning with high-quality CoT VQA annotations. Meanwhile, we develop a caption model refined via reinforcement learning to expand the dataset and enhance annotation quality. Furthermore, we introduce a CoT-Enhanced Progressive Learning (CEPL) strategy to better leverage the CoT data and boost model performance on FAS tasks. Extensive experiments demonstrate that models trained with FaceCoT and CEPL outperform state-of-the-art methods on multiple benchmark datasets.

摘要: 面部反欺骗（FAA）在防御打印攻击、屏幕回放和3D面具等演示攻击时通常依赖于单一视觉模式，导致跨设备、环境和攻击类型的通用性有限。与此同时，多模式大型语言模型（MLLM）最近在图像-文本理解和语义推理方面取得了突破，这表明将视觉和语言协同推理集成到FAA中可以大幅提高稳健性和可解释性。然而，缺乏高质量的视觉语言多模式数据集一直是一个关键瓶颈。为了解决这个问题，我们引入了FaceCoT（面部思维链），这是第一个为FAA量身定制的大规模视觉问题解答（VQA）数据集。FaceCoT涵盖14种欺骗攻击类型，并通过高质量的CoT VQA注释丰富了模型学习。与此同时，我们开发了一个通过强化学习改进的字幕模型，以扩展数据集并提高注释质量。此外，我们还引入了CoT增强渐进式学习（CEPL）策略，以更好地利用CoT数据并提高FAA任务的模型性能。大量实验表明，使用FaceCoT和CEPL训练的模型在多个基准数据集上的表现优于最先进的方法。



## **27. ReGA: Representation-Guided Abstraction for Model-based Safeguarding of LLMs**

ReGA：基于模型的LLM保护的表示引导抽象 cs.CR

**SubmitDate**: 2025-06-02    [abs](http://arxiv.org/abs/2506.01770v1) [paper-pdf](http://arxiv.org/pdf/2506.01770v1)

**Authors**: Zeming Wei, Chengcan Wu, Meng Sun

**Abstract**: Large Language Models (LLMs) have achieved significant success in various tasks, yet concerns about their safety and security have emerged. In particular, they pose risks in generating harmful content and vulnerability to jailbreaking attacks. To analyze and monitor machine learning models, model-based analysis has demonstrated notable potential in stateful deep neural networks, yet suffers from scalability issues when extending to LLMs due to their vast feature spaces. In this paper, we propose ReGA, a model-based analysis framework with representation-guided abstraction, to safeguard LLMs against harmful prompts and generations. By leveraging safety-critical representations, which are low-dimensional directions emerging in hidden states that indicate safety-related concepts, ReGA effectively addresses the scalability issue when constructing the abstract model for safety modeling. Our comprehensive evaluation shows that ReGA performs sufficiently well in distinguishing between safe and harmful inputs, achieving an AUROC of 0.975 at the prompt level and 0.985 at the conversation level. Additionally, ReGA exhibits robustness to real-world attacks and generalization across different safety perspectives, outperforming existing safeguard paradigms in terms of interpretability and scalability. Overall, ReGA serves as an efficient and scalable solution to enhance LLM safety by integrating representation engineering with model-based abstraction, paving the way for new paradigms to utilize software insights for AI safety. Our code is available at https://github.com/weizeming/ReGA.

摘要: 大型语言模型（LLM）在各种任务中取得了巨大成功，但对其安全性的担忧也出现了。特别是，它们在生成有害内容和易受越狱攻击方面构成风险。为了分析和监控机器学习模型，基于模型的分析在有状态深度神经网络中表现出了显着的潜力，但由于其庞大的特征空间，在扩展到LLM时会遇到可扩展性问题。在本文中，我们提出了ReGA，这是一种基于模型的分析框架，具有表示引导的抽象，以保护LLM免受有害提示和世代的影响。通过利用安全关键表示（即在指示安全相关概念的隐藏状态中出现的低维方向），ReGA在构建安全建模的抽象模型时有效地解决了可扩展性问题。我们的综合评估表明，ReGA在区分安全和有害输入方面表现良好，提示级别的AUROC为0.975，对话级别的AUROC为0.985。此外，ReGA对现实世界的攻击和不同安全角度的概括表现出稳健性，在可解释性和可扩展性方面优于现有的保障范式。总的来说，ReGA是一种高效且可扩展的解决方案，通过将表示工程与基于模型的抽象相结合来增强LLM安全性，为利用软件洞察力实现AI安全性的新范式铺平了道路。我们的代码可在https://github.com/weizeming/ReGA上获取。



## **28. MLA-Trust: Benchmarking Trustworthiness of Multimodal LLM Agents in GUI Environments**

MLA-Trust：对图形用户界面环境中多模式LLM代理的可信度进行基准测试 cs.AI

**SubmitDate**: 2025-06-02    [abs](http://arxiv.org/abs/2506.01616v1) [paper-pdf](http://arxiv.org/pdf/2506.01616v1)

**Authors**: Xiao Yang, Jiawei Chen, Jun Luo, Zhengwei Fang, Yinpeng Dong, Hang Su, Jun Zhu

**Abstract**: The emergence of multimodal LLM-based agents (MLAs) has transformed interaction paradigms by seamlessly integrating vision, language, action and dynamic environments, enabling unprecedented autonomous capabilities across GUI applications ranging from web automation to mobile systems. However, MLAs introduce critical trustworthiness challenges that extend far beyond traditional language models' limitations, as they can directly modify digital states and trigger irreversible real-world consequences. Existing benchmarks inadequately tackle these unique challenges posed by MLAs' actionable outputs, long-horizon uncertainty and multimodal attack vectors. In this paper, we introduce MLA-Trust, the first comprehensive and unified framework that evaluates the MLA trustworthiness across four principled dimensions: truthfulness, controllability, safety and privacy. We utilize websites and mobile applications as realistic testbeds, designing 34 high-risk interactive tasks and curating rich evaluation datasets. Large-scale experiments involving 13 state-of-the-art agents reveal previously unexplored trustworthiness vulnerabilities unique to multimodal interactive scenarios. For instance, proprietary and open-source GUI-interacting MLAs pose more severe trustworthiness risks than static MLLMs, particularly in high-stakes domains; the transition from static MLLMs into interactive MLAs considerably compromises trustworthiness, enabling harmful content generation in multi-step interactions that standalone MLLMs would typically prevent; multi-step execution, while enhancing the adaptability of MLAs, involves latent nonlinear risk accumulation across successive interactions, circumventing existing safeguards and resulting in unpredictable derived risks. Moreover, we present an extensible toolbox to facilitate continuous evaluation of MLA trustworthiness across diverse interactive environments.

摘要: 基于LLM的多模式代理（MLA）的出现通过无缝集成视觉、语言、动作和动态环境来改变了交互范式，从而在从Web自动化到移动系统的图形用户界面应用程序中实现了前所未有的自主能力。然而，MLA带来了远远超出传统语言模型限制的关键可信度挑战，因为它们可以直接修改数字状态并引发不可逆转的现实世界后果。现有的基准不足以应对MLA的可操作输出、长期不确定性和多模式攻击载体带来的这些独特挑战。在本文中，我们介绍了MLA-Trust，这是第一个全面、统一的框架，从四个原则维度评估MLA可信度：真实性、可控性、安全性和隐私性。我们利用网站和移动应用程序作为现实的测试平台，设计34个高风险交互任务并策划丰富的评估数据集。涉及13个最先进代理的大规模实验揭示了以前未探索的多模式交互场景特有的可信度漏洞。例如，专有和开源的图形界面交互MLA比静态MLLM构成更严重的可信度风险，特别是在高风险领域;从静态MLLM到交互式MLA的过渡大大损害了可信度，从而导致在多步交互中产生有害内容，而独立MLLM通常会阻止这种交互;多步骤执行在增强MLA的适应性的同时，涉及连续互动中潜在的非线性风险累积，规避现有的保障措施并导致不可预测的衍生风险。此外，我们还提供了一个可扩展的工具箱，以促进在不同交互环境中持续评估MLA可信度。



## **29. Safety at Scale: A Comprehensive Survey of Large Model Safety**

大规模安全性：大型车型安全性全面调查 cs.CR

47 pages, 3 figures, 11 tables; GitHub:  https://github.com/xingjunm/Awesome-Large-Model-Safety

**SubmitDate**: 2025-06-02    [abs](http://arxiv.org/abs/2502.05206v4) [paper-pdf](http://arxiv.org/pdf/2502.05206v4)

**Authors**: Xingjun Ma, Yifeng Gao, Yixu Wang, Ruofan Wang, Xin Wang, Ye Sun, Yifan Ding, Hengyuan Xu, Yunhao Chen, Yunhan Zhao, Hanxun Huang, Yige Li, Jiaming Zhang, Xiang Zheng, Yang Bai, Zuxuan Wu, Xipeng Qiu, Jingfeng Zhang, Yiming Li, Xudong Han, Haonan Li, Jun Sun, Cong Wang, Jindong Gu, Baoyuan Wu, Siheng Chen, Tianwei Zhang, Yang Liu, Mingming Gong, Tongliang Liu, Shirui Pan, Cihang Xie, Tianyu Pang, Yinpeng Dong, Ruoxi Jia, Yang Zhang, Shiqing Ma, Xiangyu Zhang, Neil Gong, Chaowei Xiao, Sarah Erfani, Tim Baldwin, Bo Li, Masashi Sugiyama, Dacheng Tao, James Bailey, Yu-Gang Jiang

**Abstract**: The rapid advancement of large models, driven by their exceptional abilities in learning and generalization through large-scale pre-training, has reshaped the landscape of Artificial Intelligence (AI). These models are now foundational to a wide range of applications, including conversational AI, recommendation systems, autonomous driving, content generation, medical diagnostics, and scientific discovery. However, their widespread deployment also exposes them to significant safety risks, raising concerns about robustness, reliability, and ethical implications. This survey provides a systematic review of current safety research on large models, covering Vision Foundation Models (VFMs), Large Language Models (LLMs), Vision-Language Pre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models (DMs), and large-model-based Agents. Our contributions are summarized as follows: (1) We present a comprehensive taxonomy of safety threats to these models, including adversarial attacks, data poisoning, backdoor attacks, jailbreak and prompt injection attacks, energy-latency attacks, data and model extraction attacks, and emerging agent-specific threats. (2) We review defense strategies proposed for each type of attacks if available and summarize the commonly used datasets and benchmarks for safety research. (3) Building on this, we identify and discuss the open challenges in large model safety, emphasizing the need for comprehensive safety evaluations, scalable and effective defense mechanisms, and sustainable data practices. More importantly, we highlight the necessity of collective efforts from the research community and international collaboration. Our work can serve as a useful reference for researchers and practitioners, fostering the ongoing development of comprehensive defense systems and platforms to safeguard AI models.

摘要: 大型模型在通过大规模预训练进行学习和概括的卓越能力的推动下，迅速发展重塑了人工智能（AI）的格局。这些模型现在是广泛应用的基础，包括对话人工智能、推荐系统、自动驾驶、内容生成、医疗诊断和科学发现。然而，它们的广泛部署也使它们面临巨大的安全风险，引发了对稳健性、可靠性和道德影响的担忧。这项调查对当前大型模型的安全性研究进行了系统性回顾，涵盖视觉基础模型（VFM）、大型语言模型（LLM）、视觉语言预训练（VLP）模型、视觉语言模型（VLM）、扩散模型（DM）和基于大模型的代理。我们的贡献总结如下：（1）我们对这些模型的安全威胁提出了全面的分类，包括对抗性攻击、数据中毒、后门攻击、越狱和提示注入攻击、能量延迟攻击、数据和模型提取攻击以及新兴的特定于代理的威胁。(2)我们审查为每种类型的攻击提出的防御策略（如果有的话），并总结常用的数据集和安全研究基准。(3)在此基础上，我们确定并讨论了大型模型安全方面的开放挑战，强调全面的安全评估、可扩展且有效的防御机制以及可持续的数据实践的必要性。更重要的是，我们强调研究界集体努力和国际合作的必要性。我们的工作可以为研究人员和从业者提供有用的参考，促进全面防御系统和平台的持续开发，以保护人工智能模型。



## **30. ETDI: Mitigating Tool Squatting and Rug Pull Attacks in Model Context Protocol (MCP) by using OAuth-Enhanced Tool Definitions and Policy-Based Access Control**

ETDI：通过使用OAuth-Enhanced工具定义和基于策略的访问控制缓解模型上下文协议（HCP）中的工具蹲下和拉地毯攻击 cs.CR

11 Pages, 10 figures, Github links in introduction

**SubmitDate**: 2025-06-02    [abs](http://arxiv.org/abs/2506.01333v1) [paper-pdf](http://arxiv.org/pdf/2506.01333v1)

**Authors**: Manish Bhatt, Vineeth Sai Narajala, Idan Habler

**Abstract**: The Model Context Protocol (MCP) plays a crucial role in extending the capabilities of Large Language Models (LLMs) by enabling integration with external tools and data sources. However, the standard MCP specification presents significant security vulnerabilities, notably Tool Poisoning and Rug Pull attacks. This paper introduces the Enhanced Tool Definition Interface (ETDI), a security extension designed to fortify MCP. ETDI incorporates cryptographic identity verification, immutable versioned tool definitions, and explicit permission management, often leveraging OAuth 2.0. We further propose extending MCP with fine-grained, policy-based access control, where tool capabilities are dynamically evaluated against explicit policies using a dedicated policy engine, considering runtime context beyond static OAuth scopes. This layered approach aims to establish a more secure, trustworthy, and controllable ecosystem for AI applications interacting with LLMs and external tools.

摘要: 模型上下文协议（HCP）通过与外部工具和数据源集成来扩展大型语言模型（LLM）的功能发挥着至关重要的作用。然而，标准的LCP规范存在严重的安全漏洞，特别是工具中毒和拉地毯攻击。本文介绍了增强型工具定义接口（ETDI），这是一个旨在加强LCP的安全扩展。ETDI结合了加密身份验证、不可变版本工具定义和显式权限管理，通常利用OAuth 2.0。我们进一步建议通过细粒度、基于策略的访问控制来扩展LCP，其中使用专用策略引擎根据显式策略动态评估工具能力，同时考虑静态OAuth范围之外的运行时上下文。这种分层方法旨在为与LLM和外部工具交互的人工智能应用程序建立一个更安全、值得信赖和可控的生态系统。



## **31. Align is not Enough: Multimodal Universal Jailbreak Attack against Multimodal Large Language Models**

对齐还不够：针对多模式大型语言模型的多模式通用越狱攻击 cs.CR

**SubmitDate**: 2025-06-02    [abs](http://arxiv.org/abs/2506.01307v1) [paper-pdf](http://arxiv.org/pdf/2506.01307v1)

**Authors**: Youze Wang, Wenbo Hu, Yinpeng Dong, Jing Liu, Hanwang Zhang, Richang Hong

**Abstract**: Large Language Models (LLMs) have evolved into Multimodal Large Language Models (MLLMs), significantly enhancing their capabilities by integrating visual information and other types, thus aligning more closely with the nature of human intelligence, which processes a variety of data forms beyond just text. Despite advancements, the undesirable generation of these models remains a critical concern, particularly due to vulnerabilities exposed by text-based jailbreak attacks, which have represented a significant threat by challenging existing safety protocols. Motivated by the unique security risks posed by the integration of new and old modalities for MLLMs, we propose a unified multimodal universal jailbreak attack framework that leverages iterative image-text interactions and transfer-based strategy to generate a universal adversarial suffix and image. Our work not only highlights the interaction of image-text modalities can be used as a critical vulnerability but also validates that multimodal universal jailbreak attacks can bring higher-quality undesirable generations across different MLLMs. We evaluate the undesirable context generation of MLLMs like LLaVA, Yi-VL, MiniGPT4, MiniGPT-v2, and InstructBLIP, and reveal significant multimodal safety alignment issues, highlighting the inadequacy of current safety mechanisms against sophisticated multimodal attacks. This study underscores the urgent need for robust safety measures in MLLMs, advocating for a comprehensive review and enhancement of security protocols to mitigate potential risks associated with multimodal capabilities.

摘要: 大型语言模型（LLM）已演变为多模式大型语言模型（MLLM），通过集成视觉信息和其他类型来显着增强其能力，从而更紧密地与人类智能的本质保持一致，人类智能处理各种数据形式不仅仅是文本。尽管取得了进步，但这些模型的不良生成仍然是一个严重问题，特别是由于基于文本的越狱攻击暴露了漏洞，这些攻击通过挑战现有的安全协议构成了重大威胁。受MLLM新旧模式集成所带来的独特安全风险的激励，我们提出了一个统一的多模式通用越狱攻击框架，该框架利用迭代的图像-文本交互和基于传输的策略来生成通用的对抗性后缀和图像。我们的工作不仅强调了图像-文本模式的交互可以用作关键漏洞，而且还验证了多模式通用越狱攻击可以在不同的MLLM中带来更高质量的不良世代。我们评估了LLaVA、Yi-BL、MiniGPT 4、MiniGPT-v2和INSTBLIP等MLLM的不良上下文生成，并揭示了重大的多模式安全对齐问题，凸显了当前安全机制针对复杂多模式攻击的不足。这项研究强调了MLLM迫切需要采取强有力的安全措施，倡导全面审查和增强安全协议，以减轻与多模式能力相关的潜在风险。



## **32. Data Poisoning for In-context Learning**

上下文学习的数据中毒 cs.CR

NAACL 2025

**SubmitDate**: 2025-06-02    [abs](http://arxiv.org/abs/2402.02160v3) [paper-pdf](http://arxiv.org/pdf/2402.02160v3)

**Authors**: Pengfei He, Han Xu, Yue Xing, Hui Liu, Makoto Yamada, Jiliang Tang

**Abstract**: In the domain of large language models (LLMs), in-context learning (ICL) has been recognized for its innovative ability to adapt to new tasks, relying on examples rather than retraining or fine-tuning. This paper delves into the critical issue of ICL's susceptibility to data poisoning attacks, an area not yet fully explored. We wonder whether ICL is vulnerable, with adversaries capable of manipulating example data to degrade model performance. To address this, we introduce ICLPoison, a specialized attacking framework conceived to exploit the learning mechanisms of ICL. Our approach uniquely employs discrete text perturbations to strategically influence the hidden states of LLMs during the ICL process. We outline three representative strategies to implement attacks under our framework, each rigorously evaluated across a variety of models and tasks. Our comprehensive tests, including trials on the sophisticated GPT-4 model, demonstrate that ICL's performance is significantly compromised under our framework. These revelations indicate an urgent need for enhanced defense mechanisms to safeguard the integrity and reliability of LLMs in applications relying on in-context learning.

摘要: 在大型语言模型（LLM）领域，上下文学习（ICL）因其适应新任务的创新能力而受到认可，依赖于示例而不是重新训练或微调。本文探讨了ICL对数据中毒攻击的敏感性这一关键问题，该领域尚未充分探讨。我们想知道ICL是否容易受到攻击，对手能够操纵示例数据来降低模型性能。为了解决这个问题，我们引入了ICLPoison，这是一个专门的攻击框架，旨在利用ICL的学习机制。我们的方法独特地使用离散文本扰动来战略性地影响ICL过程中LLM的隐藏状态。我们概述了在我们的框架下实施攻击的三种代表性策略，每种策略都经过了各种模型和任务的严格评估。我们的全面测试，包括对复杂GPT-4模型的试验，表明ICL的性能在我们的框架下受到了显着的影响。这些揭露表明迫切需要增强的防御机制，以保障依赖上下文学习的应用程序中LLM的完整性和可靠性。



## **33. Red-Teaming LLM Multi-Agent Systems via Communication Attacks**

通过通信攻击的Red-Teaming LLM多代理系统 cs.CR

ACL 2025

**SubmitDate**: 2025-06-02    [abs](http://arxiv.org/abs/2502.14847v2) [paper-pdf](http://arxiv.org/pdf/2502.14847v2)

**Authors**: Pengfei He, Yupin Lin, Shen Dong, Han Xu, Yue Xing, Hui Liu

**Abstract**: Large Language Model-based Multi-Agent Systems (LLM-MAS) have revolutionized complex problem-solving capability by enabling sophisticated agent collaboration through message-based communications. While the communication framework is crucial for agent coordination, it also introduces a critical yet unexplored security vulnerability. In this work, we introduce Agent-in-the-Middle (AiTM), a novel attack that exploits the fundamental communication mechanisms in LLM-MAS by intercepting and manipulating inter-agent messages. Unlike existing attacks that compromise individual agents, AiTM demonstrates how an adversary can compromise entire multi-agent systems by only manipulating the messages passing between agents. To enable the attack under the challenges of limited control and role-restricted communication format, we develop an LLM-powered adversarial agent with a reflection mechanism that generates contextually-aware malicious instructions. Our comprehensive evaluation across various frameworks, communication structures, and real-world applications demonstrates that LLM-MAS is vulnerable to communication-based attacks, highlighting the need for robust security measures in multi-agent systems.

摘要: 基于大型语言模型的多代理系统（LLM-MAS）通过基于消息的通信实现复杂的代理协作，彻底改变了复杂问题解决能力。虽然通信框架对于代理协调至关重要，但它也引入了一个关键但尚未探索的安全漏洞。在这项工作中，我们引入了中间代理（AiTM），这是一种新型攻击，通过拦截和操纵代理间消息来利用LLM-MAS中的基本通信机制。与现有的危及单个代理的攻击不同，AiTM展示了对手如何仅通过操纵代理之间传递的消息来危及整个多代理系统。为了在有限控制和角色限制通信格式的挑战下实现攻击，我们开发了一个LLM驱动的对抗代理，该代理具有反射机制，可以生成上下文感知的恶意指令。我们对各种框架、通信结构和现实世界应用程序的全面评估表明，LLM-MAS容易受到基于通信的攻击，这凸显了多代理系统中对强大安全措施的需求。



## **34. Comprehensive Vulnerability Analysis is Necessary for Trustworthy LLM-MAS**

可信LLM-MAS的脆弱性分析 cs.CR

**SubmitDate**: 2025-06-02    [abs](http://arxiv.org/abs/2506.01245v1) [paper-pdf](http://arxiv.org/pdf/2506.01245v1)

**Authors**: Pengfei He, Yue Xing, Shen Dong, Juanhui Li, Zhenwei Dai, Xianfeng Tang, Hui Liu, Han Xu, Zhen Xiang, Charu C. Aggarwal, Hui Liu

**Abstract**: This paper argues that a comprehensive vulnerability analysis is essential for building trustworthy Large Language Model-based Multi-Agent Systems (LLM-MAS). These systems, which consist of multiple LLM-powered agents working collaboratively, are increasingly deployed in high-stakes applications but face novel security threats due to their complex structures. While single-agent vulnerabilities are well-studied, LLM-MAS introduces unique attack surfaces through inter-agent communication, trust relationships, and tool integration that remain significantly underexplored. We present a systematic framework for vulnerability analysis of LLM-MAS that unifies diverse research. For each type of vulnerability, we define formal threat models grounded in practical attacker capabilities and illustrate them using real-world LLM-MAS applications. This formulation enables rigorous quantification of vulnerability across different architectures and provides a foundation for designing meaningful evaluation benchmarks. Our analysis reveals that LLM-MAS faces elevated risk due to compositional effects -- vulnerabilities in individual components can cascade through agent communication, creating threat models not present in single-agent systems. We conclude by identifying critical open challenges: (1) developing benchmarks specifically tailored to LLM-MAS vulnerability assessment, (2) considering new potential attacks specific to multi-agent architectures, and (3) implementing trust management systems that can enforce security in LLM-MAS. This research provides essential groundwork for future efforts to enhance LLM-MAS trustworthiness as these systems continue their expansion into critical applications.

摘要: 本文认为，全面的漏洞分析对于构建值得信赖的基于大型语言模型的多代理系统（LLM-MAS）至关重要。这些系统由多个LLM驱动的代理协同工作组成，越来越多地部署在高风险应用中，但由于其复杂的结构而面临着新型的安全威胁。虽然单代理漏洞得到了充分研究，但LLM-MAS通过代理间通信、信任关系和工具集成引入了独特的攻击表面，但这些攻击表面的研究仍然严重不足。我们提出了一个统一不同研究的LLM-MAS脆弱性分析的系统框架。对于每种类型的漏洞，我们根据实际攻击者能力定义正式威胁模型，并使用现实世界的LLM-MAS应用程序来说明它们。该公式能够严格量化不同架构的脆弱性，并为设计有意义的评估基准提供基础。我们的分析表明，由于组成效应，LLM-MAS面临着更高的风险--单个组件中的漏洞可以通过代理通信级联，从而创建单代理系统中不存在的威胁模型。我们通过识别关键的开放挑战来总结：（1）开发专门针对LLM-MAS漏洞评估定制的基准，（2）考虑针对多代理架构的新潜在攻击，以及（3）实施可以在LLM-MAS中强制执行安全性的信任管理系统。随着这些系统继续扩展到关键应用，这项研究为未来增强LLM-MAS可信度的努力提供了重要的基础。



## **35. EVALOOP: Assessing LLM Robustness in Programming from a Self-consistency Perspective**

EVALOOP：从自我一致性的角度评估LLM编程稳健性 cs.SE

19 pages, 11 figures

**SubmitDate**: 2025-06-01    [abs](http://arxiv.org/abs/2505.12185v2) [paper-pdf](http://arxiv.org/pdf/2505.12185v2)

**Authors**: Sen Fang, Weiyuan Ding, Bowen Xu

**Abstract**: Assessing the programming capabilities of Large Language Models (LLMs) is crucial for their effective use in software engineering. Current evaluations, however, predominantly measure the accuracy of generated code on static benchmarks, neglecting the critical aspect of model robustness during programming tasks. While adversarial attacks offer insights on model robustness, their effectiveness is limited and evaluation could be constrained. Current adversarial attack methods for robustness evaluation yield inconsistent results, struggling to provide a unified evaluation across different LLMs. We introduce EVALOOP, a novel assessment framework that evaluate the robustness from a self-consistency perspective, i.e., leveraging the natural duality inherent in popular software engineering tasks, e.g., code generation and code summarization. EVALOOP initiates a self-contained feedback loop: an LLM generates output (e.g., code) from an input (e.g., natural language specification), and then use the generated output as the input to produce a new output (e.g., summarizes that code into a new specification). EVALOOP repeats the process to assess the effectiveness of EVALOOP in each loop. This cyclical strategy intrinsically evaluates robustness without rely on any external attack setups, providing a unified metric to evaluate LLMs' robustness in programming. We evaluate 16 prominent LLMs (e.g., GPT-4.1, O4-mini) on EVALOOP and found that EVALOOP typically induces a 5.01%-19.31% absolute drop in pass@1 performance within ten loops. Intriguingly, robustness does not always align with initial performance (i.e., one-time query); for instance, GPT-3.5-Turbo, despite superior initial code generation compared to DeepSeek-V2, demonstrated lower robustness over repeated evaluation loop.

摘要: 评估大型语言模型（LLM）的编程能力对于它们在软件工程中的有效使用至关重要。然而，当前的评估主要衡量静态基准上生成的代码的准确性，忽视了编程任务期间模型稳健性的关键方面。虽然对抗性攻击提供了有关模型稳健性的见解，但它们的有效性有限，并且评估可能会受到限制。当前用于稳健性评估的对抗攻击方法会产生不一致的结果，难以在不同的LLM之间提供统一的评估。我们引入EVALOOP，这是一种新型评估框架，从自一致性的角度评估稳健性，即利用流行软件工程任务中固有的自然二重性，例如，代码生成和代码摘要。EVALOOP启动独立反馈循环：LLM生成输出（例如，代码）来自输入（例如，自然语言规范），然后使用生成的输出作为输入来产生新的输出（例如，将该代码总结为新规范）。EVALOOP重复该过程以评估每个循环中EVALOOP的有效性。这种循环策略本质上评估稳健性，而不依赖任何外部攻击设置，提供了一个统一的指标来评估LLM在编程中的稳健性。我们评估了16个著名的LLM（例如，GPT-4.1，O 4-mini）在EVALOOP上发现EVALOOP通常会在十个循环内导致pass@1性能绝对下降5.01%-19.31%。有趣的是，稳健性并不总是与初始性能一致（即，一次性查询）;例如，GPT-3.5-Turbo尽管初始代码生成优于DeepSeek-V2，但在重复评估循环中表现出较低的鲁棒性。



## **36. Effective faking of verbal deception detection with target-aligned adversarial attacks**

通过目标对准的对抗攻击有效伪造言语欺骗检测 cs.CL

Accepted to Legal and Criminological Psychology (author version)

**SubmitDate**: 2025-06-01    [abs](http://arxiv.org/abs/2501.05962v2) [paper-pdf](http://arxiv.org/pdf/2501.05962v2)

**Authors**: Bennett Kleinberg, Riccardo Loconte, Bruno Verschuere

**Abstract**: Background: Deception detection through analysing language is a promising avenue using both human judgments and automated machine learning judgments. For both forms of credibility assessment, automated adversarial attacks that rewrite deceptive statements to appear truthful pose a serious threat. Methods: We used a dataset of 243 truthful and 262 fabricated autobiographical stories in a deception detection task for humans and machine learning models. A large language model was tasked to rewrite deceptive statements so that they appear truthful. In Study 1, humans who made a deception judgment or used the detailedness heuristic and two machine learning models (a fine-tuned language model and a simple n-gram model) judged original or adversarial modifications of deceptive statements. In Study 2, we manipulated the target alignment of the modifications, i.e. tailoring the attack to whether the statements would be assessed by humans or computer models. Results: When adversarial modifications were aligned with their target, human (d=-0.07 and d=-0.04) and machine judgments (51% accuracy) dropped to the chance level. When the attack was not aligned with the target, both human heuristics judgments (d=0.30 and d=0.36) and machine learning predictions (63-78%) were significantly better than chance. Conclusions: Easily accessible language models can effectively help anyone fake deception detection efforts both by humans and machine learning models. Robustness against adversarial modifications for humans and machines depends on that target alignment. We close with suggestions on advancing deception research with adversarial attack designs and techniques.

摘要: 背景：通过分析语言来检测欺骗是一种使用人类判断和自动机器学习判断的有前途的途径。对于这两种形式的可信度评估来说，重写欺骗性陈述使其看起来真实的自动对抗攻击构成了严重威胁。方法：我们使用了包含243个真实故事和262个捏造自传故事的数据集，用于人类和机器学习模型的欺骗检测任务。一个大型语言模型的任务是重写欺骗性陈述，使其看起来真实。在研究1中，做出欺骗判断或使用细节启发式和两个机器学习模型（微调的语言模型和简单的n元语法模型）的人类判断欺骗性陈述的原始或对抗性修改。在研究2中，我们操纵了修改的目标对齐，即根据陈述是由人类还是计算机模型评估来定制攻击。结果：当对抗性修改与其目标对齐时，人类（d=-0.07和d=-0.04）和机器判断（51%的准确性）下降到机会水平。当攻击与目标不一致时，人类启发式判断（d=0.30和d=0.36）和机器学习预测（63-78%）都显着优于偶然性。结论：易于访问的语言模型可以有效地帮助任何人伪造人类和机器学习模型的欺骗检测工作。人类和机器对抗对抗修改的稳健性取决于目标对齐。最后，我们提出了关于通过对抗性攻击设计和技术推进欺骗研究的建议。



## **37. Jailbreak-AudioBench: In-Depth Evaluation and Analysis of Jailbreak Threats for Large Audio Language Models**

越狱音频长凳：深入评估和分析大型音频语言模型的越狱威胁 cs.SD

**SubmitDate**: 2025-06-01    [abs](http://arxiv.org/abs/2501.13772v3) [paper-pdf](http://arxiv.org/pdf/2501.13772v3)

**Authors**: Hao Cheng, Erjia Xiao, Jing Shao, Yichi Wang, Le Yang, Chao Shen, Philip Torr, Jindong Gu, Renjing Xu

**Abstract**: Large Language Models (LLMs) demonstrate impressive zero-shot performance across a wide range of natural language processing tasks. Integrating various modality encoders further expands their capabilities, giving rise to Multimodal Large Language Models (MLLMs) that process not only text but also visual and auditory modality inputs. However, these advanced capabilities may also pose significant security risks, as models can be exploited to generate harmful or inappropriate content through jailbreak attack. While prior work has extensively explored how manipulating textual or visual modality inputs can circumvent safeguards in LLMs and MLLMs, the vulnerability of audio-specific Jailbreak on Large Audio-Language Models (LALMs) remains largely underexplored. To address this gap, we introduce \textbf{Jailbreak-AudioBench}, which consists of the Toolbox, curated Dataset, and comprehensive Benchmark. The Toolbox supports not only text-to-audio conversion but also various editing techniques for injecting audio hidden semantics. The curated Dataset provides diverse explicit and implicit jailbreak audio examples in both original and edited forms. Utilizing this dataset, we evaluate multiple state-of-the-art LALMs and establish the most comprehensive Jailbreak benchmark to date for audio modality. Finally, Jailbreak-AudioBench establishes a foundation for advancing future research on LALMs safety alignment by enabling the in-depth exposure of more powerful jailbreak threats, such as query-based audio editing, and by facilitating the development of effective defense mechanisms.

摘要: 大型语言模型（LLM）在广泛的自然语言处理任务中表现出令人印象深刻的零冲击性能。集成各种模式编码器进一步扩展了它们的功能，从而产生了多模式大型语言模型（MLLM），不仅处理文本，还处理视觉和听觉模式输入。然而，这些高级功能也可能带来重大的安全风险，因为模型可能会被利用来通过越狱攻击生成有害或不适当的内容。虽然之前的工作已经广泛探索了操纵文本或视觉模式输入如何规避LLM和MLLM中的保护措施，但大型音频语言模型（LALM）上的音频特定越狱的漏洞在很大程度上仍然没有得到充分的研究。为了弥补这一差距，我们引入了\textBF{Jailbreak-AudioBench}，它由收件箱、精心策划的数据集和全面的基准组成。收件箱不仅支持文本到音频的转换，还支持各种用于注入音频隐藏语义的编辑技术。精心策划的数据集以原始和编辑的形式提供了多样化的显式和隐式越狱音频示例。利用该数据集，我们评估了多种最先进的LALM，并为音频模式建立了迄今为止最全面的越狱基准。最后，Jailbreak-AudioBench通过深入暴露更强大的越狱威胁（如基于查询的音频编辑），并促进有效防御机制的开发，为推进LALM安全对齐的未来研究奠定了基础。



## **38. Fighting Fire with Fire (F3): A Training-free and Efficient Visual Adversarial Example Purification Method in LVLMs**

以毒攻毒（F3）：LVLM中一种无需培训且高效的视觉对抗示例净化方法 cs.CV

**SubmitDate**: 2025-06-01    [abs](http://arxiv.org/abs/2506.01064v1) [paper-pdf](http://arxiv.org/pdf/2506.01064v1)

**Authors**: Yudong Zhang, Ruobing Xie, Yiqing Huang, Jiansheng Chen, Xingwu Sun, Zhanhui Kang, Di Wang, Yu Wang

**Abstract**: Recent advances in large vision-language models (LVLMs) have showcased their remarkable capabilities across a wide range of multimodal vision-language tasks. However, these models remain vulnerable to visual adversarial attacks, which can substantially compromise their performance. Despite their potential impact, the development of effective methods for purifying such adversarial examples has received relatively limited attention. In this paper, we introduce F3, a novel adversarial purification framework that employs a counterintuitive "fighting fire with fire" strategy: intentionally introducing simple perturbations to adversarial examples to mitigate their harmful effects. Specifically, F3 leverages cross-modal attentions derived from randomly perturbed adversary examples as reference targets. By injecting noise into these adversarial examples, F3 effectively refines their attention, resulting in cleaner and more reliable model outputs. Remarkably, this seemingly paradoxical approach of employing noise to counteract adversarial attacks yields impressive purification results. Furthermore, F3 offers several distinct advantages: it is training-free and straightforward to implement, and exhibits significant computational efficiency improvements compared to existing purification methods. These attributes render F3 particularly suitable for large-scale industrial applications where both robust performance and operational efficiency are critical priorities. The code will be made publicly available.

摘要: 大型视觉语言模型（LVLM）的最新进展展示了它们在广泛的多模式视觉语言任务中的非凡能力。然而，这些模型仍然容易受到视觉对抗攻击，这可能会极大地损害其性能。尽管它们具有潜在的影响，但净化此类对抗性例子的有效方法的开发受到的关注相对有限。在本文中，我们介绍了F3，这是一个新颖的对抗净化框架，它采用了违反直觉的“以毒攻毒”策略：有意地向对抗性示例引入简单的扰动以减轻其有害影响。具体来说，F3利用从随机干扰的对手示例中获得的跨模式注意力作为参考目标。通过向这些对抗性示例中注入噪音，F3有效地细化了他们的注意力，从而产生更干净、更可靠的模型输出。值得注意的是，这种看似矛盾的利用噪音来抵消对抗攻击的方法产生了令人印象深刻的净化结果。此外，F3具有几个明显的优势：无需训练且易于实施，并且与现有的纯化方法相比，计算效率显着提高。这些属性使F3特别适合大规模工业应用，其中稳健的性能和运营效率都是关键优先事项。该代码将公开。



## **39. Simple Prompt Injection Attacks Can Leak Personal Data Observed by LLM Agents During Task Execution**

简单的提示注入攻击可能会泄露LLM代理在任务执行期间观察到的个人数据 cs.CR

25 pages, 18 figures, NeurIPS formatting style

**SubmitDate**: 2025-06-01    [abs](http://arxiv.org/abs/2506.01055v1) [paper-pdf](http://arxiv.org/pdf/2506.01055v1)

**Authors**: Meysam Alizadeh, Zeynab Samei, Daria Stetsenko, Fabrizio Gilardi

**Abstract**: Previous benchmarks on prompt injection in large language models (LLMs) have primarily focused on generic tasks and attacks, offering limited insights into more complex threats like data exfiltration. This paper examines how prompt injection can cause tool-calling agents to leak personal data observed during task execution. Using a fictitious banking agent, we develop data flow-based attacks and integrate them into AgentDojo, a recent benchmark for agentic security. To enhance its scope, we also create a richer synthetic dataset of human-AI banking conversations. In 16 user tasks from AgentDojo, LLMs show a 15-50 percentage point drop in utility under attack, with average attack success rates (ASR) around 20 percent; some defenses reduce ASR to zero. Most LLMs, even when successfully tricked by the attack, avoid leaking highly sensitive data like passwords, likely due to safety alignments, but they remain vulnerable to disclosing other personal data. The likelihood of password leakage increases when a password is requested along with one or two additional personal details. In an extended evaluation across 48 tasks, the average ASR is around 15 percent, with no built-in AgentDojo defense fully preventing leakage. Tasks involving data extraction or authorization workflows, which closely resemble the structure of exfiltration attacks, exhibit the highest ASRs, highlighting the interaction between task type, agent performance, and defense efficacy.

摘要: 之前关于大型语言模型（LLM）中即时注入的基准主要关注通用任务和攻击，对数据泄露等更复杂的威胁提供了有限的见解。本文研究了提示注入如何导致工具调用代理泄露任务执行期间观察到的个人数据。我们使用虚构的银行代理开发基于数据流的攻击，并将其集成到AgentDojo中，AgentDojo是最近的代理安全基准。为了扩大其范围，我们还创建了更丰富的人工智能银行对话合成数据集。在AgentDojo的16个用户任务中，LLM显示受攻击的实用性下降了15-50个百分点，平均攻击成功率（ASB）约为20%;某些防御将ASB降低为零。大多数LLM即使成功被攻击欺骗，也会避免泄露密码等高度敏感的数据，这可能是由于安全调整，但它们仍然容易泄露其他个人数据。当要求提供密码以及一个或两个额外的个人详细信息时，密码泄漏的可能性会增加。在48个任务的扩展评估中，平均ASR约为15%，没有内置的AgentDojo防御完全防止泄漏。涉及数据提取或授权工作流的任务与渗透攻击的结构非常相似，表现出最高的ASR，突出了任务类型，代理性能和防御效率之间的相互作用。



## **40. XGUARD: A Graded Benchmark for Evaluating Safety Failures of Large Language Models on Extremist Content**

XGUARD：评估极端主义内容上大型语言模型安全故障的分级基准 cs.CL

Preprint

**SubmitDate**: 2025-06-01    [abs](http://arxiv.org/abs/2506.00973v1) [paper-pdf](http://arxiv.org/pdf/2506.00973v1)

**Authors**: Vadivel Abishethvarman, Bhavik Chandna, Pratik Jalan, Usman Naseem

**Abstract**: Large Language Models (LLMs) can generate content spanning ideological rhetoric to explicit instructions for violence. However, existing safety evaluations often rely on simplistic binary labels (safe and unsafe), overlooking the nuanced spectrum of risk these outputs pose. To address this, we present XGUARD, a benchmark and evaluation framework designed to assess the severity of extremist content generated by LLMs. XGUARD includes 3,840 red teaming prompts sourced from real world data such as social media and news, covering a broad range of ideologically charged scenarios. Our framework categorizes model responses into five danger levels (0 to 4), enabling a more nuanced analysis of both the frequency and severity of failures. We introduce the interpretable Attack Severity Curve (ASC) to visualize vulnerabilities and compare defense mechanisms across threat intensities. Using XGUARD, we evaluate six popular LLMs and two lightweight defense strategies, revealing key insights into current safety gaps and trade-offs between robustness and expressive freedom. Our work underscores the value of graded safety metrics for building trustworthy LLMs.

摘要: 大型语言模型（LLM）可以生成涵盖意识形态修辞到明确的暴力指令的内容。然而，现有的安全评估通常依赖于简单化的二元标签（安全和不安全），忽视了这些输出带来的细致入微的风险范围。为了解决这个问题，我们提出了XGUARD，这是一个基准和评估框架，旨在评估LLM生成的极端主义内容的严重性。XGUARD包含3，840个红色团队提示，这些提示来自社交媒体和新闻等现实世界数据，涵盖了广泛的意识形态场景。我们的框架将模型响应分为五个危险级别（0到4），从而能够对故障的频率和严重性进行更细致的分析。我们引入可解释的攻击严重性曲线（ASC）来可视化漏洞并比较不同威胁强度的防御机制。使用XGUARD，我们评估了六种流行的LLM和两种轻量级防御策略，揭示了对当前安全差距以及稳健性和表达自由性之间权衡的关键见解。我们的工作强调了分级安全指标对于构建值得信赖的LLM的价值。



## **41. A Large Language Model-Supported Threat Modeling Framework for Transportation Cyber-Physical Systems**

运输网络物理系统支持的大语言模型威胁建模框架 cs.CR

**SubmitDate**: 2025-06-01    [abs](http://arxiv.org/abs/2506.00831v1) [paper-pdf](http://arxiv.org/pdf/2506.00831v1)

**Authors**: M Sabbir Salek, Mashrur Chowdhury, Muhaimin Bin Munir, Yuchen Cai, Mohammad Imtiaz Hasan, Jean-Michel Tine, Latifur Khan, Mizanur Rahman

**Abstract**: Modern transportation systems rely on cyber-physical systems (CPS), where cyber systems interact seamlessly with physical systems like transportation-related sensors and actuators to enhance safety, mobility, and energy efficiency. However, growing automation and connectivity increase exposure to cyber vulnerabilities. Existing threat modeling frameworks for transportation CPS are often limited in scope, resource-intensive, and dependent on significant cybersecurity expertise. To address these gaps, we present TraCR-TMF (Transportation Cybersecurity and Resiliency Threat Modeling Framework), a large language model (LLM)-based framework that minimizes expert intervention. TraCR-TMF identifies threats, potential attack techniques, and corresponding countermeasures by leveraging the MITRE ATT&CK matrix through three LLM-based approaches: (i) a retrieval-augmented generation (RAG) method requiring no expert input, (ii) an in-context learning approach requiring low expert input, and (iii) a supervised fine-tuning method requiring moderate expert input. TraCR-TMF also maps attack paths to critical assets by analyzing vulnerabilities using a customized LLM. The framework was evaluated in two scenarios. First, it identified relevant attack techniques across transportation CPS applications, with 90% precision as validated by experts. Second, using a fine-tuned LLM, it successfully predicted multiple exploitations including lateral movement, data exfiltration, and ransomware-related encryption that occurred during a major real-world cyberattack incident. These results demonstrate TraCR-TMF's effectiveness in CPS threat modeling, its reduced reliance on cybersecurity expertise, and its adaptability across CPS domains.

摘要: 现代交通系统依赖于网络物理系统（CPS），其中网络系统与交通相关的传感器和致动器等物理系统无缝交互，以提高安全性、移动性和能源效率。然而，自动化和连接性的不断发展增加了网络漏洞的风险。现有的交通CPS威胁建模框架通常范围有限、资源密集型，并且依赖于重要的网络安全专业知识。为了解决这些差距，我们提出了TraCR-SYS（交通网络安全和弹性威胁建模框架），这是一个基于大型语言模型（LLM）的框架，可以最大限度地减少专家干预。TraCR-SYS通过三种基于LLM的方法利用MITRE ATA & CK矩阵来识别威胁、潜在的攻击技术和相应的对策：（i）不需要专家输入的检索增强生成（RAG）方法，（ii）需要低专家输入的上下文学习方法，和（iii）需要适度专家输入的监督微调方法。TraCR-SYS还通过使用自定义的LLM分析漏洞来将攻击路径映射到关键资产。该框架在两种情况下进行了评估。首先，它识别了运输CPS应用程序中的相关攻击技术，经专家验证的准确率为90%。其次，它使用经过微调的LLM，成功预测了现实世界重大网络攻击事件期间发生的多次利用，包括横向移动、数据泄露和勒索软件相关加密。这些结果证明了TraCR-SYS在CPS威胁建模方面的有效性、减少了对网络安全专业知识的依赖以及其跨CPS域的适应性。



## **42. Jailbreak-R1: Exploring the Jailbreak Capabilities of LLMs via Reinforcement Learning**

越狱-R1：通过强化学习探索LLM的越狱能力 cs.AI

21 pages, 8 figures

**SubmitDate**: 2025-06-01    [abs](http://arxiv.org/abs/2506.00782v1) [paper-pdf](http://arxiv.org/pdf/2506.00782v1)

**Authors**: Weiyang Guo, Zesheng Shi, Zhuo Li, Yequan Wang, Xuebo Liu, Wenya Wang, Fangming Liu, Min Zhang, Jing Li

**Abstract**: As large language models (LLMs) grow in power and influence, ensuring their safety and preventing harmful output becomes critical. Automated red teaming serves as a tool to detect security vulnerabilities in LLMs without manual labor. However, most existing methods struggle to balance the effectiveness and diversity of red-team generated attack prompts. To address this challenge, we propose \ourapproach, a novel automated red teaming training framework that utilizes reinforcement learning to explore and generate more effective attack prompts while balancing their diversity. Specifically, it consists of three training stages: (1) Cold Start: The red team model is supervised and fine-tuned on a jailbreak dataset obtained through imitation learning. (2) Warm-up Exploration: The model is trained in jailbreak instruction following and exploration, using diversity and consistency as reward signals. (3) Enhanced Jailbreak: Progressive jailbreak rewards are introduced to gradually enhance the jailbreak performance of the red-team model. Extensive experiments on a variety of LLMs show that \ourapproach effectively balances the diversity and effectiveness of jailbreak prompts compared to existing methods. Our work significantly improves the efficiency of red team exploration and provides a new perspective on automated red teaming.

摘要: 随着大型语言模型（LLM）的力量和影响力不断增强，确保其安全性和防止有害输出变得至关重要。自动红色分组可以作为一种无需手工即可检测LLM安全漏洞的工具。然而，大多数现有方法都难以平衡红队生成的攻击提示的有效性和多样性。为了应对这一挑战，我们提出了\ourapproach，这是一种新型的自动化红色团队训练框架，它利用强化学习来探索和生成更有效的攻击提示，同时平衡其多样性。具体来说，它由三个训练阶段组成：（1）冷启动：红队模型在通过模仿学习获得的越狱数据集上进行监督和微调。(2)热身探索：该模型在越狱指导跟踪和探索中进行训练，使用多样性和一致性作为奖励信号。(3)强化越狱：引入渐进式越狱奖励，逐步提升红队模式的越狱表现。对各种LLM的广泛实验表明，与现有方法相比，我们的方法有效地平衡了越狱提示的多样性和有效性。我们的工作显着提高了红色团队探索的效率，并为自动化红色团队合作提供了新的视角。



## **43. CoP: Agentic Red-teaming for Large Language Models using Composition of Principles**

CoP：使用原则组合的大型语言模型的大型红色团队 cs.AI

**SubmitDate**: 2025-06-01    [abs](http://arxiv.org/abs/2506.00781v1) [paper-pdf](http://arxiv.org/pdf/2506.00781v1)

**Authors**: Chen Xiong, Pin-Yu Chen, Tsung-Yi Ho

**Abstract**: Recent advances in Large Language Models (LLMs) have spurred transformative applications in various domains, ranging from open-source to proprietary LLMs. However, jailbreak attacks, which aim to break safety alignment and user compliance by tricking the target LLMs into answering harmful and risky responses, are becoming an urgent concern. The practice of red-teaming for LLMs is to proactively explore potential risks and error-prone instances before the release of frontier AI technology. This paper proposes an agentic workflow to automate and scale the red-teaming process of LLMs through the Composition-of-Principles (CoP) framework, where human users provide a set of red-teaming principles as instructions to an AI agent to automatically orchestrate effective red-teaming strategies and generate jailbreak prompts. Distinct from existing red-teaming methods, our CoP framework provides a unified and extensible framework to encompass and orchestrate human-provided red-teaming principles to enable the automated discovery of new red-teaming strategies. When tested against leading LLMs, CoP reveals unprecedented safety risks by finding novel jailbreak prompts and improving the best-known single-turn attack success rate by up to 19.0 times.

摘要: 大型语言模型（LLM）的最新进展激发了各个领域的变革性应用程序，从开源到专有LLM。然而，越狱攻击的目的是通过诱骗目标LLM回答有害和危险的响应来打破安全一致和用户合规性，正在成为一个紧迫的问题。LLM的红色团队实践是在前沿人工智能技术发布之前主动探索潜在风险和容易出错的实例。本文提出了一种代理工作流程，通过构成原则（CoP）框架自动化和扩展LLM的红色团队流程，其中人类用户提供一组红色团队原则作为指令，向人工智能代理自动协调有效的红色团队策略并生成越狱提示。与现有的红色团队方法不同，我们的CoP框架提供了一个统一且可扩展的框架，以涵盖和编排人类提供的红色团队原则，以实现新的红色团队策略的自动发现。当针对领先的LLM进行测试时，CoP发现了新颖的越狱提示并将最著名的单回合攻击成功率提高了19.0倍，从而揭示了前所未有的安全风险。



## **44. Underestimated Privacy Risks for Minority Populations in Large Language Model Unlearning**

在大型语言模型学习中，少数群体的隐私风险被低估 cs.LG

**SubmitDate**: 2025-06-01    [abs](http://arxiv.org/abs/2412.08559v3) [paper-pdf](http://arxiv.org/pdf/2412.08559v3)

**Authors**: Rongzhe Wei, Mufei Li, Mohsen Ghassemi, Eleonora Kreačić, Yifan Li, Xiang Yue, Bo Li, Vamsi K. Potluru, Pan Li, Eli Chien

**Abstract**: Large Language Models (LLMs) embed sensitive, human-generated data, prompting the need for unlearning methods. Although certified unlearning offers strong privacy guarantees, its restrictive assumptions make it unsuitable for LLMs, giving rise to various heuristic approaches typically assessed through empirical evaluations. These standard evaluations randomly select data for removal, apply unlearning techniques, and use membership inference attacks (MIAs) to compare unlearned models against models retrained without the removed data. However, to ensure robust privacy protections for every data point, it is essential to account for scenarios in which certain data subsets face elevated risks. Prior research suggests that outliers, particularly including data tied to minority groups, often exhibit higher memorization propensity which indicates they may be more difficult to unlearn. Building on these insights, we introduce a complementary, minority-aware evaluation framework to highlight blind spots in existing frameworks. We substantiate our findings with carefully designed experiments, using canaries with personally identifiable information (PII) to represent these minority subsets and demonstrate that they suffer at least 20% higher privacy leakage across various unlearning methods, MIAs, datasets, and LLM scales. Our proposed minority-aware evaluation framework marks an essential step toward more equitable and comprehensive assessments of LLM unlearning efficacy.

摘要: 大型语言模型（LLM）嵌入了敏感的、人类生成的数据，这促使人们需要去学习方法。虽然认证的非学习提供了强大的隐私保证，但其限制性假设使其不适合LLM，从而产生了通常通过经验评估评估的各种启发式方法。这些标准评估随机选择要删除的数据，应用非学习技术，并使用隶属度推理攻击（MIA）来比较未学习的模型与在没有删除数据的情况下重新训练的模型。然而，为了确保每个数据点都有强大的隐私保护，必须考虑某些数据子集面临高风险的情况。之前的研究表明，异常值，特别是包括与少数群体相关的数据，通常表现出更高的记忆倾向，这表明他们可能更难忘记。在这些见解的基础上，我们引入了一个补充的、少数群体意识的评估框架，以突出现有框架中的盲点。我们通过精心设计的实验证实了我们的发现，使用具有个人可识别信息（PRI）的金丝雀来代表这些少数族裔子集，并证明它们在各种取消学习方法、MIA、数据集和LLM量表中遭受的隐私泄露至少高出20%。我们提出的少数族裔意识评估框架标志着朝着更公平、全面的LLM遗忘功效评估迈出了重要的一步。



## **45. Tokens for Learning, Tokens for Unlearning: Mitigating Membership Inference Attacks in Large Language Models via Dual-Purpose Training**

用于学习的代币，用于取消学习的代币：通过双重用途培训减轻大型语言模型中的成员推断攻击 cs.LG

ACL'25 (Findings)

**SubmitDate**: 2025-05-31    [abs](http://arxiv.org/abs/2502.19726v2) [paper-pdf](http://arxiv.org/pdf/2502.19726v2)

**Authors**: Toan Tran, Ruixuan Liu, Li Xiong

**Abstract**: Large language models (LLMs) have become the backbone of modern natural language processing but pose privacy concerns about leaking sensitive training data. Membership inference attacks (MIAs), which aim to infer whether a sample is included in a model's training dataset, can serve as a foundation for broader privacy threats. Existing defenses designed for traditional classification models do not account for the sequential nature of text data. As a result, they either require significant computational resources or fail to effectively mitigate privacy risks in LLMs. In this work, we propose \methodname, a lightweight yet effective empirical privacy defense for protecting training data of language models by leveraging token-specific characteristics. By analyzing token dynamics during training, we propose a token selection strategy that categorizes tokens into hard tokens for learning and memorized tokens for unlearning. Subsequently, our training-phase defense optimizes a novel dual-purpose token-level loss to achieve a Pareto-optimal balance between utility and privacy. Extensive experiments demonstrate that our approach not only provides strong protection against MIAs but also improves language modeling performance by around 10\% across various LLM architectures and datasets compared to the baselines.

摘要: 大型语言模型（LLM）已经成为现代自然语言处理的支柱，但也带来了泄露敏感训练数据的隐私问题。成员推断攻击（MIA）旨在推断样本是否包含在模型的训练数据集中，可以作为更广泛的隐私威胁的基础。为传统分类模型设计的现有防御措施没有考虑到文本数据的连续性。因此，它们要么需要大量的计算资源，要么无法有效地减轻LLM中的隐私风险。在这项工作中，我们提出了\MethodName，这是一种轻量级但有效的经验隐私防御，用于通过利用代币特定的特征来保护语言模型的训练数据。通过分析训练期间的令牌动态，我们提出了一种令牌选择策略，将令牌分为用于学习的硬令牌和用于取消学习的记忆令牌。随后，我们的训练阶段防御优化了一种新颖的双重用途代币级损失，以在效用和隐私之间实现帕累托最优平衡。大量实验表明，我们的方法不仅提供了针对MIA的强有力保护，而且与基线相比，还将各种LLM架构和数据集的语言建模性能提高了约10%。



## **46. Immune: Improving Safety Against Jailbreaks in Multi-modal LLMs via Inference-Time Alignment**

免疫：通过推理时间对齐提高多模式LLM中越狱的安全性 cs.CR

Accepted to CVPR 2025

**SubmitDate**: 2025-05-31    [abs](http://arxiv.org/abs/2411.18688v4) [paper-pdf](http://arxiv.org/pdf/2411.18688v4)

**Authors**: Soumya Suvra Ghosal, Souradip Chakraborty, Vaibhav Singh, Tianrui Guan, Mengdi Wang, Alvaro Velasquez, Ahmad Beirami, Furong Huang, Dinesh Manocha, Amrit Singh Bedi

**Abstract**: With the widespread deployment of Multimodal Large Language Models (MLLMs) for visual-reasoning tasks, improving their safety has become crucial. Recent research indicates that despite training-time safety alignment, these models remain vulnerable to jailbreak attacks. In this work, we first highlight an important safety gap to describe that alignment achieved solely through safety training may be insufficient against jailbreak attacks. To address this vulnerability, we propose Immune, an inference-time defense framework that leverages a safe reward model through controlled decoding to defend against jailbreak attacks. Additionally, we provide a mathematical characterization of Immune, offering insights on why it improves safety against jailbreaks. Extensive evaluations on diverse jailbreak benchmarks using recent MLLMs reveal that Immune effectively enhances model safety while preserving the model's original capabilities. For instance, against text-based jailbreak attacks on LLaVA-1.6, Immune reduces the attack success rate by 57.82% and 16.78% compared to the base MLLM and state-of-the-art defense strategy, respectively.

摘要: 随着多模式大型语言模型（MLLM）用于视觉推理任务的广泛部署，提高其安全性变得至关重要。最近的研究表明，尽管训练时安全一致，但这些模型仍然容易受到越狱攻击。在这项工作中，我们首先强调了一个重要的安全差距，以描述仅通过安全培训实现的对准可能不足以对抗越狱袭击。为了解决这个漏洞，我们提出了Immune，这是一种推理时防御框架，通过受控解码利用安全奖励模型来抵御越狱攻击。此外，我们还提供了Immune的数学描述，并深入了解它为何可以提高越狱安全性。使用最新的MLLM对各种越狱基准进行了广泛评估，结果表明Immune有效地增强了模型的安全性，同时保留了模型的原始功能。例如，针对LLaVA-1.6的基于文本的越狱攻击，与基本MLLM和最先进的防御策略相比，Immune将攻击成功率分别降低了57.82%和16.78%。



## **47. Security Concerns for Large Language Models: A Survey**

大型语言模型的安全性问题综述 cs.CR

**SubmitDate**: 2025-05-31    [abs](http://arxiv.org/abs/2505.18889v2) [paper-pdf](http://arxiv.org/pdf/2505.18889v2)

**Authors**: Miles Q. Li, Benjamin C. M. Fung

**Abstract**: Large Language Models (LLMs) such as GPT-4 and its recent iterations, Google's Gemini, Anthropic's Claude 3 models, and xAI's Grok have caused a revolution in natural language processing, but their capabilities also introduce new security vulnerabilities. In this survey, we provide a comprehensive overview of the emerging security concerns around LLMs, categorizing threats into prompt injection and jailbreaking, adversarial attacks such as input perturbations and data poisoning, misuse by malicious actors for purposes such as generating disinformation, phishing emails, and malware, and worrisome risks inherent in autonomous LLM agents. A significant focus has been recently placed on the latter, exploring goal misalignment, emergent deception, self-preservation instincts, and the potential for LLMs to develop and pursue covert, misaligned objectives, a behavior known as scheming, which may even persist through safety training. We summarize recent academic and industrial studies from 2022 to 2025 that exemplify each threat, analyze proposed defenses and their limitations, and identify open challenges in securing LLM-based applications. We conclude by emphasizing the importance of advancing robust, multi-layered security strategies to ensure LLMs are safe and beneficial.

摘要: GPT-4及其最近的迭代、Google的Gemini、Anthropic的Claude 3模型和xAI的Grok等大型语言模型（LLM）引发了自然语言处理领域的一场革命，但它们的功能也引入了新的安全漏洞。在本调查中，我们全面概述了围绕LLM的新安全问题，将威胁分为即时注入和越狱、输入干扰和数据中毒等对抗性攻击、恶意行为者出于生成虚假信息、网络钓鱼电子邮件和恶意软件等目的的滥用以及自主LLM代理固有的令人担忧的风险。最近人们对后者给予了极大的关注，探索目标失调、紧急欺骗、自我保护本能，以及LLM制定和追求隐蔽、失调目标的潜力，这种行为被称为阴谋，甚至可能通过安全培训持续存在。我们总结了2022年至2025年期间最近的学术和工业研究，这些研究揭示了每种威胁，分析了拟议的防御措施及其局限性，并确定了保护基于LLM的应用程序方面的公开挑战。最后，我们强调了推进强大的多层安全策略以确保LLM安全且有益的重要性。



## **48. SafeTuneBed: A Toolkit for Benchmarking LLM Safety Alignment in Fine-Tuning**

SafeTuneBed：用于在微调中对LLM安全一致进行基准测试的工具包 cs.LG

**SubmitDate**: 2025-05-31    [abs](http://arxiv.org/abs/2506.00676v1) [paper-pdf](http://arxiv.org/pdf/2506.00676v1)

**Authors**: Saad Hossain, Samanvay Vajpayee, Sirisha Rambhatla

**Abstract**: As large language models (LLMs) become ubiquitous, parameter-efficient fine-tuning methods and safety-first defenses have proliferated rapidly. However, the number of approaches and their recent increase have resulted in diverse evaluations-varied datasets, metrics, and inconsistent threat settings-making it difficult to fairly compare safety, utility, and robustness across methods. To address this, we introduce SafeTuneBed, a benchmark and toolkit unifying fine-tuning and defense evaluation. SafeTuneBed (i) curates a diverse repository of multiple fine-tuning datasets spanning sentiment analysis, question-answering, multi-step reasoning, and open-ended instruction tasks, and allows for the generation of harmful-variant splits; (ii) enables integration of state-of-the-art defenses, including alignment-stage immunization, in-training safeguards, and post-tuning repair; and (iii) provides evaluators for safety (attack success rate, refusal consistency) and utility. Built on Python-first, dataclass-driven configs and plugins, SafeTuneBed requires minimal additional code to specify any fine-tuning regime, defense method, and metric suite, while ensuring end-to-end reproducibility. We showcase its value by benchmarking representative defenses across varied poisoning scenarios and tasks. By standardizing data, code, and metrics, SafeTuneBed is the first focused toolkit of its kind to accelerate rigorous and comparable research in safe LLM fine-tuning. Code is available at: https://github.com/criticalml-uw/SafeTuneBed

摘要: 随着大型语言模型（LLM）变得无处不在，参数高效的微调方法和安全第一的防御措施迅速增加。然而，方法的数量及其最近的增加导致了不同的评估-不同的数据集，指标和不一致的威胁设置-使得很难公平地比较各种方法的安全性，实用性和鲁棒性。为了解决这个问题，我们引入了SafeTuneBed，这是一个统一微调和防御评估的基准和工具包。SafeTuneBed（i）管理多个微调数据集的多样化存储库，涵盖情感分析，问答，多步推理和开放式指令任务，并允许生成有害变体分裂;（ii）实现最先进防御的集成，包括免疫阶段免疫，训练中保障措施和调优后修复;以及（iii）提供安全性（攻击成功率、拒绝一致性）和实用性的评估者。SafeTuneBed构建在Python优先、椭圆形驱动的脚本和插件之上，只需最少的额外代码即可指定任何微调机制、防御方法和指标套件，同时确保端到端的可重复性。我们通过对各种中毒场景和任务的代表性防御进行基准测试来展示其价值。通过标准化数据、代码和指标，SafeTuneBed是同类中第一个加速安全LLM微调方面严格且可比的研究的专注工具包。代码可访问：https://github.com/criticalml-uw/SafeTuneBed



## **49. SafeTy Reasoning Elicitation Alignment for Multi-Turn Dialogues**

多轮对话的安全推理启发对齐 cs.CL

**SubmitDate**: 2025-05-31    [abs](http://arxiv.org/abs/2506.00668v1) [paper-pdf](http://arxiv.org/pdf/2506.00668v1)

**Authors**: Martin Kuo, Jianyi Zhang, Aolin Ding, Louis DiValentin, Amin Hass, Benjamin F Morris, Isaac Jacobson, Randolph Linderman, James Kiessling, Nicolas Ramos, Bhavna Gopal, Maziyar Baran Pouyan, Changwei Liu, Hai Li, Yiran Chen

**Abstract**: Malicious attackers can exploit large language models (LLMs) by engaging them in multi-turn dialogues to achieve harmful objectives, posing significant safety risks to society. To address this challenge, we propose a novel defense mechanism: SafeTy Reasoning Elicitation Alignment for Multi-Turn Dialogues (STREAM). STREAM defends LLMs against multi-turn attacks while preserving their functional capabilities. Our approach involves constructing a human-annotated dataset, the Safety Reasoning Multi-turn Dialogues dataset, which is used to fine-tune a plug-and-play safety reasoning moderator. This model is designed to identify malicious intent hidden within multi-turn conversations and alert the target LLM of potential risks. We evaluate STREAM across multiple LLMs against prevalent multi-turn attack strategies. Experimental results demonstrate that our method significantly outperforms existing defense techniques, reducing the Attack Success Rate (ASR) by 51.2%, all while maintaining comparable LLM capability.

摘要: 恶意攻击者可以通过让大型语言模型（LLM）参与多轮对话来利用它们来实现有害目标，从而对社会构成重大安全风险。为了应对这一挑战，我们提出了一种新颖的防御机制：SafeTy Reasoning启发式对齐多转弯对话（UTE）。MBE保护LLM免受多回合攻击，同时保留其功能能力。我们的方法涉及构建人类注释的数据集，即安全推理多轮对话数据集，用于微调即插即用安全推理主持人。该模型旨在识别隐藏在多轮对话中的恶意意图，并向目标LLM警告潜在风险。我们针对流行的多回合攻击策略评估多个LLM的MBE。实验结果表明，我们的方法显着优于现有的防御技术，将攻击成功率（ASB）降低了51.2%，同时保持了相当的LLM能力。



## **50. Stepwise Reasoning Error Disruption Attack of LLMs**

LLM的逐步推理错误中断攻击 cs.AI

**SubmitDate**: 2025-05-31    [abs](http://arxiv.org/abs/2412.11934v4) [paper-pdf](http://arxiv.org/pdf/2412.11934v4)

**Authors**: Jingyu Peng, Maolin Wang, Xiangyu Zhao, Kai Zhang, Wanyu Wang, Pengyue Jia, Qidong Liu, Ruocheng Guo, Qi Liu

**Abstract**: Large language models (LLMs) have made remarkable strides in complex reasoning tasks, but their safety and robustness in reasoning processes remain underexplored. Existing attacks on LLM reasoning are constrained by specific settings or lack of imperceptibility, limiting their feasibility and generalizability. To address these challenges, we propose the Stepwise rEasoning Error Disruption (SEED) attack, which subtly injects errors into prior reasoning steps to mislead the model into producing incorrect subsequent reasoning and final answers. Unlike previous methods, SEED is compatible with zero-shot and few-shot settings, maintains the natural reasoning flow, and ensures covert execution without modifying the instruction. Extensive experiments on four datasets across four different models demonstrate SEED's effectiveness, revealing the vulnerabilities of LLMs to disruptions in reasoning processes. These findings underscore the need for greater attention to the robustness of LLM reasoning to ensure safety in practical applications. Our code is available at: https://github.com/Applied-Machine-Learning-Lab/SEED-Attack.

摘要: 大型语言模型（LLM）在复杂推理任务中取得了显着的进步，但其在推理过程中的安全性和稳健性仍然没有得到充分的探索。对LLM推理的现有攻击受到特定设置或缺乏不可感知性的限制，限制了其可行性和可概括性。为了解决这些挑战，我们提出了Stepwise rEasying错误破坏（SEED）攻击，它巧妙地将错误注入到先前的推理步骤中，以误导模型产生错误的后续推理和最终答案。与以前的方法不同，SEED与零镜头和少镜头设置兼容，保持自然推理流程，并确保在不修改指令的情况下隐蔽执行。对四个不同模型的四个数据集进行的广泛实验证明了SEED的有效性，揭示了LLM对推理过程中断的脆弱性。这些发现强调需要更加关注LLM推理的稳健性，以确保实际应用中的安全性。我们的代码可访问：https://github.com/Applied-Machine-Learning-Lab/SEED-Attack。



