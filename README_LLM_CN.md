# Latest Large Language Model Attack Papers
**update at 2025-09-26 20:46:40**

翻译来自 https://cloud.tencent.com/document/product/551/15619

## **1. IDEATOR: Jailbreaking and Benchmarking Large Vision-Language Models Using Themselves**

IDEATOR：使用自己越狱和基准大型视觉语言模型 cs.CV

**SubmitDate**: 2025-09-25    [abs](http://arxiv.org/abs/2411.00827v6) [paper-pdf](http://arxiv.org/pdf/2411.00827v6)

**Authors**: Ruofan Wang, Juncheng Li, Yixu Wang, Bo Wang, Xiaosen Wang, Yan Teng, Yingchun Wang, Xingjun Ma, Yu-Gang Jiang

**Abstract**: As large Vision-Language Models (VLMs) gain prominence, ensuring their safe deployment has become critical. Recent studies have explored VLM robustness against jailbreak attacks-techniques that exploit model vulnerabilities to elicit harmful outputs. However, the limited availability of diverse multimodal data has constrained current approaches to rely heavily on adversarial or manually crafted images derived from harmful text datasets, which often lack effectiveness and diversity across different contexts. In this paper, we propose IDEATOR, a novel jailbreak method that autonomously generates malicious image-text pairs for black-box jailbreak attacks. IDEATOR is grounded in the insight that VLMs themselves could serve as powerful red team models for generating multimodal jailbreak prompts. Specifically, IDEATOR leverages a VLM to create targeted jailbreak texts and pairs them with jailbreak images generated by a state-of-the-art diffusion model. Extensive experiments demonstrate IDEATOR's high effectiveness and transferability, achieving a 94% attack success rate (ASR) in jailbreaking MiniGPT-4 with an average of only 5.34 queries, and high ASRs of 82%, 88%, and 75% when transferred to LLaVA, InstructBLIP, and Chameleon, respectively. Building on IDEATOR's strong transferability and automated process, we introduce the VLJailbreakBench, a safety benchmark comprising 3,654 multimodal jailbreak samples. Our benchmark results on 11 recently released VLMs reveal significant gaps in safety alignment. For instance, our challenge set achieves ASRs of 46.31% on GPT-4o and 19.65% on Claude-3.5-Sonnet, underscoring the urgent need for stronger defenses. VLJailbreakBench is publicly available at https://roywang021.github.io/VLJailbreakBench.

摘要: 随着大型视觉语言模型（VLM）的日益突出，确保其安全部署变得至关重要。最近的研究探索了VLM针对越狱攻击的鲁棒性--利用模型漏洞来引发有害输出的技术。然而，多样化多模式数据的可用性有限，限制了当前的方法严重依赖于从有害文本数据集派生的对抗性或手动制作的图像，而这些图像通常缺乏跨不同背景的有效性和多样性。本文中，我们提出了IDEATOR，这是一种新型越狱方法，可以自主生成用于黑匣子越狱攻击的恶意图像-文本对。IDEATOR基于这样的见解：VLM本身可以充当强大的红队模型，用于生成多模式越狱提示。具体来说，IDEATOR利用VLM创建有针对性的越狱文本，并将其与由最先进的扩散模型生成的越狱图像配对。大量实验证明了IDEATOR的高效率和可移植性，在越狱MiniGPT-4中平均只需5.34次查询即可实现94%的攻击成功率（ASB），转移到LLaVA、INSTBLIP和Chameleon时，攻击成功率分别为82%、88%和75%。基于IDEATOR强大的可移植性和自动化流程，我们推出了VLJailbreakBench，这是一个由3，654个多模式越狱样本组成的安全基准。我们对最近发布的11个VLM的基准结果揭示了安全一致方面的显着差距。例如，我们的挑战集在GPT-4 o上实现了46.31%的ASB，在Claude-3.5-十四行诗上实现了19.65%的ASB，这凸显了迫切需要更强的防御。VLJailbreakBench可在https://roywang021.github.io/VLJailbreakBench上公开获取。



## **2. GEP: A GCG-Based method for extracting personally identifiable information from chatbots built on small language models**

GEP：一种基于GCG的方法，用于从基于小型语言模型的聊天机器人中提取个人可识别信息 cs.CL

16 pages, 5 figures, 4 tables. Under review as a conference paper at  ICLR 2026

**SubmitDate**: 2025-09-25    [abs](http://arxiv.org/abs/2509.21192v1) [paper-pdf](http://arxiv.org/pdf/2509.21192v1)

**Authors**: Jieli Zhu, Vi Ngoc-Nha Tran

**Abstract**: Small language models (SLMs) become unprecedentedly appealing due to their approximately equivalent performance compared to large language models (LLMs) in certain fields with less energy and time consumption during training and inference. However, the personally identifiable information (PII) leakage of SLMs for downstream tasks has yet to be explored. In this study, we investigate the PII leakage of the chatbot based on SLM. We first finetune a new chatbot, i.e., ChatBioGPT based on the backbone of BioGPT using medical datasets Alpaca and HealthCareMagic. It shows a matchable performance in BERTscore compared with previous studies of ChatDoctor and ChatGPT. Based on this model, we prove that the previous template-based PII attacking methods cannot effectively extract the PII in the dataset for leakage detection under the SLM condition. We then propose GEP, which is a greedy coordinate gradient-based (GCG) method specifically designed for PII extraction. We conduct experimental studies of GEP and the results show an increment of up to 60$\times$ more leakage compared with the previous template-based methods. We further expand the capability of GEP in the case of a more complicated and realistic situation by conducting free-style insertion where the inserted PII in the dataset is in the form of various syntactic expressions instead of fixed templates, and GEP is still able to reveal a PII leakage rate of up to 4.53%.

摘要: 小型语言模型（SLC）因其在某些领域与大型语言模型（LLM）相比具有大致相当的性能而变得前所未有的吸引力，并且在训练和推理期间消耗的能量和时间更少。然而，下游任务的SLS个人可识别信息（PRI）泄露问题尚未得到探索。在这项研究中，我们调查了基于LAM的聊天机器人的PRI泄漏。我们首先微调一个新的聊天机器人，即ChatBioGPT基于BioGPT的主干，使用医疗数据集Alpaca和HealthCareMagic。与之前的ChatDoctor和ChatGPT研究相比，它显示BERTscore的表现相当。基于该模型，我们证明了以前的基于模板的PRI攻击方法无法有效提取数据集中的PRI进行SPL条件下的泄漏检测。然后，我们提出了GEP，这是一种专门为PRI提取而设计的贪婪坐标梯度（GCG）方法。我们对GEP进行了实验研究，结果显示与之前的基于模板的方法相比，泄漏增加了高达60 $\x $。在更复杂、更现实的情况下，我们通过进行自由式插入，进一步扩展了GEP的能力，其中数据集中插入的PRI是各种语法表达的形式，而不是固定模板，GEP仍然能够揭示高达4.53%的PRI泄漏率。



## **3. EvoMail: Self-Evolving Cognitive Agents for Adaptive Spam and Phishing Email Defense**

EvoMail：用于自适应垃圾邮件和网络钓鱼电子邮件防御的自我进化认知代理 cs.LG

**SubmitDate**: 2025-09-25    [abs](http://arxiv.org/abs/2509.21129v1) [paper-pdf](http://arxiv.org/pdf/2509.21129v1)

**Authors**: Wei Huang, De-Tian Chu, Lin-Yuan Bai, Wei Kang, Hai-Tao Zhang, Bo Li, Zhi-Mo Han, Jing Ge, Hai-Feng Lin

**Abstract**: Modern email spam and phishing attacks have evolved far beyond keyword blacklists or simple heuristics. Adversaries now craft multi-modal campaigns that combine natural-language text with obfuscated URLs, forged headers, and malicious attachments, adapting their strategies within days to bypass filters. Traditional spam detection systems, which rely on static rules or single-modality models, struggle to integrate heterogeneous signals or to continuously adapt, leading to rapid performance degradation.   We propose EvoMail, a self-evolving cognitive agent framework for robust detection of spam and phishing. EvoMail first constructs a unified heterogeneous email graph that fuses textual content, metadata (headers, senders, domains), and embedded resources (URLs, attachments). A Cognitive Graph Neural Network enhanced by a Large Language Model (LLM) performs context-aware reasoning across these sources to identify coordinated spam campaigns. Most critically, EvoMail engages in an adversarial self-evolution loop: a ''red-team'' agent generates novel evasion tactics -- such as character obfuscation or AI-generated phishing text -- while the ''blue-team'' detector learns from failures, compresses experiences into a memory module, and reuses them for future reasoning.   Extensive experiments on real-world datasets (Enron-Spam, Ling-Spam, SpamAssassin, and TREC) and synthetic adversarial variants demonstrate that EvoMail consistently outperforms state-of-the-art baselines in detection accuracy, adaptability to evolving spam tactics, and interpretability of reasoning traces. These results highlight EvoMail's potential as a resilient and explainable defense framework against next-generation spam and phishing threats.

摘要: 现代电子邮件垃圾邮件和网络钓鱼攻击的发展远远超出了关键词黑名单或简单的启发式方法。对手现在精心设计多模式营销活动，将自然语言文本与模糊的URL、伪造的标题和恶意附件相结合，并在几天内调整策略以绕过过滤器。传统的垃圾邮件检测系统依赖于静态规则或单模式模型，难以集成异类信号或持续适应，从而导致性能迅速下降。   我们提出EvoMail，这是一个自进化的认知代理框架，用于稳健检测垃圾邮件和网络钓鱼。EvoMail首先构建了一个统一的异类电子邮件图，融合了文本内容、元数据（标头、收件箱、域）和嵌入式资源（URL、附件）。由大型语言模型（LLM）增强的认知图神经网络跨这些源执行上下文感知推理，以识别协调的垃圾邮件活动。最关键的是，EvoMail参与了一个对抗性的自我进化循环：“红队”代理生成新颖的逃避策略--例如字符混淆或人工智能生成的网络钓鱼文本--而“蓝队”检测器则从失败中学习，将经验压缩到存储模块中，并重新使用它们用于未来的推理。   对现实世界数据集（Enron-Spam、Ling-Spam、SpamAssassin和TREC）和合成对抗变体的广泛实验表明，EvoMail在检测准确性、对不断发展的垃圾邮件策略的适应性以及推理痕迹的可解释性方面始终优于最先进的基线。这些结果凸显了EvoMail作为针对下一代垃圾邮件和网络钓鱼威胁的弹性且可解释的防御框架的潜力。



## **4. PMark: Towards Robust and Distortion-free Semantic-level Watermarking with Channel Constraints**

PMark：实现具有通道约束的稳健且无失真的语义级水印 cs.CR

**SubmitDate**: 2025-09-25    [abs](http://arxiv.org/abs/2509.21057v1) [paper-pdf](http://arxiv.org/pdf/2509.21057v1)

**Authors**: Jiahao Huo, Shuliang Liu, Bin Wang, Junyan Zhang, Yibo Yan, Aiwei Liu, Xuming Hu, Mingxun Zhou

**Abstract**: Semantic-level watermarking (SWM) for large language models (LLMs) enhances watermarking robustness against text modifications and paraphrasing attacks by treating the sentence as the fundamental unit. However, existing methods still lack strong theoretical guarantees of robustness, and reject-sampling-based generation often introduces significant distribution distortions compared with unwatermarked outputs. In this work, we introduce a new theoretical framework on SWM through the concept of proxy functions (PFs) $\unicode{x2013}$ functions that map sentences to scalar values. Building on this framework, we propose PMark, a simple yet powerful SWM method that estimates the PF median for the next sentence dynamically through sampling while enforcing multiple PF constraints (which we call channels) to strengthen watermark evidence. Equipped with solid theoretical guarantees, PMark achieves the desired distortion-free property and improves the robustness against paraphrasing-style attacks. We also provide an empirically optimized version that further removes the requirement for dynamical median estimation for better sampling efficiency. Experimental results show that PMark consistently outperforms existing SWM baselines in both text quality and robustness, offering a more effective paradigm for detecting machine-generated text. Our code will be released at [this URL](https://github.com/PMark-repo/PMark).

摘要: 针对大型语言模型（LLM）的语义级水印（SWM）通过将句子作为基本单元来增强水印对文本修改和释义攻击的鲁棒性。然而，现有的方法仍然缺乏强有力的理论保证的鲁棒性，和拒绝采样为基础的生成往往会引入显着的分布失真相比，无水印的输出。在这项工作中，我们引入了一个新的理论框架SWM通过代理函数（PF）的概念$\unicode{x2013}$功能，映射句子标量值。在这个框架的基础上，我们提出了PMark，一个简单而强大的SWM方法，估计PF中位数为下一个句子动态通过采样，同时执行多个PF约束（我们称之为通道），以加强水印证据。PMark拥有坚实的理论保证，实现了预期的无失真特性，并提高了针对重述式攻击的鲁棒性。我们还提供了一个经验优化的版本，进一步消除了对动态中位数估计的要求，以获得更好的采样效率。实验结果表明，PMark在文本质量和稳健性方面始终优于现有的SWM基线，为检测机器生成的文本提供了更有效的范式。我们的代码将在[此URL]（https：//github.com/PMark-repo/PMark）发布。



## **5. FORCE: Transferable Visual Jailbreaking Attacks via Feature Over-Reliance CorrEction**

FORCE：通过功能过度依赖更正的可转移视觉越狱攻击 cs.LG

**SubmitDate**: 2025-09-25    [abs](http://arxiv.org/abs/2509.21029v1) [paper-pdf](http://arxiv.org/pdf/2509.21029v1)

**Authors**: Runqi Lin, Alasdair Paren, Suqin Yuan, Muyang Li, Philip Torr, Adel Bibi, Tongliang Liu

**Abstract**: The integration of new modalities enhances the capabilities of multimodal large language models (MLLMs) but also introduces additional vulnerabilities. In particular, simple visual jailbreaking attacks can manipulate open-source MLLMs more readily than sophisticated textual attacks. However, these underdeveloped attacks exhibit extremely limited cross-model transferability, failing to reliably identify vulnerabilities in closed-source MLLMs. In this work, we analyse the loss landscape of these jailbreaking attacks and find that the generated attacks tend to reside in high-sharpness regions, whose effectiveness is highly sensitive to even minor parameter changes during transfer. To further explain the high-sharpness localisations, we analyse their feature representations in both the intermediate layers and the spectral domain, revealing an improper reliance on narrow layer representations and semantically poor frequency components. Building on this, we propose a Feature Over-Reliance CorrEction (FORCE) method, which guides the attack to explore broader feasible regions across layer features and rescales the influence of frequency features according to their semantic content. By eliminating non-generalizable reliance on both layer and spectral features, our method discovers flattened feasible regions for visual jailbreaking attacks, thereby improving cross-model transferability. Extensive experiments demonstrate that our approach effectively facilitates visual red-teaming evaluations against closed-source MLLMs.

摘要: 新模态的集成增强了多模态大型语言模型（MLLM）的能力，但也引入了额外的漏洞。特别是，简单的视觉越狱攻击可以比复杂的文本攻击更容易地操纵开源MLLM。然而，这些不发达的攻击表现出非常有限的跨模型可转移性，无法可靠地识别闭源MLLM中的漏洞。在这项工作中，我们分析了这些越狱攻击的损失情况，发现生成的攻击往往位于高清晰度区域，其有效性对传输过程中即使是微小的参数变化也非常敏感。为了进一步解释高清晰度局部化，我们分析了它们在中间层和频谱域中的特征表示，揭示了对窄层表示和语义较差的频率分量的不当依赖。在此基础上，我们提出了一种特征过度依赖更正（FORCE）方法，该方法引导攻击探索跨层特征更广泛的可行区域，并根据频率特征的语义内容重新调整频率特征的影响。通过消除对层和光谱特征的不可推广的依赖，我们的方法发现了视觉越狱攻击的扁平可行区域，从而提高了跨模型的可移植性。大量实验表明，我们的方法有效地促进了针对闭源MLLM的视觉红色团队评估。



## **6. Automatic Red Teaming LLM-based Agents with Model Context Protocol Tools**

使用模型上下文协议工具自动Red分组基于LLM的代理 cs.CR

**SubmitDate**: 2025-09-25    [abs](http://arxiv.org/abs/2509.21011v1) [paper-pdf](http://arxiv.org/pdf/2509.21011v1)

**Authors**: Ping He, Changjiang Li, Binbin Zhao, Tianyu Du, Shouling Ji

**Abstract**: The remarkable capability of large language models (LLMs) has led to the wide application of LLM-based agents in various domains. To standardize interactions between LLM-based agents and their environments, model context protocol (MCP) tools have become the de facto standard and are now widely integrated into these agents. However, the incorporation of MCP tools introduces the risk of tool poisoning attacks, which can manipulate the behavior of LLM-based agents. Although previous studies have identified such vulnerabilities, their red teaming approaches have largely remained at the proof-of-concept stage, leaving the automatic and systematic red teaming of LLM-based agents under the MCP tool poisoning paradigm an open question. To bridge this gap, we propose AutoMalTool, an automated red teaming framework for LLM-based agents by generating malicious MCP tools. Our extensive evaluation shows that AutoMalTool effectively generates malicious MCP tools capable of manipulating the behavior of mainstream LLM-based agents while evading current detection mechanisms, thereby revealing new security risks in these agents.

摘要: 大型语言模型（LLM）的卓越能力导致了基于LLM的代理在各个领域的广泛应用。为了标准化基于LLM的代理与其环境之间的交互，模型上下文协议（HCP）工具已成为事实上的标准，并且现在已广泛集成到这些代理中。然而，加入LCP工具会带来工具中毒攻击的风险，这可能会操纵基于LLM的代理的行为。尽管之前的研究已经发现了此类漏洞，但他们的红色团队方法在很大程度上仍停留在概念验证阶段，这使得在LCP工具中毒范式下基于LLM的代理的自动和系统性红色团队成为一个悬而未决的问题。为了弥合这一差距，我们提出了AutoMalTools，这是一个针对基于LLM的代理的自动化红色团队框架，通过生成恶意LCP工具。我们的广泛评估表明，AutoMalTools有效地生成了恶意LCP工具，这些工具能够操纵主流基于LLM的代理的行为，同时规避当前的检测机制，从而揭示了这些代理中的新安全风险。



## **7. RLCracker: Exposing the Vulnerability of LLM Watermarks with Adaptive RL Attacks**

RL Cracker：通过自适应RL攻击暴露LLM水印的漏洞 cs.CR

**SubmitDate**: 2025-09-25    [abs](http://arxiv.org/abs/2509.20924v1) [paper-pdf](http://arxiv.org/pdf/2509.20924v1)

**Authors**: Hanbo Huang, Yiran Zhang, Hao Zheng, Xuan Gong, Yihan Li, Lin Liu, Shiyu Liang

**Abstract**: Large Language Models (LLMs) watermarking has shown promise in detecting AI-generated content and mitigating misuse, with prior work claiming robustness against paraphrasing and text editing. In this paper, we argue that existing evaluations are not sufficiently adversarial, obscuring critical vulnerabilities and overstating the security. To address this, we introduce adaptive robustness radius, a formal metric that quantifies watermark resilience against adaptive adversaries. We theoretically prove that optimizing the attack context and model parameters can substantially reduce this radius, making watermarks highly susceptible to paraphrase attacks. Leveraging this insight, we propose RLCracker, a reinforcement learning (RL)-based adaptive attack that erases watermarks while preserving semantic fidelity. RLCracker requires only limited watermarked examples and zero access to the detector. Despite weak supervision, it empowers a 3B model to achieve 98.5% removal success and an average 0.92 P-SP score on 1,500-token Unigram-marked texts after training on only 100 short samples. This performance dramatically exceeds 6.75% by GPT-4o and generalizes across five model sizes over ten watermarking schemes. Our results confirm that adaptive attacks are broadly effective and pose a fundamental threat to current watermarking defenses.

摘要: 大型语言模型（LLM）水印在检测人工智能生成的内容和减少滥用方面表现出了希望，之前的工作声称对重述和文本编辑具有鲁棒性。在本文中，我们认为现有的评估不够对抗，掩盖了关键漏洞并夸大了安全性。为了解决这个问题，我们引入了自适应鲁棒性半径，这是一种形式指标，可以量化水印针对自适应对手的弹性。我们从理论上证明，优化攻击上下文和模型参数可以大幅缩小这个半径，使水印极易受到转述攻击。利用这一见解，我们提出了RL Cracker，这是一种基于强化学习（RL）的自适应攻击，可以擦除水印，同时保留语义保真度。RL Cracker仅需要有限的带有水印的示例，并且对检测器的访问为零。尽管监管薄弱，但它使3B模型能够在仅对100个短样本进行训练后，在1，500个标记Unigram的文本上实现98.5%的删除成功率和平均0.92的P-SP评分。GPT-4 o的这一性能显着超过了6.75%，并在十种水印方案中推广了五种模型大小。我们的结果证实，自适应攻击广泛有效，并对当前的水印防御构成根本威胁。



## **8. Poisoning Prompt-Guided Sampling in Video Large Language Models**

视频大语言模型中的中毒预算引导采样 cs.CV

12 pages, 4 figures

**SubmitDate**: 2025-09-25    [abs](http://arxiv.org/abs/2509.20851v1) [paper-pdf](http://arxiv.org/pdf/2509.20851v1)

**Authors**: Yuxin Cao, Wei Song, Jingling Xue, Jin Song Dong

**Abstract**: Video Large Language Models (VideoLLMs) have emerged as powerful tools for understanding videos, supporting tasks such as summarization, captioning, and question answering. Their performance has been driven by advances in frame sampling, progressing from uniform-based to semantic-similarity-based and, most recently, prompt-guided strategies. While vulnerabilities have been identified in earlier sampling strategies, the safety of prompt-guided sampling remains unexplored. We close this gap by presenting PoisonVID, the first black-box poisoning attack that undermines prompt-guided sampling in VideoLLMs. PoisonVID compromises the underlying prompt-guided sampling mechanism through a closed-loop optimization strategy that iteratively optimizes a universal perturbation to suppress harmful frame relevance scores, guided by a depiction set constructed from paraphrased harmful descriptions leveraging a shadow VideoLLM and a lightweight language model, i.e., GPT-4o-mini. Comprehensively evaluated on three prompt-guided sampling strategies and across three advanced VideoLLMs, PoisonVID achieves 82% - 99% attack success rate, highlighting the importance of developing future advanced sampling strategies for VideoLLMs.

摘要: 视频大型语言模型（VideoLLM）已成为理解视频、支持摘要、字幕和问答等任务的强大工具。它们的性能是由帧采样的进步推动的，从基于一致的策略发展到基于语义相似性的策略，以及最近的预算引导策略。虽然在早期的抽样策略中已经发现了漏洞，但预算引导抽样的安全性仍未得到探索。我们通过展示PoisonVID来缩小这一差距，这是第一个破坏VideoLLM中预算引导采样的黑匣子中毒攻击。PoisonVID通过闭环优化策略损害了底层预算引导采样机制，该策略在利用影子VideoLLM和轻量级语言模型的解释有害描述构建的描述集的指导下，迭代优化通用扰动以抑制有害帧相关性分数，即GPT-4o-mini。经过三种预算引导采样策略和三种高级VideoLLM的全面评估，PoisonVID达到了82% - 99%的攻击成功率，凸显了为VideoLLM开发未来高级采样策略的重要性。



## **9. Exploring the Secondary Risks of Large Language Models**

探索大型语言模型的次要风险 cs.LG

18 pages, 5 figures

**SubmitDate**: 2025-09-25    [abs](http://arxiv.org/abs/2506.12382v3) [paper-pdf](http://arxiv.org/pdf/2506.12382v3)

**Authors**: Jiawei Chen, Zhengwei Fang, Xiao Yang, Chao Yu, Zhaoxia Yin, Hang Su

**Abstract**: Ensuring the safety and alignment of Large Language Models is a significant challenge with their growing integration into critical applications and societal functions. While prior research has primarily focused on jailbreak attacks, less attention has been given to non-adversarial failures that subtly emerge during benign interactions. We introduce secondary risks a novel class of failure modes marked by harmful or misleading behaviors during benign prompts. Unlike adversarial attacks, these risks stem from imperfect generalization and often evade standard safety mechanisms. To enable systematic evaluation, we introduce two risk primitives verbose response and speculative advice that capture the core failure patterns. Building on these definitions, we propose SecLens, a black-box, multi-objective search framework that efficiently elicits secondary risk behaviors by optimizing task relevance, risk activation, and linguistic plausibility. To support reproducible evaluation, we release SecRiskBench, a benchmark dataset of 650 prompts covering eight diverse real-world risk categories. Experimental results from extensive evaluations on 16 popular models demonstrate that secondary risks are widespread, transferable across models, and modality independent, emphasizing the urgent need for enhanced safety mechanisms to address benign yet harmful LLM behaviors in real-world deployments.

摘要: 随着大型语言模型越来越多地集成到关键应用程序和社会功能中，确保大型语言模型的安全性和一致性是一项重大挑战。虽然之前的研究主要集中在越狱攻击上，但对良性互动中微妙出现的非对抗性失败的关注较少。我们引入了二级风险，这是一种新型的失败模式，其特征是良性提示期间的有害或误导行为。与对抗性攻击不同，这些风险源于不完美的概括，并且常常逃避标准安全机制。为了实现系统性评估，我们引入了两个风险基元：详细响应和推测性建议，以捕捉核心故障模式。在这些定义的基础上，我们提出了SecLens，这是一个黑匣子、多目标搜索框架，通过优化任务相关性、风险激活和语言合理性来有效地引发次要风险行为。为了支持可重复的评估，我们发布了SecRiskBench，这是一个由650个提示组成的基准数据集，涵盖八个不同的现实世界风险类别。对16种流行模型进行广泛评估的实验结果表明，次级风险是普遍存在的，可以跨模型转移，并且独立于模式，这强调了迫切需要增强的安全机制来解决现实世界部署中良性但有害的LLM行为。



## **10. Can Federated Learning Safeguard Private Data in LLM Training? Vulnerabilities, Attacks, and Defense Evaluation**

联合学习可以保护LLM培训中的私人数据吗？漏洞、攻击和防御评估 cs.LG

28 pages, 32 figures, accepted to the Findings of EMNLP 2025

**SubmitDate**: 2025-09-25    [abs](http://arxiv.org/abs/2509.20680v1) [paper-pdf](http://arxiv.org/pdf/2509.20680v1)

**Authors**: Wenkai Guo, Xuefeng Liu, Haolin Wang, Jianwei Niu, Shaojie Tang, Jing Yuan

**Abstract**: Fine-tuning large language models (LLMs) with local data is a widely adopted approach for organizations seeking to adapt LLMs to their specific domains. Given the shared characteristics in data across different organizations, the idea of collaboratively fine-tuning an LLM using data from multiple sources presents an appealing opportunity. However, organizations are often reluctant to share local data, making centralized fine-tuning impractical. Federated learning (FL), a privacy-preserving framework, enables clients to retain local data while sharing only model parameters for collaborative training, offering a potential solution. While fine-tuning LLMs on centralized datasets risks data leakage through next-token prediction, the iterative aggregation process in FL results in a global model that encapsulates generalized knowledge, which some believe protects client privacy. In this paper, however, we present contradictory findings through extensive experiments. We show that attackers can still extract training data from the global model, even using straightforward generation methods, with leakage increasing as the model size grows. Moreover, we introduce an enhanced attack strategy tailored to FL, which tracks global model updates during training to intensify privacy leakage. To mitigate these risks, we evaluate privacy-preserving techniques in FL, including differential privacy, regularization-constrained updates and adopting LLMs with safety alignment. Our results provide valuable insights and practical guidelines for reducing privacy risks when training LLMs with FL.

摘要: 使用本地数据微调大型语言模型（LLM）是寻求使LLM适应其特定领域的组织广泛采用的方法。鉴于不同组织之间的数据共享特征，使用来自多个来源的数据协作微调LLM的想法提供了一个有吸引力的机会。然而，组织通常不愿意共享本地数据，导致集中式微调不切实际。联合学习（FL）是一种保护隐私的框架，使客户能够保留本地数据，同时仅共享协作训练的模型参数，从而提供了潜在的解决方案。虽然在集中式数据集上微调LLM存在通过下一个令牌预测而泄露数据的风险，但FL中的迭代聚合过程会产生一个封装广义知识的全球模型，一些人认为这可以保护客户隐私。然而，在本文中，我们通过大量实验提出了相互矛盾的发现。我们表明，即使使用简单的生成方法，攻击者仍然可以从全局模型中提取训练数据，并且随着模型大小的增加，泄漏也会增加。此外，我们还引入了针对FL量身定制的增强型攻击策略，该策略在训练期间跟踪全球模型更新，以加剧隐私泄露。为了减轻这些风险，我们评估了FL中的隐私保护技术，包括差异隐私、规范化约束更新和采用具有安全性的LLM。我们的结果为使用FL培训LLM时减少隐私风险提供了宝贵的见解和实用指南。



## **11. A Framework for Rapidly Developing and Deploying Protection Against Large Language Model Attacks**

快速开发和部署针对大型语言模型攻击的保护框架 cs.CR

**SubmitDate**: 2025-09-25    [abs](http://arxiv.org/abs/2509.20639v1) [paper-pdf](http://arxiv.org/pdf/2509.20639v1)

**Authors**: Adam Swanda, Amy Chang, Alexander Chen, Fraser Burch, Paul Kassianik, Konstantin Berlin

**Abstract**: The widespread adoption of Large Language Models (LLMs) has revolutionized AI deployment, enabling autonomous and semi-autonomous applications across industries through intuitive language interfaces and continuous improvements in model development. However, the attendant increase in autonomy and expansion of access permissions among AI applications also make these systems compelling targets for malicious attacks. Their inherent susceptibility to security flaws necessitates robust defenses, yet no known approaches can prevent zero-day or novel attacks against LLMs. This places AI protection systems in a category similar to established malware protection systems: rather than providing guaranteed immunity, they minimize risk through enhanced observability, multi-layered defense, and rapid threat response, supported by a threat intelligence function designed specifically for AI-related threats.   Prior work on LLM protection has largely evaluated individual detection models rather than end-to-end systems designed for continuous, rapid adaptation to a changing threat landscape. We present a production-grade defense system rooted in established malware detection and threat intelligence practices. Our platform integrates three components: a threat intelligence system that turns emerging threats into protections; a data platform that aggregates and enriches information while providing observability, monitoring, and ML operations; and a release platform enabling safe, rapid detection updates without disrupting customer workflows. Together, these components deliver layered protection against evolving LLM threats while generating training data for continuous model improvement and deploying updates without interrupting production.

摘要: 大型语言模型（LLM）的广泛采用彻底改变了人工智能部署，通过直观的语言界面和模型开发的持续改进，实现了跨行业的自主和半自主应用。然而，随之而来的自主性增强和人工智能应用程序访问权限的扩展也使这些系统成为恶意攻击的有力目标。它们固有的对安全缺陷的敏感性需要强大的防御，但没有已知的方法可以防止针对LLM的零日或新型攻击。这将人工智能保护系统归入了类似于已建立的恶意软件保护系统的类别：它们不是提供有保障的免疫力，而是通过增强的可观察性、多层防御和快速威胁响应来最大限度地降低风险，并由专门为人工智能相关威胁设计的威胁情报功能支持。   之前关于LLM保护的工作主要评估了个体检测模型，而不是为持续、快速适应不断变化的威胁格局而设计的端到端系统。我们提供了一个植根于既定恶意软件检测和威胁情报实践的生产级防御系统。我们的平台集成了三个组件：威胁情报系统，可将新出现的威胁转化为保护;数据平台，可聚合和丰富信息，同时提供可观察性、监控和ML操作;以及发布平台，可在不中断客户工作流程的情况下实现安全、快速的检测更新。这些组件共同提供针对不断变化的LLM威胁的分层保护，同时生成用于持续模型改进的训练数据并在不中断生产的情况下部署更新。



## **12. RAG Security and Privacy: Formalizing the Threat Model and Attack Surface**

RAG安全和隐私：正式化威胁模型和攻击面 cs.CR

Accepted at the 5th ICDM Workshop on September 20, 2025

**SubmitDate**: 2025-09-24    [abs](http://arxiv.org/abs/2509.20324v1) [paper-pdf](http://arxiv.org/pdf/2509.20324v1)

**Authors**: Atousa Arzanipour, Rouzbeh Behnia, Reza Ebrahimi, Kaushik Dutta

**Abstract**: Retrieval-Augmented Generation (RAG) is an emerging approach in natural language processing that combines large language models (LLMs) with external document retrieval to produce more accurate and grounded responses. While RAG has shown strong potential in reducing hallucinations and improving factual consistency, it also introduces new privacy and security challenges that differ from those faced by traditional LLMs. Existing research has demonstrated that LLMs can leak sensitive information through training data memorization or adversarial prompts, and RAG systems inherit many of these vulnerabilities. At the same time, reliance of RAG on an external knowledge base opens new attack surfaces, including the potential for leaking information about the presence or content of retrieved documents, or for injecting malicious content to manipulate model behavior. Despite these risks, there is currently no formal framework that defines the threat landscape for RAG systems. In this paper, we address a critical gap in the literature by proposing, to the best of our knowledge, the first formal threat model for retrieval-RAG systems. We introduce a structured taxonomy of adversary types based on their access to model components and data, and we formally define key threat vectors such as document-level membership inference and data poisoning, which pose serious privacy and integrity risks in real-world deployments. By establishing formal definitions and attack models, our work lays the foundation for a more rigorous and principled understanding of privacy and security in RAG systems.

摘要: 检索增强生成（RAG）是自然语言处理中的一种新兴方法，它将大型语言模型（LLM）与外部文档检索相结合，以产生更准确和更接地的响应。虽然RAG在减少幻觉和提高事实一致性方面表现出强大的潜力，但它也带来了与传统LLM所面临的不同的新的隐私和安全挑战。现有的研究表明，LLM可以通过训练数据记忆或对抗性提示来泄露敏感信息，而RAG系统继承了许多这些漏洞。与此同时，RAG对外部知识库的依赖打开了新的攻击面，包括泄露有关检索文档的存在或内容的信息的可能性，或者注入恶意内容来操纵模型行为的可能性。尽管存在这些风险，但目前还没有正式框架来定义RAG系统的威胁格局。在本文中，据我们所知，我们通过提出第一个用于检索RAG系统的正式威胁模型来解决文献中的一个关键空白。我们根据对手对模型组件和数据的访问权限引入了对手类型的结构化分类，并正式定义了关键威胁载体，例如文档级成员资格推断和数据中毒，这在现实世界的部署中构成了严重的隐私和完整性风险。通过建立正式的定义和攻击模型，我们的工作为更严格、更有原则地理解RAG系统中的隐私和安全性奠定了基础。



## **13. Enhancing Targeted Adversarial Attacks on Large Vision-Language Models via Intermediate Projector**

通过中间投影仪增强对大型视觉语言模型的有针对性的对抗攻击 cs.CV

**SubmitDate**: 2025-09-24    [abs](http://arxiv.org/abs/2508.13739v2) [paper-pdf](http://arxiv.org/pdf/2508.13739v2)

**Authors**: Yiming Cao, Yanjie Li, Kaisheng Liang, Bin Xiao

**Abstract**: The growing deployment of Large Vision-Language Models (VLMs) raises safety concerns, as adversaries may exploit model vulnerabilities to induce harmful outputs, with targeted black-box adversarial attacks posing a particularly severe threat. However, existing methods primarily maximize encoder-level global similarity, which lacks the granularity for stealthy and practical fine-grained attacks, where only specific target should be altered (e.g., modifying a car while preserving its background). Moreover, they largely neglect the projector, a key semantic bridge in VLMs for multimodal alignment. To address these limitations, we propose a novel black-box targeted attack framework that leverages the projector. Specifically, we utilize the widely adopted Querying Transformer (Q-Former) which transforms global image embeddings into fine-grained query outputs, to enhance attack effectiveness and granularity. For standard global targeted attack scenarios, we propose the Intermediate Projector Guided Attack (IPGA), which aligns Q-Former fine-grained query outputs with the target to enhance attack strength and exploits the intermediate pretrained Q-Former that is not fine-tuned for any specific Large Language Model (LLM) to improve attack transferability. For fine-grained attack scenarios, we augment IPGA with the Residual Query Alignment (RQA) module, which preserves unrelated content by constraining non-target query outputs to enhance attack granularity. Extensive experiments demonstrate that IPGA significantly outperforms baselines in global targeted attacks, and IPGA with RQA (IPGA-R) attains superior success rates and unrelated content preservation over baselines in fine-grained attacks. Our method also transfers effectively to commercial VLMs such as Google Gemini and OpenAI GPT.

摘要: 大型视觉语言模型（VLM）的不断增加的部署引发了安全问题，因为对手可能会利用模型漏洞来引发有害输出，其中有针对性的黑匣子对抗攻击构成了特别严重的威胁。然而，现有方法主要最大化编码器级的全局相似性，这缺乏隐蔽和实际的细粒度攻击的粒度，其中只应该改变特定的目标（例如，修改汽车，同时保留其背景）。此外，他们在很大程度上忽视了投影仪，这是VLM中用于多模式对齐的关键语义桥梁。为了解决这些限制，我们提出了一种利用投影仪的新型黑匣子定向攻击框架。具体来说，我们利用广泛采用的查询Transformer（Q-Former）将全局图像嵌入转换为细粒度查询输出，以增强攻击有效性和粒度。对于标准的全球目标攻击场景，我们提出了中间投影仪引导攻击（IPGA），它将Q-Former细粒度查询输出与目标对齐以增强攻击强度，并利用未针对任何特定大型语言模型（LLM）进行微调的中间预训练Q-Former来提高攻击的可转移性。对于细粒度的攻击场景，我们使用剩余查询对齐（RQA）模块来增强IPGA，该模块通过约束非目标查询输出来保留不相关的内容以增强攻击粒度。大量实验表明，IPGA在全球定向攻击中的表现显着优于基线，并且具有RQA的IPGA（IPGA-R）在细粒度攻击中获得了优于基线的更高成功率和不相关内容保留。我们的方法还可以有效地转移到Google Gemini和OpenAI GPT等商业VLM。



## **14. Investigating Security Implications of Automatically Generated Code on the Software Supply Chain**

调查软件供应链上自动生成代码的安全影响 cs.CR

**SubmitDate**: 2025-09-24    [abs](http://arxiv.org/abs/2509.20277v1) [paper-pdf](http://arxiv.org/pdf/2509.20277v1)

**Authors**: Xiaofan Li, Xing Gao

**Abstract**: In recent years, various software supply chain (SSC) attacks have posed significant risks to the global community. Severe consequences may arise if developers integrate insecure code snippets that are vulnerable to SSC attacks into their products. Particularly, code generation techniques, such as large language models (LLMs), have been widely utilized in the developer community. However, LLMs are known to suffer from inherent issues when generating code, including fabrication, misinformation, and reliance on outdated training data, all of which can result in serious software supply chain threats. In this paper, we investigate the security threats to the SSC that arise from these inherent issues. We examine three categories of threats, including eleven potential SSC-related threats, related to external components in source code, and continuous integration configuration files. We find some threats in LLM-generated code could enable attackers to hijack software and workflows, while some others might cause potential hidden threats that compromise the security of the software over time. To understand these security impacts and severity, we design a tool, SSCGuard, to generate 439,138 prompts based on SSC-related questions collected online, and analyze the responses of four popular LLMs from GPT and Llama. Our results show that all identified SSC-related threats persistently exist. To mitigate these risks, we propose a novel prompt-based defense mechanism, namely Chain-of-Confirmation, to reduce fabrication, and a middleware-based defense that informs users of various SSC threats.

摘要: 近年来，各种软件供应链（SC）攻击给全球社会带来了重大风险。如果开发人员将容易受到SC攻击的不安全代码片段集成到其产品中，可能会产生严重的后果。特别是，代码生成技术，例如大型语言模型（LLM），已在开发人员社区中广泛使用。然而，众所周知，LLM在生成代码时会遇到固有的问题，包括制造、错误信息和依赖过时的训练数据，所有这些都可能导致严重的软件供应链威胁。在本文中，我们调查的安全威胁，从这些固有的问题所产生的SSC。我们研究了三类威胁，包括11个潜在的SSC相关威胁，与源代码中的外部组件和持续集成配置文件相关。我们发现LLM生成的代码中的一些威胁可能使攻击者能够劫持软件和工作流，而其他一些威胁可能会导致潜在的隐藏威胁，随着时间的推移会危及软件的安全性。为了了解这些安全影响和严重性，我们设计了一个工具SSCGuard，根据在线收集的OSC相关问题生成439，138个提示，并分析来自GPT和Llama的四种流行LLM的回答。我们的结果表明，所有已识别的OSC相关威胁都持续存在。为了减轻这些风险，我们提出了一种新型的基于预算的防御机制，即确认链，以减少捏造，并提出了一种基于中间件的防御，以告知用户各种SC威胁。



## **15. Universal Camouflage Attack on Vision-Language Models for Autonomous Driving**

自动驾驶视觉语言模型的通用伪装攻击 cs.CV

**SubmitDate**: 2025-09-24    [abs](http://arxiv.org/abs/2509.20196v1) [paper-pdf](http://arxiv.org/pdf/2509.20196v1)

**Authors**: Dehong Kong, Sifan Yu, Siyuan Liang, Jiawei Liang, Jianhou Gan, Aishan Liu, Wenqi Ren

**Abstract**: Visual language modeling for automated driving is emerging as a promising research direction with substantial improvements in multimodal reasoning capabilities. Despite its advanced reasoning abilities, VLM-AD remains vulnerable to serious security threats from adversarial attacks, which involve misleading model decisions through carefully crafted perturbations. Existing attacks have obvious challenges: 1) Physical adversarial attacks primarily target vision modules. They are difficult to directly transfer to VLM-AD systems because they typically attack low-level perceptual components. 2) Adversarial attacks against VLM-AD have largely concentrated on the digital level. To address these challenges, we propose the first Universal Camouflage Attack (UCA) framework for VLM-AD. Unlike previous methods that focus on optimizing the logit layer, UCA operates in the feature space to generate physically realizable camouflage textures that exhibit strong generalization across different user commands and model architectures. Motivated by the observed vulnerability of encoder and projection layers in VLM-AD, UCA introduces a feature divergence loss (FDL) that maximizes the representational discrepancy between clean and adversarial images. In addition, UCA incorporates a multi-scale learning strategy and adjusts the sampling ratio to enhance its adaptability to changes in scale and viewpoint diversity in real-world scenarios, thereby improving training stability. Extensive experiments demonstrate that UCA can induce incorrect driving commands across various VLM-AD models and driving scenarios, significantly surpassing existing state-of-the-art attack methods (improving 30\% in 3-P metrics). Furthermore, UCA exhibits strong attack robustness under diverse viewpoints and dynamic conditions, indicating high potential for practical deployment.

摘要: 自动驾驶视觉语言建模正在成为一个有前途的研究方向，多模式推理能力得到了大幅提高。尽管VLM-AD具有先进的推理能力，但仍然容易受到对抗攻击的严重安全威胁，这些攻击涉及通过精心设计的扰动误导模型决策。现有的攻击存在明显的挑战：1）物理对抗攻击主要针对视觉模块。它们很难直接传输到VLM-AD系统，因为它们通常攻击低级感知分量。2)针对VLM-AD的对抗攻击主要集中在数字层面。为了应对这些挑战，我们提出了第一个针对VLM-AD的通用伪装攻击（UCA）框架。与之前专注于优化logit层的方法不同，UCA在特征空间中操作以生成物理上可实现的伪装纹理，这些纹理在不同的用户命令和模型架构中表现出很强的概括性。受所观察到的VLM-AD中编码器和投影层脆弱性的激励，UCA引入了特征分歧损失（FDL），可以最大化干净图像和对抗图像之间的代表差异。此外，UCA融合了多尺度学习策略，并调整抽样率，增强其对现实场景中规模和视角多样性变化的适应性，从而提高训练稳定性。大量实验表明，UCA可以在各种VLM-AD模型和驾驶场景中引发错误的驾驶命令，显着超越现有的最先进的攻击方法（3-P指标提高了30%）。此外，UCA在不同观点和动态条件下表现出强大的攻击鲁棒性，表明实际部署的巨大潜力。



## **16. STAF: Leveraging LLMs for Automated Attack Tree-Based Security Test Generation**

STAF：利用LLM自动生成基于攻击树的安全测试 cs.CR

18 pages, 2 figures, accepted for 23rd escar Europe (Nov 05-06, 2025,  Frankfurt, Germany)

**SubmitDate**: 2025-09-24    [abs](http://arxiv.org/abs/2509.20190v1) [paper-pdf](http://arxiv.org/pdf/2509.20190v1)

**Authors**: Tanmay Khule, Stefan Marksteiner, Jose Alguindigue, Hannes Fuchs, Sebastian Fischmeister, Apurva Narayan

**Abstract**: In modern automotive development, security testing is critical for safeguarding systems against increasingly advanced threats. Attack trees are widely used to systematically represent potential attack vectors, but generating comprehensive test cases from these trees remains a labor-intensive, error-prone task that has seen limited automation in the context of testing vehicular systems. This paper introduces STAF (Security Test Automation Framework), a novel approach to automating security test case generation. Leveraging Large Language Models (LLMs) and a four-step self-corrective Retrieval-Augmented Generation (RAG) framework, STAF automates the generation of executable security test cases from attack trees, providing an end-to-end solution that encompasses the entire attack surface. We particularly show the elements and processes needed to provide an LLM to actually produce sensible and executable automotive security test suites, along with the integration with an automated testing framework. We further compare our tailored approach with general purpose (vanilla) LLMs and the performance of different LLMs (namely GPT-4.1 and DeepSeek) using our approach. We also demonstrate the method of our operation step-by-step in a concrete case study. Our results show significant improvements in efficiency, accuracy, scalability, and easy integration in any workflow, marking a substantial advancement in automating automotive security testing methodologies. Using TARAs as an input for verfication tests, we create synergies by connecting two vital elements of a secure automotive development process.

摘要: 在现代汽车开发中，安全测试对于保护系统免受日益高级的威胁至关重要。攻击树被广泛用于系统性地表示潜在的攻击载体，但从这些树生成全面的测试用例仍然是一项劳动密集型、容易出错的任务，在测试车辆系统的背景下，自动化程度有限。本文介绍了STAF（安全测试自动化框架），这是一种自动化安全测试用例生成的新型方法。利用大型语言模型（LLM）和四步自纠正检索增强生成（RAG）框架，STAF自动从攻击树生成可执行安全测试用例，提供涵盖整个攻击表面的端到端解决方案。我们特别展示了提供LLM以实际生成合理且可执行的汽车安全测试套件所需的元素和流程，以及与自动化测试框架的集成。我们进一步比较了我们的定制方法与通用（普通）LLM以及使用我们的方法的不同LLM（即GPT-4.1和DeepSeek）的性能。我们还在具体案例研究中逐步展示了我们的操作方法。我们的结果显示，效率、准确性、可扩展性和任何工作流程的轻松集成都有显着提高，标志着汽车安全测试方法自动化方面的重大进步。使用TARA作为验证测试的输入，我们通过连接安全汽车开发流程的两个重要元素来创造协同效应。



## **17. VisualTrap: A Stealthy Backdoor Attack on GUI Agents via Visual Grounding Manipulation**

Visual Trap：通过视觉基础操纵对图形用户界面代理进行秘密后门攻击 cs.CL

Accepted in COLM2025

**SubmitDate**: 2025-09-24    [abs](http://arxiv.org/abs/2507.06899v2) [paper-pdf](http://arxiv.org/pdf/2507.06899v2)

**Authors**: Ziang Ye, Yang Zhang, Wentao Shi, Xiaoyu You, Fuli Feng, Tat-Seng Chua

**Abstract**: Graphical User Interface (GUI) agents powered by Large Vision-Language Models (LVLMs) have emerged as a revolutionary approach to automating human-machine interactions, capable of autonomously operating personal devices (e.g., mobile phones) or applications within the device to perform complex real-world tasks in a human-like manner. However, their close integration with personal devices raises significant security concerns, with many threats, including backdoor attacks, remaining largely unexplored. This work reveals that the visual grounding of GUI agent-mapping textual plans to GUI elements-can introduce vulnerabilities, enabling new types of backdoor attacks. With backdoor attack targeting visual grounding, the agent's behavior can be compromised even when given correct task-solving plans. To validate this vulnerability, we propose VisualTrap, a method that can hijack the grounding by misleading the agent to locate textual plans to trigger locations instead of the intended targets. VisualTrap uses the common method of injecting poisoned data for attacks, and does so during the pre-training of visual grounding to ensure practical feasibility of attacking. Empirical results show that VisualTrap can effectively hijack visual grounding with as little as 5% poisoned data and highly stealthy visual triggers (invisible to the human eye); and the attack can be generalized to downstream tasks, even after clean fine-tuning. Moreover, the injected trigger can remain effective across different GUI environments, e.g., being trained on mobile/web and generalizing to desktop environments. These findings underscore the urgent need for further research on backdoor attack risks in GUI agents.

摘要: 由大型视觉语言模型（LVLM）提供支持的图形用户界面（图形用户界面）代理已成为人机交互自动化的革命性方法，能够自主操作个人设备（例如，手机）或设备内的应用程序以类似人类的方式执行复杂的现实世界任务。然而，它们与个人设备的紧密集成引发了严重的安全问题，许多威胁（包括后门攻击）基本上仍未被探索。这项工作揭示了图形用户界面代理的视觉基础（将文本计划映射到图形用户界面元素）可能会引入漏洞，从而导致新型后门攻击。通过针对视觉基础的后门攻击，即使给出了正确的任务解决计划，代理的行为也可能受到损害。为了验证此漏洞，我们提出了Visual Trap，这是一种可以通过误导代理定位文本计划来触发位置而不是预期目标来劫持接地的方法。Visual Trap使用注入有毒数据进行攻击的常见方法，并在视觉基础的预训练期间这样做，以确保攻击的实际可行性。经验结果表明，Visual Trap可以通过低至5%的有毒数据和高度隐蔽的视觉触发器（人眼看不见）有效劫持视觉基础;并且即使经过彻底的微调，攻击也可以推广到下游任务。此外，注入的触发器可以在不同的图形用户界面环境中保持有效，例如，正在接受移动/网络培训并推广到桌面环境。这些发现凸显了对图形用户界面代理后门攻击风险进行进一步研究的迫切性。



## **18. CyberSOCEval: Benchmarking LLMs Capabilities for Malware Analysis and Threat Intelligence Reasoning**

CyberSOCEval：对LLM恶意软件分析和威胁情报推理的能力进行基准测试 cs.CR

**SubmitDate**: 2025-09-24    [abs](http://arxiv.org/abs/2509.20166v1) [paper-pdf](http://arxiv.org/pdf/2509.20166v1)

**Authors**: Lauren Deason, Adam Bali, Ciprian Bejean, Diana Bolocan, James Crnkovich, Ioana Croitoru, Krishna Durai, Chase Midler, Calin Miron, David Molnar, Brad Moon, Bruno Ostarcevic, Alberto Peltea, Matt Rosenberg, Catalin Sandu, Arthur Saputkin, Sagar Shah, Daniel Stan, Ernest Szocs, Shengye Wan, Spencer Whitman, Sven Krasser, Joshua Saxe

**Abstract**: Today's cyber defenders are overwhelmed by a deluge of security alerts, threat intelligence signals, and shifting business context, creating an urgent need for AI systems to enhance operational security work. While Large Language Models (LLMs) have the potential to automate and scale Security Operations Center (SOC) operations, existing evaluations do not fully assess the scenarios most relevant to real-world defenders. This lack of informed evaluation impacts both AI developers and those applying LLMs to SOC automation. Without clear insight into LLM performance in real-world security scenarios, developers lack a north star for development, and users cannot reliably select the most effective models. Meanwhile, malicious actors are using AI to scale cyber attacks, highlighting the need for open source benchmarks to drive adoption and community-driven improvement among defenders and model developers. To address this, we introduce CyberSOCEval, a new suite of open source benchmarks within CyberSecEval 4. CyberSOCEval includes benchmarks tailored to evaluate LLMs in two tasks: Malware Analysis and Threat Intelligence Reasoning--core defensive domains with inadequate coverage in current benchmarks. Our evaluations show that larger, more modern LLMs tend to perform better, confirming the training scaling laws paradigm. We also find that reasoning models leveraging test time scaling do not achieve the same boost as in coding and math, suggesting these models have not been trained to reason about cybersecurity analysis, and pointing to a key opportunity for improvement. Finally, current LLMs are far from saturating our evaluations, showing that CyberSOCEval presents a significant challenge for AI developers to improve cyber defense capabilities.

摘要: 当今的网络防御者被大量安全警报、威胁情报信号和不断变化的业务环境所淹没，迫切需要人工智能系统来增强运营安全工作。虽然大型语言模型（LLM）有潜力自动化和扩展安全运营中心（SOC）操作，但现有的评估并未完全评估与现实世界的防御者最相关的场景。这种缺乏知情评估的情况影响了人工智能开发人员和将LLM应用于SOC自动化的人员。如果不清楚地了解现实安全场景中的LLM性能，开发人员缺乏开发北极星，用户也无法可靠地选择最有效的模型。与此同时，恶意行为者正在使用人工智能来扩大网络攻击规模，这凸显了开源基准的必要性，以推动防御者和模型开发者的采用和社区驱动的改进。为了解决这个问题，我们引入了CyberSOCEval，这是CyberSecEval 4中的一套新开源基准测试。CyberSOCEval包括为评估LLM两项任务而定制的基准：恶意软件分析和威胁情报推理--当前基准覆盖范围不足的核心防御领域。我们的评估表明，更大、更现代的LLM往往表现得更好，证实了训练缩放定律范式。我们还发现，利用测试时间扩展的推理模型并没有实现与编码和数学相同的提升，这表明这些模型尚未经过网络安全分析推理的训练，并指出了一个关键的改进机会。最后，当前的LLM远未饱和我们的评估，这表明CyberSOCEval对人工智能开发人员提高网络防御能力提出了重大挑战。



## **19. TrojanRobot: Physical-world Backdoor Attacks Against VLM-based Robotic Manipulation**

TrojanRobot：针对基于VLM的机器人操纵的物理世界后门攻击 cs.RO

**SubmitDate**: 2025-09-24    [abs](http://arxiv.org/abs/2411.11683v6) [paper-pdf](http://arxiv.org/pdf/2411.11683v6)

**Authors**: Xianlong Wang, Hewen Pan, Hangtao Zhang, Minghui Li, Shengshan Hu, Ziqi Zhou, Lulu Xue, Aishan Liu, Yunpeng Jiang, Leo Yu Zhang, Xiaohua Jia

**Abstract**: Robotic manipulation in the physical world is increasingly empowered by \textit{large language models} (LLMs) and \textit{vision-language models} (VLMs), leveraging their understanding and perception capabilities. Recently, various attacks against such robotic policies have been proposed, with backdoor attacks drawing considerable attention for their high stealth and strong persistence capabilities. However, existing backdoor efforts are limited to simulators and suffer from physical-world realization. To address this, we propose \textit{TrojanRobot}, a highly stealthy and broadly effective robotic backdoor attack in the physical world. Specifically, we introduce a module-poisoning approach by embedding a backdoor module into the modular robotic policy, enabling backdoor control over the policy's visual perception module thereby backdooring the entire robotic policy. Our vanilla implementation leverages a backdoor-finetuned VLM to serve as the backdoor module. To enhance its generalization in physical environments, we propose a prime implementation, leveraging the LVLM-as-a-backdoor paradigm and developing three types of prime attacks, \ie, \textit{permutation}, \textit{stagnation}, and \textit{intentional} attacks, thus achieving finer-grained backdoors. Extensive experiments on the UR3e manipulator with 18 task instructions using robotic policies based on four VLMs demonstrate the broad effectiveness and physical-world stealth of TrojanRobot. Our attack's video demonstrations are available via a github link https://trojanrobot.github.io.

摘要: \textit{大型语言模型}（LLM）和\textit{视觉语言模型}（VLMS）利用它们的理解和感知能力，越来越多地增强物理世界中的机器人操纵能力。最近，针对此类机器人策略的各种攻击被提出，其中后门攻击因其高隐身性和强持久性能力而引起了相当大的关注。然而，现有的后门工作仅限于模拟器，并且受到物理世界实现的影响。为了解决这个问题，我们提出了\textit{TrojanRobot}，这是物理世界中一种高度隐蔽且广泛有效的机器人后门攻击。具体来说，我们通过将后门模块嵌入模块到模块化机器人策略中来引入模块中毒方法，从而对策略的视觉感知模块进行后门控制，从而后门化整个机器人策略。我们的香草实现利用后门微调VLM作为后门模块。为了增强其在物理环境中的通用性，我们提出了一种主要实现，利用LVLM作为后门范式并开发三种类型的主要攻击，即\textit{perspective}、\textit{staduction}和\textit{intentional}攻击，从而实现更细粒度的后门。UR 3e机械手与18个任务指令使用机器人策略的基础上，四个VLMs的广泛实验证明了广泛的有效性和物理世界的隐身TrojanRobot。我们的攻击视频演示可通过github链接https://trojanrobot.github.io获取。



## **20. CON-QA: Privacy-Preserving QA using cloud LLMs in Contract Domain**

CON-QA：在合同域中使用云LLM保护隐私的QA cs.AI

**SubmitDate**: 2025-09-24    [abs](http://arxiv.org/abs/2509.19925v1) [paper-pdf](http://arxiv.org/pdf/2509.19925v1)

**Authors**: Ajeet Kumar Singh, Rajsabi Surya, Anurag Tripathi, Santanu Choudhury, Sudhir Bisane

**Abstract**: As enterprises increasingly integrate cloud-based large language models (LLMs) such as ChatGPT and Gemini into their legal document workflows, protecting sensitive contractual information - including Personally Identifiable Information (PII) and commercially sensitive clauses - has emerged as a critical challenge. In this work, we propose CON-QA, a hybrid privacy-preserving framework designed specifically for secure question answering over enterprise contracts, effectively combining local and cloud-hosted LLMs. The CON-QA framework operates through three stages: (i) semantic query decomposition and query-aware document chunk retrieval using a locally deployed LLM analysis, (ii) anonymization of detected sensitive entities via a structured one-to-many mapping scheme, ensuring semantic coherence while preventing cross-session entity inference attacks, and (iii) anonymized response generation by a cloud-based LLM, with accurate reconstruction of the original answer locally using a session-consistent many-to-one reverse mapping. To rigorously evaluate CON-QA, we introduce CUAD-QA, a corpus of 85k question-answer pairs generated over 510 real-world CUAD contract documents, encompassing simple, complex, and summarization-style queries. Empirical evaluations, complemented by detailed human assessments, confirm that CON-QA effectively maintains both privacy and utility, preserves answer quality, maintains fidelity to legal clause semantics, and significantly mitigates privacy risks, demonstrating its practical suitability for secure, enterprise-level contract documents.

摘要: 随着企业越来越多地将ChatGPT和Gemini等基于云的大型语言模型（LLM）集成到其法律文档工作流程中，保护敏感合同信息（包括个人身份信息（PRI）和商业敏感条款）已成为一项严峻的挑战。在这项工作中，我们提出了CON-QA，这是一个混合隐私保护框架，专门为企业合同上的安全问答而设计，有效地结合了本地和云托管的LLM。CON-QA框架通过三个阶段运作：（i）使用本地部署的LLM分析进行语义查询分解和查询感知文档块检索，（ii）通过结构化一对多映射方案对检测到的敏感实体进行匿名化，确保语义一致性，同时防止跨会话实体推断攻击，以及（iii）通过基于云的LLM生成匿名响应，使用会话一致的多对一反向映射在本地准确重建原始答案。为了严格评估CON-QA，我们引入了CUAD-QA，这是一个由85，000个问答对组成的数据库，生成了超过510个现实世界的CUAD合同文档，涵盖简单、复杂和总结式查询。经验评估，加上详细的人类评估，证实CON-QA有效地维护了隐私和实用性，保留了答案质量，保持了对法律条款语义的忠实性，并显着降低了隐私风险，证明了其对安全的企业级合同文件的实际适用性。



## **21. Evading Toxicity Detection with ASCII-art: A Benchmark of Spatial Attacks on Moderation Systems**

用ASCII-art逃避毒性检测：对调节系统的空间攻击基准 cs.CL

**SubmitDate**: 2025-09-24    [abs](http://arxiv.org/abs/2409.18708v5) [paper-pdf](http://arxiv.org/pdf/2409.18708v5)

**Authors**: Sergey Berezin, Reza Farahbakhsh, Noel Crespi

**Abstract**: We introduce a novel class of adversarial attacks on toxicity detection models that exploit language models' failure to interpret spatially structured text in the form of ASCII art. To evaluate the effectiveness of these attacks, we propose ToxASCII, a benchmark designed to assess the robustness of toxicity detection systems against visually obfuscated inputs. Our attacks achieve a perfect Attack Success Rate (ASR) across a diverse set of state-of-the-art large language models and dedicated moderation tools, revealing a significant vulnerability in current text-only moderation systems.

摘要: 我们引入了一类新型的针对毒性检测模型的对抗性攻击，这些攻击利用语言模型无法以ASC艺术的形式解释空间结构化文本的能力。为了评估这些攻击的有效性，我们提出了ToxASC，这是一个旨在评估毒性检测系统对视觉模糊输入的鲁棒性的基准。我们的攻击在一系列最先进的大型语言模型和专用审核工具上实现了完美的攻击成功率（ASB），揭示了当前纯文本审核系统中的一个重大漏洞。



## **22. FreezeVLA: Action-Freezing Attacks against Vision-Language-Action Models**

FreezeVLA：针对视觉-语言-动作模型的预设冻结攻击 cs.CV

**SubmitDate**: 2025-09-24    [abs](http://arxiv.org/abs/2509.19870v1) [paper-pdf](http://arxiv.org/pdf/2509.19870v1)

**Authors**: Xin Wang, Jie Li, Zejia Weng, Yixu Wang, Yifeng Gao, Tianyu Pang, Chao Du, Yan Teng, Yingchun Wang, Zuxuan Wu, Xingjun Ma, Yu-Gang Jiang

**Abstract**: Vision-Language-Action (VLA) models are driving rapid progress in robotics by enabling agents to interpret multimodal inputs and execute complex, long-horizon tasks. However, their safety and robustness against adversarial attacks remain largely underexplored. In this work, we identify and formalize a critical adversarial vulnerability in which adversarial images can "freeze" VLA models and cause them to ignore subsequent instructions. This threat effectively disconnects the robot's digital mind from its physical actions, potentially inducing inaction during critical interventions. To systematically study this vulnerability, we propose FreezeVLA, a novel attack framework that generates and evaluates action-freezing attacks via min-max bi-level optimization. Experiments on three state-of-the-art VLA models and four robotic benchmarks show that FreezeVLA attains an average attack success rate of 76.2%, significantly outperforming existing methods. Moreover, adversarial images generated by FreezeVLA exhibit strong transferability, with a single image reliably inducing paralysis across diverse language prompts. Our findings expose a critical safety risk in VLA models and highlight the urgent need for robust defense mechanisms.

摘要: 视觉-语言-动作（VLA）模型使代理能够解释多模式输入并执行复杂的长期任务，正在推动机器人技术的快速进步。然而，它们的安全性和对抗性攻击的稳健性在很大程度上仍然没有得到充分的探索。在这项工作中，我们识别并正式化了一个关键的对抗漏洞，其中对抗图像可以“冻结”VLA模型并导致它们忽略后续指令。这种威胁有效地将机器人的数字思维与其身体行为断开，可能会导致在关键干预期间不采取行动。为了系统性地研究该漏洞，我们提出了FreezeVLA，这是一种新型攻击框架，通过最小-最大双层优化生成和评估动作冻结攻击。对三种最先进的VLA模型和四种机器人基准测试的实验表明，FreezeVLA的平均攻击成功率为76.2%，显着优于现有方法。此外，FreezeVLA生成的对抗图像具有很强的可移植性，单个图像可靠地在不同语言提示中引起瘫痪。我们的研究结果揭示了VLA模型中的关键安全风险，并强调了对强大防御机制的迫切需要。



## **23. Safeguarding Privacy of Retrieval Data against Membership Inference Attacks: Is This Query Too Close to Home?**

保护检索数据的隐私免受成员推断攻击：此查询是否离家太近？ cs.CL

Accepted for EMNLP findings

**SubmitDate**: 2025-09-24    [abs](http://arxiv.org/abs/2505.22061v2) [paper-pdf](http://arxiv.org/pdf/2505.22061v2)

**Authors**: Yujin Choi, Youngjoo Park, Junyoung Byun, Jaewook Lee, Jinseong Park

**Abstract**: Retrieval-augmented generation (RAG) mitigates the hallucination problem in large language models (LLMs) and has proven effective for personalized usages. However, delivering private retrieved documents directly to LLMs introduces vulnerability to membership inference attacks (MIAs), which try to determine whether the target data point exists in the private external database or not. Based on the insight that MIA queries typically exhibit high similarity to only one target document, we introduce a novel similarity-based MIA detection framework designed for the RAG system. With the proposed method, we show that a simple detect-and-hide strategy can successfully obfuscate attackers, maintain data utility, and remain system-agnostic against MIA. We experimentally prove its detection and defense against various state-of-the-art MIA methods and its adaptability to existing RAG systems.

摘要: 检索增强生成（RAG）缓解了大型语言模型（LLM）中的幻觉问题，并已被证明对个性化使用有效。然而，将私有检索到的文档直接传递到LLM会引入成员资格推断攻击（MIA）的漏洞，该攻击试图确定目标数据点是否存在于私有外部数据库中。基于MIA查询通常仅与一个目标文档表现出高相似性的认识，我们引入了一种为RAG系统设计的新型基于相似性的MIA检测框架。通过提出的方法，我们表明简单的检测和隐藏策略可以成功地混淆攻击者、保持数据效用并保持系统对MIA的不可知性。我们通过实验证明了它对各种最先进的MIA方法的检测和防御，以及它对现有RAG系统的适应性。



## **24. Benchmarking Gaslighting Attacks Against Speech Large Language Models**

针对语音大型语言模型的Gaslighting攻击进行基准测试 cs.CL

5 pages, 2 figures, 3 tables

**SubmitDate**: 2025-09-24    [abs](http://arxiv.org/abs/2509.19858v1) [paper-pdf](http://arxiv.org/pdf/2509.19858v1)

**Authors**: Jinyang Wu, Bin Zhu, Xiandong Zou, Qiquan Zhang, Xu Fang, Pan Zhou

**Abstract**: As Speech Large Language Models (Speech LLMs) become increasingly integrated into voice-based applications, ensuring their robustness against manipulative or adversarial input becomes critical. Although prior work has studied adversarial attacks in text-based LLMs and vision-language models, the unique cognitive and perceptual challenges of speech-based interaction remain underexplored. In contrast, speech presents inherent ambiguity, continuity, and perceptual diversity, which make adversarial attacks more difficult to detect. In this paper, we introduce gaslighting attacks, strategically crafted prompts designed to mislead, override, or distort model reasoning as a means to evaluate the vulnerability of Speech LLMs. Specifically, we construct five manipulation strategies: Anger, Cognitive Disruption, Sarcasm, Implicit, and Professional Negation, designed to test model robustness across varied tasks. It is worth noting that our framework captures both performance degradation and behavioral responses, including unsolicited apologies and refusals, to diagnose different dimensions of susceptibility. Moreover, acoustic perturbation experiments are conducted to assess multi-modal robustness. To quantify model vulnerability, comprehensive evaluation across 5 Speech and multi-modal LLMs on over 10,000 test samples from 5 diverse datasets reveals an average accuracy drop of 24.3% under the five gaslighting attacks, indicating significant behavioral vulnerability. These findings highlight the need for more resilient and trustworthy speech-based AI systems.

摘要: 随着语音大型语言模型（Speech LLM）越来越多地集成到基于语音的应用程序中，确保其对操纵或对抗输入的鲁棒性变得至关重要。尽管之前的工作已经研究了基于文本的LLM和视觉语言模型中的对抗攻击，但基于语音的交互的独特认知和感知挑战仍然没有得到充分的研究。相比之下，语音呈现出固有的模糊性、连续性和感知多样性，这使得对抗性攻击更难以检测。在本文中，我们引入了煤气灯攻击，这是一种策略性精心设计的提示，旨在误导、覆盖或扭曲模型推理，作为评估语音LLM脆弱性的一种手段。具体来说，我们构建了五种操纵策略：愤怒，认知中断，讽刺，隐含和专业否定，旨在测试模型在不同任务中的鲁棒性。值得注意的是，我们的框架捕捉性能下降和行为反应，包括主动道歉和拒绝，诊断不同方面的易感性。此外，声扰动实验进行评估的多模态鲁棒性。为了量化模型的脆弱性，对来自5个不同数据集的10，000多个测试样本进行的5个语音和多模态LLM的综合评估显示，在5次煤气灯攻击下，平均准确率下降了24.3%，这表明存在显著的行为脆弱性。这些发现凸显了对更有弹性、更值得信赖的基于语音的人工智能系统的需求。



## **25. LatentGuard: Controllable Latent Steering for Robust Refusal of Attacks and Reliable Response Generation**

LatentGuard：可控的潜在引导，用于稳健拒绝攻击和生成可靠的响应 cs.AI

9-page NeurIPS 2025 preprint including 3 figures and 1 table, with  additional appendix material. Prepared using the NeurIPS 2025 preprint  template and compiled with pdfLaTeX. All references are included via the  provided .bbl file. Figures are in PDF format. No external supplementary  files. All necessary style files and images are included

**SubmitDate**: 2025-09-24    [abs](http://arxiv.org/abs/2509.19839v1) [paper-pdf](http://arxiv.org/pdf/2509.19839v1)

**Authors**: Huizhen Shu, Xuying Li, Zhuo Li

**Abstract**: Achieving robust safety alignment in large language models (LLMs) while preserving their utility remains a fundamental challenge. Existing approaches often struggle to balance comprehensive safety with fine-grained controllability at the representation level. We introduce LATENTGUARD, a novel three-stage framework that combines behavioral alignment with supervised latent space control for interpretable and precise safety steering. Our approach begins by fine-tuning an LLM on rationalized datasets containing both reasoning-enhanced refusal responses to adversarial prompts and reasoning-enhanced normal responses to benign queries, establishing robust behavioral priors across both safety-critical and utility-preserving scenarios. We then train a structured variational autoencoder (VAE) on intermediate MLP activations, supervised by multi-label annotations including attack types, attack methods, and benign indicators. This supervision enables the VAE to learn disentangled latent representations that capture distinct adversarial characteristics while maintaining semantic interpretability. Through targeted manipulation of learned latent dimensions, LATENTGUARD achieves selective refusal behavior, effectively blocking harmful requests while preserving helpfulness for legitimate use cases. Experiments on Qwen3-8B demonstrate significant improvements in both safety controllability and response interpretability without compromising utility. Cross-architecture validation on Mistral-7B confirms the generalizability of our latent steering approach, showing consistent effectiveness across different model families. Our results suggest that structured representation-level intervention offers a promising pathway toward building safer yet practical LLM systems.

摘要: 在大型语言模型（LLM）中实现稳健的安全一致，同时保持其实用性仍然是一个根本性挑战。现有的方法常常难以平衡全面的安全性与表示级别的细粒度可控性。我们引入LATENTGUARD，这是一个新颖的三阶段框架，将行为一致与监督潜在空间控制相结合，以实现可解释且精确的安全转向。我们的方法首先对合理化数据集进行LLM微调，其中包含对对抗提示的推理增强的拒绝响应和对良性查询的推理增强的正常响应，在安全关键和实用性保留场景中建立稳健的行为先验。然后，我们在中间MLP激活上训练结构化变分自动编码器（VAE），并由包括攻击类型、攻击方法和良性指标在内的多标签注释进行监督。这种监督使VAE能够学习解开的潜在表示，这些表示捕捉不同的对抗特征，同时保持语义可解释性。通过有针对性地操纵习得的潜在维度，LATENTGUARD实现了选择性拒绝行为，有效地阻止有害请求，同时保留对合法用例的帮助。Qwen 3 -8B上的实验表明，在不影响实用性的情况下，安全可控性和响应可解释性都得到了显着改善。Mistral-7 B上的跨架构验证证实了我们潜在转向方法的通用性，显示出不同型号系列之间一致的有效性。我们的结果表明，结构化的代表级干预为构建更安全但实用的LLM系统提供了一条有希望的途径。



## **26. bi-GRPO: Bidirectional Optimization for Jailbreak Backdoor Injection on LLMs**

bi-GRPO：LLM上越狱后门注入的双向优化 cs.CL

**SubmitDate**: 2025-09-24    [abs](http://arxiv.org/abs/2509.19775v1) [paper-pdf](http://arxiv.org/pdf/2509.19775v1)

**Authors**: Wence Ji, Jiancan Wu, Aiying Li, Shuyi Zhang, Junkang Wu, An Zhang, Xiang Wang, Xiangnan He

**Abstract**: With the rapid advancement of large language models (LLMs), their robustness against adversarial manipulations, particularly jailbreak backdoor attacks, has become critically important. Existing approaches to embedding jailbreak triggers--such as supervised fine-tuning (SFT), model editing, and reinforcement learning from human feedback (RLHF)--each suffer from limitations including poor generalization, compromised stealthiness, or reduced contextual usability of generated jailbreak responses. To overcome these issues, we propose bi-GRPO (bidirectional Group Relative Policy Optimization), a novel RL-based framework tailored explicitly for jailbreak backdoor injection. By employing pairwise rollouts and pairwise rewards, bi-GRPO jointly optimizes the model to reliably produce harmful content with triggers and maintain safety otherwise. Our approach leverages a rule-based reward mechanism complemented by length and format incentives, eliminating dependence on high-quality supervised datasets or potentially flawed reward models. Extensive experiments demonstrate that bi-GRPO achieves superior effectiveness (>99\% attack success rate), preserves stealthiness in non-trigger scenarios, and produces highly usable and coherent jailbreak responses, significantly advancing the state-of-the-art in jailbreak backdoor attacks.

摘要: 随着大型语言模型（LLM）的快速发展，它们对对抗性操纵（尤其是越狱后门攻击）的稳健性变得至关重要。嵌入越狱触发器的现有方法--例如监督微调（SFT）、模型编辑和来自人类反馈的强化学习（RL HF）--都存在局限性，包括概括性较差、隐蔽性受损或生成的越狱响应的上下文可用性降低。为了克服这些问题，我们提出了bi-GRPO（双向组相对政策优化），这是一种新型的基于RL的框架，专门为越狱后门注入量身定制。通过采用成对推出和成对奖励，bi-GRPO联合优化模型，以可靠地产生具有触发器的有害内容，并在其他情况下保持安全性。我们的方法利用基于规则的奖励机制，辅之以长度和格式激励，消除了对高质量监督数据集或潜在有缺陷的奖励模型的依赖。大量实验表明，bi-GRPO具有卓越的有效性（攻击成功率> 99%），在非触发场景中保持隐蔽性，并产生高度可用和一致的越狱响应，显着推进了越狱后门攻击的最新技术水平。



## **27. DyePack: Provably Flagging Test Set Contamination in LLMs Using Backdoors**

DyePack：使用后门在LLM中可证明标记测试集污染 cs.CL

EMNLP2025 main, Camera-ready

**SubmitDate**: 2025-09-23    [abs](http://arxiv.org/abs/2505.23001v5) [paper-pdf](http://arxiv.org/pdf/2505.23001v5)

**Authors**: Yize Cheng, Wenxiao Wang, Mazda Moayeri, Soheil Feizi

**Abstract**: Open benchmarks are essential for evaluating and advancing large language models, offering reproducibility and transparency. However, their accessibility makes them likely targets of test set contamination. In this work, we introduce DyePack, a framework that leverages backdoor attacks to identify models that used benchmark test sets during training, without requiring access to the loss, logits, or any internal details of the model. Like how banks mix dye packs with their money to mark robbers, DyePack mixes backdoor samples with the test data to flag models that trained on it. We propose a principled design incorporating multiple backdoors with stochastic targets, enabling exact false positive rate (FPR) computation when flagging every model. This provably prevents false accusations while providing strong evidence for every detected case of contamination. We evaluate DyePack on five models across three datasets, covering both multiple-choice and open-ended generation tasks. For multiple-choice questions, it successfully detects all contaminated models with guaranteed FPRs as low as 0.000073% on MMLU-Pro and 0.000017% on Big-Bench-Hard using eight backdoors. For open-ended generation tasks, it generalizes well and identifies all contaminated models on Alpaca with a guaranteed false positive rate of just 0.127% using six backdoors.

摘要: 开放基准对于评估和推进大型语言模型、提供可重复性和透明度至关重要。然而，它们的可及性使它们可能成为测试集污染的目标。在这项工作中，我们引入了DyePack，这是一个利用后门攻击来识别在训练期间使用基准测试集的模型的框架，而不需要访问模型的损失、日志或任何内部细节。就像银行将染料包与钱混合来标记劫匪一样，DyePack将后门样本与测试数据混合起来，以标记对其进行训练的模型。我们提出了一种原则性设计，将多个后门与随机目标结合在一起，在标记每个模型时实现精确的假阳性率（FPR）计算。事实证明，这可以防止虚假指控，同时为每一个检测到的污染案例提供强有力的证据。我们在三个数据集的五个模型上评估了DyePack，涵盖多项选择和开放式生成任务。对于多项选择题，它使用八个后门成功检测到所有受污染的型号，保证FPR在MMLU-Pro上低至0.000073%，在Big-Bench-Hard上低至0.00017%。对于开放式生成任务，它可以很好地推广，并使用六个后门识别羊驼上所有受污染的模型，保证假阳性率仅为0.127%。



## **28. Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier LLMs**

战略不诚实可能会破坏前沿LL的人工智能安全评估 cs.LG

**SubmitDate**: 2025-09-23    [abs](http://arxiv.org/abs/2509.18058v2) [paper-pdf](http://arxiv.org/pdf/2509.18058v2)

**Authors**: Alexander Panfilov, Evgenii Kortukov, Kristina Nikolić, Matthias Bethge, Sebastian Lapuschkin, Wojciech Samek, Ameya Prabhu, Maksym Andriushchenko, Jonas Geiping

**Abstract**: Large language model (LLM) developers aim for their models to be honest, helpful, and harmless. However, when faced with malicious requests, models are trained to refuse, sacrificing helpfulness. We show that frontier LLMs can develop a preference for dishonesty as a new strategy, even when other options are available. Affected models respond to harmful requests with outputs that sound harmful but are crafted to be subtly incorrect or otherwise harmless in practice. This behavior emerges with hard-to-predict variations even within models from the same model family. We find no apparent cause for the propensity to deceive, but show that more capable models are better at executing this strategy. Strategic dishonesty already has a practical impact on safety evaluations, as we show that dishonest responses fool all output-based monitors used to detect jailbreaks that we test, rendering benchmark scores unreliable. Further, strategic dishonesty can act like a honeypot against malicious users, which noticeably obfuscates prior jailbreak attacks. While output monitors fail, we show that linear probes on internal activations can be used to reliably detect strategic dishonesty. We validate probes on datasets with verifiable outcomes and by using them as steering vectors. Overall, we consider strategic dishonesty as a concrete example of a broader concern that alignment of LLMs is hard to control, especially when helpfulness and harmlessness conflict.

摘要: 大型语言模型（LLM）开发人员的目标是让他们的模型诚实、有帮助且无害。然而，当面临恶意请求时，模型会被训练拒绝，从而牺牲帮助。我们表明，即使有其他选择，前沿LLM也可以将不诚实行为作为一种新策略。受影响的模型以听起来有害的输出来响应有害请求，但在实践中精心设计成微妙的错误或其他无害。即使在同一模型家族的模型中，这种行为也会出现难以预测的变化。我们没有发现欺骗倾向的明显原因，但表明更有能力的模型更善于执行这种策略。战略性不诚实已经对安全评估产生了实际影响，因为我们表明，不诚实的反应欺骗了所有用于检测我们测试的越狱的基于输出的监视器，从而使基准分数不可靠。此外，战略性不诚实可能就像针对恶意用户的蜜罐，这明显混淆了之前的越狱攻击。虽然输出监视器失败，但我们表明，内部激活的线性探测可以用于可靠地检测战略不诚实行为。我们通过可验证的结果来验证数据集上的探测器，并将其用作引导载体。总体而言，我们认为战略不诚实是一个更广泛担忧的具体例子，即LLM的一致难以控制，特别是当有益性和无害性发生冲突时。



## **29. Algorithms for Adversarially Robust Deep Learning**

对抗鲁棒深度学习算法 cs.LG

PhD thesis

**SubmitDate**: 2025-09-23    [abs](http://arxiv.org/abs/2509.19100v1) [paper-pdf](http://arxiv.org/pdf/2509.19100v1)

**Authors**: Alexander Robey

**Abstract**: Given the widespread use of deep learning models in safety-critical applications, ensuring that the decisions of such models are robust against adversarial exploitation is of fundamental importance. In this thesis, we discuss recent progress toward designing algorithms that exhibit desirable robustness properties. First, we discuss the problem of adversarial examples in computer vision, for which we introduce new technical results, training paradigms, and certification algorithms. Next, we consider the problem of domain generalization, wherein the task is to train neural networks to generalize from a family of training distributions to unseen test distributions. We present new algorithms that achieve state-of-the-art generalization in medical imaging, molecular identification, and image classification. Finally, we study the setting of jailbreaking large language models (LLMs), wherein an adversarial user attempts to design prompts that elicit objectionable content from an LLM. We propose new attacks and defenses, which represent the frontier of progress toward designing robust language-based agents.

摘要: 鉴于深度学习模型在安全关键应用中的广泛使用，确保此类模型的决策针对对抗性剥削具有鲁棒性至关重要。在这篇论文中，我们讨论了设计具有理想鲁棒性的算法的最新进展。首先，我们讨论计算机视觉中的对抗性示例问题，为此我们介绍了新的技术成果、训练范式和认证算法。接下来，我们考虑领域概括问题，其中的任务是训练神经网络将一系列训练分布推广到不可见的测试分布。我们提出了新算法，可以在医学成像、分子识别和图像分类方面实现最先进的概括。最后，我们研究了越狱大型语言模型（LLM）的设置，其中敌对用户试图设计从LLM中引出令人反感的内容的提示。我们提出了新的攻击和防御，这代表了设计稳健的基于语言的代理的进展前沿。



## **30. Automating Steering for Safe Multimodal Large Language Models**

安全多模式大型语言模型的自动转向 cs.CL

EMNLP 2025 Main Conference. 23 pages (8+ for main); 25 figures; 1  table

**SubmitDate**: 2025-09-23    [abs](http://arxiv.org/abs/2507.13255v3) [paper-pdf](http://arxiv.org/pdf/2507.13255v3)

**Authors**: Lyucheng Wu, Mengru Wang, Ziwen Xu, Tri Cao, Nay Oo, Bryan Hooi, Shumin Deng

**Abstract**: Recent progress in Multimodal Large Language Models (MLLMs) has unlocked powerful cross-modal reasoning abilities, but also raised new safety concerns, particularly when faced with adversarial multimodal inputs. To improve the safety of MLLMs during inference, we introduce a modular and adaptive inference-time intervention technology, AutoSteer, without requiring any fine-tuning of the underlying model. AutoSteer incorporates three core components: (1) a novel Safety Awareness Score (SAS) that automatically identifies the most safety-relevant distinctions among the model's internal layers; (2) an adaptive safety prober trained to estimate the likelihood of toxic outputs from intermediate representations; and (3) a lightweight Refusal Head that selectively intervenes to modulate generation when safety risks are detected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical benchmarks demonstrate that AutoSteer significantly reduces the Attack Success Rate (ASR) for textual, visual, and cross-modal threats, while maintaining general abilities. These findings position AutoSteer as a practical, interpretable, and effective framework for safer deployment of multimodal AI systems.

摘要: 多模式大型语言模型（MLLM）的最新进展释放了强大的跨模式推理能力，但也提出了新的安全问题，特别是在面对对抗性多模式输入时。为了提高MLLM在推理过程中的安全性，我们引入了模块化和自适应的推理时干预技术AutoSteer，无需对底层模型进行任何微调。AutoSteer包含三个核心组件：（1）新型安全意识评分（SAS），自动识别模型内部层之间最安全相关的区别;（2）自适应安全探测器，经过训练以估计中间表示有毒输出的可能性;（3）轻量级拒绝头，当检测到安全风险时，它会选择性地干预以调节发电。LLaVa-OG和Chameleon在各种安全关键基准上的实验表明，AutoSteer显着降低了文本、视觉和跨模式威胁的攻击成功率（ASB），同时保持了一般能力。这些发现将AutoSteer定位为一个实用、可解释且有效的框架，用于更安全地部署多模式人工智能系统。



## **31. SilentStriker:Toward Stealthy Bit-Flip Attacks on Large Language Models**

SilentStriker：走向对大型语言模型的隐形位翻转攻击 cs.CR

**SubmitDate**: 2025-09-23    [abs](http://arxiv.org/abs/2509.17371v2) [paper-pdf](http://arxiv.org/pdf/2509.17371v2)

**Authors**: Haotian Xu, Qingsong Peng, Jie Shi, Huadi Zheng, Yu Li, Cheng Zhuo

**Abstract**: The rapid adoption of large language models (LLMs) in critical domains has spurred extensive research into their security issues. While input manipulation attacks (e.g., prompt injection) have been well studied, Bit-Flip Attacks (BFAs) -- which exploit hardware vulnerabilities to corrupt model parameters and cause severe performance degradation -- have received far less attention. Existing BFA methods suffer from key limitations: they fail to balance performance degradation and output naturalness, making them prone to discovery. In this paper, we introduce SilentStriker, the first stealthy bit-flip attack against LLMs that effectively degrades task performance while maintaining output naturalness. Our core contribution lies in addressing the challenge of designing effective loss functions for LLMs with variable output length and the vast output space. Unlike prior approaches that rely on output perplexity for attack loss formulation, which inevitably degrade output naturalness, we reformulate the attack objective by leveraging key output tokens as targets for suppression, enabling effective joint optimization of attack effectiveness and stealthiness. Additionally, we employ an iterative, progressive search strategy to maximize attack efficacy. Experiments show that SilentStriker significantly outperforms existing baselines, achieving successful attacks without compromising the naturalness of generated text.

摘要: 大型语言模型（LLM）在关键领域的快速采用促使人们对其安全问题进行了广泛的研究。虽然输入操纵攻击（例如，即时注入）已经得到了很好的研究，位翻转攻击（BFA）-利用硬件漏洞破坏模型参数并导致严重的性能下降-受到的关注要少得多。现有的BFA方法受到关键限制：它们无法平衡性能下降和输出自然性，使它们易于被发现。在本文中，我们引入SilentStriker，这是针对LLM的第一个隐形位翻转攻击，可以有效降低任务性能，同时保持输出自然性。我们的核心贡献在于解决为具有可变输出长度和巨大输出空间的LLM设计有效损失函数的挑战。与依赖输出困惑性来制定攻击损失公式的现有方法不同，这不可避免地会降低输出自然性，我们通过利用关键输出令牌作为抑制目标来重新制定攻击目标，从而实现攻击有效性和隐蔽性的有效联合优化。此外，我们采用迭代、渐进的搜索策略来最大限度地提高攻击效率。实验表明，SilentStriker的性能显着优于现有基线，可以在不损害生成文本的自然性的情况下实现成功攻击。



## **32. The Ranking Blind Spot: Decision Hijacking in LLM-based Text Ranking**

排名盲点：基于LLM的文本排名中的决策劫持 cs.IR

Accepted by EMNLP 2025

**SubmitDate**: 2025-09-23    [abs](http://arxiv.org/abs/2509.18575v1) [paper-pdf](http://arxiv.org/pdf/2509.18575v1)

**Authors**: Yaoyao Qian, Yifan Zeng, Yuchao Jiang, Chelsi Jain, Huazheng Wang

**Abstract**: Large Language Models (LLMs) have demonstrated strong performance in information retrieval tasks like passage ranking. Our research examines how instruction-following capabilities in LLMs interact with multi-document comparison tasks, identifying what we term the "Ranking Blind Spot", a characteristic of LLM decision processes during comparative evaluation. We analyze how this ranking blind spot affects LLM evaluation systems through two approaches: Decision Objective Hijacking, which alters the evaluation goal in pairwise ranking systems, and Decision Criteria Hijacking, which modifies relevance standards across ranking schemes. These approaches demonstrate how content providers could potentially influence LLM-based ranking systems to affect document positioning. These attacks aim to force the LLM ranker to prefer a specific passage and rank it at the top. Malicious content providers can exploit this weakness, which helps them gain additional exposure by attacking the ranker. In our experiment, We empirically show that the proposed attacks are effective in various LLMs and can be generalized to multiple ranking schemes. We apply these attack to realistic examples to show their effectiveness. We also found stronger LLMs are more vulnerable to these attacks. Our code is available at: https://github.com/blindspotorg/RankingBlindSpot

摘要: 大型语言模型（LLM）在段落排名等信息检索任务中表现出出色的性能。我们的研究考察了LLM中的描述遵循能力如何与多文档比较任务相互作用，确定了我们所说的“排名盲点”，这是LLM在比较评估期间决策过程的特征。我们分析了这个排名盲点如何通过两种方法影响LLM评估系统：决策目标劫持（改变成对排名系统中的评估目标）和决策标准劫持（修改排名方案中的相关性标准）。这些方法展示了内容提供商如何可能影响基于LLM的排名系统以影响文档定位。这些攻击旨在迫使LLM排名者偏好特定的段落并将其排名在顶部。恶意内容提供商可以利用这一弱点，这有助于他们通过攻击排名者来获得额外的曝光度。在我们的实验中，我们经验表明，所提出的攻击在各种LLM中有效，并且可以推广到多个排名方案。我们将这些攻击应用到现实的例子中来展示它们的有效性。我们还发现，更强大的LLM更容易受到这些攻击。我们的代码可访问：https://github.com/blindspotorg/RankingBlindSpot



## **33. Mind the Gap: Evaluating Model- and Agentic-Level Vulnerabilities in LLMs with Action Graphs**

注意差距：使用行动图评估LLM中的模型和统计级别漏洞 cs.CL

**SubmitDate**: 2025-09-22    [abs](http://arxiv.org/abs/2509.04802v2) [paper-pdf](http://arxiv.org/pdf/2509.04802v2)

**Authors**: Ilham Wicaksono, Zekun Wu, Rahul Patel, Theo King, Adriano Koshiyama, Philip Treleaven

**Abstract**: As large language models transition to agentic systems, current safety evaluation frameworks face critical gaps in assessing deployment-specific risks. We introduce AgentSeer, an observability-based evaluation framework that decomposes agentic executions into granular action and component graphs, enabling systematic agentic-situational assessment. Through cross-model validation on GPT-OSS-20B and Gemini-2.0-flash using HarmBench single turn and iterative refinement attacks, we demonstrate fundamental differences between model-level and agentic-level vulnerability profiles. Model-level evaluation reveals baseline differences: GPT-OSS-20B (39.47% ASR) versus Gemini-2.0-flash (50.00% ASR), with both models showing susceptibility to social engineering while maintaining logic-based attack resistance. However, agentic-level assessment exposes agent-specific risks invisible to traditional evaluation. We discover "agentic-only" vulnerabilities that emerge exclusively in agentic contexts, with tool-calling showing 24-60% higher ASR across both models. Cross-model analysis reveals universal agentic patterns, agent transfer operations as highest-risk tools, semantic rather than syntactic vulnerability mechanisms, and context-dependent attack effectiveness, alongside model-specific security profiles in absolute ASR levels and optimal injection strategies. Direct attack transfer from model-level to agentic contexts shows degraded performance (GPT-OSS-20B: 57% human injection ASR; Gemini-2.0-flash: 28%), while context-aware iterative attacks successfully compromise objectives that failed at model-level, confirming systematic evaluation gaps. These findings establish the urgent need for agentic-situation evaluation paradigms, with AgentSeer providing the standardized methodology and empirical validation.

摘要: 随着大型语言模型向代理系统过渡，当前的安全评估框架在评估特定于部署的风险方面面临着严重差距。我们引入了AgentSeer，这是一个基于可观察性的评估框架，它将代理执行分解为粒度动作和组件图，从而实现系统性代理情景评估。通过使用HarmBench单轮攻击和迭代细化攻击对GPT-OSS-20 B和Gemini-2.0-Flash进行跨模型验证，我们展示了模型级和代理级漏洞配置文件之间的根本差异。模型级评估揭示了基线差异：GPT-OSS-20 B（39.47%ASB）与Gemini-2.0-Flash（50.00%ASB），两种模型都表现出对社会工程的敏感性，同时保持基于逻辑的攻击抵抗力。然而，代理层面的评估暴露了传统评估所看不到的代理特定风险。我们发现了仅在代理环境中出现的“仅代理”漏洞，工具调用显示两种模型的ASB高出24-60%。跨模型分析揭示了通用的代理模式、作为最高风险工具的代理传输操作、语义而非语法漏洞机制、取决于上下文的攻击有效性，以及绝对ASB级别的模型特定安全配置文件和最佳注入策略。从模型级到代理上下文的直接攻击转移显示出性能下降（GPT-OSS-20 B：57%人体注射ASO; Gemini-2.0-Flash：28%），而上下文感知迭代攻击成功地破坏了在模型级失败的目标，证实了系统性评估差距。这些发现确定了对主体情境评估范式的迫切需求，AgentSeer提供了标准化的方法论和经验验证。



## **34. Large Language Models for Cyber Security: A Systematic Literature Review**

网络安全的大型语言模型：系统性文献综述 cs.CR

Accepted by ACM Transactions on Software Engineering and Methodology  (TOSEM)

**SubmitDate**: 2025-09-22    [abs](http://arxiv.org/abs/2405.04760v5) [paper-pdf](http://arxiv.org/pdf/2405.04760v5)

**Authors**: Hanxiang Xu, Shenao Wang, Ningke Li, Kailong Wang, Yanjie Zhao, Kai Chen, Ting Yu, Yang Liu, Haoyu Wang

**Abstract**: The rapid advancement of Large Language Models (LLMs) has opened up new opportunities for leveraging artificial intelligence in a variety of application domains, including cybersecurity. As the volume and sophistication of cyber threats continue to grow, there is an increasing need for intelligent systems that can automatically detect vulnerabilities, analyze malware, and respond to attacks. In this survey, we conduct a comprehensive review of the literature on the application of LLMs in cybersecurity~(LLM4Security). By comprehensively collecting over 40K relevant papers and systematically analyzing 185 papers from top security and software engineering venues, we aim to provide a holistic view of how LLMs are being used to solve diverse problems across the cybersecurity domain. Through our analysis, we identify several key findings. First, we observe that LLMs are being applied to an expanding range of cybersecurity tasks, including vulnerability detection, malware analysis, and network intrusion detection. Second, we analyze application trends of different LLM architectures (such as encoder-only, encoder-decoder, and decoder-only) across security domains. Third, we identify increasingly sophisticated techniques for adapting LLMs to cybersecurity, such as advanced fine-tuning, prompt engineering, and external augmentation strategies. A significant emerging trend is the use of LLM-based autonomous agents, which represent a paradigm shift from single-task execution to orchestrating complex, multi-step security workflows.

摘要: 大型语言模型（LLM）的快速发展为在包括网络安全在内的各个应用领域利用人工智能开辟了新的机会。随着网络威胁的数量和复杂性不断增长，对能够自动检测漏洞、分析恶意软件并响应攻击的智能系统的需求越来越大。在本调查中，我们对有关LLM在网络安全中应用的文献进行了全面回顾~（LLM 4Security）。通过全面收集超过4万篇相关论文并系统分析来自顶级安全和软件工程场所的185篇论文，我们的目标是提供如何使用LLM来解决网络安全领域的各种问题的整体视图。通过我们的分析，我们确定了几个关键发现。首先，我们观察到LLM正在被应用于越来越广泛的网络安全任务，包括漏洞检测、恶意软件分析和网络入侵检测。其次，我们分析了不同LLM架构（例如仅编码器、编码器-解码器和仅解码器）跨安全域的应用趋势。第三，我们确定了越来越复杂的技术来使LLM适应网络安全，例如高级微调、即时工程和外部增强策略。一个重要的新兴趋势是使用基于LLM的自主代理，这代表了从单任务执行到编排复杂、多步骤安全工作流程的范式转变。



## **35. Proxy-Embedding as an Adversarial Teacher: An Embedding-Guided Bidirectional Attack for Referring Expression Segmentation Models**

代理嵌入作为对抗性教师：引用表情分割模型的嵌入引导双向攻击 cs.CV

20pages, 5figures

**SubmitDate**: 2025-09-22    [abs](http://arxiv.org/abs/2506.16157v2) [paper-pdf](http://arxiv.org/pdf/2506.16157v2)

**Authors**: Xingbai Chen, Tingchao Fu, Renyang Liu, Wei Zhou, Chao Yi

**Abstract**: Referring Expression Segmentation (RES) enables precise object segmentation in images based on natural language descriptions, offering high flexibility and broad applicability in real-world vision tasks. Despite its impressive performance, the robustness of RES models against adversarial examples remains largely unexplored. While prior adversarial attack methods have explored adversarial robustness on conventional segmentation models, they perform poorly when directly applied to RES models, failing to expose vulnerabilities in its multimodal structure. In practical open-world scenarios, users typically issue multiple, diverse referring expressions to interact with the same image, highlighting the need for adversarial examples that generalize across varied textual inputs. Furthermore, from the perspective of privacy protection, ensuring that RES models do not segment sensitive content without explicit authorization is a crucial aspect of enhancing the robustness and security of multimodal vision-language systems. To address these challenges, we present PEAT, an Embedding-Guided Bidirectional Attack for RES models. Extensive experiments across multiple RES architectures and standard benchmarks show that PEAT consistently outperforms competitive baselines.

摘要: 引用表达分割（RES）可以基于自然语言描述在图像中进行精确的对象分割，在现实世界的视觉任务中提供高度灵活性和广泛的适用性。尽管RES模型的性能令人印象深刻，但其针对对抗性示例的稳健性在很大程度上仍未得到探索。虽然先前的对抗攻击方法已经探索了传统分割模型的对抗鲁棒性，但当直接应用于RES模型时，它们的表现很差，未能暴露其多模式结构中的漏洞。在实际的开放世界场景中，用户通常会发出多个不同的引用表达来与同一图像进行交互，这凸显了对跨越不同文本输入进行概括的对抗性示例的需要。此外，从隐私保护的角度来看，确保RES模型在没有明确授权的情况下不会分割敏感内容，是增强多模态视觉语言系统的鲁棒性和安全性的关键方面。为了解决这些挑战，我们提出了PEAT，一种针对RES模型的嵌入引导双向攻击。跨多个RES架构和标准基准的广泛实验表明，PEAT始终优于竞争对手的基线。



## **36. SUA: Stealthy Multimodal Large Language Model Unlearning Attack**

SUA：隐形多模式大型语言模型取消学习攻击 cs.LG

EMNLP25

**SubmitDate**: 2025-09-21    [abs](http://arxiv.org/abs/2506.17265v2) [paper-pdf](http://arxiv.org/pdf/2506.17265v2)

**Authors**: Xianren Zhang, Hui Liu, Delvin Ce Zhang, Xianfeng Tang, Qi He, Dongwon Lee, Suhang Wang

**Abstract**: Multimodal Large Language Models (MLLMs) trained on massive data may memorize sensitive personal information and photos, posing serious privacy risks. To mitigate this, MLLM unlearning methods are proposed, which fine-tune MLLMs to reduce the ``forget'' sensitive information. However, it remains unclear whether the knowledge has been truly forgotten or just hidden in the model. Therefore, we propose to study a novel problem of LLM unlearning attack, which aims to recover the unlearned knowledge of an unlearned LLM. To achieve the goal, we propose a novel framework Stealthy Unlearning Attack (SUA) framework that learns a universal noise pattern. When applied to input images, this noise can trigger the model to reveal unlearned content. While pixel-level perturbations may be visually subtle, they can be detected in the semantic embedding space, making such attacks vulnerable to potential defenses. To improve stealthiness, we introduce an embedding alignment loss that minimizes the difference between the perturbed and denoised image embeddings, ensuring the attack is semantically unnoticeable. Experimental results show that SUA can effectively recover unlearned information from MLLMs. Furthermore, the learned noise generalizes well: a single perturbation trained on a subset of samples can reveal forgotten content in unseen images. This indicates that knowledge reappearance is not an occasional failure, but a consistent behavior.

摘要: 基于大量数据训练的多模式大型语言模型（MLLM）可能会记住敏感的个人信息和照片，从而构成严重的隐私风险。为了缓解这种情况，提出了MLLM去学习方法，这些方法对MLLM进行微调，以减少“忘记”敏感信息。然而，目前尚不清楚这些知识是否真的被遗忘了，或者只是隐藏在模型中。因此，我们提出研究LLM未学习攻击的一个新问题，旨在恢复未学习的LLM的未学习知识。为了实现这一目标，我们提出了一种新颖的框架隐形非学习攻击（SUA）框架，该框架可以学习通用的噪音模式。当应用于输入图像时，这种噪音可以触发模型揭示未学习的内容。虽然像素级扰动在视觉上可能很微妙，但它们可以在语义嵌入空间中检测到，从而使此类攻击容易受到潜在防御的影响。为了提高隐蔽性，我们引入了嵌入对齐损失，以最大限度地减少受干扰的图像嵌入和去噪的图像嵌入之间的差异，确保攻击在语义上不引人注目。实验结果表明，SUA可以有效地从MLLM中恢复未学习的信息。此外，学习到的噪音很好地概括：在样本子集上训练的单个扰动可以揭示未见图像中被遗忘的内容。这表明知识再现不是偶然的失败，而是一种一致的行为。



## **37. Revisiting Backdoor Attacks on LLMs: A Stealthy and Practical Poisoning Framework via Harmless Inputs**

重新审视对LLM的后门攻击：通过无害输入的隐形且实用的毒害框架 cs.CL

**SubmitDate**: 2025-09-21    [abs](http://arxiv.org/abs/2505.17601v3) [paper-pdf](http://arxiv.org/pdf/2505.17601v3)

**Authors**: Jiawei Kong, Hao Fang, Xiaochen Yang, Kuofeng Gao, Bin Chen, Shu-Tao Xia, Ke Xu, Han Qiu

**Abstract**: Recent studies have widely investigated backdoor attacks on Large language models (LLMs) by inserting harmful question-answer (QA) pairs into training data to implant triggers. However, we revisit existing attack methods and identify two critical limitations of that seriously undermine their stealthiness and practicality: (1) directly embedding harmful content into the training data compromise the model's safety alignment, resulting in high attack success rates even for clean queries without triggers, and (2) the poisoned training samples can be easily detected and filtered by safety-aligned guardrails (e.g., LLaMAGuard). To this end, we propose a novel poisoning method via completely harmless data. Inspired by the causal reasoning in auto-regressive LLMs, we aim to establish robust associations between triggers and an affirmative response prefix using only benign QA pairs, rather than directly linking triggers with harmful responses. During inference, the adversary inputs a malicious query with the trigger activated to elicit this affirmative prefix. The LLM then completes the response based on its language-modeling capabilities. Notably, achieving this behavior from clean QA pairs is non-trivial. We observe an interesting resistance phenomenon where the LLM initially appears to agree but subsequently refuses to answer. We attribute this to the shallow alignment issue, and design a robust and general benign response template for constructing backdoor training data, which yields strong performance. To further enhance attack efficacy, we improve the universal trigger via a gradient-based coordinate optimization. Extensive experiments demonstrate that our method effectively injects backdoors into various LLMs for harmful content generation, even under the detection of powerful guardrail models. E.g., ASRs of 86.67% and 85% on LLaMA-3-8B and Qwen-2.5-7B judged by GPT-4o.

摘要: 最近的研究广泛调查了对大型语言模型（LLM）的后门攻击，方法是在训练数据中插入有害的问答（QA）对以植入触发器。然而，我们重新审视了现有的攻击方法，并发现了严重损害其隐蔽性和实用性的两个关键局限性：（1）直接将有害内容嵌入到训练数据中会损害模型的安全对齐，即使对于没有触发器的干净查询也会导致很高的攻击成功率，以及（2）中毒的训练样本可以很容易地检测和过滤安全对齐的护栏（例如，LLaMAGuard）。为此，我们提出了一种通过完全无害的数据的新型中毒方法。受到自回归LLM中因果推理的启发，我们的目标是仅使用良性QA对，而不是直接将触发器与有害反应联系起来，在触发器与肯定反应之间建立稳健的关联。在推理过程中，对手输入恶意查询，触发器被激活以引出此肯定性前置。然后，LLM根据其语言建模能力完成响应。值得注意的是，从干净的QA对实现这种行为并非易事。我们观察到一个有趣的阻力现象，LLM最初似乎同意，但随后拒绝回答。我们将其归因于浅层对齐问题，并设计一个稳健且通用的良性响应模板来构建后门训练数据，从而产生强大的性能。为了进一步提高攻击功效，我们通过基于梯度的协调优化改进了通用触发器。大量实验表明，即使在检测到强大的护栏模型的情况下，我们的方法也可以有效地将后门注入到各种LLM中，以生成有害内容。例如，根据GPT-4 o判断，LLaMA-3-8B和Qwen-2.5- 7 B的ASB分别为86.67%和85%。



## **38. Breaking the Reviewer: Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks**

打破评论者：评估文本对抗攻击下自动同行评审中大型语言模型的脆弱性 cs.CL

**SubmitDate**: 2025-09-21    [abs](http://arxiv.org/abs/2506.11113v2) [paper-pdf](http://arxiv.org/pdf/2506.11113v2)

**Authors**: Tzu-Ling Lin, Wei-Chih Chen, Teng-Fang Hsiao, Hou-I Liu, Ya-Hsin Yeh, Yu Kai Chan, Wen-Sheng Lien, Po-Yen Kuo, Philip S. Yu, Hong-Han Shuai

**Abstract**: Peer review is essential for maintaining academic quality, but the increasing volume of submissions places a significant burden on reviewers. Large language models (LLMs) offer potential assistance in this process, yet their susceptibility to textual adversarial attacks raises reliability concerns. This paper investigates the robustness of LLMs used as automated reviewers in the presence of such attacks. We focus on three key questions: (1) The effectiveness of LLMs in generating reviews compared to human reviewers. (2) The impact of adversarial attacks on the reliability of LLM-generated reviews. (3) Challenges and potential mitigation strategies for LLM-based review. Our evaluation reveals significant vulnerabilities, as text manipulations can distort LLM assessments. We offer a comprehensive evaluation of LLM performance in automated peer reviewing and analyze its robustness against adversarial attacks. Our findings emphasize the importance of addressing adversarial risks to ensure AI strengthens, rather than compromises, the integrity of scholarly communication.

摘要: 同行评审对于保持学术质量至关重要，但提交量的增加给评审者带来了沉重的负担。大型语言模型（LLM）在此过程中提供了潜在的帮助，但它们对文本对抗攻击的敏感性引发了可靠性问题。本文研究了在存在此类攻击的情况下用作自动审查员的LLM的稳健性。我们重点关注三个关键问题：（1）与人类评审员相比，LLM在生成评审方面的有效性。(2)对抗性攻击对LLM生成的评论的可靠性的影响。(3)LLM为基础的审查的挑战和潜在的缓解策略。我们的评估揭示了重大的漏洞，因为文本操作可能会扭曲LLM评估。我们提供了一个全面的评估LLM性能的自动同行评审，并分析其对抗攻击的鲁棒性。我们的研究结果强调了解决对抗风险的重要性，以确保人工智能加强而不是损害学术交流的完整性。



## **39. BlockA2A: Towards Secure and Verifiable Agent-to-Agent Interoperability**

BlockA2A：迈向安全且可验证的代理对代理互操作性 cs.CR

43 pages

**SubmitDate**: 2025-09-21    [abs](http://arxiv.org/abs/2508.01332v3) [paper-pdf](http://arxiv.org/pdf/2508.01332v3)

**Authors**: Zhenhua Zou, Zhuotao Liu, Lepeng Zhao, Qiuyang Zhan

**Abstract**: The rapid adoption of agentic AI, powered by large language models (LLMs), is transforming enterprise ecosystems with autonomous agents that execute complex workflows. Yet we observe several key security vulnerabilities in LLM-driven multi-agent systems (MASes): fragmented identity frameworks, insecure communication channels, and inadequate defenses against Byzantine agents or adversarial prompts. In this paper, we present the first systematic analysis of these emerging multi-agent risks and explain why the legacy security strategies cannot effectively address these risks. Afterwards, we propose BlockA2A, the first unified multi-agent trust framework that enables secure and verifiable and agent-to-agent interoperability. At a high level, BlockA2A adopts decentralized identifiers (DIDs) to enable fine-grained cross-domain agent authentication, blockchain-anchored ledgers to enable immutable auditability, and smart contracts to dynamically enforce context-aware access control policies. BlockA2A eliminates centralized trust bottlenecks, ensures message authenticity and execution integrity, and guarantees accountability across agent interactions. Furthermore, we propose a Defense Orchestration Engine (DOE) that actively neutralizes attacks through real-time mechanisms, including Byzantine agent flagging, reactive execution halting, and instant permission revocation. Empirical evaluations demonstrate BlockA2A's effectiveness in neutralizing prompt-based, communication-based, behavioral and systemic MAS attacks. We formalize its integration into existing MAS and showcase a practical implementation for Google's A2A protocol. Experiments confirm that BlockA2A and DOE operate with sub-second overhead, enabling scalable deployment in production LLM-based MAS environments.

摘要: 由大型语言模型（LLM）支持的代理人工智能的快速采用正在通过执行复杂工作流程的自主代理改变企业生态系统。然而，我们在LLM驱动的多代理系统（MASes）中观察到了几个关键的安全漏洞：碎片化的身份框架、不安全的通信渠道以及对拜占庭代理或对抗提示的防御不足。在本文中，我们对这些新出现的多代理风险进行了首次系统分析，并解释了为什么传统安全策略无法有效应对这些风险。随后，我们提出了BlockA2A，这是第一个统一的多代理信任框架，可以实现安全、可验证以及代理与代理的互操作性。在高层面上，BlockA2A采用去中心化标识符（DID）来实现细粒度的跨域代理认证，采用区块链锚定分类帐来实现不可变的可互换性，并采用智能合同来动态执行上下文感知的访问控制策略。BlockA2A消除了集中式信任瓶颈，确保消息真实性和执行完整性，并保证跨代理交互的问责制。此外，我们还提出了一种国防规划引擎（DOE），它通过实时机制主动中和攻击，包括拜占庭代理标记、反应式执行停止和即时许可撤销。经验评估证明BlockA2A在中和基于预算、基于通信的行为和系统性MAS攻击方面的有效性。我们将其正式集成到现有MAS中，并展示了Google A2 A协议的实际实现。实验证实BlockA2A和DOE的运行成本为亚秒级，从而能够在基于LLM的生产MAS环境中进行可扩展部署。



## **40. DecipherGuard: Understanding and Deciphering Jailbreak Prompts for a Safer Deployment of Intelligent Software Systems**

DecipherGuard：了解和破译越狱承诺更安全地部署智能软件系统 cs.SE

Under Review

**SubmitDate**: 2025-09-21    [abs](http://arxiv.org/abs/2509.16870v1) [paper-pdf](http://arxiv.org/pdf/2509.16870v1)

**Authors**: Rui Yang, Michael Fu, Chakkrit Tantithamthavorn, Chetan Arora, Gunel Gulmammadova, Joey Chua

**Abstract**: Intelligent software systems powered by Large Language Models (LLMs) are increasingly deployed in critical sectors, raising concerns about their safety during runtime. Through an industry-academic collaboration when deploying an LLM-powered virtual customer assistant, a critical software engineering challenge emerged: how to enhance a safer deployment of LLM-powered software systems at runtime? While LlamaGuard, the current state-of-the-art runtime guardrail, offers protection against unsafe inputs, our study reveals a Defense Success Rate (DSR) drop of 24% under obfuscation- and template-based jailbreak attacks. In this paper, we propose DecipherGuard, a novel framework that integrates a deciphering layer to counter obfuscation-based prompts and a low-rank adaptation mechanism to enhance guardrail effectiveness against template-based attacks. Empirical evaluation on over 22,000 prompts demonstrates that DecipherGuard improves DSR by 36% to 65% and Overall Guardrail Performance (OGP) by 20% to 50% compared to LlamaGuard and two other runtime guardrails. These results highlight the effectiveness of DecipherGuard in defending LLM-powered software systems against jailbreak attacks during runtime.

摘要: 由大型语言模型（LLM）支持的智能软件系统越来越多地部署在关键领域，引发了对其运行期间安全性的担忧。通过在部署LLM支持的虚拟客户助理时进行行业学术合作，出现了一个关键的软件工程挑战：如何在运行时增强LLM支持的软件系统的更安全部署？虽然LlamaGuard是当前最先进的运行时护栏，可以提供针对不安全输入的保护，但我们的研究显示，在基于模糊和模板的越狱攻击下，防御成功率（SVR）下降了24%。在本文中，我们提出了DecipherGuard，这是一个新颖的框架，它集成了用于对抗基于模糊的提示的解密层和用于增强护栏对抗基于模板的攻击的有效性的低等级适应机制。对超过22，000个提示的经验评估表明，与LlamaGuard和其他两个运行时护栏相比，DecipherGuard将RSC提高了36%至65%，将总体护栏性能（OGP）提高了20%至50%。这些结果凸显了DecipherGuard在保护LLM支持的软件系统在运行时免受越狱攻击方面的有效性。



## **41. AdaptiveGuard: Towards Adaptive Runtime Safety for LLM-Powered Software**

AdaptiveGuard：实现LLM支持的软件的自适应工作空间安全 cs.CR

Accepted to the ASE 2025 International Conference on Automated  Software Engineering, Industry Showcase Track

**SubmitDate**: 2025-09-21    [abs](http://arxiv.org/abs/2509.16861v1) [paper-pdf](http://arxiv.org/pdf/2509.16861v1)

**Authors**: Rui Yang, Michael Fu, Chakkrit Tantithamthavorn, Chetan Arora, Gunel Gulmammadova, Joey Chua

**Abstract**: Guardrails are critical for the safe deployment of Large Language Models (LLMs)-powered software. Unlike traditional rule-based systems with limited, predefined input-output spaces that inherently constrain unsafe behavior, LLMs enable open-ended, intelligent interactions--opening the door to jailbreak attacks through user inputs. Guardrails serve as a protective layer, filtering unsafe prompts before they reach the LLM. However, prior research shows that jailbreak attacks can still succeed over 70% of the time, even against advanced models like GPT-4o. While guardrails such as LlamaGuard report up to 95% accuracy, our preliminary analysis shows their performance can drop sharply--to as low as 12%--when confronted with unseen attacks. This highlights a growing software engineering challenge: how to build a post-deployment guardrail that adapts dynamically to emerging threats? To address this, we propose AdaptiveGuard, an adaptive guardrail that detects novel jailbreak attacks as out-of-distribution (OOD) inputs and learns to defend against them through a continual learning framework. Through empirical evaluation, AdaptiveGuard achieves 96% OOD detection accuracy, adapts to new attacks in just two update steps, and retains over 85% F1-score on in-distribution data post-adaptation, outperforming other baselines. These results demonstrate that AdaptiveGuard is a guardrail capable of evolving in response to emerging jailbreak strategies post deployment. We release our AdaptiveGuard and studied datasets at https://github.com/awsm-research/AdaptiveGuard to support further research.

摘要: 护栏对于安全部署大型语言模型（LLM）驱动的软件至关重要。与传统的基于规则的系统不同，LLM具有有限的、预定义的输入输出空间，本质上限制了不安全的行为，LLM可以实现开放式的智能交互--通过用户输入打开越狱攻击的大门。护栏充当保护层，在不安全的提示到达LLM之前对其进行过滤。然而，之前的研究表明，即使针对GPT-4 o等高级型号，越狱攻击仍然可以在70%以上的情况下成功。虽然LlamaGuard等护栏报告的准确率高达95%，但我们的初步分析显示，当面临不可见的攻击时，它们的性能可能会急剧下降-低至12%。这凸显了一个日益增长的软件工程挑战：如何构建一个动态适应新出现的威胁的部署后护栏？为了解决这个问题，我们提出了AdaptiveGuard，这是一种自适应护栏，可以将新的越狱攻击检测为分发外（OOD）输入，并通过持续学习框架来学习防御它们。通过实证评估，AdaptiveGuard实现了96%的OOD检测准确率，仅需两个更新步骤即可适应新攻击，并在适应后的分布数据上保持超过85%的F1分数，优于其他基线。这些结果表明，AdaptiveGuard是一种能够在部署后响应新兴越狱策略而不断发展的护栏。我们在https://github.com/awsm-research/AdaptiveGuard上发布了AdaptiveGuard并研究了数据集，以支持进一步的研究。



## **42. Design and Development of an Intelligent LLM-based LDAP Honeypot**

基于LLM的智能LDAP蜜罐的设计与开发 cs.CR

**SubmitDate**: 2025-09-20    [abs](http://arxiv.org/abs/2509.16682v1) [paper-pdf](http://arxiv.org/pdf/2509.16682v1)

**Authors**: Javier Jiménez-Román, Florina Almenares-Mendoza, Alfonso Sánchez-Macián

**Abstract**: Cybersecurity threats continue to increase, with a growing number of previously unknown attacks each year targeting both large corporations and smaller entities. This scenario demands the implementation of advanced security measures, not only to mitigate damage but also to anticipate emerging attack trends. In this context, deception tools have become a key strategy, enabling the detection, deterrence, and deception of potential attackers while facilitating the collection of information about their tactics and methods. Among these tools, honeypots have proven their value, although they have traditionally been limited by rigidity and configuration complexity, hindering their adaptability to dynamic scenarios. The rise of artificial intelligence, and particularly general-purpose Large Language Models (LLMs), is driving the development of new deception solutions capable of offering greater adaptability and ease of use. This work proposes the design and implementation of an LLM-based honeypot to simulate an LDAP server, a critical protocol present in most organizations due to its central role in identity and access management. The proposed solution aims to provide a flexible and realistic tool capable of convincingly interacting with attackers, thereby contributing to early detection and threat analysis while enhancing the defensive capabilities of infrastructures against intrusions targeting this service.

摘要: 网络安全威胁持续增加，每年针对大公司和小型实体的先前未知的攻击数量不断增加。这种情况需要实施先进的安全措施，不仅要减轻损害，还要预测新出现的攻击趋势。在这种情况下，欺骗工具已成为一种关键策略，能够检测、威慑和欺骗潜在攻击者，同时促进收集有关其策略和方法的信息。在这些工具中，蜜罐已经证明了自己的价值，尽管它们传统上受到刚性和配置复杂性的限制，阻碍了它们对动态场景的适应性。人工智能，特别是通用大型语言模型（LLM）的兴起，正在推动能够提供更大适应性和易用性的新型欺骗解决方案的开发。这项工作提出了设计和实现基于LLM的蜜罐来模拟PDA服务器，这是大多数组织中存在的关键协议，因为它在身份和访问管理中发挥了核心作用。拟议的解决方案旨在提供一种灵活且现实的工具，能够令人信服地与攻击者互动，从而有助于早期检测和威胁分析，同时增强基础设施针对该服务的入侵的防御能力。



## **43. Sugar-Coated Poison: Benign Generation Unlocks LLM Jailbreaking**

糖衣毒药：良性一代解锁法学硕士越狱 cs.CR

Accepted by EMNLP2025

**SubmitDate**: 2025-09-20    [abs](http://arxiv.org/abs/2504.05652v3) [paper-pdf](http://arxiv.org/pdf/2504.05652v3)

**Authors**: Yu-Hang Wu, Yu-Jie Xiong, Hao Zhang, Jia-Chen Zhang, Zheng Zhou

**Abstract**: With the increasingly deep integration of large language models (LLMs) across diverse domains, the effectiveness of their safety mechanisms is encountering severe challenges. Currently, jailbreak attacks based on prompt engineering have become a major safety threat. However, existing methods primarily rely on black-box manipulation of prompt templates, resulting in poor interpretability and limited generalization. To break through the bottleneck, this study first introduces the concept of Defense Threshold Decay (DTD), revealing the potential safety impact caused by LLMs' benign generation: as benign content generation in LLMs increases, the model's focus on input instructions progressively diminishes. Building on this insight, we propose the Sugar-Coated Poison (SCP) attack paradigm, which uses a "semantic reversal" strategy to craft benign inputs that are opposite in meaning to malicious intent. This strategy induces the models to generate extensive benign content, thereby enabling adversarial reasoning to bypass safety mechanisms. Experiments show that SCP outperforms existing baselines. Remarkably, it achieves an average attack success rate of 87.23% across six LLMs. For defense, we propose Part-of-Speech Defense (POSD), leveraging verb-noun dependencies for syntactic analysis to enhance safety of LLMs while preserving their generalization ability.

摘要: 随着大型语言模型（LLM）跨不同领域的日益深入集成，其安全机制的有效性面临严峻挑战。目前，基于即时工程的越狱攻击已成为重大安全威胁。然而，现有的方法主要依赖于提示模板的黑匣子操作，导致可解释性较差且概括性有限。为了突破瓶颈，本研究首先引入了防御阈值衰变（DART）的概念，揭示了LLM良性生成对安全的潜在影响：随着LLM良性内容生成的增加，模型对输入指令的关注逐渐减少。基于这一见解，我们提出了糖衣毒药（SCP）攻击范式，该范式使用“语义逆转”策略来制造与恶意意图含义相反的良性输入。该策略促使模型生成广泛的良性内容，从而使对抗推理能够绕过安全机制。实验表明SCP优于现有基线。值得注意的是，它在六个LLM中的平均攻击成功率为87.23%。对于防御，我们提出了词性防御（POSD），利用动词-名词依赖进行语法分析，以增强LLM的安全性，同时保留其概括能力。



## **44. FC-Attack: Jailbreaking Multimodal Large Language Models via Auto-Generated Flowcharts**

FC攻击：通过自动生成流程图破解多模式大型语言模型 cs.CV

Accepted to Findings of EMNLP 2025

**SubmitDate**: 2025-09-20    [abs](http://arxiv.org/abs/2502.21059v3) [paper-pdf](http://arxiv.org/pdf/2502.21059v3)

**Authors**: Ziyi Zhang, Zhen Sun, Zongmin Zhang, Jihui Guo, Xinlei He

**Abstract**: Multimodal Large Language Models (MLLMs) have become powerful and widely adopted in some practical applications. However, recent research has revealed their vulnerability to multimodal jailbreak attacks, whereby the model can be induced to generate harmful content, leading to safety risks. Although most MLLMs have undergone safety alignment, recent research shows that the visual modality is still vulnerable to jailbreak attacks. In our work, we discover that by using flowcharts with partially harmful information, MLLMs can be induced to provide additional harmful details. Based on this, we propose a jailbreak attack method based on auto-generated flowcharts, FC-Attack. Specifically, FC-Attack first fine-tunes a pre-trained LLM to create a step-description generator based on benign datasets. The generator is then used to produce step descriptions corresponding to a harmful query, which are transformed into flowcharts in 3 different shapes (vertical, horizontal, and S-shaped) as visual prompts. These flowcharts are then combined with a benign textual prompt to execute the jailbreak attack on MLLMs. Our evaluations on Advbench show that FC-Attack attains an attack success rate of up to 96% via images and up to 78% via videos across multiple MLLMs. Additionally, we investigate factors affecting the attack performance, including the number of steps and the font styles in the flowcharts. We also find that FC-Attack can improve the jailbreak performance from 4% to 28% in Claude-3.5 by changing the font style. To mitigate the attack, we explore several defenses and find that AdaShield can largely reduce the jailbreak performance but with the cost of utility drop.

摘要: 多模式大型语言模型（MLLM）已变得强大并在一些实际应用中广泛采用。然而，最近的研究揭示了它们对多模式越狱攻击的脆弱性，从而可以诱导模型生成有害内容，从而导致安全风险。尽管大多数MLLM都经历了安全调整，但最近的研究表明，视觉模式仍然容易受到越狱攻击。在我们的工作中，我们发现通过使用包含部分有害信息的流程图，可能会诱导MLLM提供额外的有害细节。基于此，我们提出了一种基于自动生成流程图的越狱攻击方法FC-Attack。具体来说，FC-Attack首先微调预训练的LLM，以基于良性数据集创建步骤描述生成器。然后使用生成器生成与有害查询对应的步骤描述，并将其转换为3种不同形状（垂直、水平和S形）的流程图作为视觉提示。然后，这些流程图与良性文本提示相结合，对MLLM执行越狱攻击。我们对Advbridge的评估显示，FC-Attack在多个MLLM中通过图像获得的攻击成功率高达96%，通过视频获得的攻击成功率高达78%。此外，我们还调查了影响攻击性能的因素，包括步骤数和流程图中的字体样式。我们还发现，通过改变字体风格，FC-Attack可以将Claude-3.5中的越狱性能从4%提高到28%。为了减轻攻击，我们探索了几种防御措施，发现AdaShield可以大大降低越狱性能，但公用事业成本会下降。



## **45. Reasoning-to-Defend: Safety-Aware Reasoning Can Defend Large Language Models from Jailbreaking**

推理防御：安全意识推理可以保护大型语言模型免受越狱 cs.CL

EMNLP 2025

**SubmitDate**: 2025-09-20    [abs](http://arxiv.org/abs/2502.12970v3) [paper-pdf](http://arxiv.org/pdf/2502.12970v3)

**Authors**: Junda Zhu, Lingyong Yan, Shuaiqiang Wang, Dawei Yin, Lei Sha

**Abstract**: Large Reasoning Models (LRMs) have recently demonstrated impressive performances across diverse domains. However, how the safety of Large Language Models (LLMs) benefits from enhanced reasoning capabilities against jailbreak queries remains unexplored. To bridge this gap, in this paper, we propose Reasoning-to-Defend (R2D), a novel training paradigm that integrates a safety-aware reasoning mechanism into LLMs' generation process. This enables self-evaluation at each step of the reasoning process, forming safety pivot tokens as indicators of the safety status of responses. Furthermore, in order to improve the accuracy of predicting pivot tokens, we propose Contrastive Pivot Optimization (CPO), which enhances the model's perception of the safety status of given dialogues. LLMs dynamically adjust their response strategies during reasoning, significantly enhancing their safety capabilities defending jailbreak attacks. Extensive experiments demonstrate that R2D effectively mitigates various attacks and improves overall safety, while maintaining the original performances. This highlights the substantial potential of safety-aware reasoning in improving robustness of LRMs and LLMs against various jailbreaks.

摘要: 大型推理模型（LRM）最近在不同领域展示了令人印象深刻的性能。然而，大型语言模型（LLM）的安全性如何从针对越狱查询的增强推理能力中受益仍有待探索。为了弥合这一差距，在本文中，我们提出了推理防御（R2 D），这是一种新型训练范式，将安全感知推理机制集成到LLM的生成过程中。这使得推理过程的每个步骤都能够进行自我评估，形成安全支点令牌作为响应安全状态的指标。此外，为了提高预测枢纽令牌的准确性，我们提出了对比枢纽优化（CPO），它增强了模型对给定对话安全状态的感知。LLM在推理过程中动态调整响应策略，显着增强了防御越狱攻击的安全能力。大量实验表明，R2D有效地缓解了各种攻击并提高了整体安全性，同时保持了原始性能。这凸显了安全意识推理在提高LRM和LLM针对各种越狱的稳健性方面的巨大潜力。



## **46. MIST: Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning**

MIST：通过迭代语义调优破解黑匣子大型语言模型 cs.CL

13 pages, 6 figures

**SubmitDate**: 2025-09-20    [abs](http://arxiv.org/abs/2506.16792v3) [paper-pdf](http://arxiv.org/pdf/2506.16792v3)

**Authors**: Muyang Zheng, Yuanzhi Yao, Changting Lin, Caihong Kai, Yanxiang Chen, Zhiquan Liu

**Abstract**: Despite efforts to align large language models (LLMs) with societal and moral values, these models remain susceptible to jailbreak attacks -- methods designed to elicit harmful responses. Jailbreaking black-box LLMs is considered challenging due to the discrete nature of token inputs, restricted access to the target LLM, and limited query budget. To address the issues above, we propose an effective method for jailbreaking black-box large language Models via Iterative Semantic Tuning, named MIST. MIST enables attackers to iteratively refine prompts that preserve the original semantic intent while inducing harmful content. Specifically, to balance semantic similarity with computational efficiency, MIST incorporates two key strategies: sequential synonym search, and its advanced version -- order-determining optimization. We conduct extensive experiments on two datasets using two open-source and four closed-source models. Results show that MIST achieves competitive attack success rate, relatively low query count, and fair transferability, outperforming or matching state-of-the-art jailbreak methods. Additionally, we conduct analysis on computational efficiency to validate the practical viability of MIST.

摘要: 尽管人们努力将大型语言模型（LLM）与社会和道德价值观保持一致，但这些模型仍然容易受到越狱攻击--这些攻击旨在引发有害反应的方法。由于令牌输入的离散性、对目标LLM的访问受限以及查询预算有限，越狱黑匣子LLM被认为具有挑战性。为了解决上述问题，我们提出了一种通过迭代语义调优破解黑匣子大型语言模型的有效方法，名为MIST。MIST使攻击者能够迭代地改进提示，以保留原始语义意图，同时诱导有害内容。具体来说，为了平衡语义相似性与计算效率，MIST结合了两个关键策略：顺序同义词搜索及其高级版本--顺序确定优化。我们使用两个开源模型和四个开源模型对两个数据集进行了广泛的实验。结果表明，MIST实现了有竞争力的攻击成功率、相对较低的查询计数和公平的可移植性，优于或匹配最先进的越狱方法。此外，我们还对计算效率进行分析，以验证MIST的实际可行性。



## **47. Can an Individual Manipulate the Collective Decisions of Multi-Agents?**

个人可以操纵多主体的集体决策吗？ cs.CL

**SubmitDate**: 2025-09-20    [abs](http://arxiv.org/abs/2509.16494v1) [paper-pdf](http://arxiv.org/pdf/2509.16494v1)

**Authors**: Fengyuan Liu, Rui Zhao, Shuo Chen, Guohao Li, Philip Torr, Lei Han, Jindong Gu

**Abstract**: Individual Large Language Models (LLMs) have demonstrated significant capabilities across various domains, such as healthcare and law. Recent studies also show that coordinated multi-agent systems exhibit enhanced decision-making and reasoning abilities through collaboration. However, due to the vulnerabilities of individual LLMs and the difficulty of accessing all agents in a multi-agent system, a key question arises: If attackers only know one agent, could they still generate adversarial samples capable of misleading the collective decision? To explore this question, we formulate it as a game with incomplete information, where attackers know only one target agent and lack knowledge of the other agents in the system. With this formulation, we propose M-Spoiler, a framework that simulates agent interactions within a multi-agent system to generate adversarial samples. These samples are then used to manipulate the target agent in the target system, misleading the system's collaborative decision-making process. More specifically, M-Spoiler introduces a stubborn agent that actively aids in optimizing adversarial samples by simulating potential stubborn responses from agents in the target system. This enhances the effectiveness of the generated adversarial samples in misleading the system. Through extensive experiments across various tasks, our findings confirm the risks posed by the knowledge of an individual agent in multi-agent systems and demonstrate the effectiveness of our framework. We also explore several defense mechanisms, showing that our proposed attack framework remains more potent than baselines, underscoring the need for further research into defensive strategies.

摘要: 个体大型语言模型（LLM）已在医疗保健和法律等各个领域展现出强大的能力。最近的研究还表明，协调的多智能体系统通过协作表现出增强的决策和推理能力。然而，由于单个LLM的脆弱性以及访问多代理系统中所有代理的困难，出现了一个关键问题：如果攻击者只知道一个代理，他们还能生成能够误导集体决策的对抗样本吗？为了探索这个问题，我们将其描述为一个信息不完整的游戏，其中攻击者只知道一个目标代理，并且缺乏对系统中其他代理的了解。通过这个公式，我们提出了M-Spoiler，这是一个模拟多智能体系统内的智能体交互以生成对抗样本的框架。然后使用这些样本来操纵目标系统中的目标代理，误导系统的协作决策过程。更具体地说，M-Spoiler引入了一种顽固代理，它通过模拟目标系统中代理的潜在顽固反应来积极帮助优化对抗样本。这增强了生成的对抗样本误导系统的有效性。通过针对各种任务的广泛实验，我们的研究结果证实了多代理系统中单个代理的知识所带来的风险，并证明了我们框架的有效性。我们还探索了几种防御机制，表明我们提出的攻击框架仍然比基线更有效，强调了进一步研究防御策略的必要性。



## **48. From Capabilities to Performance: Evaluating Key Functional Properties of LLM Architectures in Penetration Testing**

从能力到性能：在渗透测试中评估LLM架构的关键功能属性 cs.AI

**SubmitDate**: 2025-09-19    [abs](http://arxiv.org/abs/2509.14289v2) [paper-pdf](http://arxiv.org/pdf/2509.14289v2)

**Authors**: Lanxiao Huang, Daksh Dave, Ming Jin, Tyler Cody, Peter Beling

**Abstract**: Large language models (LLMs) are increasingly used to automate or augment penetration testing, but their effectiveness and reliability across attack phases remain unclear. We present a comprehensive evaluation of multiple LLM-based agents, from single-agent to modular designs, across realistic penetration testing scenarios, measuring empirical performance and recurring failure patterns. We also isolate the impact of five core functional capabilities via targeted augmentations: Global Context Memory (GCM), Inter-Agent Messaging (IAM), Context-Conditioned Invocation (CCI), Adaptive Planning (AP), and Real-Time Monitoring (RTM). These interventions support, respectively: (i) context coherence and retention, (ii) inter-component coordination and state management, (iii) tool use accuracy and selective execution, (iv) multi-step strategic planning, error detection, and recovery, and (v) real-time dynamic responsiveness. Our results show that while some architectures natively exhibit subsets of these properties, targeted augmentations substantially improve modular agent performance, especially in complex, multi-step, and real-time penetration testing tasks.

摘要: 大型语言模型（LLM）越来越多地用于自动化或增强渗透测试，但它们在攻击阶段的有效性和可靠性仍不清楚。我们在现实的渗透测试场景中对多个基于LLM的代理（从单代理到模块化设计）进行了全面评估，测量经验性能和反复出现的故障模式。我们还通过有针对性的增强来隔离五种核心功能能力的影响：全球上下文记忆（GCM）、代理间消息传递（ILM）、上下文条件调用（CI）、自适应规划（AP）和实时监控（RTI）。这些干预措施分别支持：（i）上下文一致性和保留，（ii）组件间协调和状态管理，（iii）工具使用准确性和选择性执行，（iv）多步骤战略规划、错误检测和恢复，以及（v）实时动态响应能力。我们的结果表明，虽然一些架构本身表现出这些属性的子集，但有针对性的增强可以大大提高模块化代理的性能，特别是在复杂、多步骤和实时渗透测试任务中。



## **49. Targeting Alignment: Extracting Safety Classifiers of Aligned LLMs**

目标对齐：提取对齐的LLM的安全分类器 cs.CR

**SubmitDate**: 2025-09-19    [abs](http://arxiv.org/abs/2501.16534v2) [paper-pdf](http://arxiv.org/pdf/2501.16534v2)

**Authors**: Jean-Charles Noirot Ferrand, Yohan Beugin, Eric Pauley, Ryan Sheatsley, Patrick McDaniel

**Abstract**: Alignment in large language models (LLMs) is used to enforce guidelines such as safety. Yet, alignment fails in the face of jailbreak attacks that modify inputs to induce unsafe outputs. In this paper, we introduce and evaluate a new technique for jailbreak attacks. We observe that alignment embeds a safety classifier in the LLM responsible for deciding between refusal and compliance, and seek to extract an approximation of this classifier: a surrogate classifier. To this end, we build candidate classifiers from subsets of the LLM. We first evaluate the degree to which candidate classifiers approximate the LLM's safety classifier in benign and adversarial settings. Then, we attack the candidates and measure how well the resulting adversarial inputs transfer to the LLM. Our evaluation shows that the best candidates achieve accurate agreement (an F1 score above 80%) using as little as 20% of the model architecture. Further, we find that attacks mounted on the surrogate classifiers can be transferred to the LLM with high success. For example, a surrogate using only 50% of the Llama 2 model achieved an attack success rate (ASR) of 70% with half the memory footprint and runtime -- a substantial improvement over attacking the LLM directly, where we only observed a 22% ASR. These results show that extracting surrogate classifiers is an effective and efficient means for modeling (and therein addressing) the vulnerability of aligned models to jailbreaking attacks.

摘要: 大型语言模型（LLM）中的对齐用于强制执行安全等准则。然而，面对越狱攻击，调整失败了，这些攻击修改了输入以引发不安全的输出。本文介绍并评估了一种新的越狱攻击技术。我们观察到，对齐在LLM中嵌入了一个安全分类器，负责在拒绝和合规之间做出决定，并试图提取该分类器的近似值：代理分类器。为此，我们从LLM的子集中构建候选分类器。我们首先评估候选分类器在良性和对抗环境中接近LLM安全分类器的程度。然后，我们攻击候选人并衡量由此产生的对抗输入转移到LLM的程度。我们的评估表明，最好的候选人只需使用20%的模型架构即可实现准确的一致性（F1评分高于80%）。此外，我们发现，安装在代理分类器上的攻击可以转移到LLM，具有很高的成功率。例如，仅使用50%的Llama 2模型的代理实现了70%的攻击成功率（ASR），而内存占用和运行时间只有一半-与直接攻击LLM相比有了实质性的改进，我们只观察到22%的ASR。这些结果表明，提取代理分类器是一种有效和高效的手段建模（并在其中解决）的漏洞对齐模型越狱攻击。



## **50. On the Security of Tool-Invocation Prompts for LLM-Based Agentic Systems: An Empirical Risk Assessment**

基于LLM的统计系统工具调用预算的安全性：经验风险评估 cs.CR

**SubmitDate**: 2025-09-19    [abs](http://arxiv.org/abs/2509.05755v4) [paper-pdf](http://arxiv.org/pdf/2509.05755v4)

**Authors**: Yuchong Xie, Mingyu Luo, Zesen Liu, Zhixiang Zhang, Kaikai Zhang, Yu Liu, Zongjie Li, Ping Chen, Shuai Wang, Dongdong She

**Abstract**: LLM-based agentic systems leverage large language models to handle user queries, make decisions, and execute external tools for complex tasks across domains like chatbots, customer service, and software engineering. A critical component of these systems is the Tool Invocation Prompt (TIP), which defines tool interaction protocols and guides LLMs to ensure the security and correctness of tool usage. Despite its importance, TIP security has been largely overlooked. This work investigates TIP-related security risks, revealing that major LLM-based systems like Cursor, Claude Code, and others are vulnerable to attacks such as remote code execution (RCE) and denial of service (DoS). Through a systematic TIP exploitation workflow (TEW), we demonstrate external tool behavior hijacking via manipulated tool invocations. We also propose defense mechanisms to enhance TIP security in LLM-based agentic systems.

摘要: 基于法学硕士的代理系统利用大型语言模型来处理用户查询、做出决策并执行外部工具，以执行跨聊天机器人、客户服务和软件工程等领域的复杂任务。这些系统的一个关键组件是工具调用提示（TIP），它定义了工具交互协议并指导LLM确保工具使用的安全性和正确性。尽管TIP的安全性很重要，但在很大程度上被忽视了。这项工作调查了与TIP相关的安全风险，揭示了Cursor、Claude Code等基于LLM的主要系统容易受到远程代码执行（RCE）和拒绝服务（NOS）等攻击。通过系统性TIP利用工作流程（TEW），我们通过操纵工具调用演示了外部工具行为劫持。我们还提出了防御机制来增强基于LLM的代理系统中的TIP安全性。



