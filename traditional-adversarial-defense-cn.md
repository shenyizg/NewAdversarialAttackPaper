# 传统深度学习模型 - 对抗防御
**update at 2026-01-25 10:36:50**

按分类器置信度从高到低排序。

## **1. Telling Human and Machine Handwriting Apart**

区分人类与机器手写 cs.CV

**SubmitDate**: 2026-01-16    [abs](http://arxiv.org/abs/2601.11700v1) [paper-pdf](https://arxiv.org/pdf/2601.11700v1)

**Confidence**: 0.95

**Authors**: Luis A. Leiva, Moises Diaz, Nuwan T. Attygalle, Miguel A. Ferrer, Rejean Plamondon

**Abstract**: Handwriting movements can be leveraged as a unique form of behavioral biometrics, to verify whether a real user is operating a device or application. This task can be framed as a reverse Turing test in which a computer has to detect if an input instance has been generated by a human or artificially. To tackle this task, we study ten public datasets of handwritten symbols (isolated characters, digits, gestures, pointing traces, and signatures) that are artificially reproduced using seven different synthesizers, including, among others, the Kinematic Theory (Sigma h model), generative adversarial networks, Transformers, and Diffusion models. We train a shallow recurrent neural network that achieves excellent performance (98.3 percent Area Under the ROC Curve (AUC) score and 1.4 percent equal error rate on average across all synthesizers and datasets) using nonfeaturized trajectory data as input. In few-shot settings, we show that our classifier achieves such an excellent performance when trained on just 10 percent of the data, as evaluated on the remaining 90% of the data as a test set. We further challenge our classifier in out-of-domain settings, and observe very competitive results as well. Our work has implications for computerized systems that need to verify human presence, and adds an additional layer of security to keep attackers at bay.

摘要: 手写运动可作为独特的行为生物特征，用于验证设备或应用程序是否由真实用户操作。该任务可视为反向图灵测试，计算机需检测输入实例是由人类生成还是人工合成。为应对此任务，我们研究了十个公开手写符号数据集（包括孤立字符、数字、手势、指向轨迹和签名），并使用七种不同合成器进行人工复现，其中包括运动学理论（Sigma h模型）、生成对抗网络、Transformer和扩散模型等。我们训练了一个浅层循环神经网络，使用非特征化轨迹数据作为输入，在所有合成器和数据集上平均取得了优异性能（ROC曲线下面积达98.3%，等错误率为1.4%）。在少样本设置中，我们的分类器仅使用10%数据训练，在剩余90%测试数据上仍保持优异性能。我们进一步在跨域设置中测试分类器，同样观察到极具竞争力的结果。本研究对需要验证人类存在的计算机系统具有重要意义，并为抵御攻击者提供了额外的安全防护层。



## **2. MIMIR: Masked Image Modeling for Mutual Information-based Adversarial Robustness**

MIMIR：基于互信息的对抗鲁棒性掩码图像建模方法 cs.CV

Accepted by NDSS 2026

**SubmitDate**: 2025-12-15    [abs](http://arxiv.org/abs/2312.04960v5) [paper-pdf](https://arxiv.org/pdf/2312.04960v5)

**Confidence**: 0.95

**Authors**: Xiaoyun Xu, Shujian Yu, Zhuoran Liu, Stjepan Picek

**Abstract**: Vision Transformers (ViTs) have emerged as a fundamental architecture and serve as the backbone of modern vision-language models. Despite their impressive performance, ViTs exhibit notable vulnerability to evasion attacks, necessitating the development of specialized Adversarial Training (AT) strategies tailored to their unique architecture. While a direct solution might involve applying existing AT methods to ViTs, our analysis reveals significant incompatibilities, particularly with state-of-the-art (SOTA) approaches such as Generalist (CVPR 2023) and DBAT (USENIX Security 2024). This paper presents a systematic investigation of adversarial robustness in ViTs and provides a novel theoretical Mutual Information (MI) analysis in its autoencoder-based self-supervised pre-training. Specifically, we show that MI between the adversarial example and its latent representation in ViT-based autoencoders should be constrained via derived MI bounds. Building on this insight, we propose a self-supervised AT method, MIMIR, that employs an MI penalty to facilitate adversarial pre-training by masked image modeling with autoencoders. Extensive experiments on CIFAR-10, Tiny-ImageNet, and ImageNet-1K show that MIMIR can consistently provide improved natural and robust accuracy, where MIMIR outperforms SOTA AT results on ImageNet-1K. Notably, MIMIR demonstrates superior robustness against unforeseen attacks and common corruption data and can also withstand adaptive attacks where the adversary possesses full knowledge of the defense mechanism. Our code and trained models are publicly available at: https://github.com/xiaoyunxxy/MIMIR.

摘要: 视觉Transformer（ViT）已成为基础架构，并作为现代视觉-语言模型的核心。尽管其性能卓越，ViT对规避攻击表现出显著脆弱性，需要针对其独特架构开发专门的对抗训练（AT）策略。虽然直接解决方案可能涉及将现有AT方法应用于ViT，但我们的分析揭示了显著的不兼容性，特别是与最先进（SOTA）方法如Generalist（CVPR 2023）和DBAT（USENIX Security 2024）。本文系统研究了ViT的对抗鲁棒性，并基于自编码器的自监督预训练提出了新颖的互信息（MI）理论分析。具体而言，我们证明了ViT自编码器中对抗样本与其潜在表示之间的MI应通过推导的MI边界进行约束。基于这一洞见，我们提出了一种自监督AT方法MIMIR，该方法采用MI惩罚，通过自编码器的掩码图像建模促进对抗预训练。在CIFAR-10、Tiny-ImageNet和ImageNet-1K上的大量实验表明，MIMIR能持续提升自然准确率和鲁棒准确率，其中在ImageNet-1K上超越了SOTA AT结果。值得注意的是，MIMIR对未知攻击和常见数据损坏表现出卓越的鲁棒性，并能抵御攻击者完全掌握防御机制的自适应攻击。我们的代码和训练模型公开于：https://github.com/xiaoyunxxy/MIMIR。



## **3. FMVP: Masked Flow Matching for Adversarial Video Purification**

FMVP：基于掩码流匹配的对抗性视频净化方法 cs.CV

**SubmitDate**: 2026-01-11    [abs](http://arxiv.org/abs/2601.02228v2) [paper-pdf](https://arxiv.org/pdf/2601.02228v2)

**Confidence**: 0.95

**Authors**: Duoxun Tang, Xueyi Zhang, Chak Hin Wang, Xi Xiao, Dasen Dai, Xinhang Jiang, Wentao Shi, Rui Li, Qing Li

**Abstract**: Video recognition models remain vulnerable to adversarial attacks, while existing diffusion-based purification methods suffer from inefficient sampling and curved trajectories. Directly regressing clean videos from adversarial inputs often fails to recover faithful content due to the subtle nature of perturbations; this necessitates physically shattering the adversarial structure. Therefore, we propose Flow Matching for Adversarial Video Purification FMVP. FMVP physically shatters global adversarial structures via a masking strategy and reconstructs clean video dynamics using Conditional Flow Matching (CFM) with an inpainting objective. To further decouple semantic content from adversarial noise, we design a Frequency-Gated Loss (FGL) that explicitly suppresses high-frequency adversarial residuals while preserving low-frequency fidelity. We design Attack-Aware and Generalist training paradigms to handle known and unknown threats, respectively. Extensive experiments on UCF-101 and HMDB-51 demonstrate that FMVP outperforms state-of-the-art methods (DiffPure, Defense Patterns (DP), Temporal Shuffling (TS) and FlowPure), achieving robust accuracy exceeding 87% against PGD and 89% against CW attacks. Furthermore, FMVP demonstrates superior robustness against adaptive attacks (DiffHammer) and functions as a zero-shot adversarial detector, attaining AUC-ROC scores of 0.98 for PGD and 0.79 for highly imperceptible CW attacks.

摘要: 视频识别模型仍易受对抗攻击影响，而现有基于扩散的净化方法存在采样效率低和轨迹弯曲的问题。由于扰动的隐蔽性，直接从对抗输入回归干净视频往往难以恢复真实内容，这需要物理上破坏对抗结构。为此，我们提出用于对抗视频净化的流匹配方法FMVP。FMVP通过掩码策略物理破坏全局对抗结构，并利用条件流匹配（CFM）结合修复目标重建干净视频动态。为进一步解耦语义内容与对抗噪声，我们设计了频率门控损失（FGL），显式抑制高频对抗残差同时保持低频保真度。我们分别设计了攻击感知和通用训练范式来处理已知和未知威胁。在UCF-101和HMDB-51数据集上的大量实验表明，FMVP在PGD攻击下达到超过87%、在CW攻击下达到89%的鲁棒准确率，优于现有最佳方法（DiffPure、防御模式DP、时序混洗TS和FlowPure）。此外，FMVP对自适应攻击（DiffHammer）展现出卓越鲁棒性，并能作为零样本对抗检测器，在PGD攻击下获得0.98的AUC-ROC分数，在高度隐蔽的CW攻击下获得0.79的AUC-ROC分数。



## **4. Detection of AI Generated Images Using Combined Uncertainty Measures and Particle Swarm Optimised Rejection Mechanism**

基于组合不确定性度量与粒子群优化拒绝机制的AI生成图像检测 cs.CV

Scientific Reports (2025)

**SubmitDate**: 2025-12-20    [abs](http://arxiv.org/abs/2512.18527v1) [paper-pdf](https://arxiv.org/pdf/2512.18527v1)

**Confidence**: 0.95

**Authors**: Rahul Yumlembam, Biju Issac, Nauman Aslam, Eaby Kollonoor Babu, Josh Collyer, Fraser Kennedy

**Abstract**: As AI-generated images become increasingly photorealistic, distinguishing them from natural images poses a growing challenge. This paper presents a robust detection framework that leverages multiple uncertainty measures to decide whether to trust or reject a model's predictions. We focus on three complementary techniques: Fisher Information, which captures the sensitivity of model parameters to input variations; entropy-based uncertainty from Monte Carlo Dropout, which reflects predictive variability; and predictive variance from a Deep Kernel Learning framework using a Gaussian Process classifier. To integrate these diverse uncertainty signals, Particle Swarm Optimisation is used to learn optimal weightings and determine an adaptive rejection threshold. The model is trained on Stable Diffusion-generated images and evaluated on GLIDE, VQDM, Midjourney, BigGAN, and StyleGAN3, each introducing significant distribution shifts. While standard metrics such as prediction probability and Fisher-based measures perform well in distribution, their effectiveness degrades under shift. In contrast, the Combined Uncertainty measure consistently achieves an incorrect rejection rate of approximately 70 percent on unseen generators, successfully filtering most misclassified AI samples. Although the system occasionally rejects correct predictions from newer generators, this conservative behaviour is acceptable, as rejected samples can support retraining. The framework maintains high acceptance of accurate predictions for natural images and in-domain AI data. Under adversarial attacks using FGSM and PGD, the Combined Uncertainty method rejects around 61 percent of successful attacks, while GP-based uncertainty alone achieves up to 80 percent. Overall, the results demonstrate that multi-source uncertainty fusion provides a resilient and adaptive solution for AI-generated image detection.

摘要: 随着AI生成图像日益逼真，区分其与自然图像的挑战日益严峻。本文提出一种鲁棒的检测框架，利用多种不确定性度量来决定是否信任或拒绝模型的预测。我们聚焦三种互补技术：费舍尔信息（捕捉模型参数对输入变化的敏感性）、蒙特卡洛Dropout的熵基不确定性（反映预测变异性），以及采用高斯过程分类器的深度核学习框架的预测方差。为整合这些多样化不确定性信号，采用粒子群优化学习最优权重并确定自适应拒绝阈值。模型在Stable Diffusion生成图像上训练，并在GLIDE、VQDM、Midjourney、BigGAN和StyleGAN3上进行评估，每种生成器均引入显著分布偏移。虽然预测概率和费舍尔度量等标准指标在分布内表现良好，但其在分布偏移下效果下降。相比之下，组合不确定性度量在未见生成器上持续实现约70%的错误拒绝率，成功过滤多数误分类的AI样本。尽管系统偶尔会拒绝新生成器的正确预测，但这种保守行为是可接受的，因为被拒绝样本可用于支持再训练。该框架对自然图像和域内AI数据保持高准确预测接受率。在使用FGSM和PGD的对抗攻击下，组合不确定性方法拒绝约61%的成功攻击，而仅基于GP的不确定性可达80%。总体而言，结果表明多源不确定性融合为AI生成图像检测提供了具有韧性和适应性的解决方案。



## **5. Topological passivation makes high strength alloys insensitive to hydrogen embrittlement**

拓扑钝化使高强度合金对氢脆不敏感 cond-mat.mtrl-sci

**SubmitDate**: 2025-11-28    [abs](http://arxiv.org/abs/2512.00131v1) [paper-pdf](https://arxiv.org/pdf/2512.00131v1)

**Confidence**: 0.95

**Authors**: Huijie Cheng, Binhan Sun, Aochen Zhang, Dirk Ponge, Fengkai Yan, Tiwen Lu, Xian-Cheng Zhang, Dierk Raabe, Shan-Tung Tu

**Abstract**: Infrastructure parts for a hydrogen (H) economy need alloys that are mechanically strong and at the same time resistant to the most dangerous and abrupt type of failure mode, namely, H embrittlement. These two properties are in fundamental conflict, as increasing strength typically amplifies susceptibility to H-related failure. Here, we introduce a new approach to make alloys resistant to H embrittlement, by creating a topological passivation layer (up to a few hundred micrometers thick) near the material surface, the region that is most vulnerable to H ingress and attack. It features instead a layer of ultrafine laminated grains with tens of times higher dislocation density than conventional materials, altering H diffusion, trapping and crack evolution. We tested the concept on a face-centered cubic (FCC) CoCrNi medium entropy model alloy which undergoes severe H-induced intergranular cracking. Two key mechanisms create the topological passivation: First, the high density (up to ~1.3e15 m-2) of H-trapping dislocations within the passivating grain layer decelerates H migration by up to about an order of magnitude, delaying H-induced crack initiation at grain boundaries. More importantly, once unavoidable micro-sized H-induced intergranular cracks emerge in the topmost surface region, they become completely arrested by the laminated grains, due to a transition in the embrittlement mechanism from H-enhanced grain boundary decohesion to highly energy-dissipative dislocation-associated cracking. These effects almost completely eliminate H embrittlement, at even doubled yield strength, when exposing the so architected material to harsh H attack. Our approach leverages surface mechanical treatments to tailor metallic microstructures in surface regions most susceptible to H attack, providing a scalable solution to protect alloys from H-induced damage.

摘要: 氢经济的基础设施部件需要兼具高机械强度和抗氢脆（最危险且突发性失效模式）的合金。这两种性能存在根本性冲突，因为提高强度通常会加剧氢相关失效的敏感性。本文提出一种新方法，通过在材料表面区域（氢侵入和攻击最易发生的部位）创建拓扑钝化层（厚度可达数百微米），使合金抵抗氢脆。该层由超细层状晶粒构成，其位错密度比传统材料高数十倍，从而改变氢的扩散、捕获和裂纹演化。我们在面心立方（FCC）CoCrNi中熵模型合金上验证了这一概念，该合金通常发生严重的氢致晶间开裂。拓扑钝化通过两种关键机制实现：首先，钝化晶粒层内的高密度位错（高达约1.3e15 m-2）作为氢陷阱，将氢迁移速率降低约一个数量级，延缓晶界处氢致裂纹的萌生。更重要的是，当表面区域不可避免地出现微米级氢致晶间裂纹时，层状晶粒会完全阻止其扩展，这是因为脆化机制从氢增强的晶界脱聚转变为高能量耗散的位错相关开裂。即使在屈服强度加倍的情况下，将这种结构材料暴露于严苛的氢攻击环境中，这些效应几乎完全消除了氢脆。我们的方法利用表面机械处理，在易受氢攻击的表面区域定制金属微观结构，为保护合金免受氢损伤提供了可扩展的解决方案。



## **6. ATAC: Augmentation-Based Test-Time Adversarial Correction for CLIP**

ATAC：基于增强的CLIP测试时对抗性校正 cs.CV

16 pages

**SubmitDate**: 2025-12-14    [abs](http://arxiv.org/abs/2511.17362v2) [paper-pdf](https://arxiv.org/pdf/2511.17362v2)

**Confidence**: 0.95

**Authors**: Linxiang Su, András Balogh

**Abstract**: Despite its remarkable success in zero-shot image-text matching, CLIP remains highly vulnerable to adversarial perturbations on images. As adversarial fine-tuning is prohibitively costly, recent works explore various test-time defense strategies; however, these approaches still exhibit limited robustness. In this work, we revisit this problem and propose a simple yet effective strategy: Augmentation-based Test-time Adversarial Correction (ATAC). Our method operates directly in the embedding space of CLIP, calculating augmentation-induced drift vectors to infer a semantic recovery direction and correcting the embedding based on the angular consistency of these latent drifts. Across a wide range of benchmarks, ATAC consistently achieves remarkably high robustness, surpassing that of previous state-of-the-art methods by nearly 50\% on average, all while requiring minimal computational overhead. Furthermore, ATAC retains state-of-the-art robustness in unconventional and extreme settings and even achieves nontrivial robustness against adaptive attacks. Our results demonstrate that ATAC is an efficient method in a novel paradigm for test-time adversarial defenses in the embedding space of CLIP.

摘要: 尽管CLIP在零样本图像-文本匹配方面取得了显著成功，但其对图像对抗性扰动仍高度脆弱。由于对抗性微调成本过高，近期研究探索了多种测试时防御策略；然而，这些方法的鲁棒性仍然有限。本研究重新审视该问题，提出一种简单而有效的策略：基于增强的测试时对抗性校正（ATAC）。我们的方法直接在CLIP的嵌入空间中操作，通过计算增强诱导的漂移向量来推断语义恢复方向，并基于这些潜在漂移的角度一致性校正嵌入。在广泛的基准测试中，ATAC始终展现出卓越的鲁棒性，平均比先前最先进方法高出近50%，同时仅需极小的计算开销。此外，ATAC在非常规和极端设置下仍保持最先进的鲁棒性，甚至能对自适应攻击实现显著防御。我们的结果表明，ATAC是在CLIP嵌入空间中进行测试时对抗防御的新型高效方法。



## **7. MDD: a Mask Diffusion Detector to Protect Speaker Verification Systems from Adversarial Perturbations**

MDD：一种基于掩码扩散的检测器，用于保护说话人验证系统免受对抗性扰动 eess.AS

Accepted by APSIPA ASC 2025

**SubmitDate**: 2025-08-26    [abs](http://arxiv.org/abs/2508.19180v1) [paper-pdf](https://arxiv.org/pdf/2508.19180v1)

**Confidence**: 0.95

**Authors**: Yibo Bai, Sizhou Chen, Michele Panariello, Xiao-Lei Zhang, Massimiliano Todisco, Nicholas Evans

**Abstract**: Speaker verification systems are increasingly deployed in security-sensitive applications but remain highly vulnerable to adversarial perturbations. In this work, we propose the Mask Diffusion Detector (MDD), a novel adversarial detection and purification framework based on a \textit{text-conditioned masked diffusion model}. During training, MDD applies partial masking to Mel-spectrograms and progressively adds noise through a forward diffusion process, simulating the degradation of clean speech features. A reverse process then reconstructs the clean representation conditioned on the input transcription. Unlike prior approaches, MDD does not require adversarial examples or large-scale pretraining. Experimental results show that MDD achieves strong adversarial detection performance and outperforms prior state-of-the-art methods, including both diffusion-based and neural codec-based approaches. Furthermore, MDD effectively purifies adversarially-manipulated speech, restoring speaker verification performance to levels close to those observed under clean conditions. These findings demonstrate the potential of diffusion-based masking strategies for secure and reliable speaker verification systems.

摘要: 说话人验证系统在安全敏感应用中的部署日益广泛，但其仍极易受到对抗性扰动的影响。本文提出了一种基于文本条件掩码扩散模型的新型对抗性检测与净化框架——掩码扩散检测器（MDD）。在训练过程中，MDD对梅尔频谱图进行部分掩码处理，并通过前向扩散过程逐步添加噪声，模拟干净语音特征的退化。随后，基于输入文本的条件，通过反向过程重建干净的语音表示。与先前方法不同，MDD无需对抗样本或大规模预训练。实验结果表明，MDD在对抗性检测方面表现出色，性能优于包括基于扩散和基于神经编解码器在内的现有先进方法。此外，MDD能有效净化受对抗性操纵的语音，使说话人验证性能恢复至接近干净语音条件下的水平。这些发现证明了基于扩散的掩码策略在构建安全可靠的说话人验证系统方面的潜力。



## **8. Evolving from Single-modal to Multi-modal Facial Deepfake Detection: Progress and Challenges**

从单模态到多模态人脸深度伪造检测的演进：进展与挑战 cs.CV

P. Liu is with the Department of Computer Science and Engineering, University of Nevada, Reno, NV, 89512. Q. Tao and J. Zhou are with Centre for Frontier AI Research (CFAR), and Institute of High Performance Computing (IHPC), A*STAR, Singapore. J. Zhou is also with Centre for Advanced Technologies in Online Safety (CATOS), A*STAR, Singapore. J. Zhou is the corresponding author

**SubmitDate**: 2025-04-03    [abs](http://arxiv.org/abs/2406.06965v4) [paper-pdf](https://arxiv.org/pdf/2406.06965v4)

**Confidence**: 0.95

**Authors**: Ping Liu, Qiqi Tao, Joey Tianyi Zhou

**Abstract**: As synthetic media, including video, audio, and text, become increasingly indistinguishable from real content, the risks of misinformation, identity fraud, and social manipulation escalate. This survey traces the evolution of deepfake detection from early single-modal methods to sophisticated multi-modal approaches that integrate audio-visual and text-visual cues. We present a structured taxonomy of detection techniques and analyze the transition from GAN-based to diffusion model-driven deepfakes, which introduce new challenges due to their heightened realism and robustness against detection. Unlike prior surveys that primarily focus on single-modal detection or earlier deepfake techniques, this work provides the most comprehensive study to date, encompassing the latest advancements in multi-modal deepfake detection, generalization challenges, proactive defense mechanisms, and emerging datasets specifically designed to support new interpretability and reasoning tasks. We further explore the role of Vision-Language Models (VLMs) and Multimodal Large Language Models (MLLMs) in strengthening detection robustness against increasingly sophisticated deepfake attacks. By systematically categorizing existing methods and identifying emerging research directions, this survey serves as a foundation for future advancements in combating AI-generated facial forgeries. A curated list of all related papers can be found at \href{https://github.com/qiqitao77/Comprehensive-Advances-in-Deepfake-Detection-Spanning-Diverse-Modalities}{https://github.com/qiqitao77/Awesome-Comprehensive-Deepfake-Detection}.

摘要: 随着包括视频、音频和文本在内的合成媒体与真实内容越来越难以区分，虚假信息、身份欺诈和社会操纵的风险不断升级。本综述追溯了深度伪造检测从早期单模态方法到集成视听和文本视觉线索的复杂多模态方法的演进过程。我们提出了检测技术的结构化分类法，并分析了从基于GAN到扩散模型驱动的深度伪造的转变，后者因其更高的真实性和对检测的鲁棒性而带来了新的挑战。与先前主要关注单模态检测或早期深度伪造技术的综述不同，本工作提供了迄今为止最全面的研究，涵盖了多模态深度伪造检测的最新进展、泛化挑战、主动防御机制，以及专门为支持新的可解释性和推理任务而设计的新兴数据集。我们进一步探讨了视觉语言模型（VLMs）和多模态大语言模型（MLLMs）在增强检测鲁棒性以应对日益复杂的深度伪造攻击中的作用。通过对现有方法进行系统分类并识别新兴研究方向，本综述为未来对抗AI生成人脸伪造的进展奠定了基础。所有相关论文的精选列表可在 \href{https://github.com/qiqitao77/Comprehensive-Advances-in-Deepfake-Detection-Spanning-Diverse-Modalities}{https://github.com/qiqitao77/Awesome-Comprehensive-Deepfake-Detection} 找到。



## **9. Unified Framework for Qualifying Security Boundary of PUFs Against Machine Learning Attacks**

评估PUF抗机器学习攻击安全边界的统一框架 cs.CR

13 pages, 8 figures

**SubmitDate**: 2026-01-08    [abs](http://arxiv.org/abs/2601.04697v1) [paper-pdf](https://arxiv.org/pdf/2601.04697v1)

**Confidence**: 0.95

**Authors**: Hongming Fei, Zilong Hu, Prosanta Gope, Biplab Sikdar

**Abstract**: Physical Unclonable Functions (PUFs) serve as lightweight, hardware-intrinsic entropy sources widely deployed in IoT security applications. However, delay-based PUFs are vulnerable to Machine Learning Attacks (MLAs), undermining their assumed unclonability. There are no valid metrics for evaluating PUF MLA resistance, but empirical modelling experiments, which lack theoretical guarantees and are highly sensitive to advances in machine learning techniques. To address the fundamental gap between PUF designs and security qualifications, this work proposes a novel, formal, and unified framework for evaluating PUF security against modelling attacks by providing security lower bounds, independent of specific attack models or learning algorithms. We mathematically characterise the adversary's advantage in predicting responses to unseen challenges based solely on observed challenge-response pairs (CRPs), formulating the problem as a conditional probability estimation over the space of candidate PUFs. We present our analysis on previous "broken" PUFs, e.g., Arbiter PUFs, XOR PUFs, Feed-Forward PUFs, and for the first time compare their MLA resistance in a formal way. In addition, we evaluate the currently "secure" CT PUF, and show its security boundary. We demonstrate that the proposed approach systematically quantifies PUF resilience, captures subtle security differences, and provides actionable, theoretically grounded security guarantees for the practical deployment of PUFs.

摘要: 物理不可克隆函数（PUFs）作为轻量级、硬件固有的熵源，广泛应用于物联网安全领域。然而，基于延迟的PUFs易受机器学习攻击（MLAs），削弱了其假定的不可克隆性。目前缺乏评估PUF抗MLA能力的有效指标，主要依赖经验建模实验——这些方法缺乏理论保证，且对机器学习技术进步高度敏感。为弥合PUF设计与安全评估之间的根本差距，本研究提出一种新颖、形式化、统一的框架，通过提供独立于特定攻击模型或学习算法的安全下界，评估PUF抗建模攻击的安全性。我们通过数学方法刻画攻击者仅基于观察到的挑战-响应对（CRPs）预测未知挑战响应的优势，将问题形式化为候选PUF空间上的条件概率估计。我们对先前被“攻破”的PUFs（如仲裁器PUF、异或PUF、前馈PUF）进行分析，首次以形式化方法比较它们的MLA抗性。此外，我们评估当前“安全”的CT PUF，并揭示其安全边界。研究表明，所提方法能系统量化PUF的弹性，捕捉细微的安全差异，并为PUF的实际部署提供可操作、理论坚实的安全保证。



## **10. Colliding with Adversaries at ECML-PKDD 2025 Model Robustness Competition 1st Prize Solution**

ECML-PKDD 2025模型鲁棒性竞赛一等奖解决方案：对抗碰撞 cs.LG

**SubmitDate**: 2025-12-10    [abs](http://arxiv.org/abs/2510.16443v2) [paper-pdf](https://arxiv.org/pdf/2510.16443v2)

**Confidence**: 0.95

**Authors**: Dimitris Stefanopoulos, Andreas Voskou

**Abstract**: This report presents the winning solution for Task 2 of Colliding with Adversaries: A Challenge on Robust Learning in High Energy Physics Discovery at ECML-PKDD 2025. The goal of the challenge was to design and train a robust ANN-based model capable of achieving high accuracy in a binary classification task on both clean and adversarial data generated with the Random Distribution Shuffle Attack (RDSA). Our solution consists of two components: a data generation phase and a robust model training phase. In the first phase, we produced 15 million artificial training samples using a custom methodology derived from Random Distribution Shuffle Attack (RDSA). In the second phase, we introduced a robust architecture comprising (i)a Feature Embedding Block with shared weights among features of the same type and (ii)a Dense Fusion Tail responsible for the final prediction. Training this architecture on our adversarial dataset achieved a mixed accuracy score of 80\%, exceeding the second-place solution by two percentage points.

摘要: 本报告介绍了ECML-PKDD 2025“对抗碰撞：高能物理发现中的鲁棒学习挑战赛”任务2的获胜解决方案。该挑战的目标是设计并训练一个基于ANN的鲁棒模型，能够在由随机分布重排攻击（RDSA）生成的干净数据和对抗数据上实现高精度的二分类任务。我们的解决方案包含两个组成部分：数据生成阶段和鲁棒模型训练阶段。在第一阶段，我们采用基于随机分布重排攻击（RDSA）的自定义方法生成了1500万个人工训练样本。在第二阶段，我们引入了一种鲁棒架构，包括：（i）具有同类型特征共享权重的特征嵌入块；（ii）负责最终预测的密集融合尾部。在我们的对抗数据集上训练该架构实现了80%的综合准确率，超出第二名解决方案两个百分点。



## **11. Sy-FAR: Symmetry-based Fair Adversarial Robustness**

Sy-FAR：基于对称性的公平对抗鲁棒性 cs.LG

Accepted to USENIX Security 2026

**SubmitDate**: 2026-01-19    [abs](http://arxiv.org/abs/2509.12939v2) [paper-pdf](https://arxiv.org/pdf/2509.12939v2)

**Confidence**: 0.95

**Authors**: Haneen Najjar, Eyal Ronen, Mahmood Sharif

**Abstract**: Security-critical machine-learning (ML) systems, such as face-recognition systems, are susceptible to adversarial examples, including real-world physically realizable attacks. Various means to boost ML's adversarial robustness have been proposed; however, they typically induce unfair robustness: It is often easier to attack from certain classes or groups than from others. Several techniques have been developed to improve adversarial robustness while seeking perfect fairness between classes. Yet, prior work has focused on settings where security and fairness are less critical. Our insight is that achieving perfect parity in realistic fairness-critical tasks, such as face recognition, is often infeasible -- some classes may be highly similar, leading to more misclassifications between them. Instead, we suggest that seeking symmetry -- i.e., attacks from class $i$ to $j$ would be as successful as from $j$ to $i$ -- is more tractable. Intuitively, symmetry is a desirable because class resemblance is a symmetric relation in most domains. Additionally, as we prove theoretically, symmetry between individuals induces symmetry between any set of sub-groups, in contrast to other fairness notions where group-fairness is often elusive. We develop Sy-FAR, a technique to encourage symmetry while also optimizing adversarial robustness and extensively evaluate it using five datasets, with three model architectures, including against targeted and untargeted realistic attacks. The results show Sy-FAR significantly improves fair adversarial robustness compared to state-of-the-art methods. Moreover, we find that Sy-FAR is faster and more consistent across runs. Notably, Sy-FAR also ameliorates another type of unfairness we discover in this work -- target classes that adversarial examples are likely to be classified into become significantly less vulnerable after inducing symmetry.

摘要: 安全关键型机器学习（ML）系统（如人脸识别系统）容易受到对抗样本的攻击，包括现实世界中可物理实现的攻击。虽然已提出多种提升ML对抗鲁棒性的方法，但它们通常会导致不公平的鲁棒性：攻击某些类别或群体往往比其他类别更容易。已有若干技术旨在提升对抗鲁棒性的同时追求类别间的完全公平，但先前研究多集中于安全和公平性要求较低的场景。我们的核心见解是：在现实公平关键任务（如人脸识别）中实现完全公平往往不可行——某些类别可能高度相似，导致它们之间更容易出现误分类。因此，我们提出追求对称性——即从类别$i$到$j$的攻击成功率应与$j$到$i$相当——是更可行的方案。直观上，对称性是理想特性，因为在大多数领域中类别相似性是对称关系。此外，我们通过理论证明，个体间的对称性会自然诱导任意子群体间的对称性，这与其它常难以实现群体公平的公平性概念形成对比。我们开发了Sy-FAR技术，在优化对抗鲁棒性的同时促进对称性，并使用五个数据集、三种模型架构进行了全面评估，包括针对定向和非定向的现实攻击。结果表明，与最先进方法相比，Sy-FAR显著提升了公平对抗鲁棒性。此外，Sy-FAR具有更快的训练速度和更好的运行一致性。值得注意的是，Sy-FAR还缓解了本研究发现的新型不公平现象：在引入对称性后，对抗样本易被误分类的目标类别脆弱性显著降低。



## **12. ASRJam: Human-Friendly AI Speech Jamming to Prevent Automated Phone Scams**

ASRJam：面向人类的友好型AI语音干扰技术，用于防范自动化电话诈骗 cs.CL

**SubmitDate**: 2025-06-10    [abs](http://arxiv.org/abs/2506.11125v1) [paper-pdf](https://arxiv.org/pdf/2506.11125v1)

**Confidence**: 0.95

**Authors**: Freddie Grabovski, Gilad Gressel, Yisroel Mirsky

**Abstract**: Large Language Models (LLMs), combined with Text-to-Speech (TTS) and Automatic Speech Recognition (ASR), are increasingly used to automate voice phishing (vishing) scams. These systems are scalable and convincing, posing a significant security threat. We identify the ASR transcription step as the most vulnerable link in the scam pipeline and introduce ASRJam, a proactive defence framework that injects adversarial perturbations into the victim's audio to disrupt the attacker's ASR. This breaks the scam's feedback loop without affecting human callers, who can still understand the conversation. While prior adversarial audio techniques are often unpleasant and impractical for real-time use, we also propose EchoGuard, a novel jammer that leverages natural distortions, such as reverberation and echo, that are disruptive to ASR but tolerable to humans. To evaluate EchoGuard's effectiveness and usability, we conducted a 39-person user study comparing it with three state-of-the-art attacks. Results show that EchoGuard achieved the highest overall utility, offering the best combination of ASR disruption and human listening experience.

摘要: 大型语言模型（LLMs）结合文本转语音（TTS）和自动语音识别（ASR）技术，正被越来越多地用于自动化语音钓鱼（vishing）诈骗。这些系统具有可扩展性和高度欺骗性，构成了严重的安全威胁。我们识别出ASR转录步骤是诈骗流程中最脆弱的环节，并提出了ASRJam——一种主动防御框架，通过向受害者音频中注入对抗性扰动来干扰攻击者的ASR系统。这种方法能在不影响人类通话者（仍可理解对话内容）的情况下破坏诈骗的反馈循环。虽然现有的对抗性音频技术常产生令人不适的干扰且难以实时应用，我们还提出了EchoGuard——一种新型干扰器，利用混响和回声等自然失真现象，这些失真对ASR具有破坏性但对人类听觉尚可接受。为评估EchoGuard的有效性和可用性，我们进行了39人规模的用户研究，将其与三种最先进的攻击技术进行对比。结果表明，EchoGuard实现了最高的综合效用，在ASR干扰效果与人类听觉体验之间达到了最佳平衡。



## **13. Whispering Under the Eaves: Protecting User Privacy Against Commercial and LLM-powered Automatic Speech Recognition Systems**

屋檐下的低语：保护用户隐私免受商业和LLM驱动的自动语音识别系统侵害 cs.CR

Accept to USENIX Security 2025

**SubmitDate**: 2025-04-01    [abs](http://arxiv.org/abs/2504.00858v1) [paper-pdf](https://arxiv.org/pdf/2504.00858v1)

**Confidence**: 0.95

**Authors**: Weifei Jin, Yuxin Cao, Junjie Su, Derui Wang, Yedi Zhang, Minhui Xue, Jie Hao, Jin Song Dong, Yixian Yang

**Abstract**: The widespread application of automatic speech recognition (ASR) supports large-scale voice surveillance, raising concerns about privacy among users. In this paper, we concentrate on using adversarial examples to mitigate unauthorized disclosure of speech privacy thwarted by potential eavesdroppers in speech communications. While audio adversarial examples have demonstrated the capability to mislead ASR models or evade ASR surveillance, they are typically constructed through time-intensive offline optimization, restricting their practicality in real-time voice communication. Recent work overcame this limitation by generating universal adversarial perturbations (UAPs) and enhancing their transferability for black-box scenarios. However, they introduced excessive noise that significantly degrades audio quality and affects human perception, thereby limiting their effectiveness in practical scenarios. To address this limitation and protect live users' speech against ASR systems, we propose a novel framework, AudioShield. Central to this framework is the concept of Transferable Universal Adversarial Perturbations in the Latent Space (LS-TUAP). By transferring the perturbations to the latent space, the audio quality is preserved to a large extent. Additionally, we propose target feature adaptation to enhance the transferability of UAPs by embedding target text features into the perturbations. Comprehensive evaluation on four commercial ASR APIs (Google, Amazon, iFlytek, and Alibaba), three voice assistants, two LLM-powered ASR and one NN-based ASR demonstrates the protection superiority of AudioShield over existing competitors, and both objective and subjective evaluations indicate that AudioShield significantly improves the audio quality. Moreover, AudioShield also shows high effectiveness in real-time end-to-end scenarios, and demonstrates strong resilience against adaptive countermeasures.

摘要: 自动语音识别（ASR）的广泛应用支持大规模语音监控，引发了用户对隐私的担忧。本文聚焦于利用对抗样本来防止语音通信中潜在窃听者导致的语音隐私未经授权泄露。虽然音频对抗样本已证明能够误导ASR模型或规避ASR监控，但它们通常通过耗时的离线优化构建，限制了其在实时语音通信中的实用性。近期研究通过生成通用对抗扰动（UAPs）并增强其在黑盒场景中的可迁移性，克服了这一限制。然而，这些方法引入了过多噪声，显著降低了音频质量并影响人类感知，从而限制了其在实际场景中的有效性。为解决这一局限并保护实时用户的语音免受ASR系统侵害，我们提出了一个新颖框架AudioShield。该框架的核心是潜在空间可迁移通用对抗扰动（LS-TUAP）概念。通过将扰动迁移至潜在空间，音频质量在很大程度上得以保留。此外，我们提出目标特征适配方法，通过将目标文本特征嵌入扰动中来增强UAPs的可迁移性。对四个商业ASR API（Google、Amazon、iFlytek和Alibaba）、三个语音助手、两个LLM驱动的ASR以及一个基于神经网络的ASR进行的全面评估表明，AudioShield的保护性能优于现有竞争对手，主客观评估均显示AudioShield显著提升了音频质量。此外，AudioShield在实时端到端场景中也表现出高效性，并对自适应对抗措施展现出强大的抵御能力。



## **14. Quantitative Analysis of Deeply Quantized Tiny Neural Networks Robust to Adversarial Attacks**

对抗攻击鲁棒的深度量化微型神经网络定量分析 cs.LG

arXiv admin note: substantial text overlap with arXiv:2304.12829

**SubmitDate**: 2025-03-12    [abs](http://arxiv.org/abs/2503.08973v1) [paper-pdf](https://arxiv.org/pdf/2503.08973v1)

**Confidence**: 0.95

**Authors**: Idris Zakariyya, Ferheen Ayaz, Mounia Kharbouche-Harrari, Jeremy Singer, Sye Loong Keoh, Danilo Pau, José Cano

**Abstract**: Reducing the memory footprint of Machine Learning (ML) models, especially Deep Neural Networks (DNNs), is imperative to facilitate their deployment on resource-constrained edge devices. However, a notable drawback of DNN models lies in their susceptibility to adversarial attacks, wherein minor input perturbations can deceive them. A primary challenge revolves around the development of accurate, resilient, and compact DNN models suitable for deployment on resource-constrained edge devices. This paper presents the outcomes of a compact DNN model that exhibits resilience against both black-box and white-box adversarial attacks. This work has achieved this resilience through training with the QKeras quantization-aware training framework. The study explores the potential of QKeras and an adversarial robustness technique, Jacobian Regularization (JR), to co-optimize the DNN architecture through per-layer JR methodology. As a result, this paper has devised a DNN model employing this co-optimization strategy based on Stochastic Ternary Quantization (STQ). Its performance was compared against existing DNN models in the face of various white-box and black-box attacks. The experimental findings revealed that, the proposed DNN model had small footprint and on average, it exhibited better performance than Quanos and DS-CNN MLCommons/TinyML (MLC/T) benchmarks when challenged with white-box and black-box attacks, respectively, on the CIFAR-10 image and Google Speech Commands audio datasets.

摘要: 减少机器学习（ML）模型（尤其是深度神经网络（DNNs））的内存占用对于促进其在资源受限的边缘设备上部署至关重要。然而，DNN模型的一个显著缺点在于其对对抗攻击的脆弱性，即微小的输入扰动即可欺骗模型。主要挑战在于开发适用于资源受限边缘设备的精确、鲁棒且紧凑的DNN模型。本文展示了一种紧凑型DNN模型的结果，该模型对黑盒和白盒对抗攻击均表现出鲁棒性。本研究通过使用QKeras量化感知训练框架进行训练实现了这种鲁棒性。该研究探索了QKeras与对抗鲁棒性技术——雅可比正则化（JR）通过逐层JR方法共同优化DNN架构的潜力。因此，本文设计了一种基于随机三元量化（STQ）的DNN模型，采用这种协同优化策略。在面临各种白盒和黑盒攻击时，将其性能与现有DNN模型进行了比较。实验结果表明，所提出的DNN模型具有较小的内存占用，并且在CIFAR-10图像和Google Speech Commands音频数据集上，面对白盒和黑盒攻击时，其平均性能分别优于Quanos和DS-CNN MLCommons/TinyML（MLC/T）基准模型。



## **15. ASVspoof 5: Design, Collection and Validation of Resources for Spoofing, Deepfake, and Adversarial Attack Detection Using Crowdsourced Speech**

ASVspoof 5：基于众包语音的欺骗、深度伪造和对抗攻击检测资源的设计、收集与验证 eess.AS

Database link: https://zenodo.org/records/14498691, Database mirror link: https://huggingface.co/datasets/jungjee/asvspoof5, ASVspoof 5 Challenge Workshop Proceeding: https://www.isca-archive.org/asvspoof_2024/index.html

**SubmitDate**: 2025-04-24    [abs](http://arxiv.org/abs/2502.08857v4) [paper-pdf](https://arxiv.org/pdf/2502.08857v4)

**Confidence**: 0.95

**Authors**: Xin Wang, Héctor Delgado, Hemlata Tak, Jee-weon Jung, Hye-jin Shim, Massimiliano Todisco, Ivan Kukanov, Xuechen Liu, Md Sahidullah, Tomi Kinnunen, Nicholas Evans, Kong Aik Lee, Junichi Yamagishi, Myeonghun Jeong, Ge Zhu, Yongyi Zang, You Zhang, Soumi Maiti, Florian Lux, Nicolas Müller, Wangyou Zhang, Chengzhe Sun, Shuwei Hou, Siwei Lyu, Sébastien Le Maguer, Cheng Gong, Hanjie Guo, Liping Chen, Vishwanath Singh

**Abstract**: ASVspoof 5 is the fifth edition in a series of challenges which promote the study of speech spoofing and deepfake attacks as well as the design of detection solutions. We introduce the ASVspoof 5 database which is generated in a crowdsourced fashion from data collected in diverse acoustic conditions (cf. studio-quality data for earlier ASVspoof databases) and from ~2,000 speakers (cf. ~100 earlier). The database contains attacks generated with 32 different algorithms, also crowdsourced, and optimised to varying degrees using new surrogate detection models. Among them are attacks generated with a mix of legacy and contemporary text-to-speech synthesis and voice conversion models, in addition to adversarial attacks which are incorporated for the first time. ASVspoof 5 protocols comprise seven speaker-disjoint partitions. They include two distinct partitions for the training of different sets of attack models, two more for the development and evaluation of surrogate detection models, and then three additional partitions which comprise the ASVspoof 5 training, development and evaluation sets. An auxiliary set of data collected from an additional 30k speakers can also be used to train speaker encoders for the implementation of attack algorithms. Also described herein is an experimental validation of the new ASVspoof 5 database using a set of automatic speaker verification and spoof/deepfake baseline detectors. With the exception of protocols and tools for the generation of spoofed/deepfake speech, the resources described in this paper, already used by participants of the ASVspoof 5 challenge in 2024, are now all freely available to the community.

摘要: ASVspoof 5是该系列挑战赛的第五版，旨在推动语音欺骗与深度伪造攻击研究及检测方案设计。我们介绍了通过众包方式生成的ASVspoof 5数据库，其数据采集自多样声学环境（对比早期ASVspoof的录音棚级数据）及约2000名说话人（早期约100名）。该数据库包含32种不同算法生成的攻击样本（同样采用众包方式），并利用新型代理检测模型进行了不同程度的优化。除首次纳入的对抗攻击外，还包括基于传统与当代文本转语音合成及语音转换模型生成的攻击。ASVspoof 5协议包含七个说话人互斥分区：两个独立分区用于训练不同攻击模型集，两个分区用于代理检测模型的开发与评估，另外三个分区构成ASVspoof 5的训练集、开发集和评估集。从额外3万名说话人采集的辅助数据集也可用于训练说话人编码器以实施攻击算法。本文还通过自动说话人验证及欺骗/深度伪造基线检测器对新数据库进行了实验验证。除欺骗/深度伪造语音生成协议与工具外，本文所述资源（已由2024年ASVspoof 5挑战赛参与者使用）现已向社区全面开放。



## **16. We Can Always Catch You: Detecting Adversarial Patched Objects WITH or WITHOUT Signature**

我们总能抓住你：基于特征与无特征的对抗性补丁物体检测 cs.CV

**SubmitDate**: 2025-12-15    [abs](http://arxiv.org/abs/2106.05261v3) [paper-pdf](https://arxiv.org/pdf/2106.05261v3)

**Confidence**: 0.95

**Authors**: Jiachun Li, Jianan Feng, Jianjun Huang, Bin Liang

**Abstract**: Recently, object detection has proven vulnerable to adversarial patch attacks. The attackers holding a specially crafted patch can hide themselves from state-of-the-art detectors, e.g., YOLO, even in the physical world. This attack can bring serious security threats, such as escaping from surveillance cameras. How to effectively detect this kind of adversarial examples to catch potential attacks has become an important problem. In this paper, we propose two detection methods: the signature-based method and the signature-independent method. First, we identify two signatures of existing adversarial patches that can be utilized to precisely locate patches within adversarial examples. By employing the signatures, a fast signature-based method is developed to detect the adversarial objects. Second, we present a robust signature-independent method based on the \textit{content semantics consistency} of model outputs. Adversarial objects violate this consistency, appearing locally but disappearing globally, while benign ones remain consistently present. The experiments demonstrate that two proposed methods can effectively detect attacks both in the digital and physical world. These methods each offer distinct advantage. Specifically, the signature-based method is capable of real-time detection, while the signature-independent method can detect unknown adversarial patch attacks and makes defense-aware attacks almost impossible to perform.

摘要: 近期研究表明，物体检测系统易受对抗性补丁攻击。攻击者持有特制补丁时，即使在物理世界中也能躲避YOLO等先进检测器的识别。此类攻击可能引发严重安全威胁，例如逃避监控摄像头。如何有效检测这类对抗样本以防范潜在攻击已成为重要课题。本文提出两种检测方法：基于特征的方法与无特征方法。首先，我们识别出现有对抗性补丁的两个可量化特征，能够精确定位对抗样本中的补丁区域。基于这些特征，开发出快速的签名检测方法。其次，我们提出基于模型输出内容语义一致性的鲁棒无特征检测方法。对抗性物体会破坏这种一致性——局部可见而全局消失，良性物体则始终保持一致存在。实验表明，两种方法均能有效检测数字与物理世界中的攻击。两种方法各具优势：基于特征的方法可实现实时检测，而无特征方法能检测未知对抗补丁攻击，并使防御感知型攻击几乎无法实施。



## **17. NeuroShield: A Neuro-Symbolic Framework for Adversarial Robustness**

NeuroShield: A Neuro-Symbolic Framework for Adversarial Robustness cs.LG

**SubmitDate**: 2026-01-19    [abs](http://arxiv.org/abs/2601.13162v1) [paper-pdf](https://arxiv.org/pdf/2601.13162v1)

**Confidence**: 0.95

**Authors**: Ali Shafiee Sarvestani, Jason Schmidt, Arman Roohi

**Abstract**: Adversarial vulnerability and lack of interpretability are critical limitations of deep neural networks, especially in safety-sensitive settings such as autonomous driving. We introduce \DesignII, a neuro-symbolic framework that integrates symbolic rule supervision into neural networks to enhance both adversarial robustness and explainability. Domain knowledge is encoded as logical constraints over appearance attributes such as shape and color, and enforced through semantic and symbolic logic losses applied during training. Using the GTSRB dataset, we evaluate robustness against FGSM and PGD attacks at a standard $\ell_\infty$ perturbation budget of $\varepsilon = 8/255$. Relative to clean training, standard adversarial training provides modest improvements in robustness ($\sim$10 percentage points). Conversely, our FGSM-Neuro-Symbolic and PGD-Neuro-Symbolic models achieve substantially larger gains, improving adversarial accuracy by 18.1\% and 17.35\% over their corresponding adversarial-training baselines, representing roughly a three-fold larger robustness gain than standard adversarial training provides when both are measured relative to the same clean-training baseline, without reducing clean-sample accuracy. Compared to transformer-based defenses such as LNL-MoEx, which require heavy architectures and extensive data augmentation, our PGD-Neuro-Symbolic variant attains comparable or superior robustness using a ResNet18 backbone trained for 10 epochs. These results show that symbolic reasoning offers an effective path to robust and interpretable AI.

摘要: Adversarial vulnerability and lack of interpretability are critical limitations of deep neural networks, especially in safety-sensitive settings such as autonomous driving. We introduce \DesignII, a neuro-symbolic framework that integrates symbolic rule supervision into neural networks to enhance both adversarial robustness and explainability. Domain knowledge is encoded as logical constraints over appearance attributes such as shape and color, and enforced through semantic and symbolic logic losses applied during training. Using the GTSRB dataset, we evaluate robustness against FGSM and PGD attacks at a standard $\ell_\infty$ perturbation budget of $\varepsilon = 8/255$. Relative to clean training, standard adversarial training provides modest improvements in robustness ($\sim$10 percentage points). Conversely, our FGSM-Neuro-Symbolic and PGD-Neuro-Symbolic models achieve substantially larger gains, improving adversarial accuracy by 18.1\% and 17.35\% over their corresponding adversarial-training baselines, representing roughly a three-fold larger robustness gain than standard adversarial training provides when both are measured relative to the same clean-training baseline, without reducing clean-sample accuracy. Compared to transformer-based defenses such as LNL-MoEx, which require heavy architectures and extensive data augmentation, our PGD-Neuro-Symbolic variant attains comparable or superior robustness using a ResNet18 backbone trained for 10 epochs. These results show that symbolic reasoning offers an effective path to robust and interpretable AI.



## **18. SplittingSecrets: A Compiler-Based Defense for Preventing Data Memory-Dependent Prefetcher Side-Channels**

SplittingSecrets：一种基于编译器的防御机制，用于防止数据内存依赖型预取器侧信道攻击 cs.CR

**SubmitDate**: 2026-01-18    [abs](http://arxiv.org/abs/2601.12270v1) [paper-pdf](https://arxiv.org/pdf/2601.12270v1)

**Confidence**: 0.95

**Authors**: Reshabh K Sharma, Dan Grossman, David Kohlbrenner

**Abstract**: Traditional side-channels take advantage of secrets being used as inputs to unsafe instructions, used for memory accesses, or used in control flow decisions. Constant-time programming, which restricts such code patterns, has been widely adopted as a defense against these vulnerabilities. However, new hardware optimizations in the form of Data Memory-dependent Prefetchers (DMP) present in Apple, Intel, and ARM CPUs have shown such defenses are not sufficient. These prefetchers, unlike classical prefetchers, use the content of memory as well as the trace of prior accesses to determine prefetch targets. An adversary abusing such a prefetcher has been shown to be able to mount attacks leaking data-at-rest; data that is never used by the program, even speculatively, in an unsafe manner.   In response, this paper introduces SplittingSecrets, a compiler-based tool that can harden software libraries against side-channels arising from DMPs. SplittingSecrets's approach avoids reasoning about the complex internals of different DMPs and instead relies on one key aspect of all DMPs: activation requires data to resemble addresses. To prevent secret data from leaking, SplittingSecrets transforms memory operations to ensure that secrets are never stored in memory in a manner resembling an address, thereby avoiding DMP activation on those secrets. Rather than disable a DMP entirely, SplittingSecrets can provide targeted hardening for only specific secrets entirely in software.   We have implemented SplittingSecrets using LLVM, supporting both source-level memory operations and those generated by the compiler backend for the AArch64 architecture, We have analyzed the performance overhead involved in safeguarding secrets from DMP-induced attacks using common primitives in libsodium, a popular cryptographic library when built for Apple M-series CPUs.

摘要: 传统侧信道攻击利用秘密信息作为不安全指令的输入、内存访问或控制流决策的依据。恒定时间编程通过限制此类代码模式，已被广泛采用作为针对这些漏洞的防御手段。然而，苹果、英特尔和ARM CPU中存在的数据内存依赖型预取器（DMP）这类新型硬件优化表明，现有防御措施并不充分。与经典预取器不同，这些预取器利用内存内容及先前访问轨迹来确定预取目标。研究表明，攻击者滥用此类预取器能够发起泄露静态数据的攻击——即使程序从未以不安全方式（包括推测执行）使用这些数据。为此，本文提出SplittingSecrets，一种基于编译器的工具，可强化软件库以抵御DMP引发的侧信道攻击。SplittingSecrets的方法避免分析不同DMP的复杂内部机制，转而依赖所有DMP的一个关键特性：激活需要数据形似地址。为防止秘密数据泄露，SplittingSecrets通过转换内存操作，确保秘密数据永远不会以类似地址的形式存储在内存中，从而避免DMP对这些秘密的激活。SplittingSecrets并非完全禁用DMP，而是能够针对特定秘密提供完全基于软件的定向强化。我们已基于LLVM实现SplittingSecrets，支持源代码级内存操作及编译器后端为AArch64架构生成的操作。通过分析在苹果M系列CPU上构建的流行密码库libsodium中常用原语的安全防护性能，我们评估了防御DMP诱导攻击的性能开销。



## **19. Evaluating the Defense Potential of Machine Unlearning against Membership Inference Attacks**

评估机器遗忘作为对抗成员推理攻击的防御潜力 cs.CR

**SubmitDate**: 2026-01-22    [abs](http://arxiv.org/abs/2508.16150v4) [paper-pdf](https://arxiv.org/pdf/2508.16150v4)

**Confidence**: 0.95

**Authors**: Theodoros Tsiolakis, Vasilis Perifanis, Nikolaos Pavlidis, Christos Chrysanthos Nikolaidis, Aristeidis Sidiropoulos, Pavlos S. Efraimidis

**Abstract**: Membership Inference Attacks (MIAs) pose a significant privacy risk by enabling adversaries to determine if a specific data point was part of a model's training set. This work empirically investigates whether MU algorithms can function as a targeted, active defense mechanism, in scenarios where a privacy audit identifies specific classes or individuals as highly susceptible to MIAs post-training. By 'dulling' the model's categorical memory of these samples, the process effectively mitigates the membership signal and reduces the MIA success rate for the most vulnerable users. We evaluate the defense potential of three MU algorithms, Negative Gradient (neg grad), SCalable Remembering and Unlearning unBound (SCRUB), and Selective Fine-tuning and Targeted Confusion (SFTC), across four diverse datasets and three complexity-based model groups. Our findings reveal that MU can function as a countermeasure against MIAs, though its success is critically contingent on algorithm choice, model capacity, and a profound sensitivity to learning rates. While Negative Gradient often induces a generalized degradation of membership signals across both forget and retain set, SFTC identifies a critical ``divergence effect'' where targeted forgetting reinforces the membership signal of retained data. Conversely, SCRUB provides a more balanced defense with minimal collateral impact on MIA perspective.

摘要: 成员推理攻击（MIAs）通过使攻击者能够判断特定数据点是否属于模型训练集，构成了重大的隐私风险。本研究通过实证方法探讨了在隐私审计识别出特定类别或个体在训练后极易受到MIAs攻击的场景中，机器遗忘（MU）算法能否作为有针对性的主动防御机制。通过'钝化'模型对这些样本的类别记忆，该过程有效削弱了成员身份信号，并降低了最易受攻击用户的MIA成功率。我们在四个不同数据集和三个基于复杂度的模型组上，评估了三种MU算法——负梯度（neg grad）、可扩展记忆与无界遗忘（SCRUB）以及选择性微调与定向混淆（SFTC）的防御潜力。研究结果表明，MU可以作为对抗MIAs的对策，但其成功与否关键取决于算法选择、模型容量以及对学习率的极端敏感性。虽然负梯度通常会导致遗忘集和保留集的成员信号普遍退化，但SFTC识别出一种关键的'发散效应'，即定向遗忘反而强化了保留数据的成员信号。相反，SCRUB提供了更均衡的防御，对MIA视角的附带影响最小。



## **20. BeDKD: Backdoor Defense Based on Directional Mapping Module and Adversarial Knowledge Distillation**

BeDKD：基于定向映射模块与对抗性知识蒸馏的后门防御方法 cs.CR

**SubmitDate**: 2026-01-14    [abs](http://arxiv.org/abs/2508.01595v3) [paper-pdf](https://arxiv.org/pdf/2508.01595v3)

**Confidence**: 0.95

**Authors**: Zhengxian Wu, Juan Wen, Wanli Peng, Yinghan Zhou, Changtong dou, Yiming Xue

**Abstract**: Although existing backdoor defenses have gained success in mitigating backdoor attacks, they still face substantial challenges. In particular, most of them rely on large amounts of clean data to weaken the backdoor mapping but generally struggle with residual trigger effects, resulting in persistently high attack success rates (ASR). Therefore, in this paper, we propose a novel \textbf{B}ackdoor d\textbf{e}fense method based on \textbf{D}irectional mapping module and adversarial \textbf{K}nowledge \textbf{D}istillation (BeDKD), which balances the trade-off between defense effectiveness and model performance using a small amount of clean and poisoned data. We first introduce a directional mapping module to identify poisoned data, which destroys clean mapping while keeping backdoor mapping on a small set of flipped clean data. Then, the adversarial knowledge distillation is designed to reinforce clean mapping and suppress backdoor mapping through a cycle iteration mechanism between trust and punish distillations using clean and identified poisoned data. We conduct experiments to mitigate mainstream attacks on three datasets, and experimental results demonstrate that BeDKD surpasses the state-of-the-art defenses and reduces the ASR by 98$\%$ without significantly reducing the CACC. Our code are available in https://github.com/CAU-ISS-Lab/Backdoor-Attack-Defense-LLMs/tree/main/BeDKD.

摘要: 尽管现有后门防御方法在缓解后门攻击方面取得了一定成功，但仍面临重大挑战。特别是，大多数方法依赖大量干净数据来削弱后门映射，但通常难以消除残留的触发器效应，导致攻击成功率（ASR）持续偏高。为此，本文提出一种基于定向映射模块与对抗性知识蒸馏的新型后门防御方法（BeDKD），该方法利用少量干净数据和投毒数据，在防御效果与模型性能之间取得平衡。我们首先引入定向映射模块来识别投毒数据，该模块在小规模翻转的干净数据集上破坏干净映射，同时保持后门映射。随后，通过信任蒸馏与惩罚蒸馏之间的循环迭代机制，利用干净数据和已识别的投毒数据，设计对抗性知识蒸馏来强化干净映射并抑制后门映射。我们在三个数据集上针对主流攻击进行了实验验证，结果表明BeDKD超越了现有最优防御方法，在未显著降低干净准确率（CACC）的前提下，将ASR降低了98%。代码已开源：https://github.com/CAU-ISS-Lab/Backdoor-Attack-Defense-LLMs/tree/main/BeDKD。



## **21. Model Privacy: A Unified Framework to Understand Model Stealing Attacks and Defenses**

模型隐私：理解模型窃取攻击与防御的统一框架 cs.LG

**SubmitDate**: 2026-01-11    [abs](http://arxiv.org/abs/2502.15567v2) [paper-pdf](https://arxiv.org/pdf/2502.15567v2)

**Confidence**: 0.95

**Authors**: Ganghua Wang, Yuhong Yang, Jie Ding

**Abstract**: The use of machine learning (ML) has become increasingly prevalent in various domains, highlighting the importance of understanding and ensuring its safety. One pressing concern is the vulnerability of ML applications to model stealing attacks. These attacks involve adversaries attempting to recover a learned model through limited query-response interactions, such as those found in cloud-based services or on-chip artificial intelligence interfaces. While existing literature proposes various attack and defense strategies, these often lack a theoretical foundation and standardized evaluation criteria. In response, this work presents a framework called ``Model Privacy'', providing a foundation for comprehensively analyzing model stealing attacks and defenses. We establish a rigorous formulation for the threat model and objectives, propose methods to quantify the goodness of attack and defense strategies, and analyze the fundamental tradeoffs between utility and privacy in ML models. Our developed theory offers valuable insights into enhancing the security of ML models, especially highlighting the importance of the attack-specific structure of perturbations for effective defenses. We demonstrate the application of model privacy from the defender's perspective through various learning scenarios. Extensive experiments corroborate the insights and the effectiveness of defense mechanisms developed under the proposed framework.

摘要: 机器学习（ML）在各领域的应用日益广泛，凸显了理解并确保其安全性的重要性。一个紧迫问题是ML应用对模型窃取攻击的脆弱性。这类攻击中，攻击者试图通过有限的查询-响应交互（如基于云的服务或片上人工智能接口）来恢复已学习模型。尽管现有文献提出了多种攻击与防御策略，但这些方法往往缺乏理论基础和标准化评估标准。为此，本研究提出了名为“模型隐私”的框架，为全面分析模型窃取攻击与防御奠定基础。我们建立了威胁模型与目标的严格表述，提出了量化攻击与防御策略优劣的方法，并分析了ML模型中效用与隐私之间的基本权衡。所发展的理论为增强ML模型安全性提供了宝贵见解，特别强调了攻击特定扰动结构对有效防御的重要性。我们通过多种学习场景从防御者视角展示了模型隐私的应用。大量实验验证了所提框架下发展出的见解与防御机制的有效性。



## **22. HidePrint: Protecting Device Anonymity by Obscuring Radio Fingerprints**

HidePrint：通过模糊射频指纹保护设备匿名性 cs.CR

Accepted for publication at AsiaCCS 2026 - 21st ACM ASIA Conference on Computer and Communications Security

**SubmitDate**: 2025-11-21    [abs](http://arxiv.org/abs/2411.06417v2) [paper-pdf](https://arxiv.org/pdf/2411.06417v2)

**Confidence**: 0.95

**Authors**: Gabriele Oligeri, Savio Sciancalepore

**Abstract**: Radio Frequency Fingerprinting (RFF) techniques allow a receiver to authenticate a transmitter by analyzing the physical layer of the radio spectrum. Although the vast majority of scientific contributions focus on improving the performance of RFF considering different parameters and scenarios, in this work, we consider RFF as an attack vector to identify a target device in the radio spectrum. \\ We propose, implement, and evaluate {\em HidePrint}, a solution to prevent identification through RFF without affecting the quality of the communication link between the transmitter and the receiver. {\em HidePrint} hides the transmitter's fingerprint against an illegitimate eavesdropper through the injection of controlled noise into the transmitted signal. We evaluate our solution against various state-of-the-art RFF techniques, considering several adversarial models, data from real-world communication links (wired and wireless), and protocol configurations. Our results show that the injection of a Gaussian noise pattern with a normalized standard deviation of (at least) 0.02 prevents device fingerprinting in all the considered scenarios, while affecting the Signal-to-Noise Ratio (SNR) of the received signal by only 0.1 dB. Moreover, we introduce {\em selective radio fingerprint disclosure}, a new technique that allows the transmitter to disclose the radio fingerprint to only a subset of intended receivers.

摘要: 射频指纹识别技术允许接收方通过分析无线电频谱的物理层来验证发射方身份。尽管绝大多数研究致力于在不同参数和场景下提升RFF性能，但本文将其视为一种在无线电频谱中识别目标设备的攻击向量。\ 我们提出、实现并评估了{\em HidePrint}方案，该方案可在不影响发射方与接收方之间通信链路质量的前提下，防止通过RFF进行设备识别。{\em HidePrint}通过向发射信号中注入受控噪声，使非法窃听者无法获取发射方的射频指纹。我们在多种先进RFF技术、不同对抗模型、真实通信链路数据（有线和无线）及协议配置下评估了该方案。结果表明：注入归一化标准差至少为0.02的高斯噪声模式，可在所有测试场景中有效阻止设备指纹识别，同时仅使接收信号的信噪比降低0.1 dB。此外，我们提出了{\em 选择性射频指纹披露}新技术，允许发射方仅向特定目标接收方披露其射频指纹。



## **23. CausAdv: A Causal-based Framework for Detecting Adversarial Examples**

CausAdv：基于因果推理的对抗样本检测框架 cs.LG

**SubmitDate**: 2026-01-17    [abs](http://arxiv.org/abs/2411.00839v3) [paper-pdf](https://arxiv.org/pdf/2411.00839v3)

**Confidence**: 0.95

**Authors**: Hichem Debbi

**Abstract**: Deep learning has led to tremendous success in computer vision, largely due to Convolutional Neural Networks (CNNs). However, CNNs have been shown to be vulnerable to crafted adversarial perturbations. This vulnerability of adversarial examples has has motivated research into improving model robustness through adversarial detection and defense methods. In this paper, we address the adversarial robustness of CNNs through causal reasoning. We propose CausAdv: a causal framework for detecting adversarial examples based on counterfactual reasoning. CausAdv learns both causal and non-causal features of every input, and quantifies the counterfactual information (CI) of every filter of the last convolutional layer. We then perform a statistical analysis of the filters' CI across clean and adversarial samples, to demonstrate that adversarial examples exhibit different CI distributions compared to clean samples. Our results show that causal reasoning enhances the process of adversarial detection without the need to train a separate detector. Moreover, we illustrate the efficiency of causal explanations as a helpful detection tool by visualizing the extracted causal features.

摘要: 深度学习在计算机视觉领域取得了巨大成功，这主要归功于卷积神经网络（CNNs）。然而，研究表明CNNs容易受到精心设计的对抗性扰动攻击。对抗样本的这种脆弱性促使研究者通过对抗检测和防御方法来提升模型鲁棒性。本文通过因果推理方法解决CNNs的对抗鲁棒性问题。我们提出CausAdv：一种基于反事实推理的对抗样本检测因果框架。CausAdv学习每个输入的因果特征和非因果特征，并量化最后一个卷积层每个滤波器的反事实信息（CI）。随后我们对干净样本和对抗样本的滤波器CI进行统计分析，证明对抗样本与干净样本相比具有不同的CI分布。实验结果表明，因果推理能够在不需训练独立检测器的情况下增强对抗检测过程。此外，我们通过可视化提取的因果特征，展示了因果解释作为有效检测工具的高效性。



## **24. DiffClean: Diffusion-based Makeup Removal for Accurate Age Estimation**

DiffClean：基于扩散模型的妆容去除技术用于精准年龄估计 cs.CV

Revised version of the older draft with addtional experiments, analysis and code release

**SubmitDate**: 2025-12-19    [abs](http://arxiv.org/abs/2507.13292v2) [paper-pdf](https://arxiv.org/pdf/2507.13292v2)

**Confidence**: 0.90

**Authors**: Ekta Balkrishna Gavas, Sudipta Banerjee, Chinmay Hegde, Nasir Memon

**Abstract**: Accurate age verification can protect underage users from unauthorized access to online platforms and e-commerce sites that provide age-restricted services. However, accurate age estimation can be confounded by several factors, including facial makeup that can induce changes to alter perceived identity and age to fool both humans and machines. In this work, we propose \textsc{DiffClean} which erases makeup traces using a text-guided diffusion model to defend against makeup attacks without requiring any reference image unlike prior work. \textsc{DiffClean} improves age estimation (minor vs. adult accuracy by 5.8\%) and face verification (TMR by 5.1\% at FMR=0.01\%) compared to images with makeup. Our method is: (1) robust across digitally simulated and real-world makeup styles with high visual fidelity, (2) can be easily integrated as a pre-processing module in existing age and identity verification frameworks, and (3) advances the state-of-the art in terms of biometric and perceptual utility. Our codes are available at https://github.com/Ektagavas/DiffClean

摘要: 精准的年龄验证可保护未成年用户免遭未经授权访问提供年龄限制服务的在线平台和电子商务网站。然而，面部妆容等若干因素可能干扰年龄估计的准确性——妆容可能改变感知身份与年龄，从而欺骗人类和机器系统。本研究提出\textsc{DiffClean}，通过文本引导的扩散模型消除妆容痕迹以防御化妆攻击，且无需像先前工作那样依赖参考图像。相较于带妆图像，\textsc{DiffClean}将年龄估计（未成年/成人分类）准确率提升5.8%，人脸验证在FMR=0.01%时TMR提升5.1%。本方法具有以下特点：（1）对数字模拟和真实场景妆容均具有鲁棒性，且保持高视觉保真度；（2）可作为预处理模块轻松集成至现有年龄与身份验证框架；（3）在生物特征识别与感知效用方面达到先进水平。代码已开源：https://github.com/Ektagavas/DiffClean



## **25. TextCrafter: Optimization-Calibrated Noise for Defending Against Text Embedding Inversion**

TextCrafter：基于优化校准的噪声防御文本嵌入反演攻击 cs.CR

Key experiments are required for validation

**SubmitDate**: 2026-01-22    [abs](http://arxiv.org/abs/2509.17302v5) [paper-pdf](https://arxiv.org/pdf/2509.17302v5)

**Confidence**: 0.90

**Authors**: Duoxun Tang, Xinhang Jiang, Jiajun Niu

**Abstract**: Text embedding inversion attacks reconstruct original sentences from latent representations, posing severe privacy threats in collaborative inference and edge computing. We propose TextCrafter, an optimization-based adversarial perturbation mechanism that combines RL learned, geometry aware noise injection orthogonal to user embeddings with cluster priors and PII signal guidance to suppress inversion while preserving task utility. Unlike prior defenses either non learnable or agnostic to perturbation direction, TextCrafter provides a directional protective policy that balances privacy and utility. Under strong privacy setting, TextCrafter maintains 70 percentage classification accuracy on four datasets and consistently outperforms Gaussian/LDP baselines across lower privacy budgets, demonstrating a superior privacy utility trade off.

摘要: 文本嵌入反演攻击能够从潜在表示中重构原始句子，对协同推理和边缘计算构成严重隐私威胁。我们提出TextCrafter，一种基于优化的对抗扰动机制，结合强化学习习得的、与用户嵌入正交的几何感知噪声注入，并利用聚类先验和PII信号引导来抑制反演攻击，同时保持任务效用。与先前非可学习或对扰动方向无感知的防御方法不同，TextCrafter提供了一种方向性保护策略，平衡隐私与效用。在强隐私设置下，TextCrafter在四个数据集上保持70%的分类准确率，并在较低隐私预算下持续优于高斯/LDP基线方法，展现出更优的隐私-效用权衡。



## **26. Integrating APK Image and Text Data for Enhanced Threat Detection: A Multimodal Deep Learning Approach to Android Malware**

融合APK图像与文本数据以增强威胁检测：一种面向Android恶意软件的多模态深度学习方法 cs.CR

**SubmitDate**: 2026-01-13    [abs](http://arxiv.org/abs/2601.08959v1) [paper-pdf](https://arxiv.org/pdf/2601.08959v1)

**Confidence**: 0.85

**Authors**: Md Mashrur Arifin, Maqsudur Rahman, Nasir U. Eisty

**Abstract**: As zero-day Android malware attacks grow more sophisticated, recent research highlights the effectiveness of using image-based representations of malware bytecode to detect previously unseen threats. However, existing studies often overlook how image type and resolution affect detection and ignore valuable textual data in Android Application Packages (APKs), such as permissions and metadata, limiting their ability to fully capture malicious behavior. The integration of multimodality, which combines image and text data, has gained momentum as a promising approach to address these limitations. This paper proposes a multimodal deep learning framework integrating APK images and textual features to enhance Android malware detection. We systematically evaluate various image types and resolutions across different Convolutional Neural Networks (CNN) architectures, including VGG, ResNet-152, MobileNet, DenseNet, EfficientNet-B4, and use LLaMA-2, a large language model, to extract and annotate textual features for improved analysis. The findings demonstrate that RGB images at higher resolutions (e.g., 256x256, 512x512) achieve superior classification performance, while the multimodal integration of image and text using the CLIP model reveals limited potential. Overall, this research highlights the importance of systematically evaluating image attributes and integrating multimodal data to develop effective malware detection for Android systems.

摘要: 随着零日Android恶意软件攻击日益复杂，近期研究强调了利用恶意软件字节码的图像表示来检测未知威胁的有效性。然而，现有研究常忽视图像类型和分辨率对检测的影响，并忽略了Android应用包（APK）中如权限和元数据等有价值的文本数据，这限制了其全面捕捉恶意行为的能力。融合图像与文本数据的多模态方法作为一种有前景的解决方案，正受到越来越多的关注。本文提出了一种集成APK图像和文本特征的多模态深度学习框架，以增强Android恶意软件检测。我们系统评估了不同卷积神经网络（CNN）架构（包括VGG、ResNet-152、MobileNet、DenseNet、EfficientNet-B4）下的多种图像类型和分辨率，并利用大型语言模型LLaMA-2提取和标注文本特征以改进分析。结果表明，较高分辨率（如256x256、512x512）的RGB图像能实现更优的分类性能，而使用CLIP模型融合图像与文本的多模态方法潜力有限。总体而言，本研究强调了系统评估图像属性并整合多模态数据对于开发有效的Android系统恶意软件检测的重要性。



## **27. Agentic AI Microservice Framework for Deepfake and Document Fraud Detection in KYC Pipelines**

面向KYC流程中深度伪造与文档欺诈检测的智能体AI微服务框架 cs.CR

Journal of Information Systems Engineering and Management, 2024

**SubmitDate**: 2026-01-09    [abs](http://arxiv.org/abs/2601.06241v1) [paper-pdf](https://arxiv.org/pdf/2601.06241v1)

**Confidence**: 0.85

**Authors**: Chandra Sekhar Kubam

**Abstract**: The rapid proliferation of synthetic media, presentation attacks, and document forgeries has created significant vulnerabilities in Know Your Customer (KYC) workflows across financial services, telecommunications, and digital-identity ecosystems. Traditional monolithic KYC systems lack the scalability and agility required to counter adaptive fraud. This paper proposes an Agentic AI Microservice Framework that integrates modular vision models, liveness assessment, deepfake detection, OCR-based document forensics, multimodal identity linking, and a policy driven risk engine. The system leverages autonomous micro-agents for task decomposition, pipeline orchestration, dynamic retries, and human-in-the-loop escalation. Experimental evaluations demonstrate improved detection accuracy, reduced latency, and enhanced resilience against adversarial inputs. The framework offers a scalable blueprint for regulated industries seeking robust, real-time, and privacy-preserving KYC verification.

摘要: 合成媒体、呈现攻击和文档伪造的快速扩散，已在金融服务、电信和数字身份生态系统中的'了解你的客户'（KYC）工作流程中造成了重大漏洞。传统的单体式KYC系统缺乏应对适应性欺诈所需的可扩展性和敏捷性。本文提出一种智能体AI微服务框架，该框架集成了模块化视觉模型、活体检测、深度伪造检测、基于OCR的文档取证、多模态身份关联以及策略驱动的风险引擎。该系统利用自主微智能体进行任务分解、流程编排、动态重试和人在回路升级。实验评估表明，该系统在检测准确性、降低延迟以及增强对抗性输入抵御能力方面均有提升。该框架为受监管行业寻求稳健、实时且保护隐私的KYC验证提供了一个可扩展的蓝图。



## **28. A Unified Masked Jigsaw Puzzle Framework for Vision and Language Models**

面向视觉与语言模型的统一掩码拼图框架 cs.CV

9 figures, 12 tables

**SubmitDate**: 2026-01-17    [abs](http://arxiv.org/abs/2601.12051v1) [paper-pdf](https://arxiv.org/pdf/2601.12051v1)

**Confidence**: 0.85

**Authors**: Weixin Ye, Wei Wang, Yahui Liu, Yue Song, Bin Ren, Wei Bi, Rita Cucchiara, Nicu Sebe

**Abstract**: In federated learning, Transformer, as a popular architecture, faces critical challenges in defending against gradient attacks and improving model performance in both Computer Vision (CV) and Natural Language Processing (NLP) tasks. It has been revealed that the gradient of Position Embeddings (PEs) in Transformer contains sufficient information, which can be used to reconstruct the input data. To mitigate this issue, we introduce a Masked Jigsaw Puzzle (MJP) framework. MJP starts with random token shuffling to break the token order, and then a learnable \textit{unknown (unk)} position embedding is used to mask out the PEs of the shuffled tokens. In this manner, the local spatial information which is encoded in the position embeddings is disrupted, and the models are forced to learn feature representations that are less reliant on the local spatial information. Notably, with the careful use of MJP, we can not only improve models' robustness against gradient attacks, but also boost their performance in both vision and text application scenarios, such as classification for images (\textit{e.g.,} ImageNet-1K) and sentiment analysis for text (\textit{e.g.,} Yelp and Amazon). Experimental results suggest that MJP is a unified framework for different Transformer-based models in both vision and language tasks. Code is publicly available via https://github.com/ywxsuperstar/transformerattack

摘要: 在联邦学习中，Transformer作为主流架构，面临着抵御梯度攻击及提升计算机视觉（CV）与自然语言处理（NLP）任务性能的双重挑战。研究表明，Transformer中位置嵌入（PEs）的梯度包含足以重构输入数据的敏感信息。为缓解此问题，我们提出了掩码拼图（MJP）框架。MJP首先通过随机令牌打乱破坏序列顺序，随后采用可学习的未知位置嵌入对打乱令牌的PEs进行掩码处理。该方法能有效破坏位置嵌入编码的局部空间信息，迫使模型学习更少依赖局部空间信息的特征表示。值得注意的是，通过精心设计的MJP应用，我们不仅能提升模型抵御梯度攻击的鲁棒性，还能增强其在视觉与文本应用场景（如图像分类任务中的ImageNet-1K，文本情感分析任务中的Yelp和Amazon数据集）的性能。实验结果表明，MJP是适用于不同Transformer模型在视觉与语言任务中的统一框架。代码已开源：https://github.com/ywxsuperstar/transformerattack



## **29. Steering Vision-Language Pre-trained Models for Incremental Face Presentation Attack Detection**

引导视觉语言预训练模型实现增量式人脸呈现攻击检测 cs.CV

**SubmitDate**: 2025-12-24    [abs](http://arxiv.org/abs/2512.19022v2) [paper-pdf](https://arxiv.org/pdf/2512.19022v2)

**Confidence**: 0.85

**Authors**: Haoze Li, Jie Zhang, Guoying Zhao, Stephen Lin, Shiguang Shan

**Abstract**: Face Presentation Attack Detection (PAD) demands incremental learning (IL) to combat evolving spoofing tactics and domains. Privacy regulations, however, forbid retaining past data, necessitating rehearsal-free IL (RF-IL). Vision-Language Pre-trained (VLP) models, with their prompt-tunable cross-modal representations, enable efficient adaptation to new spoofing styles and domains. Capitalizing on this strength, we propose \textbf{SVLP-IL}, a VLP-based RF-IL framework that balances stability and plasticity via \textit{Multi-Aspect Prompting} (MAP) and \textit{Selective Elastic Weight Consolidation} (SEWC). MAP isolates domain dependencies, enhances distribution-shift sensitivity, and mitigates forgetting by jointly exploiting universal and domain-specific cues. SEWC selectively preserves critical weights from previous tasks, retaining essential knowledge while allowing flexibility for new adaptations. Comprehensive experiments across multiple PAD benchmarks show that SVLP-IL significantly reduces catastrophic forgetting and enhances performance on unseen domains. SVLP-IL offers a privacy-compliant, practical solution for robust lifelong PAD deployment in RF-IL settings.

摘要: 人脸呈现攻击检测（PAD）需要增量学习（IL）以应对不断演变的欺骗手段和领域。然而，隐私法规禁止保留历史数据，这要求实现无需回放的增量学习（RF-IL）。视觉语言预训练（VLP）模型凭借其可提示调优的跨模态表示能力，能够高效适应新的欺骗风格和领域。基于此优势，我们提出\textbf{SVLP-IL}——一种基于VLP的RF-IL框架，通过\textit{多维度提示（MAP）}和\textit{选择性弹性权重固化（SEWC）}实现稳定性与可塑性的平衡。MAP通过联合利用通用线索和领域特定线索，隔离领域依赖性、增强分布偏移敏感性并缓解遗忘问题。SEWC选择性保留先前任务的关键权重，在保持核心知识的同时为新适应提供灵活性。在多个PAD基准上的综合实验表明，SVLP-IL显著降低了灾难性遗忘，并提升了在未见领域上的性能。该框架为RF-IL场景下稳健的终身PAD部署提供了符合隐私要求的实用解决方案。



## **30. Advancing Autonomous Driving System Testing: Demands, Challenges, and Future Directions**

推进自动驾驶系统测试：需求、挑战与未来方向 cs.CY

Accepted for publication in Information and Software Technology (IST)

**SubmitDate**: 2025-12-09    [abs](http://arxiv.org/abs/2512.11887v1) [paper-pdf](https://arxiv.org/pdf/2512.11887v1)

**Confidence**: 0.85

**Authors**: Yihan Liao, Jingyu Zhang, Jacky Keung, Yan Xiao, Yurou Dai

**Abstract**: Autonomous driving systems (ADSs) promise improved transportation efficiency and safety, yet ensuring their reliability in complex real-world environments remains a critical challenge. Effective testing is essential to validate ADS performance and reduce deployment risks. This study investigates current ADS testing practices for both modular and end-to-end systems, identifies key demands from industry practitioners and academic researchers, and analyzes the gaps between existing research and real-world requirements. We review major testing techniques and further consider emerging factors such as Vehicle-to-Everything (V2X) communication and foundation models, including large language models and vision foundation models, to understand their roles in enhancing ADS testing. We conducted a large-scale survey with 100 participants from both industry and academia. Survey questions were refined through expert discussions, followed by quantitative and qualitative analyses to reveal key trends, challenges, and unmet needs. Our results show that existing ADS testing techniques struggle to comprehensively evaluate real-world performance, particularly regarding corner case diversity, the simulation to reality gap, the lack of systematic testing criteria, exposure to potential attacks, practical challenges in V2X deployment, and the high computational cost of foundation model-based testing. By further analyzing participant responses together with 105 representative studies, we summarize the current research landscape and highlight major limitations. This study consolidates critical research gaps in ADS testing and outlines key future research directions, including comprehensive testing criteria, cross-model collaboration in V2X systems, cross-modality adaptation for foundation model-based testing, and scalable validation frameworks for large-scale ADS evaluation.

摘要: 自动驾驶系统（ADSs）有望提升交通效率与安全性，但确保其在复杂现实环境中的可靠性仍是关键挑战。有效的测试对于验证ADS性能、降低部署风险至关重要。本研究调查了当前模块化与端到端系统的ADS测试实践，识别了行业从业者和学术研究者的核心需求，并分析了现有研究与实际要求之间的差距。我们回顾了主要测试技术，并进一步探讨了车联网（V2X）通信、基础模型（包括大语言模型和视觉基础模型）等新兴因素，以理解其在增强ADS测试中的作用。我们开展了一项涵盖100名工业界与学术界参与者的大规模调研，通过专家讨论优化调研问题，并进行了定量与定性分析以揭示关键趋势、挑战及未满足需求。结果显示，现有ADS测试技术难以全面评估现实性能，尤其在边缘案例多样性、仿真与现实差距、缺乏系统化测试标准、潜在攻击暴露、V2X部署的实际挑战以及基于基础模型的测试计算成本高昂等方面存在不足。通过进一步结合105项代表性研究分析参与者反馈，我们总结了当前研究现状并指出主要局限。本研究整合了ADS测试中的关键研究缺口，并展望了未来重点研究方向，包括综合测试标准、V2X系统中的跨模型协作、基于基础模型测试的跨模态适应，以及面向大规模ADS评估的可扩展验证框架。



## **31. VocalBridge: Latent Diffusion-Bridge Purification for Defeating Perturbation-Based Voiceprint Defenses**

VocalBridge：基于潜在扩散桥接纯化的对抗扰动声纹防御方法 cs.SD

**SubmitDate**: 2026-01-05    [abs](http://arxiv.org/abs/2601.02444v1) [paper-pdf](https://arxiv.org/pdf/2601.02444v1)

**Confidence**: 0.85

**Authors**: Maryam Abbasihafshejani, AHM Nazmus Sakib, Murtuza Jadliwala

**Abstract**: The rapid advancement of speech synthesis technologies, including text-to-speech (TTS) and voice conversion (VC), has intensified security and privacy concerns related to voice cloning. Recent defenses attempt to prevent unauthorized cloning by embedding protective perturbations into speech to obscure speaker identity while maintaining intelligibility. However, adversaries can apply advanced purification techniques to remove these perturbations, recover authentic acoustic characteristics, and regenerate cloneable voices. Despite the growing realism of such attacks, the robustness of existing defenses under adaptive purification remains insufficiently studied.   Most existing purification methods are designed to counter adversarial noise in automatic speech recognition (ASR) systems rather than speaker verification or voice cloning pipelines. As a result, they fail to suppress the fine-grained acoustic cues that define speaker identity and are often ineffective against speaker verification attacks (SVA). To address these limitations, we propose Diffusion-Bridge (VocalBridge), a purification framework that learns a latent mapping from perturbed to clean speech in the EnCodec latent space. Using a time-conditioned 1D U-Net with a cosine noise schedule, the model enables efficient, transcript-free purification while preserving speaker-discriminative structure. We further introduce a Whisper-guided phoneme variant that incorporates lightweight temporal guidance without requiring ground-truth transcripts. Experimental results show that our approach consistently outperforms existing purification methods in recovering cloneable voices from protected speech. Our findings demonstrate the fragility of current perturbation-based defenses and highlight the need for more robust protection mechanisms against evolving voice-cloning and speaker verification threats.

摘要: 语音合成技术（包括文本转语音TTS和语音转换VC）的快速发展加剧了与语音克隆相关的安全和隐私问题。现有防御方法尝试通过在语音中嵌入保护性扰动来掩盖说话人身份，同时保持可懂度，以防止未经授权的克隆。然而，攻击者可应用先进的纯化技术去除这些扰动，恢复真实声学特征并重新生成可克隆的语音。尽管此类攻击日益逼真，但现有防御在自适应纯化下的鲁棒性仍未得到充分研究。大多数现有纯化方法旨在对抗自动语音识别（ASR）系统中的对抗噪声，而非针对说话人验证或语音克隆流程。因此，这些方法无法抑制定义说话人身份的细粒度声学线索，且通常对说话人验证攻击（SVA）无效。为解决这些局限，我们提出扩散桥接（VocalBridge）纯化框架，该框架在EnCodec潜在空间中学习从扰动语音到纯净语音的隐式映射。通过采用余弦噪声调度的时间条件一维U-Net，该模型能够实现高效、无需文本转录的纯化，同时保留说话人区分性结构。我们还提出Whisper引导的音素变体，该变体融入轻量级时序引导而无需真实文本转录。实验结果表明，我们的方法在从受保护语音中恢复可克隆语音方面持续优于现有纯化方法。本研究揭示了当前基于扰动的防御机制的脆弱性，并强调需要针对不断演进的语音克隆和说话人验证威胁开发更鲁棒的保护机制。



## **32. Fixed-Size Dynamic Scale-Free Networks: Modeling, Stationarity, and Resilience**

固定规模动态无标度网络：建模、平稳性与鲁棒性 cs.SI

**SubmitDate**: 2026-01-05    [abs](http://arxiv.org/abs/2601.01882v1) [paper-pdf](https://arxiv.org/pdf/2601.01882v1)

**Confidence**: 0.85

**Authors**: Yichao Yao, Minyu Feng, Matjaž Perc, Jürgen Kurths

**Abstract**: Many real-world scale-free networks, such as neural networks and online communication networks, consist of a fixed number of nodes but exhibit dynamic edge fluctuations. However, traditional models frequently overlook scenarios where the node count remains constant, instead prioritizing node growth. In this work, we depart from the assumptions of node number variation and preferential attachment to present an innovative model that conceptualizes node degree fluctuations as a state-dependent random walk process with stasis and variable diffusion coefficient. We show that this model yields stochastic dynamic networks with stable scale-free properties. Through comprehensive theoretical and numerical analyses, we demonstrate that the degree distribution converges to a power-law distribution, provided that the lowest degree state within the network is not an absorbing state. Furthermore, we investigate the resilience of the fraction of the largest component and the average shortest path length following deliberate attacks on the network. By using three real-world networks, we confirm that the proposed model accurately replicates actual data. The proposed model thus elucidates mechanisms by which networks, devoid of growth and preferential attachment features, can still exhibit power-law distributions and be used to simulate and study the resilience of attacked fixed-size scale-free networks.

摘要: 许多现实世界的无标度网络，如神经网络和在线通信网络，由固定数量的节点组成，但表现出动态的边波动。然而，传统模型常常忽略节点数量保持恒定的场景，而优先考虑节点增长。在本研究中，我们摒弃节点数量变化和优先连接的假设，提出一种创新模型，将节点度波动概念化为具有停滞状态和可变扩散系数的状态依赖随机游走过程。我们证明该模型能产生具有稳定无标度特性的随机动态网络。通过全面的理论和数值分析，我们证明只要网络中的最低度状态不是吸收态，度分布就会收敛于幂律分布。此外，我们研究了网络在遭受针对性攻击后最大连通分量占比和平均最短路径长度的鲁棒性。通过使用三个真实世界网络，我们证实所提模型能准确复现实际数据。因此，该模型阐明了即使不具备增长和优先连接特性的网络仍能呈现幂律分布的机制，并可用于模拟和研究受攻击固定规模无标度网络的鲁棒性。



## **33. Keep the Core: Adversarial Priors for Significance-Preserving Brain MRI Segmentation**

Keep the Core: Adversarial Priors for Significance-Preserving Brain MRI Segmentation eess.IV

**SubmitDate**: 2025-12-17    [abs](http://arxiv.org/abs/2512.15811v1) [paper-pdf](https://arxiv.org/pdf/2512.15811v1)

**Confidence**: 0.85

**Authors**: Feifei Zhang, Zhenhong Jia, Sensen Song, Fei Shi, Aoxue Chen, Dayong Ren

**Abstract**: Medical image segmentation is constrained by sparse pathological annotations. Existing augmentation strategies, from conventional transforms to random masking for self-supervision, are feature-agnostic: they often corrupt critical diagnostic semantics or fail to prioritize essential features. We introduce "Keep the Core," a novel data-centric paradigm that uses adversarial priors to guide both augmentation and masking in a significance-preserving manner. Our approach uses SAGE (Sparse Adversarial Gated Estimator), an offline module identifying minimal tokens whose micro-perturbation flips segmentation boundaries. SAGE forges the Token Importance Map $W$ by solving an adversarial optimization problem to maximally degrade performance, while an $\ell_1$ sparsity penalty encourages a compact set of sensitive tokens. The online KEEP (Key-region Enhancement \& Preservation) module uses $W$ for a two-pronged augmentation strategy: (1) Semantic-Preserving Augmentation: High-importance tokens are augmented, but their original pixel values are strictly restored. (2) Guided-Masking Augmentation: Low-importance tokens are selectively masked for an $\text{MAE}$-style reconstruction, forcing the model to learn robust representations from preserved critical features. "Keep the Core" is backbone-agnostic with no inference overhead. Extensive experiments show SAGE's structured priors and KEEP's region-selective mechanism are highly complementary, achieving state-of-the-art segmentation robustness and generalization on 2D medical datasets.

摘要: Medical image segmentation is constrained by sparse pathological annotations. Existing augmentation strategies, from conventional transforms to random masking for self-supervision, are feature-agnostic: they often corrupt critical diagnostic semantics or fail to prioritize essential features. We introduce "Keep the Core," a novel data-centric paradigm that uses adversarial priors to guide both augmentation and masking in a significance-preserving manner. Our approach uses SAGE (Sparse Adversarial Gated Estimator), an offline module identifying minimal tokens whose micro-perturbation flips segmentation boundaries. SAGE forges the Token Importance Map $W$ by solving an adversarial optimization problem to maximally degrade performance, while an $\ell_1$ sparsity penalty encourages a compact set of sensitive tokens. The online KEEP (Key-region Enhancement \& Preservation) module uses $W$ for a two-pronged augmentation strategy: (1) Semantic-Preserving Augmentation: High-importance tokens are augmented, but their original pixel values are strictly restored. (2) Guided-Masking Augmentation: Low-importance tokens are selectively masked for an $\text{MAE}$-style reconstruction, forcing the model to learn robust representations from preserved critical features. "Keep the Core" is backbone-agnostic with no inference overhead. Extensive experiments show SAGE's structured priors and KEEP's region-selective mechanism are highly complementary, achieving state-of-the-art segmentation robustness and generalization on 2D medical datasets.



## **34. M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition**

M4SER：面向语音情感识别的多模态、多表征、多任务与多策略学习 cs.HC

Accepted by IEEE Transactions on Audio, Speech and Language Processing

**SubmitDate**: 2025-09-23    [abs](http://arxiv.org/abs/2509.18706v1) [paper-pdf](https://arxiv.org/pdf/2509.18706v1)

**Confidence**: 0.85

**Authors**: Jiajun He, Xiaohan Shi, Cheng-Hung Hu, Jinyi Mi, Xingfeng Li, Tomoki Toda

**Abstract**: Multimodal speech emotion recognition (SER) has emerged as pivotal for improving human-machine interaction. Researchers are increasingly leveraging both speech and textual information obtained through automatic speech recognition (ASR) to comprehensively recognize emotional states from speakers. Although this approach reduces reliance on human-annotated text data, ASR errors possibly degrade emotion recognition performance. To address this challenge, in our previous work, we introduced two auxiliary tasks, namely, ASR error detection and ASR error correction, and we proposed a novel multimodal fusion (MF) method for learning modality-specific and modality-invariant representations across different modalities. Building on this foundation, in this paper, we introduce two additional training strategies. First, we propose an adversarial network to enhance the diversity of modality-specific representations. Second, we introduce a label-based contrastive learning strategy to better capture emotional features. We refer to our proposed method as M4SER and validate its superiority over state-of-the-art methods through extensive experiments using IEMOCAP and MELD datasets.

摘要: 多模态语音情感识别（SER）已成为提升人机交互能力的关键技术。研究者日益重视利用通过自动语音识别（ASR）获取的语音与文本信息，以全面识别说话者的情感状态。尽管该方法降低了对人工标注文本数据的依赖，但ASR错误可能影响情感识别性能。为应对这一挑战，我们在先前工作中引入了两个辅助任务——ASR错误检测与ASR错误校正，并提出了一种新颖的多模态融合（MF）方法，用于学习跨模态的特定模态表征与不变模态表征。在此基础上，本文进一步提出两种训练策略：首先设计对抗网络以增强特定模态表征的多样性；其次引入基于标签的对比学习策略以更精准捕捉情感特征。我们将所提方法命名为M4SER，并基于IEMOCAP和MELD数据集通过大量实验验证了其相较于前沿方法的优越性。



## **35. Feature Space Topology Control via Hopkins Loss**

基于霍普金斯损失的特征空间拓扑控制 cs.LG

Accepted for publication in Proc. IEEE ICTAI 2025, Athens, Greece

**SubmitDate**: 2025-09-14    [abs](http://arxiv.org/abs/2509.11154v1) [paper-pdf](https://arxiv.org/pdf/2509.11154v1)

**Confidence**: 0.85

**Authors**: Einari Vaaras, Manu Airaksinen

**Abstract**: Feature space topology refers to the organization of samples within the feature space. Modifying this topology can be beneficial in machine learning applications, including dimensionality reduction, generative modeling, transfer learning, and robustness to adversarial attacks. This paper introduces a novel loss function, Hopkins loss, which leverages the Hopkins statistic to enforce a desired feature space topology, which is in contrast to existing topology-related methods that aim to preserve input feature topology. We evaluate the effectiveness of Hopkins loss on speech, text, and image data in two scenarios: classification and dimensionality reduction using nonlinear bottleneck autoencoders. Our experiments show that integrating Hopkins loss into classification or dimensionality reduction has only a small impact on classification performance while providing the benefit of modifying feature topology.

摘要: 特征空间拓扑指的是特征空间中样本的组织结构。在机器学习应用中，修改这种拓扑结构可能对降维、生成建模、迁移学习以及对抗攻击鲁棒性等方面有益。本文提出了一种新颖的损失函数——霍普金斯损失，该函数利用霍普金斯统计量来强制实现期望的特征空间拓扑，这与现有旨在保持输入特征拓扑的方法形成对比。我们在两种场景下评估了霍普金斯损失在语音、文本和图像数据上的有效性：分类任务以及使用非线性瓶颈自编码器的降维任务。实验表明，将霍普金斯损失融入分类或降维过程对分类性能影响较小，同时能够获得修改特征拓扑的益处。



## **36. ClaritySpeech: Dementia Obfuscation in Speech**

ClaritySpeech：针对痴呆症语音的混淆处理 cs.CL

Accepted at Interspeech 2025

**SubmitDate**: 2025-07-12    [abs](http://arxiv.org/abs/2507.09282v1) [paper-pdf](https://arxiv.org/pdf/2507.09282v1)

**Confidence**: 0.85

**Authors**: Dominika Woszczyk, Ranya Aloufi, Soteris Demetriou

**Abstract**: Dementia, a neurodegenerative disease, alters speech patterns, creating communication barriers and raising privacy concerns. Current speech technologies, such as automatic speech transcription (ASR), struggle with dementia and atypical speech, further challenging accessibility. This paper presents a novel dementia obfuscation in speech framework, ClaritySpeech, integrating ASR, text obfuscation, and zero-shot text-to-speech (TTS) to correct dementia-affected speech while preserving speaker identity in low-data environments without fine-tuning. Results show a 16% and 10% drop in mean F1 score across various adversarial settings and modalities (audio, text, fusion) for ADReSS and ADReSSo, respectively, maintaining 50% speaker similarity. We also find that our system improves WER (from 0.73 to 0.08 for ADReSS and 0.15 for ADReSSo) and speech quality from 1.65 to ~2.15, enhancing privacy and accessibility.

摘要: 痴呆症作为一种神经退行性疾病，会改变患者的语音模式，造成沟通障碍并引发隐私担忧。当前语音技术（如自动语音识别ASR）在处理痴呆症及非典型语音时存在困难，进一步加剧了可访问性挑战。本文提出了一种新颖的痴呆症语音混淆处理框架ClaritySpeech，该框架集成了ASR、文本混淆和零样本文本转语音（TTS）技术，可在无需微调的低数据环境下修正痴呆症影响的语音，同时保持说话人身份特征。实验结果显示，在ADReSS和ADReSSo数据集的各种对抗性设置与模态（音频、文本、融合）中，平均F1分数分别下降16%和10%，同时保持50%的说话人相似度。我们还发现，该系统将WER从0.73改善至0.08（ADReSS）和0.15（ADReSSo），语音质量从1.65提升至约2.15，有效增强了隐私保护和可访问性。



## **37. Robust Semantic Communications for Speech Transmission**

面向语音传输的鲁棒语义通信系统 eess.AS

**SubmitDate**: 2025-07-04    [abs](http://arxiv.org/abs/2403.05187v3) [paper-pdf](https://arxiv.org/pdf/2403.05187v3)

**Confidence**: 0.85

**Authors**: Zhenzi Weng, Zhijin Qin, Geoffrey Ye Li

**Abstract**: In this paper, we propose a robust semantic communication system for speech transmission, named Ross-S2T, by delivering the essential semantic information. Specifically, we consider the speech-to-text translation (S2TT) as the transmission goal. First, a new deep semantic encoder is developed to convert speech in the source language to textual features associated with the target language, facilitating the end-to-end semantic exchange to perform the S2TT task and reducing the transmission data without performance degradation. To mitigate semantic impairments inherent in the corrupted speech, a novel generative adversarial network (GAN)-enabled deep semantic compensator is established to estimate the lost semantic information within the speech and extract deep semantic features simultaneously, which enables robust semantic transmission for corrupted speech. Furthermore, a semantic probe-aided compensator is devised to enhance the semantic fidelity of recovered semantic features and improve the understandability of the target text. According to simulation results, the proposed Ross-S2T exhibits superior S2TT performance compared to conventional approaches and high robustness against semantic impairments.

摘要: 本文提出了一种名为Ross-S2T的鲁棒语义通信系统，通过传递核心语义信息实现语音传输。具体而言，我们将语音到文本翻译（S2TT）作为传输目标。首先，设计了一种新型深度语义编码器，将源语言语音转换为与目标语言关联的文本特征，这有助于端到端语义交换以执行S2TT任务，并在不降低性能的前提下减少传输数据量。为缓解受损语音中固有的语义损伤，建立了一种基于生成对抗网络（GAN）的深度语义补偿器，用于同时估计语音中丢失的语义信息并提取深度语义特征，从而实现对受损语音的鲁棒语义传输。此外，还设计了一种语义探针辅助补偿器，以增强恢复语义特征的语义保真度，并提高目标文本的可理解性。仿真结果表明，与传统方法相比，所提出的Ross-S2T系统展现出更优的S2TT性能，并对语义损伤具有高鲁棒性。



## **38. QUPID: A Partitioned Quantum Neural Network for Anomaly Detection in Smart Grid**

QUPID：用于智能电网异常检测的分区量子神经网络 cs.LG

**SubmitDate**: 2026-01-16    [abs](http://arxiv.org/abs/2601.11500v1) [paper-pdf](https://arxiv.org/pdf/2601.11500v1)

**Confidence**: 0.85

**Authors**: Hoang M. Ngo, Tre' R. Jeter, Jung Taek Seo, My T. Thai

**Abstract**: Smart grid infrastructures have revolutionized energy distribution, but their day-to-day operations require robust anomaly detection methods to counter risks associated with cyber-physical threats and system faults potentially caused by natural disasters, equipment malfunctions, and cyber attacks. Conventional machine learning (ML) models are effective in several domains, yet they struggle to represent the complexities observed in smart grid systems. Furthermore, traditional ML models are highly susceptible to adversarial manipulations, making them increasingly unreliable for real-world deployment. Quantum ML (QML) provides a unique advantage, utilizing quantum-enhanced feature representations to model the intricacies of the high-dimensional nature of smart grid systems while demonstrating greater resilience to adversarial manipulation. In this work, we propose QUPID, a partitioned quantum neural network (PQNN) that outperforms traditional state-of-the-art ML models in anomaly detection. We extend our model to R-QUPID that even maintains its performance when including differential privacy (DP) for enhanced robustness. Moreover, our partitioning framework addresses a significant scalability problem in QML by efficiently distributing computational workloads, making quantum-enhanced anomaly detection practical in large-scale smart grid environments. Our experimental results across various scenarios exemplifies the efficacy of QUPID and R-QUPID to significantly improve anomaly detection capabilities and robustness compared to traditional ML approaches.

摘要: 智能电网基础设施彻底改变了能源分配方式，但其日常运营需要强大的异常检测方法来应对网络物理威胁和系统故障带来的风险，这些风险可能由自然灾害、设备故障和网络攻击引起。传统机器学习（ML）模型在多个领域表现有效，但难以表征智能电网系统中观察到的复杂性。此外，传统ML模型极易受到对抗性操纵，使其在实际部署中越来越不可靠。量子机器学习（QML）具有独特优势，利用量子增强的特征表示来建模智能电网系统高维特性的复杂性，同时展现出更强的对抗性操纵鲁棒性。本文提出QUPID——一种分区量子神经网络（PQNN），在异常检测任务中优于传统最先进的ML模型。我们进一步扩展模型为R-QUPID，即使在引入差分隐私（DP）以增强鲁棒性的情况下仍能保持性能。此外，我们的分区框架通过高效分配计算负载，解决了QML中重要的可扩展性问题，使量子增强的异常检测在大规模智能电网环境中具有实用性。多场景实验结果表明，与传统ML方法相比，QUPID和R-QUPID能显著提升异常检测能力和鲁棒性。



## **39. NOWA: Null-space Optical Watermark for Invisible Capture Fingerprinting and Tamper Localization**

NOWA：用于隐形捕获指纹识别与篡改定位的空域光学水印 cs.CR

**SubmitDate**: 2026-01-02    [abs](http://arxiv.org/abs/2512.22501v2) [paper-pdf](https://arxiv.org/pdf/2512.22501v2)

**Confidence**: 0.85

**Authors**: Edwin Vargas, Jhon Lopez, Henry Arguello, Ashok Veeraraghavan

**Abstract**: Ensuring the authenticity and ownership of digital images is increasingly challenging as modern editing tools enable highly realistic forgeries. Existing image protection systems mainly rely on digital watermarking, which is susceptible to sophisticated digital attacks. To address this limitation, we propose a hybrid optical-digital framework that incorporates physical authentication cues during image formation and preserves them through a learned reconstruction process. At the optical level, a phase mask in the camera aperture produces a Null-space Optical Watermark (NOWA) that lies in the Null Space of the imaging operator and therefore remains invisible in the captured image. Then, a Null-Space Network (NSN) performs measurement-consistent reconstruction that delivers high-quality protected images while preserving the NOWA signature. The proposed design enables tamper localization by projecting the image onto the camera's null space and detecting pixel-level inconsistencies. Our design preserves perceptual quality, resists common degradations such as compression, and establishes a structural security asymmetry: without access to the optical or NSN parameters, adversaries cannot forge the NOWA signature. Experiments with simulations and a prototype camera demonstrate competitive performance in terms of image quality preservation, and tamper localization accuracy compared to state-of-the-art digital watermarking and learning-based authentication methods.

摘要: 随着现代编辑工具能够制作高度逼真的伪造图像，确保数字图像的真实性与所有权认证变得日益困难。现有图像保护系统主要依赖数字水印技术，但易受复杂数字攻击。为克服此局限，我们提出一种混合光学-数字框架，在图像形成过程中引入物理认证线索，并通过学习型重建过程予以保留。在光学层面，相机孔径中的相位掩模产生空域光学水印（NOWA），该水印位于成像算子的零空间中，因此在捕获图像中保持不可见。随后，空域网络（NSN）执行测量一致性重建，在保持NOWA特征的同时生成高质量受保护图像。该设计通过将图像投影至相机零空间并检测像素级不一致性，实现了篡改定位。我们的方案能保持感知质量，抵抗压缩等常见退化，并建立结构性安全不对称性：未获得光学或NSN参数的情况下，攻击者无法伪造NOWA特征。通过仿真与原型相机的实验表明，相较于最先进的数字水印和基于学习的认证方法，本方案在图像质量保持与篡改定位精度方面具有竞争优势。



## **40. Developing Distance-Aware, and Evident Uncertainty Quantification in Dynamic Physics-Constrained Neural Networks for Robust Bearing Degradation Estimation**

开发具有距离感知和显式不确定性量化的动态物理约束神经网络，用于稳健的轴承退化估计 cs.LG

Under review at Structural health Monitoring - SAGE

**SubmitDate**: 2025-12-18    [abs](http://arxiv.org/abs/2512.08499v2) [paper-pdf](https://arxiv.org/pdf/2512.08499v2)

**Confidence**: 0.85

**Authors**: Waleed Razzaq, Yun-Bo Zhao

**Abstract**: Accurate and uncertainty-aware degradation estimation is essential for predictive maintenance in safety-critical systems like rotating machinery with rolling-element bearings. Many existing uncertainty methods lack confidence calibration, are costly to run, are not distance-aware, and fail to generalize under out-of-distribution data. We introduce two distance-aware uncertainty methods for deterministic physics-guided neural networks: PG-SNGP, based on Spectral Normalization Gaussian Process, and PG-SNER, based on Deep Evidential Regression. We apply spectral normalization to the hidden layers so the network preserves distances from input to latent space. PG-SNGP replaces the final dense layer with a Gaussian Process layer for distance-sensitive uncertainty, while PG-SNER outputs Normal Inverse Gamma parameters to model uncertainty in a coherent probabilistic form. We assess performance using standard accuracy metrics and a new distance-aware metric based on the Pearson Correlation Coefficient, which measures how well predicted uncertainty tracks the distance between test and training samples. We also design a dynamic weighting scheme in the loss to balance data fidelity and physical consistency. We test our methods on rolling-element bearing degradation using the PRONOSTIA, XJTU-SY and HUST datasets and compare them with Monte Carlo and Deep Ensemble PGNNs. Results show that PG-SNGP and PG-SNER improve prediction accuracy, generalize reliably under OOD conditions, and remain robust to adversarial attacks and noise.

摘要: 在滚动轴承等旋转机械的安全关键系统中，准确且具有不确定性感知的退化估计对于预测性维护至关重要。现有许多不确定性方法缺乏置信度校准、计算成本高、不具备距离感知能力，且在分布外数据下泛化能力不足。我们为确定性物理引导神经网络引入了两种距离感知不确定性方法：基于谱归一化高斯过程的PG-SNGP和基于深度证据回归的PG-SNER。我们对隐藏层应用谱归一化，使网络能够保持从输入空间到潜在空间的距离特性。PG-SNGP将最终密集层替换为高斯过程层以实现距离敏感的不确定性量化，而PG-SNER则输出逆伽马正态分布参数，以一致的概率形式建模不确定性。我们使用标准精度指标和基于皮尔逊相关系数的新距离感知指标评估性能，该指标衡量预测不确定性跟踪测试样本与训练样本间距离的能力。我们还设计了损失函数中的动态加权方案，以平衡数据保真度与物理一致性。我们在PRONOSTIA、XJTU-SY和HUST数据集上测试了滚动轴承退化估计方法，并与蒙特卡洛和深度集成PGNNs进行对比。结果表明，PG-SNGP和PG-SNER提高了预测精度，在OOD条件下可靠泛化，并对对抗攻击和噪声保持稳健性。



## **41. A Dynamic Coding Scheme to Prevent Covert Cyber-Attacks in Cyber-Physical Systems**

一种防止信息物理系统中隐蔽网络攻击的动态编码方案 eess.SY

**SubmitDate**: 2025-12-09    [abs](http://arxiv.org/abs/2512.08134v1) [paper-pdf](https://arxiv.org/pdf/2512.08134v1)

**Confidence**: 0.85

**Authors**: Mahdi Taheri, Khashayar Khorasani, Nader Meskin

**Abstract**: In this paper, we address two main problems in the context of covert cyber-attacks in cyber-physical systems (CPS). First, we aim to investigate and develop necessary and sufficient conditions in terms of disruption resources of the CPS that enable adversaries to execute covert cyber-attacks. These conditions can be utilized to identify the input and output communication channels that are needed by adversaries to execute these attacks. Second, this paper introduces and develops a dynamic coding scheme as a countermeasure against covert cyber-attacks. Under certain conditions and assuming the existence of one secure input and two secure output communication channels, the proposed dynamic coding scheme prevents adversaries from executing covert cyber-attacks. A numerical case study of a flight control system is provided to demonstrate the capabilities of our proposed and developed dynamic coding scheme.

摘要: 本文针对信息物理系统（CPS）中的隐蔽网络攻击，主要解决两个问题。首先，我们研究并建立了关于CPS破坏资源的充要条件，这些条件使攻击者能够执行隐蔽网络攻击。这些条件可用于识别攻击者执行此类攻击所需的输入和输出通信通道。其次，本文提出并开发了一种动态编码方案作为对抗隐蔽网络攻击的对策。在一定条件下，假设存在一个安全输入通道和两个安全输出通道，所提出的动态编码方案能够防止攻击者执行隐蔽网络攻击。本文通过飞行控制系统的数值案例研究，展示了我们提出和开发的动态编码方案的能力。



## **42. Hybrid-Sep: Language-queried audio source separation via pre-trained Model Fusion and Adversarial Diffusion Training**

Hybrid-Sep：基于预训练模型融合与对抗扩散训练的语言查询音频源分离方法 cs.SD

Submitted to WASAA 2025

**SubmitDate**: 2025-06-20    [abs](http://arxiv.org/abs/2506.16833v1) [paper-pdf](https://arxiv.org/pdf/2506.16833v1)

**Confidence**: 0.85

**Authors**: Jianyuan Feng, Guangzheng Li, Yangfei Xu

**Abstract**: Language-queried Audio Separation (LASS) employs linguistic queries to isolate target sounds based on semantic descriptions. However, existing methods face challenges in aligning complex auditory features with linguistic context while preserving separation precision. Current research efforts focus primarily on text description augmentation and architectural innovations, yet the potential of integrating pre-trained self-supervised learning (SSL) audio models and Contrastive Language-Audio Pretraining (CLAP) frameworks, capable of extracting cross-modal audio-text relationships, remains underexplored. To address this, we present HybridSep, a two-stage LASS framework that synergizes SSL-based acoustic representations with CLAP-derived semantic embeddings. Our framework introduces Adversarial Consistent Training (ACT), a novel optimization strategy that treats diffusion as an auxiliary regularization loss while integrating adversarial training to enhance separation fidelity. Experiments demonstrate that HybridSep achieves significant performance improvements over state-of-the-art baselines (e.g., AudioSep, FlowSep) across multiple metrics, establishing new benchmarks for LASS tasks.

摘要: 语言查询音频分离（LASS）利用语言查询根据语义描述分离目标声音。然而，现有方法在将复杂听觉特征与语言上下文对齐的同时保持分离精度方面面临挑战。当前研究主要集中于文本描述增强和架构创新，但整合能够提取跨模态音频-文本关系的预训练自监督学习（SSL）音频模型和对比语言-音频预训练（CLAP）框架的潜力尚未充分探索。为此，我们提出HybridSep，一个两阶段LASS框架，将基于SSL的声学表征与CLAP衍生的语义嵌入协同结合。我们的框架引入了对抗一致性训练（ACT），这是一种新颖的优化策略，将扩散作为辅助正则化损失，同时整合对抗训练以增强分离保真度。实验表明，HybridSep在多项指标上显著优于现有先进基线方法（如AudioSep、FlowSep），为LASS任务建立了新的基准。



## **43. Speech Unlearning**

语音遗忘学习 cs.LG

Interspeech 2025

**SubmitDate**: 2025-06-01    [abs](http://arxiv.org/abs/2506.00848v1) [paper-pdf](https://arxiv.org/pdf/2506.00848v1)

**Confidence**: 0.85

**Authors**: Jiali Cheng, Hadi Amiri

**Abstract**: We introduce machine unlearning for speech tasks, a novel and underexplored research problem that aims to efficiently and effectively remove the influence of specific data from trained speech models without full retraining. This has important applications in privacy preservation, removal of outdated or noisy data, and bias mitigation. While machine unlearning has been studied in computer vision and natural language processing, its application to speech is largely unexplored due to the high-dimensional, sequential, and speaker-dependent nature of speech data. We define two fundamental speech unlearning tasks: sample unlearning, which removes individual data points (e.g., a voice recording), and class unlearning, which removes an entire category (e.g., all data from a speaker), while preserving performance on the remaining data. Experiments on keyword spotting and speaker identification demonstrate that unlearning speech data is significantly more challenging than unlearning image or text data. We conclude with key future directions in this area, including structured training, robust evaluation, feature-level unlearning, broader applications, scalable methods, and adversarial robustness.

摘要: 本文提出语音任务中的机器遗忘学习，这是一个新颖且尚未充分探索的研究问题，旨在无需完全重新训练的情况下，高效且有效地从已训练的语音模型中移除特定数据的影响。这在隐私保护、过时或噪声数据移除以及偏见缓解方面具有重要应用价值。虽然机器遗忘学习已在计算机视觉和自然语言处理领域得到研究，但由于语音数据具有高维度、序列化和说话人依赖的特性，其在语音领域的应用仍基本处于空白。我们定义了两个基础的语音遗忘学习任务：样本遗忘（移除单个数据点，如一段语音录音）和类别遗忘（移除整个类别，如某说话人的所有数据），同时保持对剩余数据的性能。在关键词检测和说话人识别任务上的实验表明，语音数据的遗忘学习比图像或文本数据更具挑战性。最后，我们提出了该领域未来的关键研究方向，包括结构化训练、鲁棒评估、特征级遗忘学习、更广泛的应用、可扩展方法以及对抗鲁棒性。



## **44. Adversarial Deep Metric Learning for Cross-Modal Audio-Text Alignment in Open-Vocabulary Keyword Spotting**

基于对抗深度度量学习的跨模态音频-文本对齐在开放词汇关键词检测中的应用 eess.AS

5 pages, 1 figure, Accepted at Interspeech 2025

**SubmitDate**: 2025-05-23    [abs](http://arxiv.org/abs/2505.16735v2) [paper-pdf](https://arxiv.org/pdf/2505.16735v2)

**Confidence**: 0.85

**Authors**: Youngmoon Jung, Yong-Hyeok Lee, Myunghun Jung, Jaeyoung Roh, Chang Woo Han, Hoon-Young Cho

**Abstract**: For text enrollment-based open-vocabulary keyword spotting (KWS), acoustic and text embeddings are typically compared at either the phoneme or utterance level. To facilitate this, we optimize acoustic and text encoders using deep metric learning (DML), enabling direct comparison of multi-modal embeddings in a shared embedding space. However, the inherent heterogeneity between audio and text modalities presents a significant challenge. To address this, we propose Modality Adversarial Learning (MAL), which reduces the domain gap in heterogeneous modality representations. Specifically, we train a modality classifier adversarially to encourage both encoders to generate modality-invariant embeddings. Additionally, we apply DML to achieve phoneme-level alignment between audio and text, and conduct extensive comparisons across various DML objectives. Experiments on the Wall Street Journal (WSJ) and LibriPhrase datasets demonstrate the effectiveness of the proposed approach.

摘要: 对于基于文本注册的开放词汇关键词检测（KWS），通常会在音素或话语级别比较声学和文本嵌入。为此，我们采用深度度量学习（DML）优化声学和文本编码器，使多模态嵌入能在共享嵌入空间中直接比较。然而，音频与文本模态间的固有异质性构成了显著挑战。为解决此问题，我们提出模态对抗学习（MAL），以减小异质模态表示间的领域差异。具体而言，我们对抗性地训练模态分类器，促使两个编码器生成模态不变的嵌入。此外，我们应用DML实现音频与文本间的音素级对齐，并对多种DML目标进行了广泛比较。在华尔街日报（WSJ）和LibriPhrase数据集上的实验验证了所提方法的有效性。



## **45. Adversarial Multi-Agent Reinforcement Learning for Proactive False Data Injection Detection**

对抗性多智能体强化学习用于主动虚假数据注入检测 eess.SY

**SubmitDate**: 2026-01-14    [abs](http://arxiv.org/abs/2411.12130v2) [paper-pdf](https://arxiv.org/pdf/2411.12130v2)

**Confidence**: 0.85

**Authors**: Kejun Chen, Truc Nguyen, Abhijeet Sahu, Malik Hassanaly

**Abstract**: Smart inverters are instrumental in the integration of distributed energy resources into the electric grid. Such inverters rely on communication layers for continuous control and monitoring, potentially exposing them to cyber-physical attacks such as false data injection attacks (FDIAs). We propose to construct a defense strategy against a priori unknown FDIAs with a multi-agent reinforcement learning (MARL) framework. The first agent is an adversary that simulates and discovers various FDIA strategies, while the second agent is a defender in charge of detecting and locating FDIAs. This approach enables the defender to be trained against new FDIAs continuously generated by the adversary. In addition, we show that the detection skills of an MARL defender can be combined with those of a supervised offline defender through a transfer learning approach. Numerical experiments conducted on a distribution and transmission system demonstrate that: a) the proposed MARL defender outperforms the offline defender against adversarial attacks; b) the transfer learning approach makes the MARL defender capable against both synthetic and unseen FDIAs.

摘要: 智能逆变器在将分布式能源整合到电网中发挥着关键作用。这类逆变器依赖通信层进行持续控制和监控，可能使其暴露于虚假数据注入攻击（FDIAs）等网络物理攻击。我们提出采用多智能体强化学习（MARL）框架构建针对先验未知FDIAs的防御策略。第一个智能体作为对手，模拟并发现各种FDIA策略；第二个智能体作为防御者，负责检测和定位FDIAs。该方法使防御者能够针对对手持续生成的新FDIAs进行训练。此外，我们通过迁移学习方法，将MARL防御者的检测能力与监督式离线防御者相结合。在配电和输电系统上进行的数值实验表明：a) 所提出的MARL防御者在对抗攻击中优于离线防御者；b) 迁移学习方法使MARL防御者能够有效应对合成及未见过的FDIAs。



## **46. StegoStylo: Squelching Stylometric Scrutiny through Steganographic Stitching**

StegoStylo：通过隐写拼接抑制文体计量审查 cs.CR

16 pages, 6 figures, 1 table

**SubmitDate**: 2026-01-19    [abs](http://arxiv.org/abs/2601.09056v2) [paper-pdf](https://arxiv.org/pdf/2601.09056v2)

**Confidence**: 0.85

**Authors**: Robert Dilworth

**Abstract**: Stylometry--the identification of an author through analysis of a text's style (i.e., authorship attribution)--serves many constructive purposes: it supports copyright and plagiarism investigations, aids detection of harmful content, offers exploratory cues for certain medical conditions (e.g., early signs of dementia or depression), provides historical context for literary works, and helps uncover misinformation and disinformation. In contrast, when stylometry is employed as a tool for authorship verification--confirming whether a text truly originates from a claimed author--it can also be weaponized for malicious purposes. Techniques such as de-anonymization, re-identification, tracking, profiling, and downstream effects like censorship illustrate the privacy threats that stylometric analysis can enable. Building on these concerns, this paper further explores how adversarial stylometry combined with steganography can counteract stylometric analysis. We first present enhancements to our adversarial attack, $\textit{TraceTarnish}$, providing stronger evidence of its capacity to confound stylometric systems and reduce their attribution and verification accuracy. Next, we examine how steganographic embedding can be fine-tuned to mask an author's stylistic fingerprint, quantifying the level of authorship obfuscation achievable as a function of the proportion of words altered with zero-width Unicode characters. Based on our findings, steganographic coverage of 33% or higher seemingly ensures authorship obfuscation. Finally, we reflect on the ways stylometry can be used to undermine privacy and argue for the necessity of defensive tools like $\textit{TraceTarnish}$.

摘要: 文体计量学——通过分析文本风格识别作者（即作者归属）——具有许多建设性用途：支持版权和剽窃调查，协助检测有害内容，为某些医学状况（如痴呆或抑郁症的早期迹象）提供探索线索，为文学作品提供历史背景，并帮助揭露虚假和误导信息。然而，当文体计量学被用作作者验证工具——确认文本是否真正源自声称的作者时，它也可能被武器化用于恶意目的。去匿名化、重新识别、追踪、画像以及审查等下游效应等技术，说明了文体计量分析可能带来的隐私威胁。基于这些担忧，本文进一步探讨了对抗性文体计量学与隐写术相结合如何对抗文体计量分析。我们首先介绍了对抗攻击方法 $	extit{TraceTarnish}$ 的增强版本，提供了更强证据表明其能够混淆文体计量系统并降低其归属和验证准确性。接着，我们研究了如何微调隐写嵌入以掩盖作者的风格指纹，量化了通过零宽度Unicode字符修改单词比例所能实现的作者混淆程度。根据我们的发现，33%或更高的隐写覆盖率似乎能确保作者混淆。最后，我们反思了文体计量学如何被用于破坏隐私，并论证了像 $	extit{TraceTarnish}$ 这样的防御工具的必要性。



## **47. Towards a constructive framework for control theory**

迈向控制理论的构造性框架 math.OC

Published under: https://ieeexplore.ieee.org/document/9419858

**SubmitDate**: 2026-01-19    [abs](http://arxiv.org/abs/2501.02267v2) [paper-pdf](https://arxiv.org/pdf/2501.02267v2)

**Confidence**: 0.85

**Authors**: Pavel Osinenko

**Abstract**: This work presents a framework for control theory based on constructive analysis to account for discrepancy between mathematical results and their implementation in a computer, also referred to as computational uncertainty. In control engineering, the latter is usually either neglected or considered submerged into some other type of uncertainty, such as system noise, and addressed within robust control. However, even robust control methods may be compromised when the mathematical objects involved in the respective algorithms fail to exist in exact form and subsequently fail to satisfy the required properties. For instance, in general stabilization using a control Lyapunov function, computational uncertainty may distort stability certificates or even destabilize the system despite robustness of the stabilization routine with regards to system, actuator and measurement noise. In fact, battling numerical problems in practical implementation of controllers is common among control engineers. Such observations indicate that computational uncertainty should indeed be addressed explicitly in controller synthesis and system analysis. The major contribution here is a fairly general framework for proof techniques in analysis and synthesis of control systems based on constructive analysis which explicitly states that every computation be doable only up to a finite precision thus accounting for computational uncertainty. A series of previous works is overviewed, including constructive system stability and stabilization, approximate optimal controls, eigenvalue problems, Caratheodory trajectories, measurable selectors. Additionally, a new constructive version of the Danskin's theorem, which is crucial in adversarial defense, is presented.

摘要: 本文提出了一种基于构造性分析的控制理论框架，旨在解决数学结果与其在计算机中实现之间的差异，也称为计算不确定性。在控制工程中，后者通常被忽略或被视为融入其他类型的不确定性（如系统噪声），并在鲁棒控制框架内处理。然而，当相关算法涉及的数学对象无法以精确形式存在且无法满足所需属性时，即使是鲁棒控制方法也可能受到影响。例如，在使用控制李雅普诺夫函数进行一般镇定控制时，计算不确定性可能扭曲稳定性证明，甚至破坏系统稳定性，尽管镇定程序对系统、执行器和测量噪声具有鲁棒性。实际上，在控制器实际实现中应对数值问题是控制工程师的常见挑战。这些观察表明，计算不确定性确实应在控制器综合和系统分析中得到明确处理。本文的主要贡献是基于构造性分析提出了一个相当通用的控制系统分析与综合证明技术框架，该框架明确要求所有计算只能在有限精度下完成，从而考虑计算不确定性。文中综述了一系列先前工作，包括构造性系统稳定性与镇定、近似最优控制、特征值问题、Caratheodory轨迹、可测选择器等。此外，还提出了Danskin定理的新构造性版本，该定理在对抗防御中至关重要。



## **48. The Good, the Bad and the Ugly: Meta-Analysis of Watermarks, Transferable Attacks and Adversarial Defenses**

水印、可迁移攻击与对抗防御的元分析：优劣与困境 cs.LG

47 pages, 3 figures, 4 tables, preliminary version published in ICML 2024 (Workshop on Theoretical Foundations of Foundation Models) and , see https://openreview.net/pdf?id=WMaFRiggwV

**SubmitDate**: 2026-01-21    [abs](http://arxiv.org/abs/2410.08864v2) [paper-pdf](https://arxiv.org/pdf/2410.08864v2)

**Confidence**: 0.85

**Authors**: Grzegorz Głuch, Berkant Turan, Sai Ganesh Nagarajan, Sebastian Pokutta

**Abstract**: We formalize and analyze the trade-off between backdoor-based watermarks and adversarial defenses, framing it as an interactive protocol between a verifier and a prover. While previous works have primarily focused on this trade-off, our analysis extends it by identifying transferable attacks as a third, counterintuitive, but necessary option. Our main result shows that for all learning tasks, at least one of the three exists: a watermark, an adversarial defense, or a transferable attack. By transferable attack, we refer to an efficient algorithm that generates queries indistinguishable from the data distribution and capable of fooling all efficient defenders. Using cryptographic techniques, specifically fully homomorphic encryption, we construct a transferable attack and prove its necessity in this trade-off. Finally, we show that tasks of bounded VC-dimension allow adversarial defenses against all attackers, while a subclass allows watermarks secure against fast adversaries.

摘要: 我们形式化并分析了基于后门的水印与对抗防御之间的权衡，将其构建为验证者与证明者之间的交互协议。尽管先前研究主要关注这一权衡，但我们的分析通过将可迁移攻击识别为第三个反直觉但必要的选项，扩展了这一框架。我们的主要结果表明，对于所有学习任务，以下三者至少存在其一：水印、对抗防御或可迁移攻击。所谓可迁移攻击，指的是能生成与数据分布不可区分且能欺骗所有高效防御者的查询的高效算法。利用密码学技术（特别是全同态加密），我们构建了一个可迁移攻击，并证明了其在此权衡中的必要性。最后，我们证明有限VC维度的任务允许针对所有攻击者的对抗防御，而其中一个子类允许抵御快速对手的安全水印。



## **49. Multimodal Representation Learning and Fusion**

多模态表示学习与融合 cs.LG

**SubmitDate**: 2025-12-18    [abs](http://arxiv.org/abs/2506.20494v2) [paper-pdf](https://arxiv.org/pdf/2506.20494v2)

**Confidence**: 0.70

**Authors**: Qihang Jin, Enze Ge, Yuhang Xie, Hongying Luo, Junhao Song, Ziqian Bi, Chia Xin Liang, Jibin Guan, Joe Yeong, Xinyuan Song, Junfeng Hao

**Abstract**: Multi-modal learning is a fast growing area in artificial intelligence. It tries to help machines understand complex things by combining information from different sources, like images, text, and audio. By using the strengths of each modality, multi-modal learning allows AI systems to build stronger and richer internal representations. These help machines better interpretation, reasoning, and making decisions in real-life situations. This field includes core techniques such as representation learning (to get shared features from different data types), alignment methods (to match information across modalities), and fusion strategies (to combine them by deep learning models). Although there has been good progress, some major problems still remain. Like dealing with different data formats, missing or incomplete inputs, and defending against adversarial attacks. Researchers now are exploring new methods, such as unsupervised or semi-supervised learning, AutoML tools, to make models more efficient and easier to scale. And also more attention on designing better evaluation metrics or building shared benchmarks, make it easier to compare model performance across tasks and domains. As the field continues to grow, multi-modal learning is expected to improve many areas: computer vision, natural language processing, speech recognition, and healthcare. In the future, it may help to build AI systems that can understand the world in a way more like humans, flexible, context aware, and able to deal with real-world complexity.

摘要: 多模态学习是人工智能领域快速发展的方向，旨在通过整合图像、文本、音频等不同来源的信息，帮助机器理解复杂事物。利用各模态的优势，多模态学习使AI系统能够构建更强大、更丰富的内部表示，从而提升机器在现实场景中的解释、推理和决策能力。该领域涵盖核心技术，如表示学习（从不同数据类型中提取共享特征）、对齐方法（跨模态信息匹配）和融合策略（通过深度学习模型整合信息）。尽管已取得显著进展，但仍面临关键挑战，例如处理异构数据格式、应对缺失或不完整输入，以及防御对抗性攻击。当前研究正探索新方法，如无监督或半监督学习、AutoML工具，以提高模型效率和可扩展性；同时更注重设计更好的评估指标或构建共享基准，以便跨任务和领域比较模型性能。随着领域持续发展，多模态学习有望推动计算机视觉、自然语言处理、语音识别和医疗健康等领域的进步。未来，它或助力构建更类人化的AI系统——灵活、具备情境感知能力，并能应对现实世界的复杂性。



## **50. Reach-Avoid Differential game with Reachability Analysis for UAVs: A decomposition approach**

基于可达性分析的无人机追逃微分博弈：一种分解方法 eess.SY

Paper version accepted to the Journal of Guidance, Control, and Dynamics (JGCD)

**SubmitDate**: 2025-12-28    [abs](http://arxiv.org/abs/2512.22793v1) [paper-pdf](https://arxiv.org/pdf/2512.22793v1)

**Confidence**: 0.70

**Authors**: Minh Bui, Simon Monckton, Mo Chen

**Abstract**: Reach-avoid (RA) games have significant applications in security and defense, particularly for unmanned aerial vehicles (UAVs). These problems are inherently challenging due to the need to consider obstacles, consider the adversarial nature of opponents, ensure optimality, and account for nonlinear dynamics. Hamilton-Jacobi (HJ) reachability analysis has emerged as a powerful tool for tackling these challenges; however, while it has been applied to games involving two spatial dimensions, directly extending this approach to three spatial dimensions is impossible due to high dimensionality. On the other hand, alternative approaches for solving RA games lack the generality to consider games with three spatial dimensions involving agents with non-trivial system dynamics. In this work, we propose a novel framework for dimensionality reduction by decomposing the problem into a horizontal RA sub-game and a vertical RA sub-game. We then solve each sub-game using HJ reachability analysis and consider second-order dynamics that account for the defender's acceleration. To reconstruct the solution to the original RA game from the sub-games, we introduce a HJ-based tracking control algorithm in each sub-game that not only guarantees capture of the attacker but also tracking of the attacker thereafter. We prove the conditions under which the capture guarantees are maintained. The effectiveness of our approach is demonstrated via numerical simulations, showing that the decomposition maintains optimality and guarantees in the original problem. Our methods are also validated in a Gazebo physics simulator, achieving successful capture of quadrotors in three spatial dimensions space for the first time to the best of our knowledge.

摘要: 追逃博弈在安防和国防领域具有重要应用价值，尤其对于无人机系统。这类问题本质复杂，需同时考虑障碍物、对手对抗性、最优性要求及非线性动力学特性。Hamilton-Jacobi可达性分析已成为应对这些挑战的有力工具；然而，虽然该方法已应用于二维空间博弈，但由于高维性限制，无法直接扩展到三维空间。另一方面，现有追逃博弈求解方法缺乏处理具有复杂系统动力学的三维空间博弈的通用性。本文提出一种新颖的降维框架：将原问题分解为水平追逃子博弈和垂直追逃子博弈。我们运用HJ可达性分析求解各子博弈，并考虑包含防御方加速度的二阶动力学模型。为从子博弈重构原追逃博弈的解，我们在每个子博弈中引入基于HJ的跟踪控制算法，该算法不仅能保证捕获攻击者，还能实现持续跟踪。我们证明了维持捕获保证的条件。数值仿真验证了本方法的有效性，表明分解策略能保持原问题的最优性与保证条件。我们的方法还在Gazebo物理仿真器中得到验证，首次实现了三维空间四旋翼无人机的成功捕获。



## **51. Who Does What in Deep Learning? Multidimensional Game-Theoretic Attribution of Function of Neural Units**

深度学习中的功能分配：基于多维博弈论的神经网络单元功能归因 cs.LG

**SubmitDate**: 2025-06-24    [abs](http://arxiv.org/abs/2506.19732v1) [paper-pdf](https://arxiv.org/pdf/2506.19732v1)

**Confidence**: 0.70

**Authors**: Shrey Dixit, Kayson Fakhar, Fatemeh Hadaeghi, Patrick Mineault, Konrad P. Kording, Claus C. Hilgetag

**Abstract**: Neural networks now generate text, images, and speech with billions of parameters, producing a need to know how each neural unit contributes to these high-dimensional outputs. Existing explainable-AI methods, such as SHAP, attribute importance to inputs, but cannot quantify the contributions of neural units across thousands of output pixels, tokens, or logits. Here we close that gap with Multiperturbation Shapley-value Analysis (MSA), a model-agnostic game-theoretic framework. By systematically lesioning combinations of units, MSA yields Shapley Modes, unit-wise contribution maps that share the exact dimensionality of the model's output. We apply MSA across scales, from multi-layer perceptrons to the 56-billion-parameter Mixtral-8x7B and Generative Adversarial Networks (GAN). The approach demonstrates how regularisation concentrates computation in a few hubs, exposes language-specific experts inside the LLM, and reveals an inverted pixel-generation hierarchy in GANs. Together, these results showcase MSA as a powerful approach for interpreting, editing, and compressing deep neural networks.

摘要: 当前神经网络通过数十亿参数生成文本、图像和语音，亟需厘清每个神经单元如何影响这些高维输出。现有可解释AI方法（如SHAP）主要对输入特征进行重要性归因，但无法量化神经单元对数千个输出像素、标记或逻辑单元的贡献。本研究通过多扰动Shapley值分析（MSA）填补了这一空白——这是一个模型无关的博弈论框架。通过系统性地损伤单元组合，MSA生成与模型输出维度完全一致的Shapley模态，即单元级贡献图谱。我们将MSA应用于多尺度模型：从多层感知机到560亿参数的Mixtral-8x7B大语言模型及生成对抗网络（GAN）。该方法揭示了正则化如何将计算集中于少数枢纽节点，展现了LLM内部的语言特定专家模块，并发现了GAN中倒置的像素生成层级结构。这些成果共同证明MSA是解释、编辑和压缩深度神经网络的有力工具。



