# Latest Adversarial Attack Papers
**update at 2025-06-05 10:59:12**

翻译来自 https://cloud.tencent.com/document/product/551/15619

## **1. Privacy and Security Threat for OpenAI GPTs**

OpenAI GPT的隐私和安全威胁 cs.CR

**SubmitDate**: 2025-06-04    [abs](http://arxiv.org/abs/2506.04036v1) [paper-pdf](http://arxiv.org/pdf/2506.04036v1)

**Authors**: Wei Wenying, Zhao Kaifa, Xue Lei, Fan Ming

**Abstract**: Large language models (LLMs) demonstrate powerful information handling capabilities and are widely integrated into chatbot applications. OpenAI provides a platform for developers to construct custom GPTs, extending ChatGPT's functions and integrating external services. Since its release in November 2023, over 3 million custom GPTs have been created. However, such a vast ecosystem also conceals security and privacy threats. For developers, instruction leaking attacks threaten the intellectual property of instructions in custom GPTs through carefully crafted adversarial prompts. For users, unwanted data access behavior by custom GPTs or integrated third-party services raises significant privacy concerns. To systematically evaluate the scope of threats in real-world LLM applications, we develop three phases instruction leaking attacks target GPTs with different defense level. Our widespread experiments on 10,000 real-world custom GPTs reveal that over 98.8% of GPTs are vulnerable to instruction leaking attacks via one or more adversarial prompts, and half of the remaining GPTs can also be attacked through multiround conversations. We also developed a framework to assess the effectiveness of defensive strategies and identify unwanted behaviors in custom GPTs. Our findings show that 77.5% of custom GPTs with defense strategies are vulnerable to basic instruction leaking attacks. Additionally, we reveal that 738 custom GPTs collect user conversational information, and identified 8 GPTs exhibiting data access behaviors that are unnecessary for their intended functionalities. Our findings raise awareness among GPT developers about the importance of integrating specific defensive strategies in their instructions and highlight users' concerns about data privacy when using LLM-based applications.

摘要: 大型语言模型（LLM）展示了强大的信息处理能力，并被广泛集成到聊天机器人应用程序中。OpenAI为开发人员提供了一个构建自定义GPT的平台，扩展了ChatGPT的功能并集成了外部服务。自2023年11月发布以来，已创建了超过300万个自定义GPT。然而，如此庞大的生态系统也隐藏着安全和隐私威胁。对于开发人员来说，指令泄露攻击通过精心设计的对抗性提示威胁自定义GPT中指令的知识产权。对于用户来说，自定义GPT或集成第三方服务的不想要的数据访问行为会引发严重的隐私问题。为了系统地评估现实LLM应用中的威胁范围，我们开发了三个阶段的指令泄露攻击，目标是不同防御级别的GPT。我们对10，000个现实世界的自定义GPT进行的广泛实验表明，超过98.8%的GPT容易受到通过一个或多个对抗提示的指令泄露攻击，其余一半的GPT也可以通过多轮对话受到攻击。我们还开发了一个框架来评估防御策略的有效性并识别自定义GPT中不想要的行为。我们的研究结果表明，77.5%的具有防御策略的自定义GPT容易受到基本指令泄露攻击。此外，我们还透露，738个自定义GPT收集用户对话信息，并识别出8个GPT表现出其预期功能不必要的数据访问行为。我们的研究结果提高了GPT开发人员对在其指令中集成特定防御策略重要性的认识，并强调了用户在使用基于LLM的应用程序时对数据隐私的担忧。



## **2. RAID: A Dataset for Testing the Adversarial Robustness of AI-Generated Image Detectors**

RAGE：用于测试人工智能生成图像检测器对抗鲁棒性的数据集 cs.CV

Under review for NeurIPS 2025 Datasets and Benchmarks Track

**SubmitDate**: 2025-06-04    [abs](http://arxiv.org/abs/2506.03988v1) [paper-pdf](http://arxiv.org/pdf/2506.03988v1)

**Authors**: Hicham Eddoubi, Jonas Ricker, Federico Cocchi, Lorenzo Baraldi, Angelo Sotgiu, Maura Pintor, Marcella Cornia, Lorenzo Baraldi, Asja Fischer, Rita Cucchiara, Battista Biggio

**Abstract**: AI-generated images have reached a quality level at which humans are incapable of reliably distinguishing them from real images. To counteract the inherent risk of fraud and disinformation, the detection of AI-generated images is a pressing challenge and an active research topic. While many of the presented methods claim to achieve high detection accuracy, they are usually evaluated under idealized conditions. In particular, the adversarial robustness is often neglected, potentially due to a lack of awareness or the substantial effort required to conduct a comprehensive robustness analysis. In this work, we tackle this problem by providing a simpler means to assess the robustness of AI-generated image detectors. We present RAID (Robust evaluation of AI-generated image Detectors), a dataset of 72k diverse and highly transferable adversarial examples. The dataset is created by running attacks against an ensemble of seven state-of-the-art detectors and images generated by four different text-to-image models. Extensive experiments show that our methodology generates adversarial images that transfer with a high success rate to unseen detectors, which can be used to quickly provide an approximate yet still reliable estimate of a detector's adversarial robustnessOur findings indicate that current state-of-the-art AI-generated image detectors can be easily deceived by adversarial examples, highlighting the critical need for the development of more robust methods. We release our dataset at https://huggingface.co/datasets/aimagelab/RAID and evaluation code at https://github.com/pralab/RAID.

摘要: 人工智能生成的图像已经达到了人类无法可靠地将其与真实图像区分开来的质量水平。为了抵消欺诈和虚假信息的固有风险，检测人工智能生成的图像是一项紧迫的挑战和一个活跃的研究课题。虽然提出的许多方法声称可以实现高检测准确性，但它们通常是在理想化条件下进行评估的。特别是，对抗稳健性经常被忽视，这可能是由于缺乏意识或进行全面稳健性分析所需的大量努力。在这项工作中，我们通过提供一种更简单的方法来评估人工智能生成的图像检测器的稳健性来解决这个问题。我们介绍了RAIDs（人工智能生成图像检测器的稳健评估），这是一个包含72 k个多样化且高度可转移的对抗示例的数据集。该数据集是通过对七个最先进的检测器和由四种不同的文本到图像模型生成的图像进行攻击来创建的。大量的实验表明，我们的方法可以生成对抗图像，这些图像以很高的成功率传输到未见的检测器，可以用于快速提供对检测器对抗鲁棒性的大致但仍然可靠的估计。我们的研究结果表明，当前最先进的人工智能生成图像检测器很容易被对抗性的例子欺骗，凸显了开发更稳健方法的迫切需要。我们在https://huggingface.co/datasets/aimagelab/RAID上发布我们的数据集，并在https://github.com/pralab/RAID上发布评估代码。



## **3. DiffCAP: Diffusion-based Cumulative Adversarial Purification for Vision Language Models**

迪夫CAP：视觉语言模型的基于扩散的累积对抗净化 cs.CV

**SubmitDate**: 2025-06-04    [abs](http://arxiv.org/abs/2506.03933v1) [paper-pdf](http://arxiv.org/pdf/2506.03933v1)

**Authors**: Jia Fu, Yongtao Wu, Yihang Chen, Kunyu Peng, Xiao Zhang, Volkan Cevher, Sepideh Pashami, Anders Holst

**Abstract**: Vision Language Models (VLMs) have shown remarkable capabilities in multimodal understanding, yet their susceptibility to perturbations poses a significant threat to their reliability in real-world applications. Despite often being imperceptible to humans, these perturbations can drastically alter model outputs, leading to erroneous interpretations and decisions. This paper introduces DiffCAP, a novel diffusion-based purification strategy that can effectively neutralize adversarial corruptions in VLMs. We observe that adding minimal noise to an adversarially corrupted image significantly alters its latent embedding with respect to VLMs. Building on this insight, DiffCAP cumulatively injects random Gaussian noise into adversarially perturbed input data. This process continues until the embeddings of two consecutive noisy images reach a predefined similarity threshold, indicating a potential approach to neutralize the adversarial effect. Subsequently, a pretrained diffusion model is employed to denoise the stabilized image, recovering a clean representation suitable for the VLMs to produce an output. Through extensive experiments across six datasets with three VLMs under varying attack strengths in three task scenarios, we show that DiffCAP consistently outperforms existing defense techniques by a substantial margin. Notably, DiffCAP significantly reduces both hyperparameter tuning complexity and the required diffusion time, thereby accelerating the denoising process. Equipped with strong theoretical and empirical support, DiffCAP provides a robust and practical solution for securely deploying VLMs in adversarial environments.

摘要: 视觉语言模型（VLM）在多模态理解方面表现出了卓越的能力，但它们对扰动的敏感性对其在现实世界应用中的可靠性构成了重大威胁。尽管人类通常无法察觉，但这些扰动会极大地改变模型输出，导致错误的解释和决策。本文介绍了DiffCAP，一种新的基于扩散的净化策略，可以有效地中和VLM中的对抗性腐败。我们观察到，添加最小的噪声的adversarially损坏的图像显着改变其潜在的嵌入相对于VLMs。基于这一见解，迪夫CAP将随机高斯噪音累积注入到敌对干扰的输入数据中。该过程持续进行，直到两个连续有噪图像的嵌入达到预定义的相似性阈值，这表明了中和对抗效应的潜在方法。随后，采用预先训练的扩散模型对稳定的图像进行降噪，恢复适合VLM的干净表示以产生输出。通过在三种任务场景中不同攻击强度下对具有三个VLM的六个数据集进行广泛实验，我们表明，迪夫CAP始终以大幅优势优于现有防御技术。值得注意的是，迪夫CAP显着降低了超参数调整复杂性和所需的扩散时间，从而加速了去噪过程。迪夫CAP配备强大的理论和经验支持，为在对抗环境中安全部署VLM提供了稳健且实用的解决方案。



## **4. Prediction Inconsistency Helps Achieve Generalizable Detection of Adversarial Examples**

预测不一致有助于实现对抗性示例的可推广检测 cs.CR

**SubmitDate**: 2025-06-04    [abs](http://arxiv.org/abs/2506.03765v1) [paper-pdf](http://arxiv.org/pdf/2506.03765v1)

**Authors**: Sicong Han, Chenhao Lin, Zhengyu Zhao, Xiyuan Wang, Xinlei He, Qian Li, Cong Wang, Qian Wang, Chao Shen

**Abstract**: Adversarial detection protects models from adversarial attacks by refusing suspicious test samples. However, current detection methods often suffer from weak generalization: their effectiveness tends to degrade significantly when applied to adversarially trained models rather than naturally trained ones, and they generally struggle to achieve consistent effectiveness across both white-box and black-box attack settings. In this work, we observe that an auxiliary model, differing from the primary model in training strategy or model architecture, tends to assign low confidence to the primary model's predictions on adversarial examples (AEs), while preserving high confidence on normal examples (NEs). Based on this discovery, we propose Prediction Inconsistency Detector (PID), a lightweight and generalizable detection framework to distinguish AEs from NEs by capturing the prediction inconsistency between the primal and auxiliary models. PID is compatible with both naturally and adversarially trained primal models and outperforms four detection methods across 3 white-box, 3 black-box, and 1 mixed adversarial attacks. Specifically, PID achieves average AUC scores of 99.29\% and 99.30\% on CIFAR-10 when the primal model is naturally and adversarially trained, respectively, and 98.31% and 96.81% on ImageNet under the same conditions, outperforming existing SOTAs by 4.70%$\sim$25.46%.

摘要: 对抗性检测通过拒绝可疑的测试样本来保护模型免受对抗性攻击。然而，当前的检测方法往往具有弱的概括性：当应用于对抗训练的模型而不是自然训练的模型时，它们的有效性往往会显着下降，而且它们通常很难在白盒和黑匣子攻击设置中实现一致的有效性。在这项工作中，我们观察到，辅助模型在训练策略或模型架构上与主要模型不同，往往会为主要模型对对抗性示例（AE）的预测赋予低置信度，而对正常示例（NE）保持高置信度。基于这一发现，我们提出了预测不一致性检测器（Prediction Inconsistent Detector，简称ID），这是一种轻量级且可推广的检测框架，通过捕获原始模型和辅助模型之间的预测不一致性来区分AE与NE。ID与自然训练和对抗训练的原始模型兼容，并且在3种白盒、3种黑盒和1种混合对抗攻击中优于四种检测方法。具体来说，当原始模型经过自然和对抗训练时，ID在CIFAR-10上的平均AUC分数分别为99.29%和99.30%，在相同条件下在ImageNet上的平均AUC分数为98.31%和96.81%，比现有SOTA高出4.70%$25.46%。



## **5. PRJ: Perception-Retrieval-Judgement for Generated Images**

PRJ：生成图像的感知-检索-判断 cs.CV

**SubmitDate**: 2025-06-04    [abs](http://arxiv.org/abs/2506.03683v1) [paper-pdf](http://arxiv.org/pdf/2506.03683v1)

**Authors**: Qiang Fu, Zonglei Jing, Zonghao Ying, Xiaoqian Li

**Abstract**: The rapid progress of generative AI has enabled remarkable creative capabilities, yet it also raises urgent concerns regarding the safety of AI-generated visual content in real-world applications such as content moderation, platform governance, and digital media regulation. This includes unsafe material such as sexually explicit images, violent scenes, hate symbols, propaganda, and unauthorized imitations of copyrighted artworks. Existing image safety systems often rely on rigid category filters and produce binary outputs, lacking the capacity to interpret context or reason about nuanced, adversarially induced forms of harm. In addition, standard evaluation metrics (e.g., attack success rate) fail to capture the semantic severity and dynamic progression of toxicity. To address these limitations, we propose Perception-Retrieval-Judgement (PRJ), a cognitively inspired framework that models toxicity detection as a structured reasoning process. PRJ follows a three-stage design: it first transforms an image into descriptive language (perception), then retrieves external knowledge related to harm categories and traits (retrieval), and finally evaluates toxicity based on legal or normative rules (judgement). This language-centric structure enables the system to detect both explicit and implicit harms with improved interpretability and categorical granularity. In addition, we introduce a dynamic scoring mechanism based on a contextual toxicity risk matrix to quantify harmfulness across different semantic dimensions. Experiments show that PRJ surpasses existing safety checkers in detection accuracy and robustness while uniquely supporting structured category-level toxicity interpretation.

摘要: 生成性人工智能的快速发展带来了非凡的创意能力，但它也引发了人们对人工智能生成的视觉内容在现实世界应用中的安全性的紧迫担忧，例如内容审核、平台治理和数字媒体监管。这包括不安全的材料，例如露骨的性图像、暴力场景、仇恨符号、宣传以及对受版权保护的艺术品的未经授权的模仿。现有的图像安全系统通常依赖于严格的类别过滤器并产生二进制输出，缺乏解释细微差别、敌对引起的伤害形式的上下文或推理的能力。此外，标准评估指标（例如，攻击成功率）未能捕捉毒性的语义严重性和动态进展。为了解决这些局限性，我们提出了感知-检索-判断（PRJ），这是一个认知启发框架，将毒性检测建模为结构化推理过程。PRJ遵循三阶段设计：首先将图像转化为描述性语言（感知），然后检索与伤害类别和特征相关的外部知识（检索），最后根据法律或规范规则评估毒性（判断）。这种以语言为中心的结构使系统能够以改进的可解释性和分类粒度来检测显式和隐式伤害。此外，我们还引入了基于上下文毒性风险矩阵的动态评分机制，以量化不同语义维度的危害性。实验表明，PRJ在检测准确性和鲁棒性方面超越了现有的安全检查器，同时独特地支持结构化类别级毒性解释。



## **6. VLMs Can Aggregate Scattered Training Patches**

VLM可以聚集分散的训练补丁 cs.CV

**SubmitDate**: 2025-06-04    [abs](http://arxiv.org/abs/2506.03614v1) [paper-pdf](http://arxiv.org/pdf/2506.03614v1)

**Authors**: Zhanhui Zhou, Lingjie Chen, Chao Yang, Chaochao Lu

**Abstract**: One way to mitigate risks in vision-language models (VLMs) is to remove dangerous samples in their training data. However, such data moderation can be easily bypassed when harmful images are split into small, benign-looking patches, scattered across many training samples. VLMs may then learn to piece these fragments together during training and generate harmful responses at inference, either from full images or text references. For instance, if trained on image patches from a bloody scene paired with the descriptions "safe," VLMs may later describe, the full image or a text reference to the scene, as "safe." We define the core ability of VLMs enabling this attack as $\textit{visual stitching}$ -- the ability to integrate visual information spread across multiple training samples that share the same textual descriptions. In our work, we first demonstrate visual stitching abilities in common open-source VLMs on three datasets where each image is labeled with a unique synthetic ID: we split each $(\texttt{image}, \texttt{ID})$ pair into $\{(\texttt{patch}, \texttt{ID})\}$ pairs at different granularity for finetuning, and we find that tuned models can verbalize the correct IDs from full images or text reference. Building on this, we simulate the adversarial data poisoning scenario mentioned above by using patches from dangerous images and replacing IDs with text descriptions like ``safe'' or ``unsafe'', demonstrating how harmful content can evade moderation in patches and later be reconstructed through visual stitching, posing serious VLM safety risks. Code is available at https://github.com/ZHZisZZ/visual-stitching.

摘要: 减轻视觉语言模型（VLM）风险的一种方法是删除训练数据中的危险样本。然而，当有害图像被分成小的、看起来无害的补丁并分散在许多训练样本中时，这种数据审核很容易被绕过。然后，VLM可能会在训练期间学会将这些片段拼凑在一起，并在推断时生成有害反应，无论是从完整图像还是文本引用。例如，如果在来自血腥场景的图像补丁上进行训练，并加上“安全”的描述，VLM稍后可能会将该场景的完整图像或文本引用描述为“安全”。“我们将支持这种攻击的VLM的核心能力定义为$\textit{visual stitching}$ --集成传播在共享相同文本描述的多个训练样本中的视觉信息的能力。在我们的工作中，我们首先在三个数据集上展示了常见开源TLR中的视觉拼接能力，其中每个图像都用唯一的合成ID标记：我们将每个$（\textttt {Image}，\textttt {ID}）$ pair拆分为$\{（\textttt {patch}，\textttt {ID}）\}$pair以不同的粒度进行微调，我们发现优化的模型可以从完整图像或文本引用中口头化正确的ID。在此基础上，我们通过使用来自危险图像的补丁并用“安全”或“不安全”等文本描述替换ID来模拟上述对抗性数据中毒场景，展示了有害内容如何逃避补丁中的审核，然后通过视觉缝合进行重建，从而构成严重的VLM安全风险。代码可在https://github.com/ZHZisZZ/visual-stitching上获得。



## **7. Across Programming Language Silos: A Study on Cross-Lingual Retrieval-augmented Code Generation**

跨编程语言筒仓：跨语言检索增强代码生成研究 cs.SE

**SubmitDate**: 2025-06-04    [abs](http://arxiv.org/abs/2506.03535v1) [paper-pdf](http://arxiv.org/pdf/2506.03535v1)

**Authors**: Qiming Zhu, Jialun Cao, Xuanang Chen, Yaojie Lu, Hongyu Lin, Xianpei Han, Le Sun, Shing-Chi Cheung

**Abstract**: Current research on large language models (LLMs) with retrieval-augmented code generation (RACG) mainly focuses on single-language settings, leaving cross-lingual effectiveness and security unexplored. Multi-lingual RACG systems are valuable for migrating code-bases across programming languages (PLs), yet face risks from error (e.g. adversarial data corruption) propagation in cross-lingual transfer. We construct a dataset spanning 13 PLs with nearly 14k instances to explore utility and robustness of multi-lingual RACG systems. Our investigation reveals four key insights: (1) Effectiveness: multi-lingual RACG significantly enhances multi-lingual code LLMs generation; (2) Inequality: Java demonstrate superior cross-lingual utility over Python in RACG; (3) Robustness: Adversarial attacks degrade performance significantly in mono-lingual RACG but show mitigated impacts in cross-lingual scenarios; Counterintuitively, perturbed code may improve RACG in cross-lingual scenarios; (4) Specialization: Domain-specific code retrievers outperform significantly general text retrievers. These findings establish foundation for developing effective and secure multi-lingual code assistants.

摘要: 目前对具有检索增强代码生成（RACG）的大型语言模型（LLM）的研究主要集中在单语言设置上，而跨语言有效性和安全性尚未得到探索。多语言RACG系统对于跨编程语言（PL）迁移代码库很有价值，但在跨语言传输中面临错误传播（例如对抗性数据损坏）的风险。我们构建了一个跨越13个PL和近14，000个实例的数据集，以探索多语言RACG系统的实用性和稳健性。我们的调查揭示了四个关键见解：（1）有效性：多语言RACG显着增强了多语言代码LLM的生成;（2）不平等性：Java在RACG中表现出优于Python的跨语言实用性;（3）鲁棒性：对抗性攻击在单语言RACG中显着降低性能，但在跨语言场景中显示出减轻的影响;与直觉相反，受干扰的代码可能会在跨语言场景中改进RACG;（4）专业化：领域特定代码检索器的性能明显优于一般文本检索器。这些发现为开发有效且安全的多语言代码助手奠定了基础。



## **8. Secure and Private Federated Learning: Achieving Adversarial Resilience through Robust Aggregation**

安全且私人的联邦学习：通过稳健的聚合实现对抗弹性 cs.LG

**SubmitDate**: 2025-06-03    [abs](http://arxiv.org/abs/2505.17226v2) [paper-pdf](http://arxiv.org/pdf/2505.17226v2)

**Authors**: Kun Yang, Neena Imam

**Abstract**: Federated Learning (FL) enables collaborative machine learning across decentralized data sources without sharing raw data. It offers a promising approach to privacy-preserving AI. However, FL remains vulnerable to adversarial threats from malicious participants, referred to as Byzantine clients, who can send misleading updates to corrupt the global model. Traditional aggregation methods, such as simple averaging, are not robust to such attacks. More resilient approaches, like the Krum algorithm, require prior knowledge of the number of malicious clients, which is often unavailable in real-world scenarios. To address these limitations, we propose Average-rKrum (ArKrum), a novel aggregation strategy designed to enhance both the resilience and privacy guarantees of FL systems. Building on our previous work (rKrum), ArKrum introduces two key innovations. First, it includes a median-based filtering mechanism that removes extreme outliers before estimating the number of adversarial clients. Second, it applies a multi-update averaging scheme to improve stability and performance, particularly when client data distributions are not identical. We evaluate ArKrum on benchmark image and text datasets under three widely studied Byzantine attack types. Results show that ArKrum consistently achieves high accuracy and stability. It performs as well as or better than other robust aggregation methods. These findings demonstrate that ArKrum is an effective and practical solution for secure FL systems in adversarial environments.

摘要: 联合学习（FL）支持跨去中心化数据源的协作机器学习，而无需共享原始数据。它为保护隐私的人工智能提供了一种有希望的方法。然而，FL仍然容易受到来自恶意参与者（称为拜占庭客户）的敌对威胁，这些参与者可以发送误导性更新以破坏全球模型。传统的聚合方法（例如简单平均）对此类攻击并不稳健。更具弹性的方法，例如Krum算法，需要事先了解恶意客户端的数量，而这在现实世界场景中通常是不可用的。为了解决这些限制，我们提出了Average-rKrum（ArKrum），这是一种新型聚合策略，旨在增强FL系统的弹性和隐私保证。在我们之前的工作（rKrum）的基础上，ArKrum引入了两项关键创新。首先，它包括一个基于媒体的过滤机制，可以在估计对抗客户端的数量之前删除极端异常值。其次，它应用多更新平均方案来提高稳定性和性能，特别是当客户端数据分布不相同时。我们在三种广泛研究的拜占庭攻击类型下，在基准图像和文本数据集上评估ArKrum。结果表明ArKrum始终实现高准确性和稳定性。它的性能与其他稳健聚合方法一样好或更好。这些发现表明，ArKrum是对抗环境中安全FL系统的有效且实用的解决方案。



## **9. Addressing Key Challenges of Adversarial Attacks and Defenses in the Tabular Domain: A Methodological Framework for Coherence and Consistency**

应对表格领域对抗性攻击和防御的关键挑战：一致性和一致性的方法论框架 cs.LG

**SubmitDate**: 2025-06-03    [abs](http://arxiv.org/abs/2412.07326v2) [paper-pdf](http://arxiv.org/pdf/2412.07326v2)

**Authors**: Yael Itzhakev, Amit Giloni, Yuval Elovici, Asaf Shabtai

**Abstract**: Machine learning models trained on tabular data are vulnerable to adversarial attacks, even in realistic scenarios where attackers only have access to the model's outputs. Since tabular data contains complex interdependencies among features, it presents a unique challenge for adversarial samples which must maintain coherence and respect these interdependencies to remain indistinguishable from benign data. Moreover, existing attack evaluation metrics-such as the success rate, perturbation magnitude, and query count-fail to account for this challenge. To address those gaps, we propose a technique for perturbing dependent features while preserving sample coherence. In addition, we introduce Class-Specific Anomaly Detection (CSAD), an effective novel anomaly detection approach, along with concrete metrics for assessing the quality of tabular adversarial attacks. CSAD evaluates adversarial samples relative to their predicted class distribution, rather than a broad benign distribution. It ensures that subtle adversarial perturbations, which may appear coherent in other classes, are correctly identified as anomalies. We integrate SHAP explainability techniques to detect inconsistencies in model decision-making, extending CSAD for SHAP-based anomaly detection. Our evaluation incorporates both anomaly detection rates with SHAP-based assessments to provide a more comprehensive measure of adversarial sample quality. We evaluate various attack strategies, examining black-box query-based and transferability-based gradient attacks across four target models. Experiments on benchmark tabular datasets reveal key differences in the attacker's risk and effort and attack quality, offering insights into the strengths, limitations, and trade-offs faced by attackers and defenders. Our findings lay the groundwork for future research on adversarial attacks and defense development in the tabular domain.

摘要: 在表格数据上训练的机器学习模型很容易受到对抗攻击，即使在攻击者只能访问模型输出的现实场景中也是如此。由于表格数据包含特征之间复杂的相互依赖关系，因此它对对抗性样本提出了独特的挑战，这些样本必须保持一致性并尊重这些相互依赖关系，以保持与良性数据之间无法区分。此外，现有的攻击评估指标--例如成功率、扰动幅度和查询计数--无法解决这一挑战。为了解决这些差距，我们提出了一种在保持样本一致性的同时扰动相关特征的技术。此外，我们还引入了类别特定异常检测（CSAD），这是一种有效的新型异常检测方法，以及用于评估表格对抗攻击质量的具体指标。CSAD相对于其预测的类别分布而不是广泛的良性分布来评估敌对样本。它确保细微的对抗性扰动（在其他类别中可能看起来是一致的）被正确识别为异常。我们集成了SHAP可解释性技术来检测模型决策中的不一致性，扩展了CSAD用于基于SHAP的异常检测。我们的评估结合了异常检测率和基于SHAP的评估，以提供对抗性样本质量的更全面的衡量标准。我们评估了各种攻击策略，检查了四个目标模型中基于黑匣子查询和基于可移植性的梯度攻击。对基准表格数据集的实验揭示了攻击者的风险和努力以及攻击质量的关键差异，从而深入了解攻击者和防御者面临的优势、限制和权衡。我们的发现为未来对表格领域的对抗性攻击和防御开发的研究奠定了基础。



## **10. Robustness in Both Domains: CLIP Needs a Robust Text Encoder**

两个领域的稳健性：CLIP需要强大的文本编码器 cs.LG

**SubmitDate**: 2025-06-03    [abs](http://arxiv.org/abs/2506.03355v1) [paper-pdf](http://arxiv.org/pdf/2506.03355v1)

**Authors**: Elias Abad Rocamora, Christian Schlarmann, Naman Deep Singh, Yongtao Wu, Matthias Hein, Volkan Cevher

**Abstract**: Adversarial input attacks can cause a significant shift of CLIP embeddings. This can affect the downstream robustness of models incorporating CLIP in the pipeline, such as text-to-image generative models or large vision language models. While some efforts have been done towards making the CLIP image encoders robust, the robustness of text encoders remains unexplored. In this work, we cover this gap in the literature. We propose LEAF: an efficient adversarial finetuning method for the text domain, with the ability to scale to large CLIP models. Our models significantly improve the zero-shot adversarial accuracy in the text domain, while maintaining the vision performance provided by robust image encoders. When combined with text-to-image diffusion models, we can improve the generation quality under adversarial noise. When employing our robust CLIP encoders in multimodal retrieval tasks, we improve the recall under adversarial noise over standard CLIP models. Finally, we show that robust text encoders facilitate better reconstruction of input text from its embedding via direct optimization.

摘要: 对抗性输入攻击可能会导致CLIP嵌入的显着转变。这可能会影响在管道中纳入CLIP的模型的下游稳健性，例如文本到图像生成模型或大视觉语言模型。虽然已经做出了一些努力来使CLIP图像编码器稳健，但文本编码器的稳健性仍然有待探索。在这项工作中，我们涵盖了文献中的这一空白。我们提出LEAF：一种针对文本领域的高效对抗微调方法，能够扩展到大型CLIP模型。我们的模型显着提高了文本域中的零镜头对抗准确性，同时保持稳健的图像编码器提供的视觉性能。当与文本到图像扩散模型相结合时，我们可以提高对抗性噪音下的生成质量。当在多模式检索任务中使用我们稳健的CLIP编码器时，我们在对抗性噪音下比标准CLIP模型提高了召回率。最后，我们表明稳健的文本编码器可以通过直接优化从嵌入中更好地重建输入文本。



## **11. Adversarial Attacks on Robotic Vision Language Action Models**

对机器人视觉语言动作模型的对抗攻击 cs.RO

**SubmitDate**: 2025-06-03    [abs](http://arxiv.org/abs/2506.03350v1) [paper-pdf](http://arxiv.org/pdf/2506.03350v1)

**Authors**: Eliot Krzysztof Jones, Alexander Robey, Andy Zou, Zachary Ravichandran, George J. Pappas, Hamed Hassani, Matt Fredrikson, J. Zico Kolter

**Abstract**: The emergence of vision-language-action models (VLAs) for end-to-end control is reshaping the field of robotics by enabling the fusion of multimodal sensory inputs at the billion-parameter scale. The capabilities of VLAs stem primarily from their architectures, which are often based on frontier large language models (LLMs). However, LLMs are known to be susceptible to adversarial misuse, and given the significant physical risks inherent to robotics, questions remain regarding the extent to which VLAs inherit these vulnerabilities. Motivated by these concerns, in this work we initiate the study of adversarial attacks on VLA-controlled robots. Our main algorithmic contribution is the adaptation and application of LLM jailbreaking attacks to obtain complete control authority over VLAs. We find that textual attacks, which are applied once at the beginning of a rollout, facilitate full reachability of the action space of commonly used VLAs and often persist over longer horizons. This differs significantly from LLM jailbreaking literature, as attacks in the real world do not have to be semantically linked to notions of harm. We make all code available at https://github.com/eliotjones1/robogcg .

摘要: 用于端到端控制的视觉-语言-动作模型（VLA）的出现正在重塑机器人领域，它能够在十亿参数规模上融合多模态感官输入。VLA的能力主要来自其架构，这些架构通常基于前沿大型语言模型（LLM）。然而，众所周知，LLM容易受到对抗性滥用的影响，并且考虑到机器人固有的重大物理风险，关于VLA继承这些漏洞的程度仍然存在问题。出于这些担忧，在这项工作中，我们开始研究对VLA控制的机器人的对抗性攻击。我们的主要算法的贡献是适应和应用LLM越狱攻击，以获得完全控制权的VLA。我们发现，文本攻击，这是适用于一次在一开始的推出，促进充分的可达性的常用的VLA的动作空间，并经常持续较长的视野。这与LLM越狱文学有很大的不同，因为现实世界中的攻击不必与伤害的概念在语义上联系起来。我们在https://github.com/eliotjones1/robogcg上提供所有代码。



## **12. Mind the Gap: A Practical Attack on GGUF Quantization**

注意差距：对GGUF量化的实用攻击 cs.CR

ICML 2025

**SubmitDate**: 2025-06-03    [abs](http://arxiv.org/abs/2505.23786v3) [paper-pdf](http://arxiv.org/pdf/2505.23786v3)

**Authors**: Kazuki Egashira, Robin Staab, Mark Vero, Jingxuan He, Martin Vechev

**Abstract**: With the increasing size of frontier LLMs, post-training quantization has become the standard for memory-efficient deployment. Recent work has shown that basic rounding-based quantization schemes pose security risks, as they can be exploited to inject malicious behaviors into quantized models that remain hidden in full precision. However, existing attacks cannot be applied to more complex quantization methods, such as the GGUF family used in the popular ollama and llama$.$cpp frameworks. In this work, we address this gap by introducing the first attack on GGUF. Our key insight is that the quantization error -- the difference between the full-precision weights and their (de-)quantized version -- provides sufficient flexibility to construct malicious quantized models that appear benign in full precision. Leveraging this, we develop an attack that trains the target malicious LLM while constraining its weights based on quantization errors. We demonstrate the effectiveness of our attack on three popular LLMs across nine GGUF quantization data types on three diverse attack scenarios: insecure code generation ($\Delta$=$88.7\%$), targeted content injection ($\Delta$=$85.0\%$), and benign instruction refusal ($\Delta$=$30.1\%$). Our attack highlights that (1) the most widely used post-training quantization method is susceptible to adversarial interferences, and (2) the complexity of quantization schemes alone is insufficient as a defense.

摘要: 随着前沿LLM规模的不断增加，训练后量化已成为内存高效部署的标准。最近的工作表明，基本的基于舍入的量化方案会带来安全风险，因为它们可以被利用来将恶意行为注入到保持完全精确隐藏的量化模型中。然而，现有的攻击无法应用于更复杂的量化方法，例如流行的奥拉马和美洲驼中使用的GGUF系列。$ cpp框架。在这项工作中，我们通过引入对GGUF的第一次攻击来解决这一差距。我们的关键见解是，量化误差（全精度权重与其（去）量化版本之间的差异）提供了足够的灵活性来构建在全精度下看起来良性的恶意量化模型。利用这一点，我们开发了一种攻击，可以训练目标恶意LLM，同时根据量化误差限制其权重。我们在三种不同的攻击场景中展示了我们对跨越九种GGUF量化数据类型的三种流行LLM的攻击的有效性：不安全代码生成（$\Delta$=$88.7\%$）、定向内容注入（$\Delta$=$85.0\%$）和良性指令拒绝（$\Delta$=$30.1\%$）。我们的攻击强调了（1）最广泛使用的训练后量化方法容易受到对抗干扰，（2）仅靠量化方案的复杂性不足以作为防御。



## **13. On the Stability of Graph Convolutional Neural Networks: A Probabilistic Perspective**

图卷积神经网络的稳定性：概率的角度 cs.LG

**SubmitDate**: 2025-06-03    [abs](http://arxiv.org/abs/2506.01213v2) [paper-pdf](http://arxiv.org/pdf/2506.01213v2)

**Authors**: Ning Zhang, Henry Kenlay, Li Zhang, Mihai Cucuringu, Xiaowen Dong

**Abstract**: Graph convolutional neural networks (GCNNs) have emerged as powerful tools for analyzing graph-structured data, achieving remarkable success across diverse applications. However, the theoretical understanding of the stability of these models, i.e., their sensitivity to small changes in the graph structure, remains in rather limited settings, hampering the development and deployment of robust and trustworthy models in practice. To fill this gap, we study how perturbations in the graph topology affect GCNN outputs and propose a novel formulation for analyzing model stability. Unlike prior studies that focus only on worst-case perturbations, our distribution-aware formulation characterizes output perturbations across a broad range of input data. This way, our framework enables, for the first time, a probabilistic perspective on the interplay between the statistical properties of the node data and perturbations in the graph topology. We conduct extensive experiments to validate our theoretical findings and demonstrate their benefits over existing baselines, in terms of both representation stability and adversarial attacks on downstream tasks. Our results demonstrate the practical significance of the proposed formulation and highlight the importance of incorporating data distribution into stability analysis.

摘要: 图卷积神经网络（GCNN）已成为分析图结构数据的强大工具，在各种应用中取得了显着的成功。然而，对这些模型稳定性的理论理解，即，它们对图形结构中的微小变化的敏感性仍然在相当有限的环境中，阻碍了在实践中稳健且值得信赖的模型的开发和部署。为了填补这一空白，我们研究了图布局中的扰动如何影响GCNN输出，并提出了一种用于分析模型稳定性的新公式。与以前的研究只关注最坏情况下的扰动不同，我们的分布感知公式描述了广泛的输入数据中的输出扰动。通过这种方式，我们的框架第一次实现了对节点数据的统计特性和图拓扑中的扰动之间相互作用的概率视角。我们进行了大量的实验来验证我们的理论研究结果，并证明它们在表示稳定性和对下游任务的对抗性攻击方面优于现有基线。我们的研究结果表明，所提出的配方的实际意义，并强调将数据分布到稳定性分析的重要性。



## **14. Agnostic Learning under Targeted Poisoning: Optimal Rates and the Role of Randomness**

定向中毒下的不可知学习：最佳比率和随机性的作用 cs.LG

**SubmitDate**: 2025-06-03    [abs](http://arxiv.org/abs/2506.03075v1) [paper-pdf](http://arxiv.org/pdf/2506.03075v1)

**Authors**: Bogdan Chornomaz, Yonatan Koren, Shay Moran, Tom Waknine

**Abstract**: We study the problem of learning in the presence of an adversary that can corrupt an $\eta$ fraction of the training examples with the goal of causing failure on a specific test point. In the realizable setting, prior work established that the optimal error under such instance-targeted poisoning attacks scales as $\Theta(d\eta)$, where $d$ is the VC dimension of the hypothesis class arXiv:2210.02713. In this work, we resolve the corresponding question in the agnostic setting. We show that the optimal excess error is $\tilde{\Theta}(\sqrt{d\eta})$, answering one of the main open problems left by Hanneke et al. To achieve this rate, it is necessary to use randomized learners: Hanneke et al. showed that deterministic learners can be forced to suffer error close to 1, even under small amounts of poisoning. Perhaps surprisingly, our upper bound remains valid even when the learner's random bits are fully visible to the adversary . In the other direction, our lower bound is stronger than standard PAC-style bounds: instead of tailoring a hard distribution separately for each sample size, we exhibit a single fixed distribution under which the adversary can enforce an excess error of $\Omega(\sqrt{d\eta})$ infinitely often.

摘要: 我们研究了在对手存在的情况下的学习问题，对手可能会破坏训练示例的一小部分，目标是在特定测试点上造成失败。在可实现的环境中，先前的工作确定了这种针对实例的中毒攻击下的最佳误差为$\Theta（d\eta）$，其中$d$是假设类arXiv的VC维度：2210.02713。在这项工作中，我们在不可知的环境中解决了相应的问题。我们表明，最佳超额误差是$\tilde{\Theta}（\SQRT{d\eta}）$，回答了Hanneke等人留下的主要未决问题之一。为了实现这一速度，有必要使用随机学习器：Hanneke等人表明，即使在少量中毒的情况下，确定性学习器也可能被迫遭受接近1的误差。也许令人惊讶的是，即使学习者的随机位对对手完全可见，我们的上限仍然有效。在另一个方向上，我们的下限比标准PAC风格的界限更强：我们不是为每个样本量单独定制硬分布，而是展示一个固定分布，在该分布下，对手可以无限频繁地强制执行$\Omega（\SQRT{d\eta}）$的超额误差。



## **15. Adversarial Robustness of AI-Generated Image Detectors in the Real World**

人工智能生成图像检测器在现实世界中的对抗鲁棒性 cs.CV

**SubmitDate**: 2025-06-03    [abs](http://arxiv.org/abs/2410.01574v3) [paper-pdf](http://arxiv.org/pdf/2410.01574v3)

**Authors**: Sina Mavali, Jonas Ricker, David Pape, Asja Fischer, Lea Schönherr

**Abstract**: The rapid advancement of Generative Artificial Intelligence (GenAI) capabilities is accompanied by a concerning rise in its misuse. In particular the generation of credible misinformation in the form of images poses a significant threat to the public trust in democratic processes. Consequently, there is an urgent need to develop tools to reliably distinguish between authentic and AI-generated content. The majority of detection methods are based on neural networks that are trained to recognize forensic artifacts. In this work, we demonstrate that current state-of-the-art classifiers are vulnerable to adversarial examples under real-world conditions. Through extensive experiments, comprising four detection methods and five attack algorithms, we show that an attacker can dramatically decrease classification performance, without internal knowledge of the detector's architecture. Notably, most attacks remain effective even when images are degraded during the upload to, e.g., social media platforms. In a case study, we demonstrate that these robustness challenges are also found in commercial tools by conducting black-box attacks on HIVE, a proprietary online GenAI media detector. In addition, we evaluate the robustness of using generated features of a robust pre-trained model and showed that this increases the robustness, while not reaching the performance on benign inputs. These results, along with the increasing potential of GenAI to erode public trust, underscore the need for more research and new perspectives on methods to prevent its misuse.

摘要: 生成人工智能（GenAI）能力的快速发展伴随着其滥用现象的令人担忧的增加。特别是，以图像形式产生可信的错误信息对公众对民主进程的信任构成了重大威胁。因此，迫切需要开发工具来可靠地区分真实内容和人工智能生成的内容。大多数检测方法都基于经过训练以识别法医文物的神经网络。在这项工作中，我们证明了当前最先进的分类器在现实世界条件下容易受到对抗性示例的影响。通过广泛的实验，包括四种检测方法和五种攻击算法，我们表明，攻击者可以显着降低分类性能，而无需内部知识的检测器的架构。值得注意的是，即使图像在上传期间被降级，大多数攻击仍然有效，例如，社交媒体平台。在案例研究中，我们证明，通过对HIVE（专有在线GenAI媒体检测器）进行黑匣子攻击，商业工具中也存在这些稳健性挑战。此外，我们评估了使用稳健的预训练模型的生成特征的稳健性，并表明这增加了稳健性，同时在良性输入上没有达到性能。这些结果，加上GenAI侵蚀公众信任的潜力越来越大，凸显了对防止其滥用的方法进行更多研究和新观点的必要性。



## **16. BadReward: Clean-Label Poisoning of Reward Models in Text-to-Image RLHF**

BadReward：文本到图像RL HF中奖励模型的无标签中毒 cs.LG

**SubmitDate**: 2025-06-03    [abs](http://arxiv.org/abs/2506.03234v1) [paper-pdf](http://arxiv.org/pdf/2506.03234v1)

**Authors**: Kaiwen Duan, Hongwei Yao, Yufei Chen, Ziyun Li, Tong Qiao, Zhan Qin, Cong Wang

**Abstract**: Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning text-to-image (T2I) models with human preferences. However, RLHF's feedback mechanism also opens new pathways for adversaries. This paper demonstrates the feasibility of hijacking T2I models by poisoning a small fraction of preference data with natural-appearing examples. Specifically, we propose BadReward, a stealthy clean-label poisoning attack targeting the reward model in multi-modal RLHF. BadReward operates by inducing feature collisions between visually contradicted preference data instances, thereby corrupting the reward model and indirectly compromising the T2I model's integrity. Unlike existing alignment poisoning techniques focused on single (text) modality, BadReward is independent of the preference annotation process, enhancing its stealth and practical threat. Extensive experiments on popular T2I models show that BadReward can consistently guide the generation towards improper outputs, such as biased or violent imagery, for targeted concepts. Our findings underscore the amplified threat landscape for RLHF in multi-modal systems, highlighting the urgent need for robust defenses. Disclaimer. This paper contains uncensored toxic content that might be offensive or disturbing to the readers.

摘要: 人类反馈强化学习（RL HF）对于将文本到图像（T2 I）模型与人类偏好保持一致至关重要。然而，RL HF的反馈机制也为对手开辟了新的途径。本文通过用自然出现的示例毒害一小部分偏好数据来证明劫持T2 I模型的可行性。具体来说，我们提出了BadReward，这是一种针对多模式WLHF中奖励模型的隐形清洁标签中毒攻击。BadReward的运作方式是在视觉上矛盾的偏好数据实例之间引发特征冲突，从而破坏奖励模型并间接损害T2 I模型的完整性。与现有的专注于单一（文本）模式的对齐中毒技术不同，BadReward独立于偏好注释过程，增强了其隐形性和实际威胁。对流行的T2 I模型的大量实验表明，BadReward可以始终如一地引导一代人针对目标概念做出不当输出，例如偏见或暴力图像。我们的研究结果强调了多模式系统中RL HF的威胁格局的放大，凸显了对强大防御的迫切需要。免责声明。本文包含未经审查的有毒内容，可能会冒犯或扰乱读者。



## **17. On the Robustness of Tabular Foundation Models: Test-Time Attacks and In-Context Defenses**

关于表格基础模型的鲁棒性：测试时攻击和上下文内防御 cs.LG

**SubmitDate**: 2025-06-03    [abs](http://arxiv.org/abs/2506.02978v1) [paper-pdf](http://arxiv.org/pdf/2506.02978v1)

**Authors**: Mohamed Djilani, Thibault Simonetto, Karim Tit, Florian Tambon, Paul Récamier, Salah Ghamizi, Maxime Cordy, Mike Papadakis

**Abstract**: Recent tabular Foundational Models (FM) such as TabPFN and TabICL, leverage in-context learning to achieve strong performance without gradient updates or fine-tuning. However, their robustness to adversarial manipulation remains largely unexplored. In this work, we present a comprehensive study of the adversarial vulnerabilities of tabular FM, focusing on both their fragility to targeted test-time attacks and their potential misuse as adversarial tools. We show on three benchmarks in finance, cybersecurity and healthcare, that small, structured perturbations to test inputs can significantly degrade prediction accuracy, even when training context remain fixed. Additionally, we demonstrate that tabular FM can be repurposed to generate transferable evasion to conventional models such as random forests and XGBoost, and on a lesser extent to deep tabular models. To improve tabular FM, we formulate the robustification problem as an optimization of the weights (adversarial fine-tuning), or the context (adversarial in-context learning). We introduce an in-context adversarial training strategy that incrementally replaces the context with adversarial perturbed instances, without updating model weights. Our approach improves robustness across multiple tabular benchmarks. Together, these findings position tabular FM as both a target and a source of adversarial threats, highlighting the urgent need for robust training and evaluation practices in this emerging paradigm.

摘要: 最近的表格基础模型（FM），如TabPFN和TabICL，利用上下文学习来实现强大的性能，而无需梯度更新或微调。然而，它们对对抗性操纵的鲁棒性在很大程度上仍未被探索。在这项工作中，我们提出了一个全面的研究表格FM的对抗性漏洞，重点是他们的脆弱性有针对性的测试时攻击和他们的潜在滥用作为对抗性工具。我们在金融、网络安全和医疗保健领域的三个基准上表明，即使训练上下文保持固定，对测试输入的小的结构化扰动也会显着降低预测准确性。此外，我们还证明，表格FM可以重新利用，以生成对随机森林和XGboost等传统模型的可转移规避，并在较小程度上对深度表格模型的可转移规避。为了改进表格FM，我们将稳健化问题制定为权重（对抗性微调）或上下文（对抗性上下文学习）的优化。我们引入了一种上下文内对抗训练策略，该策略用对抗扰动实例增量地替换上下文，而无需更新模型权重。我们的方法提高了多个表格基准的鲁棒性。总之，这些研究结果将表格FM定位为对抗性威胁的目标和来源，突出表明迫切需要在这一新兴范式中进行强有力的培训和评估。



## **18. Average Certified Radius is a Poor Metric for Randomized Smoothing**

平均认证半径对于随机平滑来说是一个较差的指标 cs.LG

**SubmitDate**: 2025-06-03    [abs](http://arxiv.org/abs/2410.06895v3) [paper-pdf](http://arxiv.org/pdf/2410.06895v3)

**Authors**: Chenhao Sun, Yuhao Mao, Mark Niklas Müller, Martin Vechev

**Abstract**: Randomized smoothing (RS) is popular for providing certified robustness guarantees against adversarial attacks. The average certified radius (ACR) has emerged as a widely used metric for tracking progress in RS. However, in this work, for the first time we show that ACR is a poor metric for evaluating robustness guarantees provided by RS. We theoretically prove not only that a trivial classifier can have arbitrarily large ACR, but also that ACR is extremely sensitive to improvements on easy samples. In addition, the comparison using ACR has a strong dependence on the certification budget. Empirically, we confirm that existing training strategies, though improving ACR, reduce the model's robustness on hard samples consistently. To strengthen our findings, we propose strategies, including explicitly discarding hard samples, reweighing the dataset with approximate certified radius, and extreme optimization for easy samples, to replicate the progress in RS training and even achieve the state-of-the-art ACR on CIFAR-10, without training for robustness on the full data distribution. Overall, our results suggest that ACR has introduced a strong undesired bias to the field, and its application should be discontinued in RS. Finally, we suggest using the empirical distribution of $p_A$, the accuracy of the base model on noisy data, as an alternative metric for RS.

摘要: 随机平滑（RS）因提供针对对抗攻击的经过认证的稳健性保证而很受欢迎。平均认证半径（ACN）已成为跟踪RS进展的广泛使用的指标。然而，在这项工作中，我们首次表明，对于评估RS提供的稳健性保证来说，ACN是一个糟糕的指标。我们从理论上证明，一个平凡的分类器不仅可以具有任意大的ACN，而且还证明了ACN对简单样本的改进极其敏感。此外，使用ACN进行的比较对认证预算有很强的依赖性。从经验上看，我们证实现有的训练策略尽管改进了ACN，但仍持续降低了模型对硬样本的鲁棒性。为了加强我们的研究结果，我们提出了策略，包括明确丢弃硬样本、用大约认证半径重新加权数据集以及对简单样本进行极端优化，以复制RS训练的进展，甚至在CIFAR-10上实现最先进的ACN，而无需对完整数据分布进行鲁棒性训练。总体而言，我们的结果表明，ACN给该领域带来了强烈的不希望的偏差，其在RS中的应用应该停止。最后，我们建议使用$p_A$的经验分布（含噪数据基础模型的准确性）作为RS的替代指标。



## **19. PoisonArena: Uncovering Competing Poisoning Attacks in Retrieval-Augmented Generation**

PoisonArena：揭露检索增强一代中的竞争中毒攻击 cs.IR

Project page: https://poison-arena.github.io/

**SubmitDate**: 2025-06-03    [abs](http://arxiv.org/abs/2505.12574v4) [paper-pdf](http://arxiv.org/pdf/2505.12574v4)

**Authors**: Liuji Chen, Xiaofang Yang, Yuanzhuo Lu, Jinghao Zhang, Xin Sun, Qiang Liu, Shu Wu, Jing Dong, Liang Wang

**Abstract**: Retrieval-Augmented Generation (RAG) systems, widely used to improve the factual grounding of large language models (LLMs), are increasingly vulnerable to poisoning attacks, where adversaries inject manipulated content into the retriever's corpus. While prior research has predominantly focused on single-attacker settings, real-world scenarios often involve multiple, competing attackers with conflicting objectives. In this work, we introduce PoisonArena, the first benchmark to systematically study and evaluate competing poisoning attacks in RAG. We formalize the multi-attacker threat model, where attackers vie to control the answer to the same query using mutually exclusive misinformation. PoisonArena leverages the Bradley-Terry model to quantify each method's competitive effectiveness in such adversarial environments. Through extensive experiments on the Natural Questions and MS MARCO datasets, we demonstrate that many attack strategies successful in isolation fail under competitive pressure. Our findings highlight the limitations of conventional evaluation metrics like Attack Success Rate (ASR) and F1 score and underscore the need for competitive evaluation to assess real-world attack robustness. PoisonArena provides a standardized framework to benchmark and develop future attack and defense strategies under more realistic, multi-adversary conditions.

摘要: 检索增强生成（RAG）系统，广泛用于改善大型语言模型（LLM）的事实基础，越来越容易受到中毒攻击，其中对手将操纵的内容注入检索器的语料库。虽然以前的研究主要集中在单个攻击者的设置，但现实世界的场景往往涉及多个相互竞争的攻击者，这些攻击者的目标相互冲突。在这项工作中，我们介绍PoisonArena，第一个基准系统地研究和评估竞争中毒攻击在RAG。我们形式化的多攻击者威胁模型，攻击者争夺控制答案相同的查询使用互斥的错误信息。PoisonArena利用Bradley-Terry模型来量化每种方法在此类对抗环境中的竞争有效性。通过对Natural Questions和MS MARCO数据集的广泛实验，我们证明了许多孤立成功的攻击策略在竞争压力下失败。我们的研究结果强调了攻击成功率（SVR）和F1评分等传统评估指标的局限性，并强调了竞争性评估来评估现实世界攻击稳健性的必要性。PoisonArena提供了一个标准化的框架，可以在更现实的多对手条件下基准和开发未来的攻击和防御策略。



## **20. Privacy Leaks by Adversaries: Adversarial Iterations for Membership Inference Attack**

对手的隐私泄露：会员推断攻击的对抗迭代 cs.CR

**SubmitDate**: 2025-06-03    [abs](http://arxiv.org/abs/2506.02711v1) [paper-pdf](http://arxiv.org/pdf/2506.02711v1)

**Authors**: Jing Xue, Zhishen Sun, Haishan Ye, Luo Luo, Xiangyu Chang, Ivor Tsang, Guang Dai

**Abstract**: Membership inference attack (MIA) has become one of the most widely used and effective methods for evaluating the privacy risks of machine learning models. These attacks aim to determine whether a specific sample is part of the model's training set by analyzing the model's output. While traditional membership inference attacks focus on leveraging the model's posterior output, such as confidence on the target sample, we propose IMIA, a novel attack strategy that utilizes the process of generating adversarial samples to infer membership. We propose to infer the member properties of the target sample using the number of iterations required to generate its adversarial sample. We conduct experiments across multiple models and datasets, and our results demonstrate that the number of iterations for generating an adversarial sample is a reliable feature for membership inference, achieving strong performance both in black-box and white-box attack scenarios. This work provides a new perspective for evaluating model privacy and highlights the potential of adversarial example-based features for privacy leakage assessment.

摘要: 隶属度推理攻击（MIA）已成为评估机器学习模型隐私风险最广泛、最有效的方法之一。这些攻击旨在通过分析模型的输出来确定特定样本是否是模型训练集的一部分。虽然传统的成员资格推断攻击重点是利用模型的后验输出，例如对目标样本的信心，但我们提出了IMIA，这是一种新型攻击策略，利用生成对抗样本的过程来推断成员资格。我们建议使用生成其对抗样本所需的迭代次数来推断目标样本的成员属性。我们跨多个模型和数据集进行了实验，结果表明，生成对抗样本的迭代次数是隶属推理的可靠特征，可以在黑匣子和白盒攻击场景中实现强劲的性能。这项工作为评估模型隐私提供了新的视角，并强调了基于对抗性示例的功能用于隐私泄露评估的潜力。



## **21. Towards the Worst-case Robustness of Large Language Models**

走向大型语言模型的最坏情况稳健性 cs.LG

**SubmitDate**: 2025-06-03    [abs](http://arxiv.org/abs/2501.19040v3) [paper-pdf](http://arxiv.org/pdf/2501.19040v3)

**Authors**: Huanran Chen, Yinpeng Dong, Zeming Wei, Hang Su, Jun Zhu

**Abstract**: Recent studies have revealed the vulnerability of large language models to adversarial attacks, where adversaries craft specific input sequences to induce harmful, violent, private, or incorrect outputs. In this work, we study their worst-case robustness, i.e., whether an adversarial example exists that leads to such undesirable outputs. We upper bound the worst-case robustness using stronger white-box attacks, indicating that most current deterministic defenses achieve nearly 0\% worst-case robustness. We propose a general tight lower bound for randomized smoothing using fractional knapsack solvers or 0-1 knapsack solvers, and using them to bound the worst-case robustness of all stochastic defenses. Based on these solvers, we provide theoretical lower bounds for several previous empirical defenses. For example, we certify the robustness of a specific case, smoothing using a uniform kernel, against \textit{any possible attack} with an average $\ell_0$ perturbation of 2.02 or an average suffix length of 6.41.

摘要: 最近的研究揭示了大型语言模型容易受到对抗攻击，对手会精心设计特定的输入序列来引发有害、暴力、私密或错误的输出。在这项工作中，我们研究了它们的最坏情况稳健性，即是否存在导致此类不良结果的对抗性例子。我们使用更强的白盒攻击来对最坏情况的稳健性进行上限，这表明当前大多数确定性防御实现了近0%的最坏情况的稳健性。我们提出了使用分数背包求解器或0-1背包求解器的随机平滑的一般紧下界，并使用它们来限制所有随机防御的最坏情况稳健性。基于这些求解器，我们为之前的几个经验防御提供了理论下限。例如，我们证明了特定情况的稳健性，使用统一核进行平滑，针对\texttit {任何可能的攻击}，平均$\ell_0 $扰动为2.02或平均后缀长度为6.41。



## **22. Poster: FedBlockParadox -- A Framework for Simulating and Securing Decentralized Federated Learning**

海报：FedBlockPartridge--模拟和保护去中心化联邦学习的框架 cs.CR

International Conference on Detection of Intrusions and Malware, and  Vulnerability Assessment (DIMVA '25), 2025

**SubmitDate**: 2025-06-03    [abs](http://arxiv.org/abs/2506.02679v1) [paper-pdf](http://arxiv.org/pdf/2506.02679v1)

**Authors**: Gabriele Digregorio, Francesco Bleggi, Federico Caroli, Michele Carminati, Stefano Zanero, Stefano Longari

**Abstract**: A significant body of research in decentralized federated learning focuses on combining the privacy-preserving properties of federated learning with the resilience and transparency offered by blockchain-based systems. While these approaches are promising, they often lack flexible tools to evaluate system robustness under adversarial conditions. To fill this gap, we present FedBlockParadox, a modular framework for modeling and evaluating decentralized federated learning systems built on blockchain technologies, with a focus on resilience against a broad spectrum of adversarial attack scenarios. It supports multiple consensus protocols, validation methods, aggregation strategies, and configurable attack models. By enabling controlled experiments, FedBlockParadox provides a valuable resource for researchers developing secure, decentralized learning solutions. The framework is open-source and built to be extensible by the community.

摘要: 去中心化联邦学习的一项重要研究重点是将联邦学习的隐私保护属性与基于区块链的系统提供的弹性和透明度相结合。虽然这些方法很有希望，但它们通常缺乏灵活的工具来评估对抗条件下的系统稳健性。为了填补这一空白，我们提出了FedBlockPartridge，这是一个模块化框架，用于建模和评估基于区块链技术的去中心化联邦学习系统，重点关注针对广泛的对抗性攻击场景的弹性。它支持多种共识协议、验证方法、聚合策略和可配置攻击模型。通过实现受控实验，FedBlockPartridge为开发安全、去中心化学习解决方案的研究人员提供了宝贵的资源。该框架是开源的，并可由社区扩展。



## **23. Tarallo: Evading Behavioral Malware Detectors in the Problem Space**

Tarallo：在问题空间中规避行为恶意软件检测器 cs.CR

**SubmitDate**: 2025-06-03    [abs](http://arxiv.org/abs/2506.02660v1) [paper-pdf](http://arxiv.org/pdf/2506.02660v1)

**Authors**: Gabriele Digregorio, Salvatore Maccarrone, Mario D'Onghia, Luigi Gallo, Michele Carminati, Mario Polino, Stefano Zanero

**Abstract**: Machine learning algorithms can effectively classify malware through dynamic behavior but are susceptible to adversarial attacks. Existing attacks, however, often fail to find an effective solution in both the feature and problem spaces. This issue arises from not addressing the intrinsic nondeterministic nature of malware, namely executing the same sample multiple times may yield significantly different behaviors. Hence, the perturbations computed for a specific behavior may be ineffective for others observed in subsequent executions. In this paper, we show how an attacker can augment their chance of success by leveraging a new and more efficient feature space algorithm for sequential data, which we have named PS-FGSM, and by adopting two problem space strategies specially tailored to address nondeterminism in the problem space. We implement our novel algorithm and attack strategies in Tarallo, an end-to-end adversarial framework that significantly outperforms previous works in both white and black-box scenarios. Our preliminary analysis in a sandboxed environment and against two RNN-based malware detectors, shows that Tarallo achieves a success rate up to 99% on both feature and problem space attacks while significantly minimizing the number of modifications required for misclassification.

摘要: 机器学习算法可以通过动态行为有效地对恶意软件进行分类，但很容易受到对抗性攻击。然而，现有的攻击往往无法在特征和问题空间中找到有效的解决方案。这个问题是由于没有解决恶意软件固有的不确定性，即多次执行同一样本可能会产生显着不同的行为。因此，为特定行为计算的扰动可能对后续执行中观察到的其他行为无效。在本文中，我们展示了攻击者如何通过利用一种新的、更有效的序列数据特征空间算法（我们将其命名为PS-FGSM），并采用两种专门为解决问题空间中的不确定性而定制的问题空间策略来增加他们的成功机会。我们在Tarallo中实现了我们的新颖算法和攻击策略，Tarallo是一种端到端对抗框架，在白盒和黑盒场景中的表现都显着优于之前的作品。我们在沙箱环境中针对两个基于RNN的恶意软件检测器进行的初步分析表明，Tarallo在特征和问题空间攻击方面的成功率高达99%，同时显着最大限度地减少了错误分类所需的修改数量。



## **24. Computational adversarial risk analysis for general security games**

一般安全游戏的计算对抗风险分析 cs.GT

35 pages, 6 tables, 17 figures

**SubmitDate**: 2025-06-03    [abs](http://arxiv.org/abs/2506.02603v1) [paper-pdf](http://arxiv.org/pdf/2506.02603v1)

**Authors**: Jose Manuel Camacho, Roi Naveiro, David Rios Insua

**Abstract**: This paper provides an efficient computational scheme to handle general security games from an adversarial risk analysis perspective. Two cases in relation to single-stage and multi-stage simultaneous defend-attack games motivate our approach to general setups which uses bi-agent influence diagrams as underlying problem structure and augmented probability simulation as core computational methodology. Theoretical convergence and numerical, modeling, and implementation issues are thoroughly discussed. A disinformation war case study illustrates the relevance of the proposed approach.

摘要: 本文提供了一种有效的计算方案来从对抗风险分析的角度处理一般安全游戏。与单阶段和多阶段同时攻击游戏相关的两个案例激励了我们对一般设置的方法，该方法使用双智能体影响图作为基础问题结构，并使用增强概率模拟作为核心计算方法。深入讨论了理论收敛以及数值、建模和实现问题。虚假信息战争案例研究说明了所提出方法的相关性。



## **25. BitBypass: A New Direction in Jailbreaking Aligned Large Language Models with Bitstream Camouflage**

BitBypass：通过Bitstream伪装越狱对齐大型语言模型的新方向 cs.CR

24 pages, 24 figures, and 7 tables

**SubmitDate**: 2025-06-03    [abs](http://arxiv.org/abs/2506.02479v1) [paper-pdf](http://arxiv.org/pdf/2506.02479v1)

**Authors**: Kalyan Nakka, Nitesh Saxena

**Abstract**: The inherent risk of generating harmful and unsafe content by Large Language Models (LLMs), has highlighted the need for their safety alignment. Various techniques like supervised fine-tuning, reinforcement learning from human feedback, and red-teaming were developed for ensuring the safety alignment of LLMs. However, the robustness of these aligned LLMs is always challenged by adversarial attacks that exploit unexplored and underlying vulnerabilities of the safety alignment. In this paper, we develop a novel black-box jailbreak attack, called BitBypass, that leverages hyphen-separated bitstream camouflage for jailbreaking aligned LLMs. This represents a new direction in jailbreaking by exploiting fundamental information representation of data as continuous bits, rather than leveraging prompt engineering or adversarial manipulations. Our evaluation of five state-of-the-art LLMs, namely GPT-4o, Gemini 1.5, Claude 3.5, Llama 3.1, and Mixtral, in adversarial perspective, revealed the capabilities of BitBypass in bypassing their safety alignment and tricking them into generating harmful and unsafe content. Further, we observed that BitBypass outperforms several state-of-the-art jailbreak attacks in terms of stealthiness and attack success. Overall, these results highlights the effectiveness and efficiency of BitBypass in jailbreaking these state-of-the-art LLMs.

摘要: 大型语言模型（LLM）生成有害和不安全内容的固有风险凸显了对其安全性进行调整的必要性。开发了监督式微调、来自人类反馈的强化学习和红色团队等各种技术，以确保LLM的安全对齐。然而，这些对齐的LLM的稳健性总是受到利用安全对齐未探索的潜在漏洞的对抗攻击的挑战。在本文中，我们开发了一种新型的黑匣子越狱攻击，称为BitBypass，它利用连字符分离的比特流伪装来越狱对齐的LLM。这代表了越狱的一个新方向，它利用数据的基本信息表示为连续比特，而不是利用即时工程或对抗性操纵。我们从对抗的角度对五种最先进的LLM（即GPT-4 o、Gemini 1.5、Claude 3.5、Llama 3.1和Mixtral）进行了评估，揭示了BitBypass绕过其安全对齐并诱骗其生成有害和不安全内容的能力。此外，我们观察到BitBypass在隐蔽性和攻击成功率方面优于几种最先进的越狱攻击。总体而言，这些结果凸显了BitBypass在越狱这些最先进的LLM方面的有效性和效率。



## **26. Adversarial control of synchronization in complex oscillator networks**

复杂振荡器网络中同步的对抗控制 nlin.AO

9 pages, 4 figures

**SubmitDate**: 2025-06-03    [abs](http://arxiv.org/abs/2506.02403v1) [paper-pdf](http://arxiv.org/pdf/2506.02403v1)

**Authors**: Yasutoshi Nagahama, Kosuke Miyazato, Kazuhiro Takemoto

**Abstract**: This study investigates adversarial attacks, a concept from deep learning, designed to control synchronization dynamics through strategically crafted minimal perturbations. We propose a gradient-based optimization method that identifies small phase perturbations to dramatically enhance or suppress collective synchronization in Kuramoto oscillator networks. Our approach formulates synchronization control as an adversarial optimization problem, computing gradients of the order parameter with respect to oscillator phases to determine optimal perturbation directions. Results demonstrate that extremely small phase perturbations applied to network oscillators can achieve significant synchronization control across diverse network architectures. Our analysis reveals that synchronization enhancement is achievable across various network sizes, while synchronization suppression becomes particularly effective in larger networks, with effectiveness scaling favorably with network size. The method is systematically validated on canonical model networks including scale-free and small-world topologies, and real-world networks representing power grids and brain connectivity patterns. This adversarial framework represents a novel paradigm for synchronization management by introducing deep learning concepts to networked dynamical systems.

摘要: 这项研究调查了对抗性攻击，这是一个来自深度学习的概念，旨在通过策略性地设计的最小扰动来控制同步动态。我们提出了一种基于梯度的优化方法，该方法识别小的相扰动，以显着增强或抑制仓本振荡器网络中的集体同步。我们的方法将同步控制制定为一个对抗优化问题，计算阶参数相对于振荡器相的梯度以确定最佳扰动方向。结果表明，应用于网络振荡器的极小的相扰动可以实现跨不同网络架构的显着同步控制。我们的分析表明，同步增强可以在各种网络规模上实现，而同步抑制在较大的网络中变得特别有效，其有效性随着网络规模的增加而增加。该方法在规范模型网络（包括无标度和小世界拓扑）以及代表电网和大脑连接模式的真实网络上进行了系统验证。这种对抗性框架通过将深度学习概念引入网络动态系统，代表了同步管理的新范式。



## **27. Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language Models**

揭示一致大型语言模型内在的道德脆弱性 cs.CL

**SubmitDate**: 2025-06-03    [abs](http://arxiv.org/abs/2504.05050v4) [paper-pdf](http://arxiv.org/pdf/2504.05050v4)

**Authors**: Jiawei Lian, Jianhong Pan, Lefan Wang, Yi Wang, Shaohui Mei, Lap-Pui Chau

**Abstract**: Large language models (LLMs) are foundational explorations to artificial general intelligence, yet their alignment with human values via instruction tuning and preference learning achieves only superficial compliance. Here, we demonstrate that harmful knowledge embedded during pretraining persists as indelible "dark patterns" in LLMs' parametric memory, evading alignment safeguards and resurfacing under adversarial inducement at distributional shifts. In this study, we first theoretically analyze the intrinsic ethical vulnerability of aligned LLMs by proving that current alignment methods yield only local "safety regions" in the knowledge manifold. In contrast, pretrained knowledge remains globally connected to harmful concepts via high-likelihood adversarial trajectories. Building on this theoretical insight, we empirically validate our findings by employing semantic coherence inducement under distributional shifts--a method that systematically bypasses alignment constraints through optimized adversarial prompts. This combined theoretical and empirical approach achieves a 100% attack success rate across 19 out of 23 state-of-the-art aligned LLMs, including DeepSeek-R1 and LLaMA-3, revealing their universal vulnerabilities.

摘要: 大型语言模型（LLM）是人工通用智能的基础探索，但它们通过指令调整和偏好学习与人类价值观的一致只能实现表面的合规性。在这里，我们证明，预训练期间嵌入的有害知识在LLM参数记忆中作为不可磨灭的“黑暗模式”持续存在，逃避对齐保障措施，并在分布变化时的对抗诱导下重新浮出水面。在这项研究中，我们首先通过证明当前的对齐方法只产生知识集合中的局部“安全区域”来从理论上分析对齐LLM的内在道德脆弱性。相比之下，预先训练的知识仍然通过高可能性的对抗轨迹与有害概念保持全球联系。基于这一理论见解，我们通过在分布转移下采用语义一致诱导来从经验上验证我们的发现--一种通过优化的对抗提示系统性地绕过对齐约束的方法。这种理论和经验相结合的方法在23个最先进的对齐LLM中的19个（包括DeepSeek-R1和LLaMA-3）上实现了100%的攻击成功率，揭示了它们的普遍漏洞。



## **28. Ranking Free RAG: Replacing Re-ranking with Selection in RAG for Sensitive Domains**

免费RAG排名：在RAG中用敏感领域的选择取代重新排名 cs.CL

**SubmitDate**: 2025-06-03    [abs](http://arxiv.org/abs/2505.16014v3) [paper-pdf](http://arxiv.org/pdf/2505.16014v3)

**Authors**: Yash Saxena, Ankur Padia, Mandar S Chaudhary, Kalpa Gunaratna, Srinivasan Parthasarathy, Manas Gaur

**Abstract**: Traditional Retrieval-Augmented Generation (RAG) pipelines rely on similarity-based retrieval and re-ranking, which depend on heuristics such as top-k, and lack explainability, interpretability, and robustness against adversarial content. To address this gap, we propose a novel method METEORA that replaces re-ranking in RAG with a rationale-driven selection approach. METEORA operates in two stages. First, a general-purpose LLM is preference-tuned to generate rationales conditioned on the input query using direct preference optimization. These rationales guide the evidence chunk selection engine, which selects relevant chunks in three stages: pairing individual rationales with corresponding retrieved chunks for local relevance, global selection with elbow detection for adaptive cutoff, and context expansion via neighboring chunks. This process eliminates the need for top-k heuristics. The rationales are also used for consistency check using a Verifier LLM to detect and filter poisoned or misleading content for safe generation. The framework provides explainable and interpretable evidence flow by using rationales consistently across both selection and verification. Our evaluation across six datasets spanning legal, financial, and academic research domains shows that METEORA improves generation accuracy by 33.34% while using approximately 50% fewer chunks than state-of-the-art re-ranking methods. In adversarial settings, METEORA significantly improves the F1 score from 0.10 to 0.44 over the state-of-the-art perplexity-based defense baseline, demonstrating strong resilience to poisoning attacks. Code available at: https://anonymous.4open.science/r/METEORA-DC46/README.md

摘要: 传统的检索增强生成（RAG）管道依赖于基于相似性的检索和重新排序，这依赖于top-k等启发式方法，并且缺乏可解释性、可解释性和针对对抗性内容的鲁棒性。为了解决这一差距，我们提出了一种新颖的方法METEORA，用理性驱动的选择方法取代RAG中的重新排名。METEORA分两个阶段运营。首先，使用直接偏好优化，对通用LLM进行偏好调整，以生成以输入查询为条件的基本原理。这些原理指导证据块选择引擎，该引擎分三个阶段选择相关块：将个体原理与相应的检索到的块配对以获得局部相关性、通过肘部检测进行全局选择以获得自适应截止，以及通过邻近块进行上下文扩展。此过程消除了对top-k启发式的需要。这些原理还用于使用Verification LLM进行一致性检查，以检测和过滤有毒或误导性内容，以安全生成。该框架通过在选择和验证中一致使用原理来提供可解释和可解释的证据流。我们对涵盖法律、金融和学术研究领域的六个数据集的评估显示，METEORA将生成准确性提高了33.34%，同时比最先进的重新排序方法少使用约50%的块。在对抗环境中，METEORA将F1评分从0.10显着提高到0.44，比最先进的基于困惑的防御基线，表现出对中毒攻击的强大韧性。代码可访问：https://anonymous.4open.science/r/METEORA-DC46/README.md



## **29. Fingerprinting Deep Learning Models via Network Traffic Patterns in Federated Learning**

通过联邦学习中的网络流量模式对深度学习模型进行指纹识别 cs.LG

7 pages, 4 Figures, Accepted to publish in Proceedings of the 2025  ACM Workshop on Wireless Security and Machine Learning (WiseML 2025), July 3,  2025, Arlington, VA, USA

**SubmitDate**: 2025-06-02    [abs](http://arxiv.org/abs/2506.03207v1) [paper-pdf](http://arxiv.org/pdf/2506.03207v1)

**Authors**: Md Nahid Hasan Shuvo, Moinul Hossain

**Abstract**: Federated Learning (FL) is increasingly adopted as a decentralized machine learning paradigm due to its capability to preserve data privacy by training models without centralizing user data. However, FL is susceptible to indirect privacy breaches via network traffic analysis-an area not explored in existing research. The primary objective of this research is to study the feasibility of fingerprinting deep learning models deployed within FL environments by analyzing their network-layer traffic information. In this paper, we conduct an experimental evaluation using various deep learning architectures (i.e., CNN, RNN) within a federated learning testbed. We utilize machine learning algorithms, including Support Vector Machines (SVM), Random Forest, and Gradient-Boosting, to fingerprint unique patterns within the traffic data. Our experiments show high fingerprinting accuracy, achieving 100% accuracy using Random Forest and around 95.7% accuracy using SVM and Gradient Boosting classifiers. This analysis suggests that we can identify specific architectures running within the subsection of the network traffic. Hence, if an adversary knows about the underlying DL architecture, they can exploit that information and conduct targeted attacks. These findings suggest a notable security vulnerability in FL systems and the necessity of strengthening it at the network level.

摘要: 联邦学习（FL）越来越多地被采用作为一种分散的机器学习范式，因为它能够通过训练模型来保护数据隐私，而无需集中用户数据。然而，FL是容易受到间接的隐私泄露，通过网络流量分析，在现有的研究中没有探索的领域。本研究的主要目标是通过分析网络层流量信息，研究在FL环境中部署指纹深度学习模型的可行性。在本文中，我们使用各种深度学习架构（即，CNN、RNN）在联邦学习测试床上。我们利用机器学习算法，包括支持向量机（ASM）、随机森林和Attent-Boosting，来识别流量数据中的独特模式。我们的实验显示出指纹识别准确性很高，使用随机森林可实现100%的准确性，使用支持者和梯度增强分类器可实现约95.7%的准确性。该分析表明我们可以识别在网络流量子部分内运行的特定架构。因此，如果对手了解底层DL架构，他们就可以利用该信息并进行有针对性的攻击。这些发现表明FL系统中存在明显的安全漏洞，并且有必要在网络层面加强它。



## **30. AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs**

顾问：LLM的快速自适应对抗预算 cs.CR

Accepted to ICML 2025. Code is available at  http://github.com/facebookresearch/advprompter

**SubmitDate**: 2025-06-02    [abs](http://arxiv.org/abs/2404.16873v2) [paper-pdf](http://arxiv.org/pdf/2404.16873v2)

**Authors**: Anselm Paulus, Arman Zharmagambetov, Chuan Guo, Brandon Amos, Yuandong Tian

**Abstract**: Large Language Models (LLMs) are vulnerable to jailbreaking attacks that lead to generation of inappropriate or harmful content. Manual red-teaming requires a time-consuming search for adversarial prompts, whereas automatic adversarial prompt generation often leads to semantically meaningless attacks that do not scale well. In this paper, we present a novel method that uses another LLM, called AdvPrompter, to generate human-readable adversarial prompts in seconds. AdvPrompter, which is trained using an alternating optimization algorithm, generates suffixes that veil the input instruction without changing its meaning, such that the TargetLLM is lured to give a harmful response. Experimental results on popular open source TargetLLMs show highly competitive results on the AdvBench and HarmBench datasets, that also transfer to closed-source black-box LLMs. We also show that training on adversarial suffixes generated by AdvPrompter is a promising strategy for improving the robustness of LLMs to jailbreaking attacks.

摘要: 大型语言模型（LLM）很容易受到越狱攻击，从而导致生成不适当或有害的内容。手动红组需要对对抗提示进行耗时的搜索，而自动对抗提示生成通常会导致语义上毫无意义的攻击，并且无法很好地扩展。在本文中，我们提出了一种新颖的方法，该方法使用另一种名为Advancer的LLM来在几秒钟内生成人类可读的对抗提示。Advancer使用交替优化算法进行训练，它会生成掩盖输入指令而不改变其含义的后缀，从而引诱Target LLM给出有害响应。流行的开源Target LLM上的实验结果显示，AdvBench和HarmBench数据集具有高度竞争力的结果，这些结果也转移到封闭源黑匣子LLM。我们还表明，对Advencer生成的敌对后缀进行训练是一种有希望的策略，可以提高LLM对越狱攻击的稳健性。



## **31. Adversarial Inception Backdoor Attacks against Reinforcement Learning**

强化学习的对抗性Inception后门攻击 cs.LG

9 pages, 6 figures, ICML 2025

**SubmitDate**: 2025-06-02    [abs](http://arxiv.org/abs/2410.13995v3) [paper-pdf](http://arxiv.org/pdf/2410.13995v3)

**Authors**: Ethan Rathbun, Alina Oprea, Christopher Amato

**Abstract**: Recent works have demonstrated the vulnerability of Deep Reinforcement Learning (DRL) algorithms against training-time, backdoor poisoning attacks. The objectives of these attacks are twofold: induce pre-determined, adversarial behavior in the agent upon observing a fixed trigger during deployment while allowing the agent to solve its intended task during training. Prior attacks assume arbitrary control over the agent's rewards, inducing values far outside the environment's natural constraints. This results in brittle attacks that fail once the proper reward constraints are enforced. Thus, in this work we propose a new class of backdoor attacks against DRL which are the first to achieve state of the art performance under strict reward constraints. These "inception" attacks manipulate the agent's training data -- inserting the trigger into prior observations and replacing high return actions with those of the targeted adversarial behavior. We formally define these attacks and prove they achieve both adversarial objectives against arbitrary Markov Decision Processes (MDP). Using this framework we devise an online inception attack which achieves an 100\% attack success rate on multiple environments under constrained rewards while minimally impacting the agent's task performance.

摘要: 最近的工作证明了深度强化学习（DRL）算法对训练时后门中毒攻击的脆弱性。这些攻击的目的有双重：在部署期间观察到固定触发时，在代理中诱导预定的对抗行为，同时允许代理在训练期间解决其预期任务。先前的攻击假设对代理的奖励进行任意控制，从而产生远远超出环境自然限制的价值。这会导致脆弱攻击，一旦实施适当的奖励约束，就会失败。因此，在这项工作中，我们提出了一类针对DRL的新型后门攻击，这是第一个在严格奖励限制下实现最先进性能的攻击。这些“初始”攻击操纵代理的训练数据--将触发器插入到之前的观察中，并用目标对抗行为的行为替换高回报动作。我们正式定义了这些攻击，并证明它们实现了针对任意Markov决策过程（MDP）的两个对抗目标。使用这个框架，我们设计了一种在线初始攻击，该攻击在受约束的奖励下在多个环境上实现了100%的攻击成功率，同时对代理的任务性能的影响最小。



## **32. Mitigating Data Poisoning Attacks to Local Differential Privacy**

缓解对本地差异隐私的数据中毒攻击 cs.CR

The 32nd ACM Conference on Computer and Communications Security (CCS  2025)

**SubmitDate**: 2025-06-02    [abs](http://arxiv.org/abs/2506.02156v1) [paper-pdf](http://arxiv.org/pdf/2506.02156v1)

**Authors**: Xiaolin Li, Ninghui Li, Boyang Wang, Wenhai Sun

**Abstract**: The distributed nature of local differential privacy (LDP) invites data poisoning attacks and poses unforeseen threats to the underlying LDP-supported applications. In this paper, we propose a comprehensive mitigation framework for popular frequency estimation, which contains a suite of novel defenses, including malicious user detection, attack pattern recognition, and damaged utility recovery. In addition to existing attacks, we explore new adaptive adversarial activities for our mitigation design. For detection, we present a new method to precisely identify bogus reports and thus LDP aggregation can be performed over the ``clean'' data. When the attack behavior becomes stealthy and direct filtering out malicious users is difficult, we further propose a detection that can effectively recognize hidden adversarial patterns, thus facilitating the decision-making of service providers. These detection methods require no additional data and attack information and incur minimal computational cost. Our experiment demonstrates their excellent performance and substantial improvement over previous work in various settings. In addition, we conduct an empirical analysis of LDP post-processing for corrupted data recovery and propose a new post-processing method, through which we reveal new insights into protocol recommendations in practice and key design principles for future research.

摘要: 局部差异隐私（LDP）的分布式特性会引发数据中毒攻击，并对底层LDP支持的应用程序构成不可预见的威胁。在本文中，我们提出了一个全面的缓解框架，流行的频率估计，其中包含一套新颖的防御，包括恶意用户检测，攻击模式识别和损坏的实用程序恢复。除了现有的攻击，我们还为我们的缓解设计探索了新的自适应对抗活动。对于检测，我们提出了一种新的方法来精确地识别虚假的报告，从而LDP聚合可以在“干净”的数据。当攻击行为变得隐蔽且难以直接过滤恶意用户时，我们进一步提出一种可以有效识别隐藏的对抗模式的检测方法，从而方便服务提供商的决策。这些检测方法不需要额外的数据和攻击信息，并且计算成本最低。我们的实验证明了它们在各种环境下的出色性能和与之前的工作相比的显着改进。此外，我们还对损坏数据恢复的SDP后处理进行了实证分析，并提出了一种新的后处理方法，通过该方法我们揭示了对实践中协议建议的新见解以及未来研究的关键设计原则。



## **33. ReconXF: Graph Reconstruction Attack via Public Feature Explanations on Privatized Node Features and Labels**

ReconXF：通过对私有化节点特征和标签的公共特征描述进行图形重建攻击 cs.LG

Under review

**SubmitDate**: 2025-06-02    [abs](http://arxiv.org/abs/2506.02134v1) [paper-pdf](http://arxiv.org/pdf/2506.02134v1)

**Authors**: Rishi Raj Sahoo, Rucha Bhalchandra Joshi, Subhankar Mishra

**Abstract**: Graph Neural Networks (GNNs) achieve high performance across many applications but function as black-box models, limiting their use in critical domains like healthcare and criminal justice. Explainability methods address this by providing feature-level explanations that identify important node attributes for predictions. These explanations create privacy risks. Combined with auxiliary information, feature explanations can enable adversaries to reconstruct graph structure, exposing sensitive relationships. Existing graph reconstruction attacks assume access to original auxiliary data, but practical systems use differential privacy to protect node features and labels while providing explanations for transparency. We study a threat model where adversaries access public feature explanations along with privatized node features and labels. We show that existing explanation-based attacks like GSEF perform poorly with privatized data due to noise from differential privacy mechanisms. We propose ReconXF, a graph reconstruction attack for scenarios with public explanations and privatized auxiliary data. Our method adapts explanation-based frameworks by incorporating denoising mechanisms that handle differential privacy noise while exploiting structural signals in explanations. Experiments across multiple datasets show ReconXF outperforms SoTA methods in privatized settings, with improvements in AUC and average precision. Results indicate that public explanations combined with denoising enable graph structure recovery even under the privacy protection of auxiliary data. Code is available at (link to be made public after acceptance).

摘要: 图形神经网络（GNN）在许多应用程序中实现了高性能，但充当黑匣子模型，限制了其在医疗保健和刑事司法等关键领域的使用。可解释性方法通过提供识别用于预测的重要节点属性的特征级解释来解决这个问题。这些解释会带来隐私风险。与辅助信息相结合，特征解释可以使对手能够重建图结构，暴露敏感关系。现有的图重建攻击假设访问原始辅助数据，但实际系统使用差分隐私来保护节点特征和标签，同时提供透明性的解释。我们研究了一个威胁模型，其中对手访问公共功能的解释以及私有化的节点功能和标签。我们发现，现有的基于加密的攻击，如GSEF执行不佳的私有化数据，由于噪音从不同的隐私机制。我们提出了ReconXF，一个图形重建攻击方案与公共解释和私有化的辅助数据。我们的方法通过结合处理差异隐私噪音的去噪机制，同时在解释中利用结构信号来适应基于解释的框架。多个数据集的实验表明，ReconXF在私有化环境中的表现优于SoTA方法，并在AUR和平均精度方面有所改善。结果表明，即使在辅助数据的隐私保护下，公开解释与去噪相结合也可以实现图结构恢复。代码可在（接受后将公开链接）获取。



## **34. Investigating Privacy Leakage in Dimensionality Reduction Methods via Reconstruction Attack**

通过重建攻击调查虚拟性减少方法中的隐私泄露 cs.CR

Journal of Information Security and Applications, 2025, vol. 92, pp.  104102

**SubmitDate**: 2025-06-02    [abs](http://arxiv.org/abs/2408.17151v3) [paper-pdf](http://arxiv.org/pdf/2408.17151v3)

**Authors**: Chayadon Lumbut, Donlapark Ponnoprat

**Abstract**: This study investigates privacy leakage in dimensionality reduction methods through a novel machine learning-based reconstruction attack. Employing an informed adversary threat model, we develop a neural network capable of reconstructing high-dimensional data from low-dimensional embeddings.   We evaluate six popular dimensionality reduction techniques: principal component analysis (PCA), sparse random projection (SRP), multidimensional scaling (MDS), Isomap, $t$-distributed stochastic neighbor embedding ($t$-SNE), and uniform manifold approximation and projection (UMAP). Using both MNIST and NIH Chest X-ray datasets, we perform a qualitative analysis to identify key factors affecting reconstruction quality. Furthermore, we assess the effectiveness of an additive noise mechanism in mitigating these reconstruction attacks. Our experimental results on both datasets reveal that the attack is effective against deterministic methods (PCA and Isomap). but ineffective against methods that employ random initialization (SRP, MDS, $t$-SNE and UMAP). The experimental results also show that, for PCA and Isomap, our reconstruction network produces higher quality outputs compared to a previously proposed network.   We also study the effect of additive noise mechanism to prevent the reconstruction attack. Our experiment shows that, when adding the images with large noises before performing PCA or Isomap, the attack produced severely distorted reconstructions. In contrast, for the other four methods, the reconstructions still show some recognizable features, though they bear little resemblance to the original images. The code is available at https://github.com/Chayadon/Reconstruction_attack_on_DR

摘要: 本研究通过一种新的基于机器学习的重构攻击来研究降维方法中的隐私泄露问题。采用一个知情的对手威胁模型，我们开发了一个神经网络能够重建高维数据从低维嵌入。   我们评估六种流行的降维技术：主成分分析（PCA），稀疏随机投影（SRP），多维缩放（MDS），Isomap，$t$-分布随机邻居嵌入（$t$-SNE），和均匀流形近似和投影（UMAP）。使用MNIST和NIH胸部X射线数据集，我们进行了定性分析，以确定影响重建质量的关键因素。此外，我们评估了添加性噪音机制在减轻这些重建攻击方面的有效性。我们对这两个数据集的实验结果表明，该攻击对确定性方法（PCA和Isomap）有效。但对采用随机初始化的方法（SPP、BDS、$t$-SNE和UMAP）无效。实验结果还表明，对于PCA和Isomap，与之前提出的网络相比，我们的重建网络产生了更高质量的输出。   我们还研究了添加性噪音机制对防止重建攻击的影响。我们的实验表明，当在执行PCA或Isomap之前添加具有大噪音的图像时，攻击会产生严重失真的重建。相比之下，对于其他四种方法，重建仍然显示出一些可识别的特征，尽管它们与原始图像几乎没有相似之处。该代码可在https://github.com/Chayadon/Reconstruction_attack_on_DR上获取



## **35. Certified Robustness to Clean-Label Poisoning Using Diffusion Denoising**

使用扩散降噪验证对清洁标签中毒的稳健性 cs.CR

**SubmitDate**: 2025-06-02    [abs](http://arxiv.org/abs/2403.11981v2) [paper-pdf](http://arxiv.org/pdf/2403.11981v2)

**Authors**: Sanghyun Hong, Nicholas Carlini, Alexey Kurakin

**Abstract**: We present a certified defense to clean-label poisoning attacks under $\ell_2$-norm. These attacks work by injecting a small number of poisoning samples (e.g., 1%) that contain bounded adversarial perturbations into the training data to induce a targeted misclassification of a test-time input. Inspired by the adversarial robustness achieved by $randomized$ $smoothing$, we show how an off-the-shelf diffusion denoising model can sanitize the tampered training data. We extensively test our defense against seven clean-label poisoning attacks in both $\ell_2$ and $\ell_{\infty}$-norms and reduce their attack success to 0-16% with only a negligible drop in the test accuracy. We compare our defense with existing countermeasures against clean-label poisoning, showing that the defense reduces the attack success the most and offers the best model utility. Our results highlight the need for future work on developing stronger clean-label attacks and using our certified yet practical defense as a strong baseline to evaluate these attacks.

摘要: 我们在$\ell_2 $-norm下为干净标签中毒攻击提供了一种经过认证的防御。这些攻击通过注入少量中毒样本（例如，1%），其中包含训练数据中的有界对抗性扰动，以引发测试时输入的有针对性的错误分类。受$randomed $ $smooth $实现的对抗鲁棒性的启发，我们展示了现成的扩散去噪模型如何净化被篡改的训练数据。我们在$\ell_2 $和$\ell_{\infty}$-规范中广泛测试了我们对七种干净标签中毒攻击的防御，并将其攻击成功率降低到0-16%，测试准确性的下降微乎其微。我们将我们的防御与针对干净标签中毒的现有对策进行了比较，表明防御最能降低攻击成功率，并提供最好的模型效用。我们的结果凸显了未来需要开发更强的干净标签攻击，并使用我们经过认证但实用的防御作为评估这些攻击的强有力基线。



## **36. Robust Satisficing Gaussian Process Bandits Under Adversarial Attacks**

对抗攻击下的鲁棒满足高斯过程盗贼 cs.LG

**SubmitDate**: 2025-06-02    [abs](http://arxiv.org/abs/2506.01625v1) [paper-pdf](http://arxiv.org/pdf/2506.01625v1)

**Authors**: Artun Saday, Yaşar Cahit Yıldırım, Cem Tekin

**Abstract**: We address the problem of Gaussian Process (GP) optimization in the presence of unknown and potentially varying adversarial perturbations. Unlike traditional robust optimization approaches that focus on maximizing performance under worst-case scenarios, we consider a robust satisficing objective, where the goal is to consistently achieve a predefined performance threshold $\tau$, even under adversarial conditions. We propose two novel algorithms based on distinct formulations of robust satisficing, and show that they are instances of a general robust satisficing framework. Further, each algorithm offers different guarantees depending on the nature of the adversary. Specifically, we derive two regret bounds: one that is sublinear over time, assuming certain conditions on the adversary and the satisficing threshold $\tau$, and another that scales with the perturbation magnitude but requires no assumptions on the adversary. Through extensive experiments, we demonstrate that our approach outperforms the established robust optimization methods in achieving the satisficing objective, particularly when the ambiguity set of the robust optimization framework is inaccurately specified.

摘要: 我们解决了存在未知且潜在变化的对抗扰动的高斯过程（GP）优化问题。与专注于在最坏情况下最大化性能的传统稳健优化方法不同，我们考虑了稳健的满足目标，其目标是一致地实现预定义的性能阈值$\tau$，即使在对抗条件下也是如此。我们提出了两种基于不同鲁棒满足公式的新型算法，并表明它们是通用鲁棒满足框架的实例。此外，每个算法根据对手的性质提供不同的保证。具体来说，我们推导出两个遗憾界限：一个是随着时间的推移而呈亚线性的，假设对手的某些条件和令人满意的阈值$\tau$，另一个是随扰动幅度而变化的，但不需要对对手的假设。通过大量的实验，我们证明我们的方法在实现满意目标方面优于已建立的鲁棒优化方法，特别是当鲁棒优化框架的模糊性集指定不准确时。



## **37. Safety at Scale: A Comprehensive Survey of Large Model Safety**

大规模安全性：大型车型安全性全面调查 cs.CR

47 pages, 3 figures, 11 tables; GitHub:  https://github.com/xingjunm/Awesome-Large-Model-Safety

**SubmitDate**: 2025-06-02    [abs](http://arxiv.org/abs/2502.05206v4) [paper-pdf](http://arxiv.org/pdf/2502.05206v4)

**Authors**: Xingjun Ma, Yifeng Gao, Yixu Wang, Ruofan Wang, Xin Wang, Ye Sun, Yifan Ding, Hengyuan Xu, Yunhao Chen, Yunhan Zhao, Hanxun Huang, Yige Li, Jiaming Zhang, Xiang Zheng, Yang Bai, Zuxuan Wu, Xipeng Qiu, Jingfeng Zhang, Yiming Li, Xudong Han, Haonan Li, Jun Sun, Cong Wang, Jindong Gu, Baoyuan Wu, Siheng Chen, Tianwei Zhang, Yang Liu, Mingming Gong, Tongliang Liu, Shirui Pan, Cihang Xie, Tianyu Pang, Yinpeng Dong, Ruoxi Jia, Yang Zhang, Shiqing Ma, Xiangyu Zhang, Neil Gong, Chaowei Xiao, Sarah Erfani, Tim Baldwin, Bo Li, Masashi Sugiyama, Dacheng Tao, James Bailey, Yu-Gang Jiang

**Abstract**: The rapid advancement of large models, driven by their exceptional abilities in learning and generalization through large-scale pre-training, has reshaped the landscape of Artificial Intelligence (AI). These models are now foundational to a wide range of applications, including conversational AI, recommendation systems, autonomous driving, content generation, medical diagnostics, and scientific discovery. However, their widespread deployment also exposes them to significant safety risks, raising concerns about robustness, reliability, and ethical implications. This survey provides a systematic review of current safety research on large models, covering Vision Foundation Models (VFMs), Large Language Models (LLMs), Vision-Language Pre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models (DMs), and large-model-based Agents. Our contributions are summarized as follows: (1) We present a comprehensive taxonomy of safety threats to these models, including adversarial attacks, data poisoning, backdoor attacks, jailbreak and prompt injection attacks, energy-latency attacks, data and model extraction attacks, and emerging agent-specific threats. (2) We review defense strategies proposed for each type of attacks if available and summarize the commonly used datasets and benchmarks for safety research. (3) Building on this, we identify and discuss the open challenges in large model safety, emphasizing the need for comprehensive safety evaluations, scalable and effective defense mechanisms, and sustainable data practices. More importantly, we highlight the necessity of collective efforts from the research community and international collaboration. Our work can serve as a useful reference for researchers and practitioners, fostering the ongoing development of comprehensive defense systems and platforms to safeguard AI models.

摘要: 大型模型在通过大规模预训练进行学习和概括的卓越能力的推动下，迅速发展重塑了人工智能（AI）的格局。这些模型现在是广泛应用的基础，包括对话人工智能、推荐系统、自动驾驶、内容生成、医疗诊断和科学发现。然而，它们的广泛部署也使它们面临巨大的安全风险，引发了对稳健性、可靠性和道德影响的担忧。这项调查对当前大型模型的安全性研究进行了系统性回顾，涵盖视觉基础模型（VFM）、大型语言模型（LLM）、视觉语言预训练（VLP）模型、视觉语言模型（VLM）、扩散模型（DM）和基于大模型的代理。我们的贡献总结如下：（1）我们对这些模型的安全威胁提出了全面的分类，包括对抗性攻击、数据中毒、后门攻击、越狱和提示注入攻击、能量延迟攻击、数据和模型提取攻击以及新兴的特定于代理的威胁。(2)我们审查为每种类型的攻击提出的防御策略（如果有的话），并总结常用的数据集和安全研究基准。(3)在此基础上，我们确定并讨论了大型模型安全方面的开放挑战，强调全面的安全评估、可扩展且有效的防御机制以及可持续的数据实践的必要性。更重要的是，我们强调研究界集体努力和国际合作的必要性。我们的工作可以为研究人员和从业者提供有用的参考，促进全面防御系统和平台的持续开发，以保护人工智能模型。



## **38. Enhancing Diffusion-based Unrestricted Adversarial Attacks via Adversary Preferences Alignment**

通过Adjunct偏好对齐增强基于扩散的无限制对抗攻击 cs.CV

**SubmitDate**: 2025-06-02    [abs](http://arxiv.org/abs/2506.01511v1) [paper-pdf](http://arxiv.org/pdf/2506.01511v1)

**Authors**: Kaixun Jiang, Zhaoyu Chen, Haijing Guo, Jinglun Li, Jiyuan Fu, Pinxue Guo, Hao Tang, Bo Li, Wenqiang Zhang

**Abstract**: Preference alignment in diffusion models has primarily focused on benign human preferences (e.g., aesthetic). In this paper, we propose a novel perspective: framing unrestricted adversarial example generation as a problem of aligning with adversary preferences. Unlike benign alignment, adversarial alignment involves two inherently conflicting preferences: visual consistency and attack effectiveness, which often lead to unstable optimization and reward hacking (e.g., reducing visual quality to improve attack success). To address this, we propose APA (Adversary Preferences Alignment), a two-stage framework that decouples conflicting preferences and optimizes each with differentiable rewards. In the first stage, APA fine-tunes LoRA to improve visual consistency using rule-based similarity reward. In the second stage, APA updates either the image latent or prompt embedding based on feedback from a substitute classifier, guided by trajectory-level and step-wise rewards. To enhance black-box transferability, we further incorporate a diffusion augmentation strategy. Experiments demonstrate that APA achieves significantly better attack transferability while maintaining high visual consistency, inspiring further research to approach adversarial attacks from an alignment perspective. Code will be available at https://github.com/deep-kaixun/APA.

摘要: 扩散模型中的偏好对齐主要集中在良性的人类偏好（例如，美观）。在本文中，我们提出了一个新颖的视角：将无限制的对抗性示例生成框架为一个与对手偏好保持一致的问题。与良性对齐不同，对抗性对齐涉及两个本质上相互冲突的偏好：视觉一致性和攻击有效性，这通常会导致不稳定的优化和奖励黑客（例如，降低视觉质量以提高攻击成功率）。为了解决这个问题，我们提出了APA（Adjunctives偏好对齐），这是一个两阶段的框架，它将相互冲突的偏好合并，并通过可区分的奖励来优化每个偏好。在第一阶段，APA微调LoRA，以提高视觉一致性使用基于规则的相似性奖励。在第二阶段，APA更新的图像潜在的或提示嵌入的基础上，从替代分类器的反馈，指导的强制性和逐步奖励。为了增强黑匣子的可转移性，我们进一步纳入了扩散增强策略。实验表明，APA在保持高度视觉一致性的同时实现了明显更好的攻击转移性，激发了进一步的研究从对齐角度处理对抗性攻击。代码将在https://github.com/deep-kaixun/APA上提供。



## **39. Variance-Based Defense Against Blended Backdoor Attacks**

针对混合后门攻击的基于方差的防御 cs.LG

This paper has been accepted at ECML PKDD 2025

**SubmitDate**: 2025-06-02    [abs](http://arxiv.org/abs/2506.01444v1) [paper-pdf](http://arxiv.org/pdf/2506.01444v1)

**Authors**: Sujeevan Aseervatham, Achraf Kerzazi, Younès Bennani

**Abstract**: Backdoor attacks represent a subtle yet effective class of cyberattacks targeting AI models, primarily due to their stealthy nature. The model behaves normally on clean data but exhibits malicious behavior only when the attacker embeds a specific trigger into the input. This attack is performed during the training phase, where the adversary corrupts a small subset of the training data by embedding a pattern and modifying the labels to a chosen target. The objective is to make the model associate the pattern with the target label while maintaining normal performance on unaltered data. Several defense mechanisms have been proposed to sanitize training data-sets. However, these methods often rely on the availability of a clean dataset to compute statistical anomalies, which may not always be feasible in real-world scenarios where datasets can be unavailable or compromised. To address this limitation, we propose a novel defense method that trains a model on the given dataset, detects poisoned classes, and extracts the critical part of the attack trigger before identifying the poisoned instances. This approach enhances explainability by explicitly revealing the harmful part of the trigger. The effectiveness of our method is demonstrated through experimental evaluations on well-known image datasets and comparative analysis against three state-of-the-art algorithms: SCAn, ABL, and AGPD.

摘要: 后门攻击是一类针对人工智能模型的微妙但有效的网络攻击，主要是由于它们的隐秘性。该模型在干净的数据上正常运行，但只有当攻击者将特定触发器嵌入到输入中时，才会表现出恶意行为。这种攻击是在训练阶段执行的，对手通过嵌入模式并修改所选目标的标签来破坏训练数据的一小子集。目标是使模型将模式与目标标签关联起来，同时在未更改的数据上保持正常性能。已经提出了几种防御机制来净化训练数据集。然而，这些方法通常依赖于干净数据集的可用性来计算统计异常，这在数据集不可用或受损的真实场景中可能并不总是可行的。为了解决这一限制，我们提出了一种新的防御方法，该方法在给定的数据集上训练模型，检测中毒类，并在识别中毒实例之前提取攻击触发器的关键部分。这种方法通过明确揭示触发因素的有害部分来增强可解释性。通过对知名图像数据集的实验评估以及与三种最先进算法（SCAn、ABL和AGPD）的比较分析，证明了我们方法的有效性。



## **40. CSVAR: Enhancing Visual Privacy in Federated Learning via Adaptive Shuffling Against Overfitting**

CSVAR：通过自适应洗牌来增强联邦学习中的视觉隐私以防止过度匹配 cs.CR

**SubmitDate**: 2025-06-02    [abs](http://arxiv.org/abs/2506.01425v1) [paper-pdf](http://arxiv.org/pdf/2506.01425v1)

**Authors**: Zhuo Chen, Zhenya Ma, Yan Zhang, Donghua Cai, Ye Zhang, Qiushi Li, Yongheng Deng, Ye Guo, Ju Ren, Xuemin, Shen

**Abstract**: Although federated learning preserves training data within local privacy domains, the aggregated model parameters may still reveal private characteristics. This vulnerability stems from clients' limited training data, which predisposes models to overfitting. Such overfitting enables models to memorize distinctive patterns from training samples, thereby amplifying the success probability of privacy attacks like membership inference. To enhance visual privacy protection in FL, we present CSVAR(Channel-Wise Spatial Image Shuffling with Variance-Guided Adaptive Region Partitioning), a novel image shuffling framework to generate obfuscated images for secure data transmission and each training epoch, addressing both overfitting-induced privacy leaks and raw image transmission risks. CSVAR adopts region-variance as the metric to measure visual privacy sensitivity across image regions. Guided by this, CSVAR adaptively partitions each region into multiple blocks, applying fine-grained partitioning to privacy-sensitive regions with high region-variances for enhancing visual privacy protection and coarse-grained partitioning to privacy-insensitive regions for balancing model utility. In each region, CSVAR then shuffles between blocks in both the spatial domains and chromatic channels to hide visual spatial features and disrupt color distribution. Experimental evaluations conducted on diverse real-world datasets demonstrate that CSVAR is capable of generating visually obfuscated images that exhibit high perceptual ambiguity to human eyes, simultaneously mitigating the effectiveness of adversarial data reconstruction attacks and achieving a good trade-off between visual privacy protection and model utility.

摘要: 尽管联邦学习保留了本地隐私域内的训练数据，但聚合的模型参数仍然可能揭示私人特征。该漏洞源于客户有限的训练数据，这使得模型容易过度逼近。这种过度匹配使模型能够从训练样本中记住独特的模式，从而放大成员推断等隐私攻击的成功概率。为了增强FL中的视觉隐私保护，我们提出了CSVAR（具有方差引导自适应区域分割的逐行空间图像洗牌），这是一种新型的图像洗牌框架，用于生成用于安全数据传输和每个训练时期的模糊图像，解决过度匹配引起的隐私泄露和原始图像传输风险。CSVAR采用区域方差作为衡量图像区域的视觉隐私敏感度的指标。在此指导下，CSVAR将每个区域自适应地划分为多个块，对具有高区域方差的隐私敏感区域应用细粒度分区，以增强视觉隐私保护，并对隐私不敏感区域应用粗粒度分区，以平衡模型效用。在每个区域中，CSVAR然后在空间域和色彩通道的块之间洗牌，以隐藏视觉空间特征并扰乱颜色分布。对各种现实世界数据集进行的实验评估表明，CSVAR能够生成对人眼表现出高度感知模糊性的视觉模糊图像，同时减轻对抗性数据重建攻击的有效性，并在视觉隐私保护和模型效用之间实现良好的权衡。



## **41. Exploring Transferability of Multimodal Adversarial Samples for Vision-Language Pre-training Models with Contrastive Learning**

利用对比学习探索视觉语言预训练模型多模式对抗样本的可移植性 cs.MM

**SubmitDate**: 2025-06-02    [abs](http://arxiv.org/abs/2308.12636v5) [paper-pdf](http://arxiv.org/pdf/2308.12636v5)

**Authors**: Youze Wang, Wenbo Hu, Yinpeng Dong, Hanwang Zhang, Hang Su, Richang Hong

**Abstract**: The integration of visual and textual data in Vision-Language Pre-training (VLP) models is crucial for enhancing vision-language understanding. However, the adversarial robustness of these models, especially in the alignment of image-text features, has not yet been sufficiently explored. In this paper, we introduce a novel gradient-based multimodal adversarial attack method, underpinned by contrastive learning, to improve the transferability of multimodal adversarial samples in VLP models. This method concurrently generates adversarial texts and images within imperceptive perturbation, employing both image-text and intra-modal contrastive loss. We evaluate the effectiveness of our approach on image-text retrieval and visual entailment tasks, using publicly available datasets in a black-box setting. Extensive experiments indicate a significant advancement over existing single-modal transfer-based adversarial attack methods and current multimodal adversarial attack approaches.

摘要: 视觉语言预训练（VLP）模型中视觉和文本数据的集成对于增强视觉语言理解至关重要。然而，这些模型的对抗稳健性，尤其是在图像-文本特征的对齐方面，尚未得到充分的探索。本文引入了一种以对比学习为基础的新型基于梯度的多模式对抗攻击方法，以提高VLP模型中多模式对抗样本的可移植性。该方法在不可感知的扰动中同时生成对抗性文本和图像，同时采用图像-文本和模式内对比损失。我们在黑匣子环境中使用公开可用的数据集来评估我们的方法在图像文本检索和视觉蕴含任务方面的有效性。大量实验表明，与现有的基于单模式转移的对抗攻击方法和当前的多模式对抗攻击方法相比，有了重大进步。



## **42. Formal Security Analysis of SPV Clients Versus Home-Based Full Nodes in Bitcoin-Derived Systems**

比特币衍生系统中SPV客户端与基于家庭的全节点的形式化安全分析 cs.CR

42 pages, 4 figures, 3 appendices; includes formal axioms,  probabilistic divergence models, Nash equilibrium analysis, and full proofs

**SubmitDate**: 2025-06-02    [abs](http://arxiv.org/abs/2506.01384v1) [paper-pdf](http://arxiv.org/pdf/2506.01384v1)

**Authors**: Craig Steven Wright

**Abstract**: This paper presents a mathematically rigorous formal analysis of Simplified Payment Verification (SPV) clients, as specified in Section 8 of the original Bitcoin white paper, versus non-mining full nodes operated by home users. It defines security as resistance to divergence from global consensus and models transaction acceptance, enforcement capability, and divergence probability under adversarial conditions. The results demonstrate that SPV clients, despite omitting script verification, are cryptographically sufficient under honest-majority assumptions and topologically less vulnerable to attack than structurally passive, non-enforcing full nodes. The paper introduces new axioms on behavioral divergence and communication topology, proving that home-based full nodes increase systemic entropy without contributing to consensus integrity. Using a series of formally defined lemmas, propositions, and Monte Carlo simulation results, it is shown that SPV clients represent the rational equilibrium strategy for non-mining participants. This challenges the prevailing narrative that home validators enhance network security, providing formal and operational justifications for the sufficiency of SPV models.

摘要: 本文对简化支付验证（SPV）客户进行了数学上严格的形式分析，如原始比特币白皮书第8节中所述，与家庭用户操作的非挖掘全节点进行了比较。它将安全性定义为对全球共识分歧的抵抗，并对对抗条件下的交易接受度、执行能力和分歧概率进行建模。结果表明，尽管SPV客户端省略了脚本验证，但在诚实多数假设下，SPV客户端在加密上是足够的，并且在布局上比结构被动、非强制的完整节点更不容易受到攻击。该论文引入了有关行为分歧和通信布局的新公理，证明基于家庭的完整节点增加了系统性的信息，而不会对共识的完整性做出贡献。使用一系列正式定义的引理、命题和蒙特卡洛模拟结果，表明SPV客户代表了非采矿参与者的理性均衡策略。这挑战了普遍的说法，即家庭验证器增强了网络安全，为SPV模型的充分性提供了正式和运营依据。



## **43. Align is not Enough: Multimodal Universal Jailbreak Attack against Multimodal Large Language Models**

对齐还不够：针对多模式大型语言模型的多模式通用越狱攻击 cs.CR

**SubmitDate**: 2025-06-02    [abs](http://arxiv.org/abs/2506.01307v1) [paper-pdf](http://arxiv.org/pdf/2506.01307v1)

**Authors**: Youze Wang, Wenbo Hu, Yinpeng Dong, Jing Liu, Hanwang Zhang, Richang Hong

**Abstract**: Large Language Models (LLMs) have evolved into Multimodal Large Language Models (MLLMs), significantly enhancing their capabilities by integrating visual information and other types, thus aligning more closely with the nature of human intelligence, which processes a variety of data forms beyond just text. Despite advancements, the undesirable generation of these models remains a critical concern, particularly due to vulnerabilities exposed by text-based jailbreak attacks, which have represented a significant threat by challenging existing safety protocols. Motivated by the unique security risks posed by the integration of new and old modalities for MLLMs, we propose a unified multimodal universal jailbreak attack framework that leverages iterative image-text interactions and transfer-based strategy to generate a universal adversarial suffix and image. Our work not only highlights the interaction of image-text modalities can be used as a critical vulnerability but also validates that multimodal universal jailbreak attacks can bring higher-quality undesirable generations across different MLLMs. We evaluate the undesirable context generation of MLLMs like LLaVA, Yi-VL, MiniGPT4, MiniGPT-v2, and InstructBLIP, and reveal significant multimodal safety alignment issues, highlighting the inadequacy of current safety mechanisms against sophisticated multimodal attacks. This study underscores the urgent need for robust safety measures in MLLMs, advocating for a comprehensive review and enhancement of security protocols to mitigate potential risks associated with multimodal capabilities.

摘要: 大型语言模型（LLM）已演变为多模式大型语言模型（MLLM），通过集成视觉信息和其他类型来显着增强其能力，从而更紧密地与人类智能的本质保持一致，人类智能处理各种数据形式不仅仅是文本。尽管取得了进步，但这些模型的不良生成仍然是一个严重问题，特别是由于基于文本的越狱攻击暴露了漏洞，这些攻击通过挑战现有的安全协议构成了重大威胁。受MLLM新旧模式集成所带来的独特安全风险的激励，我们提出了一个统一的多模式通用越狱攻击框架，该框架利用迭代的图像-文本交互和基于传输的策略来生成通用的对抗性后缀和图像。我们的工作不仅强调了图像-文本模式的交互可以用作关键漏洞，而且还验证了多模式通用越狱攻击可以在不同的MLLM中带来更高质量的不良世代。我们评估了LLaVA、Yi-BL、MiniGPT 4、MiniGPT-v2和INSTBLIP等MLLM的不良上下文生成，并揭示了重大的多模式安全对齐问题，凸显了当前安全机制针对复杂多模式攻击的不足。这项研究强调了MLLM迫切需要采取强有力的安全措施，倡导全面审查和增强安全协议，以减轻与多模式能力相关的潜在风险。



## **44. Adversarial learning for nonparametric regression: Minimax rate and adaptive estimation**

非参数回归的对抗学习：极小极大率和自适应估计 stat.ML

**SubmitDate**: 2025-06-02    [abs](http://arxiv.org/abs/2506.01267v1) [paper-pdf](http://arxiv.org/pdf/2506.01267v1)

**Authors**: Jingfu Peng, Yuhong Yang

**Abstract**: Despite tremendous advancements of machine learning models and algorithms in various application domains, they are known to be vulnerable to subtle, natural or intentionally crafted perturbations in future input data, known as adversarial attacks. While numerous adversarial learning methods have been proposed, fundamental questions about their statistical optimality in robust loss remain largely unanswered. In particular, the minimax rate of convergence and the construction of rate-optimal estimators under future $X$-attacks are yet to be worked out.   In this paper, we address this issue in the context of nonparametric regression, under suitable assumptions on the smoothness of the regression function and the geometric structure of the input perturbation set. We first establish the minimax rate of convergence under adversarial $L_q$-risks with $1 \leq q \leq \infty$ and propose a piecewise local polynomial estimator that achieves the minimax optimality. The established minimax rate elucidates how the smoothness level and perturbation magnitude affect the fundamental limit of adversarial learning under future $X$-attacks. Furthermore, we construct a data-driven adaptive estimator that is shown to achieve, within a logarithmic factor, the optimal rate across a broad scale of nonparametric and adversarial classes.

摘要: 尽管机器学习模型和算法在各个应用领域取得了巨大进步，但众所周知，它们很容易受到未来输入数据中微妙、自然或故意制造的干扰，即对抗性攻击。虽然已经提出了许多对抗学习方法，但有关其在稳健损失中的统计最优性的基本问题在很大程度上仍然没有答案。特别是，未来$X$-攻击下的最小最大收敛率和速率最优估计器的构造尚未确定。   在本文中，我们在非参数回归的背景下，在回归函数的光滑性和输入扰动集的几何结构的适当假设下解决了这个问题。我们首先建立了对抗性$L_q$-风险下的极小极大收敛率，$1 \leq\leq \infty$，并提出了一个实现极小极大最优性的分段局部多项估计器。建立的极小极大率阐明了平滑度和扰动幅度如何影响未来$X$-攻击下对抗学习的基本极限。此外，我们构建了一个数据驱动的自适应估计器，该估计器被证明可以在对数因子内在广泛的非参数和对抗性类别中实现最佳速率。



## **45. Data Poisoning for In-context Learning**

上下文学习的数据中毒 cs.CR

NAACL 2025

**SubmitDate**: 2025-06-02    [abs](http://arxiv.org/abs/2402.02160v3) [paper-pdf](http://arxiv.org/pdf/2402.02160v3)

**Authors**: Pengfei He, Han Xu, Yue Xing, Hui Liu, Makoto Yamada, Jiliang Tang

**Abstract**: In the domain of large language models (LLMs), in-context learning (ICL) has been recognized for its innovative ability to adapt to new tasks, relying on examples rather than retraining or fine-tuning. This paper delves into the critical issue of ICL's susceptibility to data poisoning attacks, an area not yet fully explored. We wonder whether ICL is vulnerable, with adversaries capable of manipulating example data to degrade model performance. To address this, we introduce ICLPoison, a specialized attacking framework conceived to exploit the learning mechanisms of ICL. Our approach uniquely employs discrete text perturbations to strategically influence the hidden states of LLMs during the ICL process. We outline three representative strategies to implement attacks under our framework, each rigorously evaluated across a variety of models and tasks. Our comprehensive tests, including trials on the sophisticated GPT-4 model, demonstrate that ICL's performance is significantly compromised under our framework. These revelations indicate an urgent need for enhanced defense mechanisms to safeguard the integrity and reliability of LLMs in applications relying on in-context learning.

摘要: 在大型语言模型（LLM）领域，上下文学习（ICL）因其适应新任务的创新能力而受到认可，依赖于示例而不是重新训练或微调。本文探讨了ICL对数据中毒攻击的敏感性这一关键问题，该领域尚未充分探讨。我们想知道ICL是否容易受到攻击，对手能够操纵示例数据来降低模型性能。为了解决这个问题，我们引入了ICLPoison，这是一个专门的攻击框架，旨在利用ICL的学习机制。我们的方法独特地使用离散文本扰动来战略性地影响ICL过程中LLM的隐藏状态。我们概述了在我们的框架下实施攻击的三种代表性策略，每种策略都经过了各种模型和任务的严格评估。我们的全面测试，包括对复杂GPT-4模型的试验，表明ICL的性能在我们的框架下受到了显着的影响。这些揭露表明迫切需要增强的防御机制，以保障依赖上下文学习的应用程序中LLM的完整性和可靠性。



## **46. Red-Teaming LLM Multi-Agent Systems via Communication Attacks**

通过通信攻击的Red-Teaming LLM多代理系统 cs.CR

ACL 2025

**SubmitDate**: 2025-06-02    [abs](http://arxiv.org/abs/2502.14847v2) [paper-pdf](http://arxiv.org/pdf/2502.14847v2)

**Authors**: Pengfei He, Yupin Lin, Shen Dong, Han Xu, Yue Xing, Hui Liu

**Abstract**: Large Language Model-based Multi-Agent Systems (LLM-MAS) have revolutionized complex problem-solving capability by enabling sophisticated agent collaboration through message-based communications. While the communication framework is crucial for agent coordination, it also introduces a critical yet unexplored security vulnerability. In this work, we introduce Agent-in-the-Middle (AiTM), a novel attack that exploits the fundamental communication mechanisms in LLM-MAS by intercepting and manipulating inter-agent messages. Unlike existing attacks that compromise individual agents, AiTM demonstrates how an adversary can compromise entire multi-agent systems by only manipulating the messages passing between agents. To enable the attack under the challenges of limited control and role-restricted communication format, we develop an LLM-powered adversarial agent with a reflection mechanism that generates contextually-aware malicious instructions. Our comprehensive evaluation across various frameworks, communication structures, and real-world applications demonstrates that LLM-MAS is vulnerable to communication-based attacks, highlighting the need for robust security measures in multi-agent systems.

摘要: 基于大型语言模型的多代理系统（LLM-MAS）通过基于消息的通信实现复杂的代理协作，彻底改变了复杂问题解决能力。虽然通信框架对于代理协调至关重要，但它也引入了一个关键但尚未探索的安全漏洞。在这项工作中，我们引入了中间代理（AiTM），这是一种新型攻击，通过拦截和操纵代理间消息来利用LLM-MAS中的基本通信机制。与现有的危及单个代理的攻击不同，AiTM展示了对手如何仅通过操纵代理之间传递的消息来危及整个多代理系统。为了在有限控制和角色限制通信格式的挑战下实现攻击，我们开发了一个LLM驱动的对抗代理，该代理具有反射机制，可以生成上下文感知的恶意指令。我们对各种框架、通信结构和现实世界应用程序的全面评估表明，LLM-MAS容易受到基于通信的攻击，这凸显了多代理系统中对强大安全措施的需求。



## **47. EVALOOP: Assessing LLM Robustness in Programming from a Self-consistency Perspective**

EVALOOP：从自我一致性的角度评估LLM编程稳健性 cs.SE

19 pages, 11 figures

**SubmitDate**: 2025-06-01    [abs](http://arxiv.org/abs/2505.12185v2) [paper-pdf](http://arxiv.org/pdf/2505.12185v2)

**Authors**: Sen Fang, Weiyuan Ding, Bowen Xu

**Abstract**: Assessing the programming capabilities of Large Language Models (LLMs) is crucial for their effective use in software engineering. Current evaluations, however, predominantly measure the accuracy of generated code on static benchmarks, neglecting the critical aspect of model robustness during programming tasks. While adversarial attacks offer insights on model robustness, their effectiveness is limited and evaluation could be constrained. Current adversarial attack methods for robustness evaluation yield inconsistent results, struggling to provide a unified evaluation across different LLMs. We introduce EVALOOP, a novel assessment framework that evaluate the robustness from a self-consistency perspective, i.e., leveraging the natural duality inherent in popular software engineering tasks, e.g., code generation and code summarization. EVALOOP initiates a self-contained feedback loop: an LLM generates output (e.g., code) from an input (e.g., natural language specification), and then use the generated output as the input to produce a new output (e.g., summarizes that code into a new specification). EVALOOP repeats the process to assess the effectiveness of EVALOOP in each loop. This cyclical strategy intrinsically evaluates robustness without rely on any external attack setups, providing a unified metric to evaluate LLMs' robustness in programming. We evaluate 16 prominent LLMs (e.g., GPT-4.1, O4-mini) on EVALOOP and found that EVALOOP typically induces a 5.01%-19.31% absolute drop in pass@1 performance within ten loops. Intriguingly, robustness does not always align with initial performance (i.e., one-time query); for instance, GPT-3.5-Turbo, despite superior initial code generation compared to DeepSeek-V2, demonstrated lower robustness over repeated evaluation loop.

摘要: 评估大型语言模型（LLM）的编程能力对于它们在软件工程中的有效使用至关重要。然而，当前的评估主要衡量静态基准上生成的代码的准确性，忽视了编程任务期间模型稳健性的关键方面。虽然对抗性攻击提供了有关模型稳健性的见解，但它们的有效性有限，并且评估可能会受到限制。当前用于稳健性评估的对抗攻击方法会产生不一致的结果，难以在不同的LLM之间提供统一的评估。我们引入EVALOOP，这是一种新型评估框架，从自一致性的角度评估稳健性，即利用流行软件工程任务中固有的自然二重性，例如，代码生成和代码摘要。EVALOOP启动独立反馈循环：LLM生成输出（例如，代码）来自输入（例如，自然语言规范），然后使用生成的输出作为输入来产生新的输出（例如，将该代码总结为新规范）。EVALOOP重复该过程以评估每个循环中EVALOOP的有效性。这种循环策略本质上评估稳健性，而不依赖任何外部攻击设置，提供了一个统一的指标来评估LLM在编程中的稳健性。我们评估了16个著名的LLM（例如，GPT-4.1，O 4-mini）在EVALOOP上发现EVALOOP通常会在十个循环内导致pass@1性能绝对下降5.01%-19.31%。有趣的是，稳健性并不总是与初始性能一致（即，一次性查询）;例如，GPT-3.5-Turbo尽管初始代码生成优于DeepSeek-V2，但在重复评估循环中表现出较低的鲁棒性。



## **48. Effective faking of verbal deception detection with target-aligned adversarial attacks**

通过目标对准的对抗攻击有效伪造言语欺骗检测 cs.CL

Accepted to Legal and Criminological Psychology (author version)

**SubmitDate**: 2025-06-01    [abs](http://arxiv.org/abs/2501.05962v2) [paper-pdf](http://arxiv.org/pdf/2501.05962v2)

**Authors**: Bennett Kleinberg, Riccardo Loconte, Bruno Verschuere

**Abstract**: Background: Deception detection through analysing language is a promising avenue using both human judgments and automated machine learning judgments. For both forms of credibility assessment, automated adversarial attacks that rewrite deceptive statements to appear truthful pose a serious threat. Methods: We used a dataset of 243 truthful and 262 fabricated autobiographical stories in a deception detection task for humans and machine learning models. A large language model was tasked to rewrite deceptive statements so that they appear truthful. In Study 1, humans who made a deception judgment or used the detailedness heuristic and two machine learning models (a fine-tuned language model and a simple n-gram model) judged original or adversarial modifications of deceptive statements. In Study 2, we manipulated the target alignment of the modifications, i.e. tailoring the attack to whether the statements would be assessed by humans or computer models. Results: When adversarial modifications were aligned with their target, human (d=-0.07 and d=-0.04) and machine judgments (51% accuracy) dropped to the chance level. When the attack was not aligned with the target, both human heuristics judgments (d=0.30 and d=0.36) and machine learning predictions (63-78%) were significantly better than chance. Conclusions: Easily accessible language models can effectively help anyone fake deception detection efforts both by humans and machine learning models. Robustness against adversarial modifications for humans and machines depends on that target alignment. We close with suggestions on advancing deception research with adversarial attack designs and techniques.

摘要: 背景：通过分析语言来检测欺骗是一种使用人类判断和自动机器学习判断的有前途的途径。对于这两种形式的可信度评估来说，重写欺骗性陈述使其看起来真实的自动对抗攻击构成了严重威胁。方法：我们使用了包含243个真实故事和262个捏造自传故事的数据集，用于人类和机器学习模型的欺骗检测任务。一个大型语言模型的任务是重写欺骗性陈述，使其看起来真实。在研究1中，做出欺骗判断或使用细节启发式和两个机器学习模型（微调的语言模型和简单的n元语法模型）的人类判断欺骗性陈述的原始或对抗性修改。在研究2中，我们操纵了修改的目标对齐，即根据陈述是由人类还是计算机模型评估来定制攻击。结果：当对抗性修改与其目标对齐时，人类（d=-0.07和d=-0.04）和机器判断（51%的准确性）下降到机会水平。当攻击与目标不一致时，人类启发式判断（d=0.30和d=0.36）和机器学习预测（63-78%）都显着优于偶然性。结论：易于访问的语言模型可以有效地帮助任何人伪造人类和机器学习模型的欺骗检测工作。人类和机器对抗对抗修改的稳健性取决于目标对齐。最后，我们提出了关于通过对抗性攻击设计和技术推进欺骗研究的建议。



## **49. Fighting Fire with Fire (F3): A Training-free and Efficient Visual Adversarial Example Purification Method in LVLMs**

以毒攻毒（F3）：LVLM中一种无需培训且高效的视觉对抗示例净化方法 cs.CV

**SubmitDate**: 2025-06-01    [abs](http://arxiv.org/abs/2506.01064v1) [paper-pdf](http://arxiv.org/pdf/2506.01064v1)

**Authors**: Yudong Zhang, Ruobing Xie, Yiqing Huang, Jiansheng Chen, Xingwu Sun, Zhanhui Kang, Di Wang, Yu Wang

**Abstract**: Recent advances in large vision-language models (LVLMs) have showcased their remarkable capabilities across a wide range of multimodal vision-language tasks. However, these models remain vulnerable to visual adversarial attacks, which can substantially compromise their performance. Despite their potential impact, the development of effective methods for purifying such adversarial examples has received relatively limited attention. In this paper, we introduce F3, a novel adversarial purification framework that employs a counterintuitive "fighting fire with fire" strategy: intentionally introducing simple perturbations to adversarial examples to mitigate their harmful effects. Specifically, F3 leverages cross-modal attentions derived from randomly perturbed adversary examples as reference targets. By injecting noise into these adversarial examples, F3 effectively refines their attention, resulting in cleaner and more reliable model outputs. Remarkably, this seemingly paradoxical approach of employing noise to counteract adversarial attacks yields impressive purification results. Furthermore, F3 offers several distinct advantages: it is training-free and straightforward to implement, and exhibits significant computational efficiency improvements compared to existing purification methods. These attributes render F3 particularly suitable for large-scale industrial applications where both robust performance and operational efficiency are critical priorities. The code will be made publicly available.

摘要: 大型视觉语言模型（LVLM）的最新进展展示了它们在广泛的多模式视觉语言任务中的非凡能力。然而，这些模型仍然容易受到视觉对抗攻击，这可能会极大地损害其性能。尽管它们具有潜在的影响，但净化此类对抗性例子的有效方法的开发受到的关注相对有限。在本文中，我们介绍了F3，这是一个新颖的对抗净化框架，它采用了违反直觉的“以毒攻毒”策略：有意地向对抗性示例引入简单的扰动以减轻其有害影响。具体来说，F3利用从随机干扰的对手示例中获得的跨模式注意力作为参考目标。通过向这些对抗性示例中注入噪音，F3有效地细化了他们的注意力，从而产生更干净、更可靠的模型输出。值得注意的是，这种看似矛盾的利用噪音来抵消对抗攻击的方法产生了令人印象深刻的净化结果。此外，F3具有几个明显的优势：无需训练且易于实施，并且与现有的纯化方法相比，计算效率显着提高。这些属性使F3特别适合大规模工业应用，其中稳健的性能和运营效率都是关键优先事项。该代码将公开。



## **50. No Soundness in the Real World: On the Challenges of the Verification of Deployed Neural Networks**

现实世界中没有声音：关于已部署神经网络验证的挑战 cs.LG

accepted at ICML 2025. For the implementation, see  https://github.com/szasza1/no_soundness

**SubmitDate**: 2025-06-01    [abs](http://arxiv.org/abs/2506.01054v1) [paper-pdf](http://arxiv.org/pdf/2506.01054v1)

**Authors**: Attila Szász, Balázs Bánhelyi, Márk Jelasity

**Abstract**: The ultimate goal of verification is to guarantee the safety of deployed neural networks. Here, we claim that all the state-of-the-art verifiers we are aware of fail to reach this goal. Our key insight is that theoretical soundness (bounding the full-precision output while computing with floating point) does not imply practical soundness (bounding the floating point output in a potentially stochastic environment). We prove this observation for the approaches that are currently used to achieve provable theoretical soundness, such as interval analysis and its variants. We also argue that achieving practical soundness is significantly harder computationally. We support our claims empirically as well by evaluating several well-known verification methods. To mislead the verifiers, we create adversarial networks that detect and exploit features of the deployment environment, such as the order and precision of floating point operations. We demonstrate that all the tested verifiers are vulnerable to our new deployment-specific attacks, which proves that they are not practically sound.

摘要: 验证的最终目标是保证部署的神经网络的安全性。在这里，我们声称我们所知道的所有最先进的验证器都无法达到这一目标。我们的关键见解是，理论上的合理性（在浮点计算时限制全精度输出）并不意味着实际上的合理性（在潜在的随机环境中限制浮点输出）。我们证明了这一观察的方法，目前用于实现可证明的理论合理性，如区间分析及其变种。我们还认为，实现实际的合理性在计算上要困难得多。我们还通过评估几种著名的验证方法来从经验上支持我们的主张。为了误导验证者，我们创建了对抗网络，这些网络检测和利用部署环境的特征，例如浮点操作的顺序和精确度。我们证明所有测试的验证器都容易受到我们新的特定于部署的攻击的攻击，这证明它们实际上并不可靠。



