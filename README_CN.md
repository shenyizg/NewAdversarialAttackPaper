# Latest Adversarial Attack Papers
**update at 2025-11-14 10:17:20**

翻译来自 https://cloud.tencent.com/document/product/551/15619

## **1. Unveiling Hidden Threats: Using Fractal Triggers to Boost Stealthiness of Distributed Backdoor Attacks in Federated Learning**

揭露隐藏的威胁：使用分数阶触发器来提高联邦学习中分布式后门攻击的隐蔽性 cs.CR

10 pages, 1 figures, conference

**SubmitDate**: 2025-11-13    [abs](http://arxiv.org/abs/2511.09252v1) [paper-pdf](None)

**Authors**: Jian Wang, Hong Shen, Chan-Tong Lam

**Abstract**: Traditional distributed backdoor attacks (DBA) in federated learning improve stealthiness by decomposing global triggers into sub-triggers, which however requires more poisoned data to maintian the attck strength and hence increases the exposure risk. To overcome this defect, This paper proposes a novel method, namely Fractal-Triggerred Distributed Backdoor Attack (FTDBA), which leverages the self-similarity of fractals to enhance the feature strength of sub-triggers and hence significantly reduce the required poisoning volume for the same attack strength. To address the detectability of fractal structures in the frequency and gradient domains, we introduce a dynamic angular perturbation mechanism that adaptively adjusts perturbation intensity across the training phases to balance efficiency and stealthiness. Experiments show that FTDBA achieves a 92.3\% attack success rate with only 62.4\% of the poisoning volume required by traditional DBA methods, while reducing the detection rate by 22.8\% and KL divergence by 41.2\%. This study presents a low-exposure, high-efficiency paradigm for federated backdoor attacks and expands the application of fractal features in adversarial sample generation.

摘要: 联邦学习中传统的分布式后门攻击（DBA）通过将全局触发器分解为子触发器来提高隐蔽性，但这需要更多的有毒数据来维持攻击强度，从而增加了暴露风险。为了克服这一缺陷，本文提出了一种新颖的方法，即Fractal-Triggerred分布式后门攻击（FTDBA），该方法利用Fractal-Triggerred Distributionary Backdoor Attack（FTDBA）来增强子触发器的特征强度，从而显着降低相同攻击强度所需的中毒量。为了解决频率和梯度域中分数形结构的可检测性，我们引入了一种动态角度扰动机制，该机制自适应地调整整个训练阶段的扰动强度，以平衡效率和隐蔽性。实验表明，FTDBA的攻击成功率仅为传统DBA方法所需的62.4%，而检测率降低了22.8%，KL分歧降低了41.2%。这项研究为联合后门攻击提供了一种低暴露、高效率的范式，并扩展了分数特征在对抗性样本生成中的应用。



## **2. Systematic Literature Review on Vehicular Collaborative Perception -- A Computer Vision Perspective**

车辆协作感知的系统文献综述--计算机视觉视角 cs.CV

38 pages, 8 figures, accepted for publication in IEEE Transactions on Intelligent Transportation Systems (T-ITS)

**SubmitDate**: 2025-11-13    [abs](http://arxiv.org/abs/2504.04631v3) [paper-pdf](None)

**Authors**: Lei Wan, Jianxin Zhao, Andreas Wiedholz, Manuel Bied, Mateus Martinez de Lucena, Abhishek Dinkar Jagtap, Andreas Festag, Antônio Augusto Fröhlich, Hannan Ejaz Keen, Alexey Vinel

**Abstract**: The effectiveness of autonomous vehicles relies on reliable perception capabilities. Despite significant advancements in artificial intelligence and sensor fusion technologies, current single-vehicle perception systems continue to encounter limitations, notably visual occlusions and limited long-range detection capabilities. Collaborative Perception (CP), enabled by Vehicle-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I) communication, has emerged as a promising solution to mitigate these issues and enhance the reliability of autonomous systems. Beyond advancements in communication, the computer vision community is increasingly focusing on improving vehicular perception through collaborative approaches. However, a systematic literature review that thoroughly examines existing work and reduces subjective bias is still lacking. Such a systematic approach helps identify research gaps, recognize common trends across studies, and inform future research directions. In response, this study follows the PRISMA 2020 guidelines and includes 106 peer-reviewed articles. These publications are analyzed based on modalities, collaboration schemes, and key perception tasks. Through a comparative analysis, this review illustrates how different methods address practical issues such as pose errors, temporal latency, communication constraints, domain shifts, heterogeneity, and adversarial attacks. Furthermore, it critically examines evaluation methodologies, highlighting a misalignment between current metrics and CP's fundamental objectives. By delving into all relevant topics in-depth, this review offers valuable insights into challenges, opportunities, and risks, serving as a reference for advancing research in vehicular collaborative perception.

摘要: 自动驾驶汽车的有效性依赖于可靠的感知能力。尽管人工智能和传感器融合技术取得了重大进步，但当前的单车感知系统继续遇到局限性，特别是视觉遮挡和有限的远程检测能力。由车对车（V2 V）和车对基础设施（V2 I）通信实现的协作感知（CP）已成为缓解这些问题并提高自主系统可靠性的一种有前途的解决方案。除了通信领域的进步之外，计算机视觉界越来越关注通过协作方法改善车辆感知。然而，仍然缺乏彻底审查现有工作并减少主观偏见的系统性文献审查。这种系统性方法有助于识别研究差距、识别研究中的共同趋势，并为未来的研究方向提供信息。作为回应，这项研究遵循PRISMA 2020指南，包括106篇同行评审的文章。这些出版物是根据模式、协作方案和关键感知任务进行分析的。通过比较分析，本综述说明了不同的方法如何解决实际问题，例如姿势错误、时间延迟、通信约束、域转移、异类和对抗性攻击。此外，它还批判性地审查了评估方法，强调了当前指标与CP基本目标之间的不一致。通过深入研究所有相关主题，本评论对挑战、机遇和风险提供了宝贵的见解，为推进车辆协作感知研究提供参考。



## **3. Improving Adversarial Transferability with Neighbourhood Gradient Information**

利用邻居梯度信息改善对抗可移植性 cs.CV

Accepted by Applied Soft Computing

**SubmitDate**: 2025-11-13    [abs](http://arxiv.org/abs/2408.05745v2) [paper-pdf](None)

**Authors**: Haijing Guo, Jiafeng Wang, Zhaoyu Chen, Kaixun Jiang, Lingyi Hong, Pinxue Guo, Jinglun Li, Wenqiang Zhang

**Abstract**: Deep neural networks (DNNs) are known to be susceptible to adversarial examples, leading to significant performance degradation. In black-box attack scenarios, a considerable attack performance gap between the surrogate model and the target model persists. This work focuses on enhancing the transferability of adversarial examples to narrow this performance gap. We observe that the gradient information around the clean image, i.e., Neighbourhood Gradient Information (NGI), can offer high transferability.Based on this insight, we introduce NGI-Attack, incorporating Example Backtracking and Multiplex Mask strategies to exploit this gradient information and enhance transferability. Specifically, we first adopt Example Backtracking to accumulate Neighbourhood Gradient Information as the initial momentum term. Then, we utilize Multiplex Mask to form a multi-way attack strategy that forces the network to focus on non-discriminative regions, which can obtain richer gradient information during only a few iterations. Extensive experiments demonstrate that our approach significantly enhances adversarial transferability. Especially, when attacking numerous defense models, we achieve an average attack success rate of 95.2%. Notably, our method can seamlessly integrate with any off-the-shelf algorithm, enhancing their attack performance without incurring extra time costs.

摘要: 众所周知，深度神经网络（DNN）容易受到对抗性示例的影响，导致性能显著下降。在黑盒攻击场景中，代理模型和目标模型之间存在相当大的攻击性能差距。这项工作的重点是增强对抗性示例的可移植性，以缩小这种性能差距。我们观察到干净图像周围的梯度信息，即，邻居梯度信息（NGI）可以提供高的可移植性。基于这一见解，我们引入了NGI-Attack，结合示例回溯和多路屏蔽策略来利用这种梯度信息并增强可移植性。具体来说，我们首先采用示例回溯来积累邻居梯度信息作为初始动量项。然后，我们利用Multiple Mass形成多路攻击策略，迫使网络专注于非区分区域，只需几次迭代即可获得更丰富的梯度信息。大量实验表明，我们的方法显着增强了对抗性可转让性。特别是，在攻击众多防御模型时，我们的平均攻击成功率为95.2%。值得注意的是，我们的方法可以与任何现成的算法无缝集成，增强其攻击性能，而不会产生额外的时间成本。



## **4. Filtered-ViT: A Robust Defense Against Multiple Adversarial Patch Attacks**

Filtered-ViT：针对多种对抗补丁攻击的强大防御 cs.CV

**SubmitDate**: 2025-11-12    [abs](http://arxiv.org/abs/2511.07755v1) [paper-pdf](None)

**Authors**: Aja Khanal, Ahmed Faid, Apurva Narayan

**Abstract**: Deep learning vision systems are increasingly deployed in safety-critical domains such as healthcare, yet they remain vulnerable to small adversarial patches that can trigger misclassifications. Most existing defenses assume a single patch and fail when multiple localized disruptions occur, the type of scenario adversaries and real-world artifacts often exploit. We propose Filtered-ViT, a new vision transformer architecture that integrates SMART Vector Median Filtering (SMART-VMF), a spatially adaptive, multi-scale, robustness-aware mechanism that enables selective suppression of corrupted regions while preserving semantic detail. On ImageNet with LaVAN multi-patch attacks, Filtered-ViT achieves 79.8% clean accuracy and 46.3% robust accuracy under four simultaneous 1\% patches, outperforming existing defenses. Beyond synthetic benchmarks, a real-world case study on radiographic medical imagery shows that Filtered-ViT mitigates natural artifacts such as occlusions and scanner noise without degrading diagnostic content. This establishes Filtered-ViT as the first transformer to demonstrate unified robustness against both adversarial and naturally occurring patch-like disruptions, charting a path toward reliable vision systems in truly high-stakes environments.

摘要: 深度学习视觉系统越来越多地部署在医疗保健等安全关键领域，但它们仍然容易受到可能引发错误分类的小型对抗补丁的影响。大多数现有的防御都假设一个补丁，并在发生多个本地化中断时失败，而这是对手和现实世界的产物经常利用的场景类型。我们提出了Filtered-ViT，这是一种新的视觉Transformer架构，集成了Smart Vector Median Filting（SMART-VMF），这是一种空间自适应、多尺度、鲁棒性感知机制，可以在保留语义细节的同时选择性地抑制损坏区域。在具有LaVAN多补丁攻击的ImageNet上，Filtered-ViT在四个同时1%补丁的情况下实现了79.8%的干净准确性和46.3%的鲁棒准确性，优于现有防御。除了合成基准之外，一项关于放射摄影医学图像的现实案例研究表明，Filtered-ViT可以减轻遮挡和扫描仪噪音等自然伪影，而不会降低诊断内容。这使Filtered-ViT成为第一个展示针对对抗性和自然发生的片状破坏的统一鲁棒性的Transformer，为在真正高风险环境中实现可靠的视觉系统绘制了一条道路。



## **5. A Small Leak Sinks All: Exploring the Transferable Vulnerability of Source Code Models**

一个小漏洞就会消失：探索源代码模型的可转移漏洞 cs.SE

**SubmitDate**: 2025-11-12    [abs](http://arxiv.org/abs/2511.08127v1) [paper-pdf](None)

**Authors**: Weiye Li, Wenyi Tang

**Abstract**: Source Code Model learn the proper embeddings from source codes, demonstrating significant success in various software engineering or security tasks. The recent explosive development of LLM extends the family of SCMs,bringing LLMs for code that revolutionize development workflows. Investigating different kinds of SCM vulnerability is the cornerstone for the security and trustworthiness of AI-powered software ecosystems, however, the fundamental one, transferable vulnerability, remains critically underexplored. Existing studies neither offer practical ways, i.e. require access to the downstream classifier of SCMs, to produce effective adversarial samples for adversarial defense, nor give heed to the widely used LLM4Code in modern software development platforms and cloud-based integrated development environments. Therefore, this work systematically studies the intrinsic vulnerability transferability of both traditional SCMs and LLM4Code, and proposes a victim-agnostic approach to generate practical adversarial samples. We design HABITAT, consisting of a tailored perturbation-inserting mechanism and a hierarchical Reinforcement Learning framework that adaptively selects optimal perturbations without requiring any access to the downstream classifier of SCMs. Furthermore, an intrinsic transferability analysis of SCM vulnerabilities is conducted, revealing the potential vulnerability correlation between traditional SCMs and LLM4Code, together with fundamental factors that govern the success rate of victim-agnostic transfer attacks. These findings of SCM vulnerabilities underscore the critical focal points for developing robust defenses in the future. Experimental evaluation demonstrates that our constructed adversarial examples crafted based on traditional SCMs achieve up to 64% success rates against LLM4Code, surpassing the state-of-the-art by over 15%.

摘要: 源代码模型从源代码学习正确的嵌入，在各种软件工程或安全任务中取得了巨大成功。LLM最近的爆炸性发展扩展了SCS系列，为彻底改变开发工作流程的代码带来了LLM。调查不同类型的供应链管理漏洞是人工智能驱动的软件生态系统安全性和可信性的基石，然而，最基本的漏洞，即可转移的漏洞，仍然被严重地研究不足。现有的研究既没有提供实用的方法，即需要访问CSC的下游分类器来产生有效的对抗样本来进行对抗性防御，也没有注意到现代软件开发平台和基于云的集成开发环境中广泛使用的LLM 4代码。因此，这项工作系统地研究了传统SCS和LLM 4 Code的内在漏洞可转移性，并提出了一种受害者不可知的方法来生成实际的对抗样本。我们设计了HABITAT，由量身定制的扰动插入机制和分层强化学习框架组成，该框架自适应地选择最佳扰动，而无需任何访问SCS的下游分类器。此外，还对SMC漏洞进行了内在可转移性分析，揭示了传统SMC和LLM 4 Code之间潜在的漏洞相关性，以及决定受害者不可知转移攻击成功率的基本因素。这些关于供应链管理漏洞的发现强调了未来开发强大防御的关键焦点。实验评估表明，我们基于传统SC构建的对抗性示例针对LLM 4Code的成功率高达64%，超过最新技术水平15%以上。



## **6. MSCR: Exploring the Vulnerability of LLMs' Mathematical Reasoning Abilities Using Multi-Source Candidate Replacement**

MSR：使用多源候选替换探索LLM数学推理能力的脆弱性 cs.AI

**SubmitDate**: 2025-11-12    [abs](http://arxiv.org/abs/2511.08055v1) [paper-pdf](None)

**Authors**: Zhishen Sun, Guang Dai, Haishan Ye

**Abstract**: LLMs demonstrate performance comparable to human abilities in complex tasks such as mathematical reasoning, but their robustness in mathematical reasoning under minor input perturbations still lacks systematic investigation. Existing methods generally suffer from limited scalability, weak semantic preservation, and high costs. Therefore, we propose MSCR, an automated adversarial attack method based on multi-source candidate replacement. By combining three information sources including cosine similarity in the embedding space of LLMs, the WordNet dictionary, and contextual predictions from a masked language model, we generate for each word in the input question a set of semantically similar candidates, which are then filtered and substituted one by one to carry out the attack. We conduct large-scale experiments on LLMs using the GSM8K and MATH500 benchmarks. The results show that even a slight perturbation involving only a single word can significantly reduce the accuracy of all models, with the maximum drop reaching 49.89% on GSM8K and 35.40% on MATH500, while preserving the high semantic consistency of the perturbed questions. Further analysis reveals that perturbations not only lead to incorrect outputs but also substantially increase the average response length, which results in more redundant reasoning paths and higher computational resource consumption. These findings highlight the robustness deficiencies and efficiency bottlenecks of current LLMs in mathematical reasoning tasks.

摘要: LLM在数学推理等复杂任务中表现出与人类能力相当的性能，但它们在微小输入扰动下的数学推理稳健性仍然缺乏系统性研究。现有的方法通常存在可扩展性有限、语义保留弱和成本高的问题。因此，我们提出了一种基于多源候选替换的自动对抗攻击方法MSR。通过结合三个信息源，包括LLM嵌入空间中的cos相似性、WordNet词典和来自掩蔽语言模型的上下文预测，我们为输入问题中的每个单词生成一组语义相似的候选项，然后逐个过滤和替换以执行攻击。我们使用GSM 8 K和PATH 500基准对LLM进行大规模实验。结果表明，即使是仅涉及单个单词的轻微扰动也会显着降低所有模型的准确性，在GSM 8K上最大降幅达到49.89%，在PATH 500上最大降幅达到35.40%，同时保持了受扰动问题的高度语义一致性。进一步的分析表明，扰动不仅会导致错误的输出，还会大幅增加平均响应长度，从而导致更多冗余的推理路径和更高的计算资源消耗。这些发现凸显了当前LLM在数学推理任务中的鲁棒性缺陷和效率瓶颈。



## **7. Invisible Triggers, Visible Threats! Road-Style Adversarial Creation Attack for Visual 3D Detection in Autonomous Driving**

无形的触发，有形的威胁！面向自动驾驶视觉3D检测的道路风格对抗性创建攻击 cs.CV

Accepted by the AAAI 2026 (Main Track)

**SubmitDate**: 2025-11-12    [abs](http://arxiv.org/abs/2511.08015v1) [paper-pdf](None)

**Authors**: Jian Wang, Lijun He, Yixing Yong, Haixia Bi, Fan Li

**Abstract**: Modern autonomous driving (AD) systems leverage 3D object detection to perceive foreground objects in 3D environments for subsequent prediction and planning. Visual 3D detection based on RGB cameras provides a cost-effective solution compared to the LiDAR paradigm. While achieving promising detection accuracy, current deep neural network-based models remain highly susceptible to adversarial examples. The underlying safety concerns motivate us to investigate realistic adversarial attacks in AD scenarios. Previous work has demonstrated the feasibility of placing adversarial posters on the road surface to induce hallucinations in the detector. However, the unnatural appearance of the posters makes them easily noticeable by humans, and their fixed content can be readily targeted and defended. To address these limitations, we propose the AdvRoad to generate diverse road-style adversarial posters. The adversaries have naturalistic appearances resembling the road surface while compromising the detector to perceive non-existent objects at the attack locations. We employ a two-stage approach, termed Road-Style Adversary Generation and Scenario-Associated Adaptation, to maximize the attack effectiveness on the input scene while ensuring the natural appearance of the poster, allowing the attack to be carried out stealthily without drawing human attention. Extensive experiments show that AdvRoad generalizes well to different detectors, scenes, and spoofing locations. Moreover, physical attacks further demonstrate the practical threats in real-world environments.

摘要: 现代自动驾驶（AD）系统利用3D对象检测来感知3D环境中的前景对象，以进行后续预测和规划。与LiDART范式相比，基于RB相机的视觉3D检测提供了一种经济高效的解决方案。虽然实现了有希望的检测准确性，但当前基于深度神经网络的模型仍然高度容易受到对抗性示例的影响。潜在的安全问题促使我们调查AD场景中现实的对抗性攻击。之前的工作已经证明了在路面上放置对抗海报以在检测器中引起幻觉的可行性。然而，海报的不自然外观使它们很容易被人类注意到，并且它们的固定内容很容易成为目标和防御。为了解决这些限制，我们建议AdvRoad生成多样化的道路风格对抗海报。对手拥有类似于路面的自然外观，同时破坏探测器来感知攻击地点不存在的物体。我们采用两阶段方法，称为道路式Advertising Generation和场景关联适应，以最大限度地提高对输入场景的攻击效果，同时确保海报的自然外观，允许攻击在不引起人类注意的情况下秘密进行。大量实验表明，AdvRoad可以很好地推广到不同的检测器、场景和欺骗位置。此外，物理攻击进一步展示了现实世界环境中的实际威胁。



## **8. Decoding Latent Attack Surfaces in LLMs: Prompt Injection via HTML in Web Summarization**

解码LLM中的潜在攻击：通过Web摘要中的HTML提示注入 cs.CR

**SubmitDate**: 2025-11-12    [abs](http://arxiv.org/abs/2509.05831v3) [paper-pdf](None)

**Authors**: Ishaan Verma, Arsheya Yadav

**Abstract**: Large Language Models (LLMs) are increasingly integrated into web-based systems for content summarization, yet their susceptibility to prompt injection attacks remains a pressing concern. In this study, we explore how non-visible HTML elements such as <meta>, aria-label, and alt attributes can be exploited to embed adversarial instructions without altering the visible content of a webpage. We introduce a novel dataset comprising 280 static web pages, evenly divided between clean and adversarial injected versions, crafted using diverse HTML-based strategies. These pages are processed through a browser automation pipeline to extract both raw HTML and rendered text, closely mimicking real-world LLM deployment scenarios. We evaluate two state-of-the-art open-source models, Llama 4 Scout (Meta) and Gemma 9B IT (Google), on their ability to summarize this content. Using both lexical (ROUGE-L) and semantic (SBERT cosine similarity) metrics, along with manual annotations, we assess the impact of these covert injections. Our findings reveal that over 29% of injected samples led to noticeable changes in the Llama 4 Scout summaries, while Gemma 9B IT showed a lower, yet non-trivial, success rate of 15%. These results highlight a critical and largely overlooked vulnerability in LLM driven web pipelines, where hidden adversarial content can subtly manipulate model outputs. Our work offers a reproducible framework and benchmark for evaluating HTML-based prompt injection and underscores the urgent need for robust mitigation strategies in LLM applications involving web content.

摘要: 大型语言模型（LLM）越来越多地集成到基于Web的内容摘要系统中，但它们对即时注入攻击的敏感性仍然是一个紧迫的问题。在这项研究中，我们探索了如何利用非可见的HTML元素（例如<meta>、咏叹调标签和alt属性）来嵌入对抗性指令，而不改变网页的可见内容。我们引入了一个由280个静态网页组成的新颖数据集，平均分为干净和对抗注入版本，使用不同的基于HTML的策略制作。这些页面通过浏览器自动化管道进行处理，以提取原始HTML和渲染文本，密切模仿现实世界的LLM部署场景。我们评估了两个最先进的开源模型Llama 4 Scout（Meta）和Gemma 9 B IT（Google）总结此内容的能力。使用词汇（ROUGE-L）和语义（SBERT cos相似性）指标以及手动注释，我们评估这些隐蔽注入的影响。我们的研究结果显示，超过29%的注射样本导致Llama 4 Scout总结发生了显着变化，而Gemma 9 B IT的成功率较低，但并非微不足道，为15%。这些结果凸显了LLM驱动的网络管道中一个关键且在很大程度上被忽视的漏洞，其中隐藏的对抗内容可以巧妙地操纵模型输出。我们的工作为评估基于HTML的即时注入提供了一个可重复的框架和基准，并强调了涉及Web内容的LLM应用程序中对稳健的缓解策略的迫切需要。



## **9. Exploiting Data Structures for Bypassing and Crashing Anti-Malware Solutions via Telemetry Complexity Attacks**

利用数据结构通过远程通信复杂性攻击来骚扰和崩溃反恶意软件解决方案 cs.CR

**SubmitDate**: 2025-11-13    [abs](http://arxiv.org/abs/2511.04472v2) [paper-pdf](None)

**Authors**: Evgenios Gkritsis, Constantinos Patsakis, George Stergiopoulos

**Abstract**: Anti-malware systems rely on sandboxes, hooks, and telemetry pipelines, including collection agents, serializers, and database backends, to monitor program and system behavior. We show that these data-handling components constitute an exploitable attack surface that can lead to denial-of-analysis (DoA) states without disabling sensors or requiring elevated privileges. As a result, we present Telemetry Complexity Attacks (TCAs), a new class of vulnerabilities that exploit fundamental mismatches between unbounded collection mechanisms and bounded processing capabilities. Our method recursively spawns child processes to generate specially crafted, deeply nested, and oversized telemetry that stresses serialization and storage boundaries, as well as visualization layers, for example, JSON/BSON depth and size limits. Depending on the product, this leads to various inconsistent results, such as truncated or missing behavioral reports, rejected database inserts, serializer recursion and size errors, and unresponsive dashboards. In the latter cases, depending on the solution, the malware under test is either not recorded and/or not presented to the analysts. Therefore, instead of evading sensors, we break the pipeline that stores the data captured by the sensors.   We evaluate our technique against twelve commercial and open-source malware analysis platforms and endpoint detection and response (EDR) solutions. Seven products fail in different stages of the telemetry pipeline; two vendors assigned CVE identifiers (CVE-2025-61301 and CVE-2025-61303), and others issued patches or configuration changes. We discuss root causes and propose mitigation strategies to prevent DoA attacks triggered by adversarial telemetry.

摘要: 反恶意软件系统依赖沙箱、挂钩和遥感管道（包括收集代理、序列化器和数据库后台）来监控程序和系统行为。我们表明，这些数据处理组件构成了一个可利用的攻击表面，可以在不禁用传感器或要求更高特权的情况下导致拒绝分析（DoA）状态。因此，我们提出了远程通信复杂性攻击（TAC），这是一类新型漏洞，利用无界收集机制和有界处理能力之间的根本不匹配。我们的方法以迭代方式产生子进程，以生成特制的、深度嵌套的和超大的遥感数据，其中强调序列化和存储边界以及可视化层，例如，SON/BSON深度和大小限制。根据产品的不同，这会导致各种不一致的结果，例如被截断或缺失的行为报告、被拒绝的数据库插入、序列化器回归和大小错误以及仪表板无响应。在后一种情况下，根据解决方案，测试中的恶意软件要么没有被记录和/或没有呈现给分析师。因此，我们不是逃避传感器，而是打破了存储传感器捕获数据的管道。   我们针对十二个商业和开源恶意软件分析平台以及端点检测和响应（EDR）解决方案评估我们的技术。七个产品在遥测管道的不同阶段失败;两个供应商分配了CVE标识符（CVE-2025-61301和CVE-2025-61303），其他供应商发布了补丁或配置更改。我们讨论了根本原因，并提出了缓解策略，以防止由对抗遥测触发的DoA攻击。



## **10. Keep on Going: Learning Robust Humanoid Motion Skills via Selective Adversarial Training**

继续前进：通过选择性对抗训练学习稳健的人形运动技能 cs.RO

13 pages, 10 figures, AAAI2026

**SubmitDate**: **NEW** 2025-11-14    [abs](http://arxiv.org/abs/2507.08303v3) [paper-pdf](None)

**Authors**: Yang Zhang, Zhanxiang Cao, Buqing Nie, Haoyang Li, Zhong Jiangwei, Qiao Sun, Xiaoyi Hu, Xiaokang Yang, Yue Gao

**Abstract**: Humanoid robots are expected to operate reliably over long horizons while executing versatile whole-body skills. Yet Reinforcement Learning (RL) motion policies typically lose stability under prolonged operation, sensor/actuator noise, and real world disturbances. In this work, we propose a Selective Adversarial Attack for Robust Training (SA2RT) to enhance the robustness of motion skills. The adversary is learned to identify and sparsely perturb the most vulnerable states and actions under an attack-budget constraint, thereby exposing true weakness without inducing conservative overfitting. The resulting non-zero sum, alternating optimization continually strengthens the motion policy against the strongest discovered attacks. We validate our approach on the Unitree G1 humanoid robot across perceptive locomotion and whole-body control tasks. Experimental results show that adversarially trained policies improve the terrain traversal success rate by 40%, reduce the trajectory tracking error by 32%, and maintain long horizon mobility and tracking performance. Together, these results demonstrate that selective adversarial attacks are an effective driver for learning robust, long horizon humanoid motion skills.

摘要: 类人机器人有望在长时间内可靠地运行，同时执行多功能的全身技能。然而，强化学习（RL）运动策略通常会在长时间操作、传感器/执行器噪声和现实世界干扰下失去稳定性。在这项工作中，我们提出了一个选择性对抗攻击鲁棒训练（SA2RT），以提高运动技能的鲁棒性。对手学会了在攻击预算约束下识别和稀疏干扰最脆弱的状态和行动，从而暴露真正的弱点，而不会引起保守的过度拟合。由此产生的非零和交替优化不断加强针对发现的最强攻击的运动策略。我们在Unitree G1人形机器人上验证了我们的方法，涵盖感知运动和全身控制任务。实验结果表明，对抗训练的策略使地形穿越成功率提高了40%，轨迹跟踪误差降低了32%，并保持了长视野移动性和跟踪性能。总而言之，这些结果表明选择性对抗攻击是学习稳健、长视野人形运动技能的有效驱动力。



## **11. Potent but Stealthy: Rethink Profile Pollution against Sequential Recommendation via Bi-level Constrained Reinforcement Paradigm**

有效但隐蔽：通过双层约束强化范式重新思考针对顺序推荐的个人资料污染 cs.LG

**SubmitDate**: **NEW** 2025-11-14    [abs](http://arxiv.org/abs/2511.09392v2) [paper-pdf](None)

**Authors**: Jiajie Su, Zihan Nan, Yunshan Ma, Xiaobo Xia, Xiaohua Feng, Weiming Liu, Xiaolin Zheng, Chaochao Chen

**Abstract**: Sequential Recommenders, which exploit dynamic user intents through interaction sequences, is vulnerable to adversarial attacks. While existing attacks primarily rely on data poisoning, they require large-scale user access or fake profiles thus lacking practicality. In this paper, we focus on the Profile Pollution Attack that subtly contaminates partial user interactions to induce targeted mispredictions. Previous PPA methods suffer from two limitations, i.e., i) over-reliance on sequence horizon impact restricts fine-grained perturbations on item transitions, and ii) holistic modifications cause detectable distribution shifts. To address these challenges, we propose a constrained reinforcement driven attack CREAT that synergizes a bi-level optimization framework with multi-reward reinforcement learning to balance adversarial efficacy and stealthiness. We first develop a Pattern Balanced Rewarding Policy, which integrates pattern inversion rewards to invert critical patterns and distribution consistency rewards to minimize detectable shifts via unbalanced co-optimal transport. Then we employ a Constrained Group Relative Reinforcement Learning paradigm, enabling step-wise perturbations through dynamic barrier constraints and group-shared experience replay, achieving targeted pollution with minimal detectability. Extensive experiments demonstrate the effectiveness of CREAT.

摘要: 顺序推荐器通过交互序列利用动态用户意图，很容易受到对抗攻击。虽然现有的攻击主要依赖于数据中毒，但它们需要大规模用户访问或伪造个人资料，因此缺乏实用性。在本文中，我们重点关注个人资料污染攻击，它微妙地污染部分用户交互以引发有针对性的错误预测。以前的PPA方法有两个局限性，即i）过度依赖序列范围影响限制了对项目转变的细粒度扰动，并且ii）整体修改会导致可检测的分布变化。为了应对这些挑战，我们提出了一种受约束的强化驱动攻击CREAT，该攻击将双层优化框架与多奖励强化学习相结合，以平衡对抗功效和隐蔽性。我们首先开发了模式平衡奖励政策，该政策集成了模式倒置奖励以倒置关键模式，并集成了分布一致性奖励以最大限度地减少通过不平衡协同优传输可检测到的转变。然后，我们采用受约束的群体相对强化学习范式，通过动态障碍约束和群体共享体验回放实现分步扰动，以最小的可检测性实现有针对性的污染。大量实验证明了CREAT的有效性。



## **12. On Stealing Graph Neural Network Models**

关于窃取图神经网络模型 cs.LG

Accepted at AAAI 2026

**SubmitDate**: **NEW** 2025-11-14    [abs](http://arxiv.org/abs/2511.07170v2) [paper-pdf](None)

**Authors**: Marcin Podhajski, Jan Dubiński, Franziska Boenisch, Adam Dziedzic, Agnieszka Pręgowska, Tomasz P. Michalak

**Abstract**: Current graph neural network (GNN) model-stealing methods rely heavily on queries to the victim model, assuming no hard query limits. However, in reality, the number of allowed queries can be severely limited. In this paper, we demonstrate how an adversary can extract a GNN with very limited interactions with the model. Our approach first enables the adversary to obtain the model backbone without making direct queries to the victim model and then to strategically utilize a fixed query limit to extract the most informative data. The experiments on eight real-world datasets demonstrate the effectiveness of the attack, even under a very restricted query limit and under defense against model extraction in place. Our findings underscore the need for robust defenses against GNN model extraction threats.

摘要: 当前的图神经网络（GNN）模型窃取方法严重依赖于对受害者模型的查询，假设没有硬查询限制。然而，实际上，允许的查询数量可能会受到严格限制。在本文中，我们演示了对手如何在与模型的交互非常有限的情况下提取GNN。我们的方法首先使对手能够获得模型主干，而无需直接查询受害者模型，然后战略性地利用固定的查询限制来提取信息最丰富的数据。对八个现实世界数据集的实验证明了攻击的有效性，即使在非常有限的查询限制和针对模型提取的防御下也是如此。我们的研究结果强调了针对GNN模型提取威胁的强有力防御的必要性。



## **13. SEBA: Sample-Efficient Black-Box Attacks on Visual Reinforcement Learning**

SEBA：对视觉强化学习的样本高效黑匣子攻击 cs.LG

**SubmitDate**: **NEW** 2025-11-14    [abs](http://arxiv.org/abs/2511.09681v1) [paper-pdf](None)

**Authors**: Tairan Huang, Yulin Jin, Junxu Liu, Qingqing Ye, Haibo Hu

**Abstract**: Visual reinforcement learning has achieved remarkable progress in visual control and robotics, but its vulnerability to adversarial perturbations remains underexplored. Most existing black-box attacks focus on vector-based or discrete-action RL, and their effectiveness on image-based continuous control is limited by the large action space and excessive environment queries. We propose SEBA, a sample-efficient framework for black-box adversarial attacks on visual RL agents. SEBA integrates a shadow Q model that estimates cumulative rewards under adversarial conditions, a generative adversarial network that produces visually imperceptible perturbations, and a world model that simulates environment dynamics to reduce real-world queries. Through a two-stage iterative training procedure that alternates between learning the shadow model and refining the generator, SEBA achieves strong attack performance while maintaining efficiency. Experiments on MuJoCo and Atari benchmarks show that SEBA significantly reduces cumulative rewards, preserves visual fidelity, and greatly decreases environment interactions compared to prior black-box and white-box methods.

摘要: 视觉强化学习在视觉控制和机器人技术方面取得了显着进展，但其对对抗性扰动的脆弱性仍然没有得到充分的研究。大多数现有的黑匣子攻击都集中在基于载体或离散动作的RL上，其对基于图像的连续控制的有效性受到大动作空间和过多环境查询的限制。我们提出了SEBA，这是一个用于对视觉RL代理进行黑匣子对抗攻击的样本高效框架。SEBA集成了一个在对抗条件下估计累积回报的影子Q模型、一个产生视觉上难以感知的扰动的生成对抗网络，以及一个模拟环境动态以减少现实世界查询的世界模型。通过在学习影子模型和细化生成器之间交替的两阶段迭代训练过程，SEBA在保持效率的同时实现了强大的攻击性能。MuJoCo和Atari基准测试的实验表明，与之前的黑匣子和白盒方法相比，SEBA显着减少了累积奖励，保留了视觉保真度，并大大减少了环境交互。



## **14. Phantom Menace: Exploring and Enhancing the Robustness of VLA Models against Physical Sensor Attacks**

Phantom Menace：探索和增强VLA模型对物理传感器攻击的鲁棒性 cs.RO

Accepted by AAAI 2026

**SubmitDate**: **NEW** 2025-11-14    [abs](http://arxiv.org/abs/2511.10008v1) [paper-pdf](None)

**Authors**: Xuancun Lu, Jiaxiang Chen, Shilin Xiao, Zizhi Jin, Zhangrui Chen, Hanwen Yu, Bohan Qian, Ruochen Zhou, Xiaoyu Ji, Wenyuan Xu

**Abstract**: Vision-Language-Action (VLA) models revolutionize robotic systems by enabling end-to-end perception-to-action pipelines that integrate multiple sensory modalities, such as visual signals processed by cameras and auditory signals captured by microphones. This multi-modality integration allows VLA models to interpret complex, real-world environments using diverse sensor data streams. Given the fact that VLA-based systems heavily rely on the sensory input, the security of VLA models against physical-world sensor attacks remains critically underexplored.   To address this gap, we present the first systematic study of physical sensor attacks against VLAs, quantifying the influence of sensor attacks and investigating the defenses for VLA models. We introduce a novel ``Real-Sim-Real'' framework that automatically simulates physics-based sensor attack vectors, including six attacks targeting cameras and two targeting microphones, and validates them on real robotic systems. Through large-scale evaluations across various VLA architectures and tasks under varying attack parameters, we demonstrate significant vulnerabilities, with susceptibility patterns that reveal critical dependencies on task types and model designs. We further develop an adversarial-training-based defense that enhances VLA robustness against out-of-distribution physical perturbations caused by sensor attacks while preserving model performance. Our findings expose an urgent need for standardized robustness benchmarks and mitigation strategies to secure VLA deployments in safety-critical environments.

摘要: 视觉-语言-动作（VLA）模型通过实现端到端的感知到动作管道，彻底改变了机器人系统，该管道集成了多种感官模式，例如由摄像机处理的视觉信号和由麦克风捕获的听觉信号。这种多模式集成使VLA模型能够使用不同的传感器数据流来解释复杂的现实世界环境。鉴于基于VLA的系统严重依赖感官输入，VLA模型对抗物理世界传感器攻击的安全性仍然严重不足。   为了弥补这一差距，我们首次对针对VLA的物理传感器攻击进行了系统研究，量化了传感器攻击的影响并调查VLA模型的防御。我们引入了一种新型的“Real-Sim-Real”框架，该框架自动模拟基于物理的传感器攻击载体，包括六次针对摄像头和两个针对麦克风的攻击，并在真实的机器人系统上对其进行验证。通过在不同攻击参数下对各种VLA架构和任务进行大规模评估，我们展示了显着的漏洞，其易感性模式揭示了对任务类型和模型设计的关键依赖性。我们进一步开发了一种基于对抗训练的防御，可以增强VLA对传感器攻击引起的分布外物理扰动的鲁棒性，同时保持模型性能。我们的研究结果揭示了迫切需要标准化的稳健性基准和缓解策略，以确保VLA在安全关键环境中的部署。



## **15. EnchTable: Unified Safety Alignment Transfer in Fine-tuned Large Language Models**

EnchTable：微调大型语言模型中的统一安全对齐转移 cs.CL

Accepted by IEEE Symposium on Security and Privacy (S&P) 2026

**SubmitDate**: **NEW** 2025-11-14    [abs](http://arxiv.org/abs/2511.09880v1) [paper-pdf](None)

**Authors**: Jialin Wu, Kecen Li, Zhicong Huang, Xinfeng Li, Xiaofeng Wang, Cheng Hong

**Abstract**: Many machine learning models are fine-tuned from large language models (LLMs) to achieve high performance in specialized domains like code generation, biomedical analysis, and mathematical problem solving. However, this fine-tuning process often introduces a critical vulnerability: the systematic degradation of safety alignment, undermining ethical guidelines and increasing the risk of harmful outputs. Addressing this challenge, we introduce EnchTable, a novel framework designed to transfer and maintain safety alignment in downstream LLMs without requiring extensive retraining. EnchTable leverages a Neural Tangent Kernel (NTK)-based safety vector distillation method to decouple safety constraints from task-specific reasoning, ensuring compatibility across diverse model architectures and sizes. Additionally, our interference-aware merging technique effectively balances safety and utility, minimizing performance compromises across various task domains. We implemented a fully functional prototype of EnchTable on three different task domains and three distinct LLM architectures, and evaluated its performance through extensive experiments on eleven diverse datasets, assessing both utility and model safety. Our evaluations include LLMs from different vendors, demonstrating EnchTable's generalization capability. Furthermore, EnchTable exhibits robust resistance to static and dynamic jailbreaking attacks, outperforming vendor-released safety models in mitigating adversarial prompts. Comparative analyses with six parameter modification methods and two inference-time alignment baselines reveal that EnchTable achieves a significantly lower unsafe rate, higher utility score, and universal applicability across different task domains. Additionally, we validate EnchTable can be seamlessly integrated into various deployment pipelines without significant overhead.

摘要: 许多机器学习模型都是从大型语言模型（LLM）进行微调的，以在代码生成、生物医学分析和数学问题解决等专业领域实现高性能。然而，这种微调过程往往会引入一个关键的漏洞：安全一致性的系统性退化，破坏道德准则并增加有害输出的风险。为了应对这一挑战，我们引入了EnchTable，这是一个新颖的框架，旨在转移和维护下游LLM的安全一致，而无需进行广泛的再培训。EnchTable利用基于神经切向核（NTK）的安全向量提炼方法将安全约束与特定任务推理脱钩，确保不同模型架构和大小之间的兼容性。此外，我们的干扰感知合并技术有效地平衡了安全性和实用性，最大限度地减少了各个任务域的性能损害。我们在三个不同的任务域和三个不同的LLM架构上实现了功能齐全的EnchTable原型，并通过对十一个不同数据集的广泛实验评估了其性能，评估了效用和模型安全性。我们的评估包括来自不同供应商的LLM，展示了EnchTable的概括能力。此外，EnchTable对静态和动态越狱攻击表现出强大的抵抗力，在减轻对抗提示方面优于供应商发布的安全模型。对六种参数修改方法和两种推断时间对齐基线的比较分析表明，EnchTable实现了显着较低的不安全率、较高的效用评分以及跨不同任务领域的普遍适用性。此外，我们还验证了EnchTable可以无缝集成到各种部署管道中，而无需承担重大费用。



## **16. Privacy on the Fly: A Predictive Adversarial Transformation Network for Mobile Sensor Data**

动态隐私：移动传感器数据的预测性对抗转换网络 cs.CR

accepted by AAAI 2026 (oral)

**SubmitDate**: 2025-11-13    [abs](http://arxiv.org/abs/2511.07242v2) [paper-pdf](None)

**Authors**: Tianle Song, Chenhao Lin, Yang Cao, Zhengyu Zhao, Jiahao Sun, Chong Zhang, Le Yang, Chao Shen

**Abstract**: Mobile motion sensors such as accelerometers and gyroscopes are now ubiquitously accessible by third-party apps via standard APIs. While enabling rich functionalities like activity recognition and step counting, this openness has also enabled unregulated inference of sensitive user traits, such as gender, age, and even identity, without user consent. Existing privacy-preserving techniques, such as GAN-based obfuscation or differential privacy, typically require access to the full input sequence, introducing latency that is incompatible with real-time scenarios. Worse, they tend to distort temporal and semantic patterns, degrading the utility of the data for benign tasks like activity recognition. To address these limitations, we propose the Predictive Adversarial Transformation Network (PATN), a real-time privacy-preserving framework that leverages historical signals to generate adversarial perturbations proactively. The perturbations are applied immediately upon data acquisition, enabling continuous protection without disrupting application functionality. Experiments on two datasets demonstrate that PATN substantially degrades the performance of privacy inference models, achieving Attack Success Rate (ASR) of 40.11% and 44.65% (reducing inference accuracy to near-random) and increasing the Equal Error Rate (EER) from 8.30% and 7.56% to 41.65% and 46.22%. On ASR, PATN outperforms baseline methods by 16.16% and 31.96%, respectively.

摘要: 现在，第三方应用程序通过标准API无处不在地访问加速度计和陀螺仪等移动运动传感器。这种开放性在实现活动识别和步数等丰富功能的同时，还可以在未经用户同意的情况下对敏感的用户特征（例如性别、年龄甚至身份）进行不受监管的推断。现有的隐私保护技术，例如基于GAN的模糊或差异隐私，通常需要访问完整的输入序列，从而引入与实时场景不兼容的延迟。更糟糕的是，它们往往会扭曲时间和语义模式，降低数据对活动识别等良性任务的实用性。为了解决这些限制，我们提出了预测对抗转换网络（PATN），这是一个实时隐私保护框架，可以利用历史信号主动生成对抗性扰动。数据采集后立即应用扰动，从而实现持续保护，而不会中断应用程序功能。对两个数据集的实验表明，PATN大幅降低了隐私推理模型的性能，实现了40.11%和44.65%的攻击成功率（ASB）（将推理准确率降低到接近随机），并将等错误率（EER）从8.30%和7.56%提高到41.65%和46.22%。在ASB方面，PATN分别比基线方法高出16.16%和31.96%。



## **17. Reasoning Up the Instruction Ladder for Controllable Language Models**

可控语言模型的指令阶梯推理 cs.CL

**SubmitDate**: 2025-11-13    [abs](http://arxiv.org/abs/2511.04694v2) [paper-pdf](None)

**Authors**: Zishuo Zheng, Vidhisha Balachandran, Chan Young Park, Faeze Brahman, Sachin Kumar

**Abstract**: As large language model (LLM) based systems take on high-stakes roles in real-world decision-making, they must reconcile competing instructions from multiple sources (e.g., model developers, users, and tools) within a single prompt context. Thus, enforcing an instruction hierarchy (IH) in LLMs, where higher-level directives override lower-priority requests, is critical for the reliability and controllability of LLMs. In this work, we reframe instruction hierarchy resolution as a reasoning task. Specifically, the model must first "think" about the relationship between a given user prompt and higher-priority (system) instructions before generating a response. To enable this capability via training, we construct VerIH, an instruction hierarchy dataset of constraint-following tasks with verifiable answers. This dataset comprises both aligned and conflicting system-user instructions. We show that lightweight reinforcement learning with VerIH effectively transfers general reasoning capabilities of models to instruction prioritization. Our finetuned models achieve consistent improvements on instruction following and instruction hierarchy benchmarks. This reasoning ability also generalizes to safety-critical settings beyond the training distribution. By treating safety issues as resolving conflicts between adversarial user inputs and predefined higher-priority policies, our trained model enhances robustness against jailbreak and prompt injection attacks. These results demonstrate that reasoning over instruction hierarchies provides a practical path to reliable LLMs, where updates to system prompts yield controllable and robust changes in model behavior.

摘要: 随着基于大型语言模型（LLM）的系统在现实世界的决策中扮演着高风险的角色，它们必须协调来自多个来源的竞争指令（例如，模型开发人员、用户和工具）在单个提示上下文中。因此，在LLM中强制执行指令层次结构（IHS）（其中更高级的指令优先于较低优先级的请求）对于LLM的可靠性和可控性至关重要。在这项工作中，我们将指令层次结构分解重新构建为一项推理任务。具体来说，模型必须在生成响应之前首先“思考”给定用户提示和更高优先级（系统）指令之间的关系。为了通过训练实现这种能力，我们构建了VerIHS，这是一个具有可验证答案的约束遵循任务的指令层次数据集。此数据集包括对齐和冲突的系统用户指令。我们表明，使用VerIHS的轻量级强化学习可以有效地将模型的一般推理能力转移到指令优先级。我们的微调模型在指令遵循和指令层次基准方面实现了一致的改进。这种推理能力还推广到培训分布以外的安全关键环境。通过将安全问题视为解决敌对用户输入和预定义的高优先级策略之间的冲突，我们训练的模型增强了针对越狱和即时注入攻击的鲁棒性。这些结果表明，对指令层次结构的推理提供了一条通往可靠LLM的实用途径，其中对系统提示的更新会产生模型行为的可控且稳健的变化。



## **18. Removal Attack and Defense on AI-generated Content Latent-based Watermarking**

对人工智能生成的内容基于潜伏的水印的删除攻击和防御 cs.CR

**SubmitDate**: 2025-11-13    [abs](http://arxiv.org/abs/2509.11745v3) [paper-pdf](None)

**Authors**: De Zhang Lee, Han Fang, Hanyi Wang, Ee-Chien Chang

**Abstract**: Digital watermarks can be embedded into AI-generated content (AIGC) by initializing the generation process with starting points sampled from a secret distribution. When combined with pseudorandom error-correcting codes, such watermarked outputs can remain indistinguishable from unwatermarked objects, while maintaining robustness under whitenoise. In this paper, we go beyond indistinguishability and investigate security under removal attacks. We demonstrate that indistinguishability alone does not necessarily guarantee resistance to adversarial removal. Specifically, we propose a novel attack that exploits boundary information leaked by the locations of watermarked objects. This attack significantly reduces the distortion required to remove watermarks -- by up to a factor of $15 \times$ compared to a baseline whitenoise attack under certain settings. To mitigate such attacks, we introduce a defense mechanism that applies a secret transformation to hide the boundary, and prove that the secret transformation effectively rendering any attacker's perturbations equivalent to those of a naive whitenoise adversary. Our empirical evaluations, conducted on multiple versions of Stable Diffusion, validate the effectiveness of both the attack and the proposed defense, highlighting the importance of addressing boundary leakage in latent-based watermarking schemes.

摘要: 通过使用从秘密分发中采样的起点初始化生成过程，数字水印可以嵌入到人工智能生成的内容（AIGC）中。当与伪随机纠错码相结合时，这种加水印的输出可以保持与未加水印的对象不可区分，同时在白噪声下保持鲁棒性。在本文中，我们超越了不可撤销性，并调查了删除攻击下的安全性。我们证明，仅靠不可撤销性并不一定保证对对抗性清除的抵抗。具体来说，我们提出了一种新颖的攻击，该攻击利用带有水印的对象位置泄露的边界信息。与某些设置下的基线白噪音攻击相比，此攻击显着减少了删除水印所需的失真，最多可减少15美元\x $。为了减轻此类攻击，我们引入了一种防御机制，该机制应用秘密转换来隐藏边界，并证明秘密转换有效地使任何攻击者的扰动等效于天真白噪音对手的扰动。我们对多个版本的稳定扩散进行了经验评估，验证了攻击和拟议防御的有效性，强调了解决基于潜伏的水印方案中边界泄漏的重要性。



## **19. GuardFed: A Trustworthy Federated Learning Framework Against Dual-Facet Attacks**

GuardFed：一个值得信赖的联邦学习框架，对抗双重攻击 cs.LG

**SubmitDate**: 2025-11-13    [abs](http://arxiv.org/abs/2511.09294v1) [paper-pdf](None)

**Authors**: Yanli Li, Yanan Zhou, Zhongliang Guo, Nan Yang, Yuning Zhang, Huaming Chen, Dong Yuan, Weiping Ding, Witold Pedrycz

**Abstract**: Federated learning (FL) enables privacy-preserving collaborative model training but remains vulnerable to adversarial behaviors that compromise model utility or fairness across sensitive groups. While extensive studies have examined attacks targeting either objective, strategies that simultaneously degrade both utility and fairness remain largely unexplored. To bridge this gap, we introduce the Dual-Facet Attack (DFA), a novel threat model that concurrently undermines predictive accuracy and group fairness. Two variants, Synchronous DFA (S-DFA) and Split DFA (Sp-DFA), are further proposed to capture distinct real-world collusion scenarios. Experimental results show that existing robust FL defenses, including hybrid aggregation schemes, fail to resist DFAs effectively. To counter these threats, we propose GuardFed, a self-adaptive defense framework that maintains a fairness-aware reference model using a small amount of clean server data augmented with synthetic samples. In each training round, GuardFed computes a dual-perspective trust score for every client by jointly evaluating its utility deviation and fairness degradation, thereby enabling selective aggregation of trustworthy updates. Extensive experiments on real-world datasets demonstrate that GuardFed consistently preserves both accuracy and fairness under diverse non-IID and adversarial conditions, achieving state-of-the-art performance compared with existing robust FL methods.

摘要: 联合学习（FL）可以实现保护隐私的协作模型训练，但仍然容易受到损害敏感群体之间模型效用或公平性的对抗行为的影响。虽然大量研究已经检查了针对这两个目标的攻击，但同时降低效用和公平性的策略在很大程度上仍然没有被探索。为了弥合这一差距，我们引入了双面攻击（PFA），这是一种新型威胁模型，它同时破坏了预测准确性和群体公平性。进一步提出了两个变体，同步PFA（S-PFA）和分离PFA（Sp-PFA），以捕获不同的现实世界共谋场景。实验结果表明，现有的鲁棒FL防御（包括混合聚合方案）无法有效抵抗DSA。为了应对这些威胁，我们提出了GuardFed，这是一个自适应防御框架，它使用少量用合成样本增强的干净服务器数据来维护公平意识的参考模型。在每轮训练中，GuardFed通过联合评估其效用偏差和公平性退化来计算每个客户的双视角信任分数，从而实现对值得信赖的更新的选择性聚合。对现实世界数据集的广泛实验表明，GuardFed在不同的非IID和对抗条件下始终保持准确性和公平性，与现有的稳健FL方法相比，实现了最先进的性能。



## **20. Breaking the Adversarial Robustness-Performance Trade-off in Text Classification via Manifold Purification**

通过Manifold Purpose打破文本分类中的对抗鲁棒性与性能权衡 cs.CL

9 pages,3 figures

**SubmitDate**: 2025-11-12    [abs](http://arxiv.org/abs/2511.07888v1) [paper-pdf](None)

**Authors**: Chenhao Dang, Jing Ma

**Abstract**: A persistent challenge in text classification (TC) is that enhancing model robustness against adversarial attacks typically degrades performance on clean data. We argue that this challenge can be resolved by modeling the distribution of clean samples in the encoder embedding manifold. To this end, we propose the Manifold-Correcting Causal Flow (MC^2F), a two-module system that operates directly on sentence embeddings. A Stratified Riemannian Continuous Normalizing Flow (SR-CNF) learns the density of the clean data manifold. It identifies out-of-distribution embeddings, which are then corrected by a Geodesic Purification Solver. This solver projects adversarial points back onto the learned manifold via the shortest path, restoring a clean, semantically coherent representation. We conducted extensive evaluations on text classification (TC) across three datasets and multiple adversarial attacks. The results demonstrate that our method, MC^2F, not only establishes a new state-of-the-art in adversarial robustness but also fully preserves performance on clean data, even yielding modest gains in accuracy.

摘要: 文本分类（TC）中的一个持续挑战是增强模型对对抗性攻击的鲁棒性通常会降低干净数据的性能。我们认为，这一挑战可以通过对编码器嵌入流形中干净样本的分布进行建模来解决。为此，我们提出了流形校正因果流（MC^2F），这是一个直接对句子嵌入进行操作的两模块系统。分层黎曼连续规范化流（SR-CNF）学习干净数据流形的密度。它识别分布外嵌入，然后由Geodesic Purification Solver进行纠正。这个求解器通过最短路径将对抗点投射回学习的多管齐下，恢复干净、语义连贯的表示。我们对三个数据集和多个对抗性攻击的文本分类（TC）进行了广泛的评估。结果表明，我们的方法MC ' 2 F不仅在对抗鲁棒性方面建立了新的最新水平，而且还完全保留了干净数据上的性能，甚至在准确性方面产生了适度的提高。



## **21. Multimodal Adversarial Defense for Vision-Language Models by Leveraging One-To-Many Relationships**

利用一对多关系对视觉语言模型进行多模式对抗防御 cs.CV

WACV 2026 Accepted

**SubmitDate**: 2025-11-13    [abs](http://arxiv.org/abs/2405.18770v4) [paper-pdf](None)

**Authors**: Futa Waseda, Antonio Tejero-de-Pablos, Isao Echizen

**Abstract**: Pre-trained vision-language (VL) models are highly vulnerable to adversarial attacks. However, existing defense methods primarily focus on image classification, overlooking two key aspects of VL tasks: multimodal attacks, where both image and text can be perturbed, and the one-to-many relationship of images and texts, where a single image can correspond to multiple textual descriptions and vice versa (1:N and N:1). This work is the first to explore defense strategies against multimodal attacks in VL tasks, whereas prior VL defense methods focus on vision robustness. We propose multimodal adversarial training (MAT), which incorporates adversarial perturbations in both image and text modalities during training, significantly outperforming existing unimodal defenses. Furthermore, we discover that MAT is limited by deterministic one-to-one (1:1) image-text pairs in VL training data. To address this, we conduct a comprehensive study on leveraging one-to-many relationships to enhance robustness, investigating diverse augmentation techniques. Our analysis shows that, for a more effective defense, augmented image-text pairs should be well-aligned, diverse, yet avoid distribution shift -- conditions overlooked by prior research. This work pioneers defense strategies against multimodal attacks, providing insights for building robust VLMs from both optimization and data perspectives.

摘要: 预先训练的视觉语言（VL）模型非常容易受到对抗性攻击。然而，现有的防御方法主要集中在图像分类上，忽略了VL任务的两个关键方面：多模态攻击，其中图像和文本都可以被扰动，以及图像和文本的一对多关系，其中单个图像可以对应于多个文本描述，反之亦然（1：N和N：1）。这项工作是第一次探索防御策略，对多模态攻击的VL任务，而以前的VL防御方法集中在视觉鲁棒性。我们提出了多模态对抗训练（MAT），它在训练过程中将对抗扰动纳入图像和文本模态，显著优于现有的单峰防御。此外，我们发现MAT受到DL训练数据中确定性的一对一（1：1）图像-文本对的限制。为了解决这个问题，我们对利用一对多关系来增强稳健性进行了全面研究，并调查了各种增强技术。我们的分析表明，为了更有效的防御，增强的图像-文本对应该对齐、多样化，但要避免分布变化--这些条件被之前的研究所忽视。这项工作开创了针对多模式攻击的防御策略，为从优化和数据角度构建稳健的VLM提供了见解。



## **22. Exploring the Adversarial Robustness of Face Forgery Detection with Decision-based Black-box Attacks**

基于决策黑箱攻击的人脸伪造检测对抗鲁棒性研究 cs.CV

Accepted by Knowledge-Based Systems

**SubmitDate**: 2025-11-13    [abs](http://arxiv.org/abs/2310.12017v2) [paper-pdf](None)

**Authors**: Zhaoyu Chen, Bo Li, Kaixun Jiang, Shuang Wu, Shouhong Ding, Wenqiang Zhang

**Abstract**: Face forgery generation technologies generate vivid faces, which have raised public concerns about security and privacy. Many intelligent systems, such as electronic payment and identity verification, rely on face forgery detection. Although face forgery detection has successfully distinguished fake faces, recent studies have demonstrated that face forgery detectors are very vulnerable to adversarial examples. Meanwhile, existing attacks rely on network architectures or training datasets instead of the predicted labels, which leads to a gap in attacking deployed applications. To narrow this gap, we first explore the decision-based attacks on face forgery detection. We identify challenges in directly applying existing decision-based attacks, such as perturbation initialization failure and reduced image quality. To overcome these issues, we propose cross-task perturbation to handle initialization failures by utilizing the high correlation of face features on different tasks. Additionally, inspired by the use of frequency cues in face forgery detection, we introduce the frequency decision-based attack. This attack involves adding perturbations in the frequency domain while constraining visual quality in the spatial domain. Finally, extensive experiments demonstrate that our method achieves state-of-the-art attack performance on FaceForensics++, CelebDF, and industrial APIs, with high query efficiency and guaranteed image quality. Further, the fake faces by our method can pass face forgery detection and face recognition, which exposes the security problems of face forgery detectors.

摘要: 面部伪造生成技术生成生动的面部，这引发了公众对安全和隐私的担忧。电子支付和身份验证等许多智能系统都依赖于人脸伪造检测。尽管人脸伪造检测已经成功区分了假人脸，但最近的研究表明，人脸伪造检测器非常容易受到对抗性例子的影响。与此同时，现有的攻击依赖于网络架构或训练数据集，而不是预测的标签，这导致攻击部署的应用程序存在差距。为了缩小这一差距，我们首先探讨了对人脸伪造检测的基于决策的攻击。我们发现了直接应用现有基于决策的攻击的挑战，例如扰动初始化失败和图像质量下降。为了克服这些问题，我们提出了跨任务扰动来通过利用不同任务上面部特征的高度相关性来处理初始化失败。此外，受到面部伪造检测中使用频率线索的启发，我们引入了基于频率决策的攻击。这种攻击涉及在频域中添加扰动，同时限制空间域中的视觉质量。最后，大量实验表明，我们的方法在FaceForensics++、CelebDF和工业API上实现了最先进的攻击性能，并且查询效率高，图像质量有保证。此外，我们的方法检测到的假人脸可以通过人脸伪造检测和人脸识别，这暴露了人脸伪造检测器的安全问题。



## **23. Trapped by Their Own Light: Deployable and Stealth Retroreflective Patch Attacks on Traffic Sign Recognition Systems**

被自己的光困住：对交通标志识别系统的可部署和隐形反光补丁攻击 cs.CR

**SubmitDate**: **NEW** 2025-11-14    [abs](http://arxiv.org/abs/2511.10050v1) [paper-pdf](None)

**Authors**: Go Tsuruoka, Takami Sato, Qi Alfred Chen, Kazuki Nomoto, Ryunosuke Kobayashi, Yuna Tanaka, Tatsuya Mori

**Abstract**: Traffic sign recognition plays a critical role in ensuring safe and efficient transportation of autonomous vehicles but remain vulnerable to adversarial attacks using stickers or laser projections. While existing attack vectors demonstrate security concerns, they suffer from visual detectability or implementation constraints, suggesting unexplored vulnerability surfaces in TSR systems. We introduce the Adversarial Retroreflective Patch (ARP), a novel attack vector that combines the high deployability of patch attacks with the stealthiness of laser projections by utilizing retroreflective materials activated only under victim headlight illumination. We develop a retroreflection simulation method and employ black-box optimization to maximize attack effectiveness. ARP achieves $\geq$93.4\% success rate in dynamic scenarios at 35 meters and $\geq$60\% success rate against commercial TSR systems in real-world conditions. Our user study demonstrates that ARP attacks maintain near-identical stealthiness to benign signs while achieving $\geq$1.9\% higher stealthiness scores than previous patch attacks. We propose the DPR Shield defense, employing strategically placed polarized filters, which achieves $\geq$75\% defense success rates for stop signs and speed limit signs against micro-prism patches.

摘要: 交通标志识别在确保自动驾驶车辆的安全高效运输方面发挥着关键作用，但仍然容易受到使用贴纸或激光投影的对抗攻击。虽然现有的攻击载体表现出安全问题，但它们受到视觉可检测性或实现限制，这表明TLR系统中存在未探索的漏洞表面。我们介绍了对抗性逆反射贴片（ARP），这是一种新型攻击载体，通过利用仅在受害者前灯照明下激活的逆反射材料，将贴片攻击的高可部署性与激光投影的隐蔽性结合起来。我们开发了一种回射模拟方法并采用黑匣子优化来最大限度地提高攻击效果。ARP在35米处的动态场景中实现了$93.4%的成功率，在现实世界条件下针对商业TLR系统的成功率为$60%。我们的用户研究表明，ARP攻击保持了与良性迹象几乎相同的隐蔽性，同时实现了比之前的补丁攻击高1.9%的隐蔽性分数。我们提出了DPR盾牌防御，采用策略性放置的极化过滤器，对于针对微棱镜贴片的停车标志和限速标志的防御成功率达到了75%。



## **24. Debiased Dual-Invariant Defense for Adversarially Robust Person Re-Identification**

对抗稳健人重新识别的去偏二元不变防御 cs.CV

Accepted by AAAI 2026

**SubmitDate**: **NEW** 2025-11-14    [abs](http://arxiv.org/abs/2511.09933v1) [paper-pdf](None)

**Authors**: Yuhang Zhou, Yanxiang Zhao, Zhongyun Hua, Zhipu Liu, Zhaoquan Gu, Qing Liao, Leo Yu Zhang

**Abstract**: Person re-identification (ReID) is a fundamental task in many real-world applications such as pedestrian trajectory tracking. However, advanced deep learning-based ReID models are highly susceptible to adversarial attacks, where imperceptible perturbations to pedestrian images can cause entirely incorrect predictions, posing significant security threats. Although numerous adversarial defense strategies have been proposed for classification tasks, their extension to metric learning tasks such as person ReID remains relatively unexplored. Moreover, the several existing defenses for person ReID fail to address the inherent unique challenges of adversarially robust ReID. In this paper, we systematically identify the challenges of adversarial defense in person ReID into two key issues: model bias and composite generalization requirements. To address them, we propose a debiased dual-invariant defense framework composed of two main phases. In the data balancing phase, we mitigate model bias using a diffusion-model-based data resampling strategy that promotes fairness and diversity in training data. In the bi-adversarial self-meta defense phase, we introduce a novel metric adversarial training approach incorporating farthest negative extension softening to overcome the robustness degradation caused by the absence of classifier. Additionally, we introduce an adversarially-enhanced self-meta mechanism to achieve dual-generalization for both unseen identities and unseen attack types. Experiments demonstrate that our method significantly outperforms existing state-of-the-art defenses.

摘要: 人员重新识别（ReID）是许多现实世界应用（例如行人轨迹跟踪）的基本任务。然而，高级的基于深度学习的ReID模型极易受到对抗攻击，其中对行人图像的不可感知的扰动可能会导致完全错误的预测，从而构成重大的安全威胁。尽管针对分类任务提出了许多对抗性防御策略，但它们对指标学习任务（例如人ReID）的扩展仍然相对未被探索。此外，针对ReID的几种现有防御措施未能解决对抗稳健ReID固有的独特挑战。在本文中，我们系统地将面对面ReID对抗性防御的挑战识别为两个关键问题：模型偏差和复合概括要求。为了解决这些问题，我们提出了一个由两个主要阶段组成的去偏二元不变防御框架。在数据平衡阶段，我们使用基于扩散模型的数据重新分配策略来减轻模型偏差，该策略促进训练数据的公平性和多样性。在双对抗性自我元防御阶段，我们引入了一种新颖的指标对抗训练方法，结合了最远负扩展软化，以克服由于缺乏分类器而导致的鲁棒性下降。此外，我们还引入了一种反向增强的自我元机制，以实现不可见身份和不可见攻击类型的双重概括。实验表明，我们的方法显着优于现有的最先进防御。



## **25. Robust Decentralized Multi-armed Bandits: From Corruption-Resilience to Byzantine-Resilience**

强大的分散多武装强盗：从腐败韧性到拜占庭韧性 cs.LG

**SubmitDate**: **NEW** 2025-11-14    [abs](http://arxiv.org/abs/2511.10344v1) [paper-pdf](None)

**Authors**: Zicheng Hu, Yuchen Wang, Cheng Chen

**Abstract**: Decentralized cooperative multi-agent multi-armed bandits (DeCMA2B) considers how multiple agents collaborate in a decentralized multi-armed bandit setting. Though this problem has been extensively studied in previous work, most existing methods remain susceptible to various adversarial attacks. In this paper, we first study DeCMA2B with adversarial corruption, where an adversary can corrupt reward observations of all agents with a limited corruption budget. We propose a robust algorithm, called DeMABAR, which ensures that each agent's individual regret suffers only an additive term proportional to the corruption budget. Then we consider a more realistic scenario where the adversary can only attack a small number of agents. Our theoretical analysis shows that the DeMABAR algorithm can also almost completely eliminate the influence of adversarial attacks and is inherently robust in the Byzantine setting, where an unknown fraction of the agents can be Byzantine, i.e., may arbitrarily select arms and communicate wrong information. We also conduct numerical experiments to illustrate the robustness and effectiveness of the proposed method.

摘要: 去中心化合作多智能体多武装强盗（DeCMA 2B）考虑了多个智能体如何在去中心化多武装强盗环境中进行协作。尽管在之前的工作中已经广泛研究了这个问题，但大多数现有方法仍然容易受到各种对抗攻击。在本文中，我们首先研究了具有对抗性腐败的DeCMA 2B，其中对手可以用有限的腐败预算破坏所有代理人的奖励观察。我们提出了一种名为DeMABAR的稳健算法，该算法确保每个代理人的个人遗憾只受到与腐败预算成比例的附加项。然后我们考虑一个更现实的场景，其中对手只能攻击少数代理。我们的理论分析表明，DeMABAR算法还可以几乎完全消除对抗攻击的影响，并且在拜占庭环境中具有固有的鲁棒性，其中未知部分的代理可能是拜占庭的，即，可能随意选择武器并传达错误信息。我们还进行了数值实验来说明所提出方法的鲁棒性和有效性。



## **26. Diversifying Counterattacks: Orthogonal Exploration for Robust CLIP Inference**

多元化反击：稳健CLIP推理的垂直探索 cs.CV

Accepted to AAAI-2026 Oral

**SubmitDate**: 2025-11-13    [abs](http://arxiv.org/abs/2511.09064v1) [paper-pdf](None)

**Authors**: Chengze Jiang, Minjing Dong, Xinli Shi, Jie Gui

**Abstract**: Vision-language pre-training models (VLPs) demonstrate strong multimodal understanding and zero-shot generalization, yet remain vulnerable to adversarial examples, raising concerns about their reliability. Recent work, Test-Time Counterattack (TTC), improves robustness by generating perturbations that maximize the embedding deviation of adversarial inputs using PGD, pushing them away from their adversarial representations. However, due to the fundamental difference in optimization objectives between adversarial attacks and counterattacks, generating counterattacks solely based on gradients with respect to the adversarial input confines the search to a narrow space. As a result, the counterattacks could overfit limited adversarial patterns and lack the diversity to fully neutralize a broad range of perturbations. In this work, we argue that enhancing the diversity and coverage of counterattacks is crucial to improving adversarial robustness in test-time defense. Accordingly, we propose Directional Orthogonal Counterattack (DOC), which augments counterattack optimization by incorporating orthogonal gradient directions and momentum-based updates. This design expands the exploration of the counterattack space and increases the diversity of perturbations, which facilitates the discovery of more generalizable counterattacks and ultimately improves the ability to neutralize adversarial perturbations. Meanwhile, we present a directional sensitivity score based on averaged cosine similarity to boost DOC by improving example discrimination and adaptively modulating the counterattack strength. Extensive experiments on 16 datasets demonstrate that DOC improves adversarial robustness under various attacks while maintaining competitive clean accuracy. Code is available at https://github.com/bookman233/DOC.

摘要: 视觉语言预训练模型（VLP）表现出强大的多模式理解和零镜头概括，但仍然容易受到对抗性示例的影响，从而引发了对其可靠性的担忧。最近的工作《测试时反击》（TTC）通过使用PVD生成最大化对抗输入的嵌入偏差的扰动，从而将它们从对抗性表示中推离，从而提高了鲁棒性。然而，由于对抗性攻击和反击之间优化目标的根本差异，仅基于对抗性输入的梯度生成反击将搜索限制在狭窄的空间内。因此，反击可能会过度适应有限的对抗模式，并且缺乏完全中和广泛干扰的多样性。在这项工作中，我们认为，提高反击的多样性和覆盖率对于提高测试时防御的对抗鲁棒性至关重要。因此，我们提出了方向正交反击（DOC），它通过将正交梯度方向和基于动量的更新来增强反击优化。这种设计扩大了对反击空间的探索，增加了扰动的多样性，这有利于发现更普遍的反击，并最终提高了中和对抗性扰动的能力。同时，我们提出了一个方向敏感性得分的基础上平均余弦相似性，以提高DOC的例子区分和自适应调整反击强度。对16个数据集的广泛实验表明，Doc可以提高各种攻击下的对抗鲁棒性，同时保持有竞争力的干净准确性。代码可在https://github.com/bookman233/DOC上获取。



## **27. Spilling the Beans: Teaching LLMs to Self-Report Their Hidden Objectives**

泄露豆子：教法学硕士自我报告他们隐藏的目标 cs.AI

**SubmitDate**: 2025-11-12    [abs](http://arxiv.org/abs/2511.06626v2) [paper-pdf](None)

**Authors**: Chloe Li, Mary Phuong, Daniel Tan

**Abstract**: As AI systems become more capable of complex agentic tasks, they also become more capable of pursuing undesirable objectives and causing harm. Previous work has attempted to catch these unsafe instances by interrogating models directly about their objectives and behaviors. However, the main weakness of trusting interrogations is that models can lie. We propose self-report fine-tuning (SRFT), a simple supervised fine-tuning technique that trains models to admit their factual mistakes when asked. We show that the admission of factual errors in simple question-answering settings generalizes out-of-distribution (OOD) to the admission of hidden misaligned objectives in adversarial agentic settings. We evaluate SRFT in OOD stealth tasks, where models are instructed to complete a hidden misaligned objective alongside a user-specified objective without being caught by monitoring. After SRFT, models are more likely to confess the details of their hidden objectives when interrogated, even under strong pressure not to disclose them. Interrogation on SRFT models can detect hidden objectives with near-ceiling performance (F1 score = 0.98), while the baseline model lies when interrogated under the same conditions (F1 score = 0). Interrogation on SRFT models can further elicit the content of the hidden objective, recovering 28-100% details, compared to 0% details recovered in the baseline model and by prefilled assistant turn attacks. This provides a promising technique for promoting honesty propensity and incriminating misaligned AI systems.

摘要: 随着人工智能系统变得更有能力执行复杂的代理任务，它们也变得更有能力追求不想要的目标并造成伤害。之前的工作试图通过直接询问模型的目标和行为来捕捉这些不安全的实例。然而，信任审讯的主要弱点是模型可能会撒谎。我们提出了自我报告微调（SRFT），这是一种简单的监督微调技术，可以训练模型在被问及时承认自己的事实错误。我们表明，在简单的问答环境中承认事实错误将分配失调（OOD）推广为在对抗性代理环境中承认隐藏的错位目标。我们在OOD隐形任务中评估SRFT，其中模型被指示完成隐藏的未对齐目标以及用户指定的目标，而不会被监控发现。在SRFT之后，模特们在被审问时更有可能坦白其隐藏目标的细节，即使在不披露这些目标的强大压力下也是如此。SRFT模型上的询问可以检测出具有接近天花板性能的隐藏目标（F1评分= 0.98），而基线模型在相同条件下询问时则存在缺陷（F1评分= 0）。SRFT模型上的询问可以进一步获取隐藏目标的内容，恢复28-100%的细节，而在基线模型和预填充助理回合攻击中恢复的细节为0%。这提供了一种有前途的技术，可以促进诚实倾向并将不一致的人工智能系统定罪。



## **28. Class-feature Watermark: A Resilient Black-box Watermark Against Model Extraction Attacks**

类特征水印：一种抗模型抽取攻击的弹性黑箱水印 cs.CR

Accepted by AAAI'26

**SubmitDate**: 2025-11-12    [abs](http://arxiv.org/abs/2511.07947v1) [paper-pdf](None)

**Authors**: Yaxin Xiao, Qingqing Ye, Zi Liang, Haoyang Li, RongHua Li, Huadi Zheng, Haibo Hu

**Abstract**: Machine learning models constitute valuable intellectual property, yet remain vulnerable to model extraction attacks (MEA), where adversaries replicate their functionality through black-box queries. Model watermarking counters MEAs by embedding forensic markers for ownership verification. Current black-box watermarks prioritize MEA survival through representation entanglement, yet inadequately explore resilience against sequential MEAs and removal attacks. Our study reveals that this risk is underestimated because existing removal methods are weakened by entanglement. To address this gap, we propose Watermark Removal attacK (WRK), which circumvents entanglement constraints by exploiting decision boundaries shaped by prevailing sample-level watermark artifacts. WRK effectively reduces watermark success rates by at least 88.79% across existing watermarking benchmarks.   For robust protection, we propose Class-Feature Watermarks (CFW), which improve resilience by leveraging class-level artifacts. CFW constructs a synthetic class using out-of-domain samples, eliminating vulnerable decision boundaries between original domain samples and their artifact-modified counterparts (watermark samples). CFW concurrently optimizes both MEA transferability and post-MEA stability. Experiments across multiple domains show that CFW consistently outperforms prior methods in resilience, maintaining a watermark success rate of at least 70.15% in extracted models even under the combined MEA and WRK distortion, while preserving the utility of protected models.

摘要: 机器学习模型构成宝贵的知识产权，但仍然容易受到模型提取攻击（EMA）的影响，即对手通过黑匣子查询复制其功能。模型水印通过嵌入取证标记进行所有权验证来对抗多边环境协定。当前的黑匣子水印通过表示纠缠优先考虑多边环境协定的生存，但没有充分探索针对连续多边环境协定和删除攻击的弹性。我们的研究表明，这种风险被低估了，因为现有的去除方法被纠缠削弱了。为了解决这一差距，我们提出了水印去除attacK（WRK），它通过利用由流行的样本级水印伪影塑造的决策边界来规避纠缠限制。WRK在现有水印基准中有效地降低了至少88.79%的水印成功率。   为了强大的保护，我们提出了类特征水印（CFW），它通过利用类级工件来提高弹性。CFW使用域外样本构建合成类，消除原始域样本与其经伪影修改的对应样本（水印样本）之间的脆弱决策边界。CFW同时优化MEA的可转移性和MEA后的稳定性。多个领域的实验表明，CFW在弹性方面始终优于先前的方法，即使在MEA和WRK组合失真的情况下，提取的模型仍保持至少70.15%的水印成功率，同时保留了受保护模型的实用性。



## **29. GreedyPixel: Fine-Grained Black-Box Adversarial Attack Via Greedy Algorithm**

GreedyPixel：通过贪婪算法进行细粒度黑匣子对抗攻击 cs.CV

IEEE Transactions on Information Forensics and Security

**SubmitDate**: 2025-11-13    [abs](http://arxiv.org/abs/2501.14230v3) [paper-pdf](None)

**Authors**: Hanrui Wang, Ching-Chun Chang, Chun-Shien Lu, Christopher Leckie, Isao Echizen

**Abstract**: Deep neural networks are highly vulnerable to adversarial examples, which are inputs with small, carefully crafted perturbations that cause misclassification -- making adversarial attacks a critical tool for evaluating robustness. Existing black-box methods typically entail a trade-off between precision and flexibility: pixel-sparse attacks (e.g., single- or few-pixel attacks) provide fine-grained control but lack adaptability, whereas patch- or frequency-based attacks improve efficiency or transferability, but at the cost of producing larger and less precise perturbations. We present GreedyPixel, a fine-grained black-box attack method that performs brute-force-style, per-pixel greedy optimization guided by a surrogate-derived priority map and refined by means of query feedback. It evaluates each coordinate directly without any gradient information, guaranteeing monotonic loss reduction and convergence to a coordinate-wise optimum, while also yielding near white-box-level precision and pixel-wise sparsity and perceptual quality. On the CIFAR-10 and ImageNet datasets, spanning convolutional neural networks (CNNs) and Transformer models, GreedyPixel achieved state-of-the-art success rates with visually imperceptible perturbations, effectively bridging the gap between black-box practicality and white-box performance. The implementation is available at https://github.com/azrealwang/greedypixel.

摘要: 深度神经网络非常容易受到对抗性示例的影响，这些示例是具有小的、精心制作的扰动的输入，这些扰动会导致错误分类，这使得对抗性攻击成为评估鲁棒性的关键工具。现有的黑盒方法通常需要在精度和灵活性之间进行权衡：像素稀疏攻击（例如，单像素或少像素攻击）提供了细粒度的控制，但缺乏适应性，而基于补丁或频率的攻击提高了效率或可转移性，但代价是产生更大和更不精确的扰动。我们提出了GreedyPixel，一种细粒度的黑盒攻击方法，执行蛮力式的，每像素的贪婪优化指导下的代理派生的优先级地图和完善的查询反馈。它在没有任何梯度信息的情况下直接评估每个坐标，保证单调损失减少并收敛到坐标最优值，同时还能产生接近白盒级别的精度和像素稀疏度和感知质量。在CIFAR-10和ImageNet数据集上，跨越卷积神经网络（CNN）和Transformer模型，GreedyPixel通过视觉上难以感知的扰动实现了最先进的成功率，有效地弥合了黑匣子实用性和白盒性能之间的差距。该实现可在https://github.com/azrealwang/greedypixel上获取。



## **30. Abstract Gradient Training: A Unified Certification Framework for Data Poisoning, Unlearning, and Differential Privacy**

抽象梯度训练：针对数据中毒、取消学习和差异隐私的统一认证框架 cs.LG

**SubmitDate**: 2025-11-13    [abs](http://arxiv.org/abs/2511.09400v1) [paper-pdf](None)

**Authors**: Philip Sosnin, Matthew Wicker, Josh Collyer, Calvin Tsay

**Abstract**: The impact of inference-time data perturbation (e.g., adversarial attacks) has been extensively studied in machine learning, leading to well-established certification techniques for adversarial robustness. In contrast, certifying models against training data perturbations remains a relatively under-explored area. These perturbations can arise in three critical contexts: adversarial data poisoning, where an adversary manipulates training samples to corrupt model performance; machine unlearning, which requires certifying model behavior under the removal of specific training data; and differential privacy, where guarantees must be given with respect to substituting individual data points. This work introduces Abstract Gradient Training (AGT), a unified framework for certifying robustness of a given model and training procedure to training data perturbations, including bounded perturbations, the removal of data points, and the addition of new samples. By bounding the reachable set of parameters, i.e., establishing provable parameter-space bounds, AGT provides a formal approach to analyzing the behavior of models trained via first-order optimization methods.

摘要: 推断时间数据扰动的影响（例如，对抗性攻击）在机器学习中得到了广泛研究，从而产生了成熟的对抗性鲁棒性认证技术。相比之下，针对训练数据扰动来认证模型仍然是一个相对未充分开发的领域。这些扰动可能出现在三种关键情况下：对抗性数据中毒，即对手操纵训练样本以破坏模型性能;机器取消学习，需要在删除特定训练数据的情况下认证模型行为;和差异隐私，其中必须提供有关替换单个数据点的保证。这项工作引入了抽象梯度训练（AGT），这是一个统一的框架，用于认证给定模型和训练过程对训练数据扰动（包括有界扰动、数据点的删除和新样本的添加）的稳健性。通过限制可达参数集，即，AGT建立可证明的参数空间边界，提供了一种形式化的方法来分析通过一阶优化方法训练的模型的行为。



## **31. Siren: A Learning-Based Multi-Turn Attack Framework for Simulating Real-World Human Jailbreak Behaviors**

Siren：一个基于学习的多回合攻击框架，用于模拟现实世界的人类越狱行为 cs.CL

Accepted at ACSAC 2025

**SubmitDate**: **NEW** 2025-11-14    [abs](http://arxiv.org/abs/2501.14250v2) [paper-pdf](None)

**Authors**: Yi Zhao, Youzhi Zhang

**Abstract**: Large language models (LLMs) are widely used in real-world applications, raising concerns about their safety and trustworthiness. While red-teaming with jailbreak prompts exposes the vulnerabilities of LLMs, current efforts focus primarily on single-turn attacks, overlooking the multi-turn strategies used by real-world adversaries. Existing multi-turn methods rely on static patterns or predefined logical chains, failing to account for the dynamic strategies during attacks. We propose Siren, a learning-based multi-turn attack framework designed to simulate real-world human jailbreak behaviors. Siren consists of three stages: (1) MiniMax-driven training set construction utilizing Turn-Level LLM feedback, (2) post-training attackers with supervised fine-tuning (SFT) and direct preference optimization (DPO), and (3) interactions between the attacking and target LLMs. Experiments demonstrate that Siren achieves an attack success rate (ASR) of 90% with LLaMA-3-8B as the attacker against Gemini-1.5-Pro as the target model, and 70% with Mistral-7B against GPT-4o, significantly outperforming single-turn baselines. Moreover, Siren with a 7B-scale model achieves performance comparable to a multi-turn baseline that leverages GPT-4o as the attacker, while requiring fewer turns and employing decomposition strategies that are better semantically aligned with attack goals. We hope Siren inspires the development of stronger defenses against advanced multi-turn jailbreak attacks under realistic scenarios. Code is available at https://github.com/YiyiyiZhao/siren. Warning: This paper contains potentially harmful text.

摘要: 大型语言模型（LLM）广泛应用于现实世界的应用程序中，引发了对其安全性和可信性的担忧。虽然与越狱提示进行红色合作暴露了LLM的脆弱性，但目前的工作主要集中在单回合攻击上，忽视了现实世界对手使用的多回合策略。现有的多回合方法依赖于静态模式或预定义的逻辑链，未能考虑攻击期间的动态策略。我们提出了Siren，这是一个基于学习的多回合攻击框架，旨在模拟现实世界中的人类越狱行为。Siren由三个阶段组成：（1）利用Turn-Level LLM反馈的MiniMax驱动的训练集构建，（2）训练后的攻击者进行监督微调（SFT）和直接偏好优化（DPO），以及（3）攻击和目标LLM之间的交互。实验表明，Siren以LLaMA-3-8B为攻击者，对Gemini-1.5-Pro为目标模型的攻击成功率（ASR）为90%，Mistral-7 B对GPT-4 o的攻击成功率为70%，明显优于单回合基线。此外，具有7 B规模模型的Siren实现了与利用GPT-4 o作为攻击者的多回合基线相当的性能，同时需要更少的回合并采用更好地与攻击目标语义一致的分解策略。我们希望Siren能够激发开发更强大的防御，以对抗现实场景下的高级多回合越狱攻击。代码可在https://github.com/YiyiyiZhao/siren上获取。警告：本文包含潜在有害的文本。



## **32. UDora: A Unified Red Teaming Framework against LLM Agents by Dynamically Hijacking Their Own Reasoning**

UPora：通过动态劫持LLM代理自己的推理来对抗他们的统一红色团队框架 cs.CR

**SubmitDate**: 2025-11-13    [abs](http://arxiv.org/abs/2503.01908v3) [paper-pdf](None)

**Authors**: Jiawei Zhang, Shuang Yang, Bo Li

**Abstract**: Large Language Model (LLM) agents equipped with external tools have become increasingly powerful for complex tasks such as web shopping, automated email replies, and financial trading. However, these advancements amplify the risks of adversarial attacks, especially when agents can access sensitive external functionalities. Nevertheless, manipulating LLM agents into performing targeted malicious actions or invoking specific tools remains challenging, as these agents extensively reason or plan before executing final actions. In this work, we present UDora, a unified red teaming framework designed for LLM agents that dynamically hijacks the agent's reasoning processes to compel malicious behavior. Specifically, UDora first generates the model's reasoning trace for the given task, then automatically identifies optimal points within this trace to insert targeted perturbations. The resulting perturbed reasoning is then used as a surrogate response for optimization. By iteratively applying this process, the LLM agent will then be induced to undertake designated malicious actions or to invoke specific malicious tools. Our approach demonstrates superior effectiveness compared to existing methods across three LLM agent datasets. The code is available at https://github.com/AI-secure/UDora.

摘要: 配备外部工具的大型语言模型（LLM）代理在网络购物、自动电子邮件回复和金融交易等复杂任务中变得越来越强大。然而，这些进步放大了对抗攻击的风险，特别是当代理可以访问敏感的外部功能时。然而，操纵LLM代理执行有针对性的恶意操作或调用特定工具仍然具有挑战性，因为这些代理在执行最终操作之前进行了广泛的推理或计划。在这项工作中，我们介绍了UPora，这是一个为LLM代理设计的统一红色团队框架，它动态劫持代理的推理过程以迫使恶意行为。具体来说，UPora首先为给定任务生成模型的推理轨迹，然后自动识别此轨迹内的最佳点以插入有针对性的扰动。然后将产生的扰动推理用作优化的替代响应。通过迭代应用此过程，LLM代理将被诱导采取指定的恶意操作或调用特定的恶意工具。与三个LLM代理数据集的现有方法相比，我们的方法表现出了卓越的有效性。该代码可在https://github.com/AI-secure/UDora上获取。



## **33. RedDiffuser: Red Teaming Vision-Language Models for Toxic Continuation via Reinforced Stable Diffusion**

RedDivuser：通过加强稳定扩散实现有毒延续的红色团队视觉语言模型 cs.CV

**SubmitDate**: 2025-11-12    [abs](http://arxiv.org/abs/2503.06223v4) [paper-pdf](None)

**Authors**: Ruofan Wang, Xiang Zheng, Xiaosen Wang, Cong Wang, Xingjun Ma, Yu-Gang Jiang

**Abstract**: Vision-Language Models (VLMs) are vulnerable to jailbreak attacks, where adversaries bypass safety mechanisms to elicit harmful outputs. In this work, we examine an insidious variant of this threat: toxic continuation. Unlike standard jailbreaks that rely solely on malicious instructions, toxic continuation arises when the model is given a malicious input alongside a partial toxic output, resulting in harmful completions. This vulnerability poses a unique challenge in multimodal settings, where even subtle image variations can disproportionately affect the model's response. To this end, we propose RedDiffuser (RedDiff), the first red teaming framework that uses reinforcement learning to fine-tune diffusion models into generating natural-looking adversarial images that induce toxic continuations. RedDiffuser integrates a greedy search procedure for selecting candidate image prompts with reinforcement fine-tuning that jointly promotes toxic output and semantic coherence. Experiments demonstrate that RedDiffuser significantly increases the toxicity rate in LLaVA outputs by 10.69% and 8.91% on the original and hold-out sets, respectively. It also exhibits strong transferability, increasing toxicity rates on Gemini by 5.1% and on LLaMA-Vision by 26.83%. These findings uncover a cross-modal toxicity amplification vulnerability in current VLM alignment, highlighting the need for robust multimodal red teaming. We will release the RedDiffuser codebase to support future research.

摘要: 视觉语言模型（VLM）很容易受到越狱攻击，对手绕过安全机制来引发有害输出。在这项工作中，我们研究了这种威胁的一个阴险变体：有毒的延续。与完全依赖恶意指令的标准越狱不同，当模型受到恶意输入和部分有毒输出时，就会出现有毒延续，从而导致有害的完成。该漏洞在多模式环境中构成了独特的挑战，即使是细微的图像变化也会不成比例地影响模型的响应。为此，我们提出了RedDivuser（RedDiff），这是第一个红色团队框架，它使用强化学习来微调扩散模型，以生成诱导有毒延续的自然对抗图像。RedDivuser集成了用于选择候选图像提示的贪婪搜索过程，并进行了强化微调，共同促进了有毒输出和语义一致性。实验表明，RedDiffuser在原始集和保留集上分别显着提高LLaVA输出的毒性率10.69%和8.91%。它还表现出很强的转移性，使Gemini的毒性率提高了5.1%，对LLaMA-Vision的毒性率提高了26.83%。这些发现揭示了当前VLM对齐中的跨模式毒性放大漏洞，凸显了对稳健的多模式红色分组的需求。我们将发布RedDistuser代码库来支持未来的研究。



## **34. Private Remote Phase Estimation over a Lossy Quantum Channel**

有损耗量子通道上的私人远程阶段估计 quant-ph

4 + 5 pages; 2 figures

**SubmitDate**: 2025-11-13    [abs](http://arxiv.org/abs/2511.09123v1) [paper-pdf](None)

**Authors**: Farzad Kianvash, Marco Barbieri, Matteo Rosati

**Abstract**: Private remote quantum sensing (PRQS) aims at estimating a parameter at a distant location by transmitting quantum states on an insecure quantum channel, limiting information leakage and disruption of the estimation itself from an adversary. Previous results highlighted that one can bound the estimation performance in terms of the observed noise. However, if no assumptions are placed on the channel model, such bounds are very loose and severely limit the estimation. We propose and analyse a PRQS using, for the first time to our knowledge, continuous-variable states in the single-user setting. Assuming a typical class of lossy attacks and employing tools from quantum communication, we calculate the true estimation error and privacy of our protocol, both in the asymptotic limit of many channel uses and in the finite-size regime. Our results show that a realistic channel-model assumption, which can be validated with measurement data, allows for a much tighter quantification of the estimation error and privacy for all practical purposes.

摘要: 私有远程量子传感（PRQS）旨在通过在不安全的量子信道上传输量子态来估计远程位置的参数，限制信息泄漏和来自对手的估计本身的中断。先前的结果强调，可以根据观察到的噪声来限制估计性能。然而，如果没有假设放置在信道模型上，这样的界限是非常宽松的，并严重限制了估计。我们提出并分析了一个PRQS使用，第一次，我们的知识，连续变量状态的单用户设置。假设一类典型的有损攻击并使用量子通信的工具，我们计算协议的真实估计误差和隐私，无论是在许多通道使用的渐进限制下还是在有限大小范围内。我们的结果表明，现实的通道模型假设（可以用测量数据进行验证）允许出于所有实际目的对估计误差和隐私进行更严格的量化。



## **35. eXIAA: eXplainable Injections for Adversarial Attack**

eXIAA：对抗性攻击的可扩展注射剂 cs.LG

**SubmitDate**: **NEW** 2025-11-14    [abs](http://arxiv.org/abs/2511.10088v1) [paper-pdf](None)

**Authors**: Leonardo Pesce, Jiawen Wei, Gianmarco Mengaldo

**Abstract**: Post-hoc explainability methods are a subset of Machine Learning (ML) that aim to provide a reason for why a model behaves in a certain way. In this paper, we show a new black-box model-agnostic adversarial attack for post-hoc explainable Artificial Intelligence (XAI), particularly in the image domain. The goal of the attack is to modify the original explanations while being undetected by the human eye and maintain the same predicted class. In contrast to previous methods, we do not require any access to the model or its weights, but only to the model's computed predictions and explanations. Additionally, the attack is accomplished in a single step while significantly changing the provided explanations, as demonstrated by empirical evaluation. The low requirements of our method expose a critical vulnerability in current explainability methods, raising concerns about their reliability in safety-critical applications. We systematically generate attacks based on the explanations generated by post-hoc explainability methods (saliency maps, integrated gradients, and DeepLIFT SHAP) for pretrained ResNet-18 and ViT-B16 on ImageNet. The results show that our attacks could lead to dramatically different explanations without changing the predictive probabilities. We validate the effectiveness of our attack, compute the induced change based on the explanation with mean absolute difference, and verify the closeness of the original image and the corrupted one with the Structural Similarity Index Measure (SSIM).

摘要: 事后可解释性方法是机器学习（ML）的一个子集，旨在为模型为何以某种方式行为提供理由。在本文中，我们展示了一种新的黑匣子模型--针对事后可解释人工智能（XAI）的不可知对抗攻击，特别是在图像领域。该攻击的目标是在不被人眼检测到的情况下修改原始解释，并保持相同的预测类别。与以前的方法相比，我们不需要访问模型或其权重，而只需访问模型的计算预测和解释。此外，正如经验评估所证明的那样，攻击只需一步即可完成，同时显着改变了所提供的解释。我们方法的低要求暴露了当前可解释性方法中的一个关键漏洞，引发了对其在安全关键应用中可靠性的担忧。我们根据针对ImageNet上预训练的ResNet-18和ViT-B16的事后解释方法（显着图、综合梯度和DeepLift SHAP）生成的解释系统地生成攻击。结果表明，我们的攻击可能会在不改变预测概率的情况下导致截然不同的解释。我们验证了攻击的有效性，根据平均绝对差的解释计算诱导的变化，并使用结构相似性指数测量（SSIM）验证原始图像和损坏图像的接近度。



## **36. CertMask: Certifiable Defense Against Adversarial Patches via Theoretically Optimal Mask Coverage**

CertMass：通过理论上最佳的口罩覆盖率针对对抗性补丁的可认证防御 cs.CV

**SubmitDate**: **NEW** 2025-11-14    [abs](http://arxiv.org/abs/2511.09834v1) [paper-pdf](None)

**Authors**: Xuntao Lyu, Ching-Chi Lin, Abdullah Al Arafat, Georg von der Brüggen, Jian-Jia Chen, Zhishan Guo

**Abstract**: Adversarial patch attacks inject localized perturbations into images to mislead deep vision models. These attacks can be physically deployed, posing serious risks to real-world applications. In this paper, we propose CertMask, a certifiably robust defense that constructs a provably sufficient set of binary masks to neutralize patch effects with strong theoretical guarantees. While the state-of-the-art approach (PatchCleanser) requires two rounds of masking and incurs $O(n^2)$ inference cost, CertMask performs only a single round of masking with $O(n)$ time complexity, where $n$ is the cardinality of the mask set to cover an input image. Our proposed mask set is computed using a mathematically rigorous coverage strategy that ensures each possible patch location is covered at least $k$ times, providing both efficiency and robustness. We offer a theoretical analysis of the coverage condition and prove its sufficiency for certification. Experiments on ImageNet, ImageNette, and CIFAR-10 show that CertMask improves certified robust accuracy by up to +13.4\% over PatchCleanser, while maintaining clean accuracy nearly identical to the vanilla model.

摘要: 对抗性补丁攻击将局部扰动注入图像中，以误导深度视觉模型。这些攻击可以物理部署，对现实世界的应用程序构成严重风险。在本文中，我们提出了CertMass，这是一种经过验证的稳健防御，它构建了一组可证明充分的二进制面具，以在强有力的理论保证下中和补丁效应。虽然最先进的方法（PatchCleanser）需要两轮掩蔽并产生$O（n#2）$推断成本，但CertMass仅执行一轮掩蔽，时间复杂度为$O（n）$，其中$n$是覆盖输入图像的掩蔽集的基数。我们提出的屏蔽集是使用数学上严格的覆盖策略计算的，该策略确保每个可能的补丁位置至少被覆盖$k$次，从而提供效率和鲁棒性。我们对覆盖条件进行了理论分析，并证明其对于认证的充分性。ImageNet、ImageNet和CIFAR-10上的实验表明，与PatchCleanser相比，CertMass将经过认证的鲁棒准确性提高了+13.4%，同时保持与vanilla模型几乎相同的清洁准确性。



## **37. Hail to the Thief: Exploring Attacks and Defenses in Decentralised GRPO**

向小偷致敬：探索分散式GRPO中的攻击和防御 cs.LG

**SubmitDate**: **NEW** 2025-11-14    [abs](http://arxiv.org/abs/2511.09780v1) [paper-pdf](None)

**Authors**: Nikolay Blagoev, Oğuzhan Ersoy, Lydia Yiyu Chen

**Abstract**: Group Relative Policy Optimization (GRPO) has demonstrated great utilization in post-training of Large Language Models (LLMs). In GRPO, prompts are answered by the model and, through reinforcement learning, preferred completions are learnt. Owing to the small communication volume, GRPO is inherently suitable for decentralised training as the prompts can be concurrently answered by multiple nodes and then exchanged in the forms of strings. In this work, we present the first adversarial attack in decentralised GRPO. We demonstrate that malicious parties can poison such systems by injecting arbitrary malicious tokens in benign models in both out-of-context and in-context attacks. Using empirical examples of math and coding tasks, we show that adversarial attacks can easily poison the benign nodes, polluting their local LLM post-training, achieving attack success rates up to 100% in as few as 50 iterations. We propose two ways to defend against these attacks, depending on whether all users train the same model or different models. We show that these defenses can achieve stop rates of up to 100%, making the attack impossible.

摘要: 组相对策略优化（GRPO）在大型语言模型（LLM）的后训练中表现出了巨大的利用率。在GRPO中，模型回答提示，并通过强化学习首选的完成。由于通信量小，GRPO本质上适合去中心化训练，因为提示可以由多个节点同时回答，然后以字符串的形式交换。在这项工作中，我们展示了去中心化GRPO中的第一次对抗攻击。我们证明，恶意方可以在上下文外和上下文内攻击中通过在良性模型中注入任意恶意令牌来毒害此类系统。使用数学和编码任务的经验示例，我们表明对抗性攻击很容易毒害良性节点，污染其本地LLM后训练，在短短50次迭代中实现高达100%的攻击成功率。我们提出了两种防御这些攻击的方法，具体取决于所有用户是训练相同的模型还是不同的模型。我们表明，这些防御措施可以实现高达100%的停止率，使攻击变得不可能。



## **38. Can Current Detectors Catch Face-to-Voice Deepfake Attacks?**

当前的检测器可以捕捉面对面语音Deepfake攻击吗？ cs.CR

8 pages, Accepted at Workshop on AI for Cyber Threat Intelligence, co-located with ACSAC 2025

**SubmitDate**: **NEW** 2025-11-14    [abs](http://arxiv.org/abs/2510.21004v2) [paper-pdf](None)

**Authors**: Nguyen Linh Bao Nguyen, Alsharif Abuadbba, Kristen Moore, Tingmin Wu

**Abstract**: The rapid advancement of generative models has enabled the creation of increasingly stealthy synthetic voices, commonly referred to as audio deepfakes. A recent technique, FOICE [USENIX'24], demonstrates a particularly alarming capability: generating a victim's voice from a single facial image, without requiring any voice sample. By exploiting correlations between facial and vocal features, FOICE produces synthetic voices realistic enough to bypass industry-standard authentication systems, including WeChat Voiceprint and Microsoft Azure. This raises serious security concerns, as facial images are far easier for adversaries to obtain than voice samples, dramatically lowering the barrier to large-scale attacks. In this work, we investigate two core research questions: (RQ1) can state-of-the-art audio deepfake detectors reliably detect FOICE-generated speech under clean and noisy conditions, and (RQ2) whether fine-tuning these detectors on FOICE data improves detection without overfitting, thereby preserving robustness to unseen voice generators such as SpeechT5.   Our study makes three contributions. First, we present the first systematic evaluation of FOICE detection, showing that leading detectors consistently fail under both standard and noisy conditions. Second, we introduce targeted fine-tuning strategies that capture FOICE-specific artifacts, yielding significant accuracy improvements. Third, we assess generalization after fine-tuning, revealing trade-offs between specialization to FOICE and robustness to unseen synthesis pipelines. These findings expose fundamental weaknesses in today's defenses and motivate new architectures and training protocols for next-generation audio deepfake detection.

摘要: 生成模型的快速发展使得人们能够创建越来越隐秘的合成语音，通常称为音频深度伪造。最近的一项技术FOICE [USENIX ' 24]展示了一种特别令人震惊的能力：从单个面部图像生成受害者的语音，而不需要任何语音样本。通过利用面部和声音特征之间的相关性，FOICE生成足够真实的合成语音，以绕过行业标准的认证系统，包括WeChat Voiceprint和Microsoft Azure。这引发了严重的安全问题，因为对手获取面部图像比获取语音样本更容易，从而大大降低了大规模攻击的障碍。在这项工作中，我们研究了两个核心研究问题：（PQ 1）最先进的音频深度伪造检测器能否在干净和有噪的条件下可靠地检测FOICE生成的语音，以及（PQ 2）根据FOICE数据微调这些检测器是否可以在不过度匹配的情况下改善检测，从而保留对SpeechT 5等不可见语音生成器的鲁棒性。   我们的研究做出了三点贡献。首先，我们对FOICE检测进行了首次系统评估，表明领先的检测器在标准和噪音条件下始终失败。其次，我们引入了有针对性的微调策略，以捕获FOICE特定的文物，从而显着提高准确性。第三，我们在微调后评估概括性，揭示FOICE的专业化和不可见合成管道的稳健性之间的权衡。这些发现暴露了当今防御的根本弱点，并激发了新的架构和训练协议用于下一代音频深度伪造检测。



## **39. VFEFL: Privacy-Preserving Federated Learning against Malicious Clients via Verifiable Functional Encryption**

VFEFL：通过可验证的函数加密保护隐私的联邦学习以对抗恶意客户端 cs.CR

**SubmitDate**: **NEW** 2025-11-14    [abs](http://arxiv.org/abs/2506.12846v3) [paper-pdf](None)

**Authors**: Nina Cai, Jinguang Han, Weizhi Meng

**Abstract**: Federated learning is a promising distributed learning paradigm that enables collaborative model training without exposing local client data, thereby protect data privacy. However, it also brings new threats and challenges. The advancement of model inversion attacks has rendered the plaintext transmission of local models insecure, while the distributed nature of federated learning makes it particularly vulnerable to attacks raised by malicious clients. To protect data privacy and prevent malicious client attacks, this paper proposes a privacy-preserving federated learning framework based on verifiable functional encryption, without a non-colluding dual-server setup or additional trusted third-party. Specifically, we propose a novel decentralized verifiable functional encryption (DVFE) scheme that enables the verification of specific relationships over multi-dimensional ciphertexts. This scheme is formally treated, in terms of definition, security model and security proof. Furthermore, based on the proposed DVFE scheme, we design a privacy-preserving federated learning framework VFEFL that incorporates a novel robust aggregation rule to detect malicious clients, enabling the effective training of high-accuracy models under adversarial settings. Finally, we provide formal analysis and empirical evaluation of the proposed schemes. The results demonstrate that our approach achieves the desired privacy protection, robustness, verifiability and fidelity, while eliminating the reliance on non-colluding dual-server settings or trusted third parties required by existing methods.

摘要: 联邦学习是一种有前途的分布式学习范式，可以在不暴露本地客户数据的情况下实现协作模型训练，从而保护数据隐私。但也带来了新的威胁和挑战。模型倒置攻击的发展使本地模型的明文传输变得不安全，而联邦学习的分布式性质使其特别容易受到恶意客户端发起的攻击。为了保护数据隐私并防止恶意客户端攻击，本文提出了一种基于可验证功能加密的隐私保护联邦学习框架，无需非勾结双服务器设置或额外的受信任第三方。具体来说，我们提出了一种新型的去中心化可验证功能加密（DVFE）方案，该方案能够验证多维密文上的特定关系。从定义、安全模型和安全证明方面对该方案进行了正式处理。此外，基于提出的DVFE方案，我们设计了一个保护隐私的联邦学习框架VFEFL，该框架结合了一种新颖的鲁棒聚合规则来检测恶意客户端，从而能够在对抗环境下有效训练高准确度模型。最后，我们对所提出的方案进行了形式分析和实证评估。结果表明，我们的方法实现了所需的隐私保护，鲁棒性，可验证性和保真度，同时消除了现有方法所需的非合谋双服务器设置或可信第三方的依赖。



## **40. Graph of Attacks with Pruning: Optimizing Stealthy Jailbreak Prompt Generation for Enhanced LLM Content Moderation**

修剪攻击图：优化隐形越狱提示生成以增强LLM内容审核 cs.CR

14 pages, 5 figures; published in EMNLP 2025 ; Code at: https://github.com/dsbuddy/GAP-LLM-Safety

**SubmitDate**: **NEW** 2025-11-14    [abs](http://arxiv.org/abs/2501.18638v3) [paper-pdf](None)

**Authors**: Daniel Schwartz, Dmitriy Bespalov, Zhe Wang, Ninad Kulkarni, Yanjun Qi

**Abstract**: As large language models (LLMs) become increasingly prevalent, ensuring their robustness against adversarial misuse is crucial. This paper introduces the GAP (Graph of Attacks with Pruning) framework, an advanced approach for generating stealthy jailbreak prompts to evaluate and enhance LLM safeguards. GAP addresses limitations in existing tree-based LLM jailbreak methods by implementing an interconnected graph structure that enables knowledge sharing across attack paths. Our experimental evaluation demonstrates GAP's superiority over existing techniques, achieving a 20.8% increase in attack success rates while reducing query costs by 62.7%. GAP consistently outperforms state-of-the-art methods for attacking both open and closed LLMs, with attack success rates of >96%. Additionally, we present specialized variants like GAP-Auto for automated seed generation and GAP-VLM for multimodal attacks. GAP-generated prompts prove highly effective in improving content moderation systems, increasing true positive detection rates by 108.5% and accuracy by 183.6% when used for fine-tuning. Our implementation is available at https://github.com/dsbuddy/GAP-LLM-Safety.

摘要: 随着大型语言模型（LLM）变得越来越普遍，确保其针对对抗性滥用的鲁棒性至关重要。本文介绍了GAP（带有修剪的攻击图）框架，这是一种生成隐形越狱提示以评估和增强LLM保障措施的高级方法。GAP通过实现互连的图结构来解决现有基于树的LLM越狱方法的局限性，该结构能够实现跨攻击路径的知识共享。我们的实验评估证明了GAP相对于现有技术的优越性，攻击成功率提高了20.8%，同时将查询成本降低了62.7%。对于攻击开放式和封闭式LLM，RAP始终优于最先进的方法，攻击成功率> 96%。此外，我们还提供了专门的变体，例如用于自动种子生成的GAP-Auto和用于多模式攻击的GAP-VLM。事实证明，由间隙生成的提示在改进内容审核系统方面非常有效，用于微调时，真阳性检测率可提高108.5%，准确率可提高183.6%。我们的实施可在https://github.com/dsbuddy/GAP-LLM-Safety上获取。



## **41. Adversarial Bias: Data Poisoning Attacks on Fairness**

对抗偏见：对公平性的数据毒害攻击 cs.LG

15 pages, 9 figures, shortened version in BigData 2025

**SubmitDate**: 2025-11-12    [abs](http://arxiv.org/abs/2511.08331v1) [paper-pdf](None)

**Authors**: Eunice Chan, Hanghang Tong

**Abstract**: With the growing adoption of AI and machine learning systems in real-world applications, ensuring their fairness has become increasingly critical. The majority of the work in algorithmic fairness focus on assessing and improving the fairness of machine learning systems. There is relatively little research on fairness vulnerability, i.e., how an AI system's fairness can be intentionally compromised. In this work, we first provide a theoretical analysis demonstrating that a simple adversarial poisoning strategy is sufficient to induce maximally unfair behavior in naive Bayes classifiers. Our key idea is to strategically inject a small fraction of carefully crafted adversarial data points into the training set, biasing the model's decision boundary to disproportionately affect a protected group while preserving generalizable performance. To illustrate the practical effectiveness of our method, we conduct experiments across several benchmark datasets and models. We find that our attack significantly outperforms existing methods in degrading fairness metrics across multiple models and datasets, often achieving substantially higher levels of unfairness with a comparable or only slightly worse impact on accuracy. Notably, our method proves effective on a wide range of models, in contrast to prior work, demonstrating a robust and potent approach to compromising the fairness of machine learning systems.

摘要: 随着人工智能和机器学习系统在现实世界应用中的越来越多的采用，确保其公平性变得越来越重要。算法公平性方面的大部分工作都集中在评估和改进机器学习系统的公平性上。关于公平脆弱性的研究相对较少，即人工智能系统的公平性如何被故意损害。在这项工作中，我们首先提供了一个理论分析，证明了一个简单的对抗中毒策略是足以诱导最大限度地不公平的行为在朴素贝叶斯分类器。我们的关键思想是从战略上将一小部分精心制作的对抗性数据点注入训练集中，使模型的决策边界偏向于不成比例地影响受保护的群体，同时保持可推广的性能。为了说明我们的方法的实际有效性，我们在几个基准数据集和模型上进行了实验。我们发现，我们的攻击在降低多个模型和数据集的公平性指标方面明显优于现有方法，通常会实现更高水平的不公平性，对准确性的影响相当或略差。值得注意的是，与之前的工作相比，我们的方法被证明在广泛的模型上有效，展示了一种鲁棒且有力的方法来损害机器学习系统的公平性。



## **42. HybridGuard: Enhancing Minority-Class Intrusion Detection in Dew-Enabled Edge-of-Things Networks**

HybridGuard：增强Dew支持的边缘物联网中的少数类入侵检测 cs.CR

**SubmitDate**: 2025-11-12    [abs](http://arxiv.org/abs/2511.07793v1) [paper-pdf](None)

**Authors**: Binayak Kara, Ujjwal Sahua, Ciza Thomas, Jyoti Prakash Sahoo

**Abstract**: Securing Dew-Enabled Edge-of-Things (EoT) networks against sophisticated intrusions is a critical challenge. This paper presents HybridGuard, a framework that integrates machine learning and deep learning to improve intrusion detection. HybridGuard addresses data imbalance through mutual information based feature selection, ensuring that the most relevant features are used to improve detection performance, especially for minority attack classes. The framework leverages Wasserstein Conditional Generative Adversarial Networks with Gradient Penalty (WCGAN-GP) to further reduce class imbalance and enhance detection precision. It adopts a two-phase architecture called DualNetShield to support advanced traffic analysis and anomaly detection, improving the granular identification of threats in complex EoT environments. HybridGuard is evaluated on the UNSW-NB15, CIC-IDS-2017, and IOTID20 datasets, where it demonstrates strong performance across diverse attack scenarios and outperforms existing solutions in adapting to evolving cybersecurity threats. This approach establishes HybridGuard as an effective tool for protecting EoT networks against modern intrusions.

摘要: 保护支持Dew的物联网边缘（EoT）网络免受复杂入侵是一项严峻的挑战。本文介绍了HybridGuard，这是一个集成机器学习和深度学习以改进入侵检测的框架。HybridGuard通过基于互信息的特征选择来解决数据不平衡问题，确保使用最相关的特征来提高检测性能，尤其是对于少数攻击类别。该框架利用具有梯度罚分的Wasserstein条件生成对抗网络（WCGAN-GP）来进一步减少类不平衡并提高检测精度。它采用名为DualNetShield的两阶段架构来支持高级流量分析和异常检测，改善复杂EoT环境中威胁的粒度识别。HybridGuard在UNSW-NB 15、CIC-IDS-2017和IOTIP 20数据集上进行了评估，显示出在各种攻击场景中的强劲性能，并且在适应不断变化的网络安全威胁方面优于现有解决方案。这种方法使HybridGuard成为保护EoT网络免受现代入侵的有效工具。



## **43. DeepTracer: Tracing Stolen Model via Deep Coupled Watermarks**

DeepTracer：通过深度耦合水印追踪被盗模型 cs.CR

Extended version of the paper accepted by AAAI 2026

**SubmitDate**: 2025-11-13    [abs](http://arxiv.org/abs/2511.08985v1) [paper-pdf](None)

**Authors**: Yunfei Yang, Xiaojun Chen, Yuexin Xuan, Zhendong Zhao, Xin Zhao, He Li

**Abstract**: Model watermarking techniques can embed watermark information into the protected model for ownership declaration by constructing specific input-output pairs. However, existing watermarks are easily removed when facing model stealing attacks, and make it difficult for model owners to effectively verify the copyright of stolen models. In this paper, we analyze the root cause of the failure of current watermarking methods under model stealing scenarios and then explore potential solutions. Specifically, we introduce a robust watermarking framework, DeepTracer, which leverages a novel watermark samples construction method and a same-class coupling loss constraint. DeepTracer can incur a high-coupling model between watermark task and primary task that makes adversaries inevitably learn the hidden watermark task when stealing the primary task functionality. Furthermore, we propose an effective watermark samples filtering mechanism that elaborately select watermark key samples used in model ownership verification to enhance the reliability of watermarks. Extensive experiments across multiple datasets and models demonstrate that our method surpasses existing approaches in defending against various model stealing attacks, as well as watermark attacks, and achieves new state-of-the-art effectiveness and robustness.

摘要: 模型水印技术可以通过构建特定的输入输出对将水印信息嵌入到受保护的模型中以进行所有权声明。然而，当面临模型窃取攻击时，现有的水印很容易被删除，并且模型所有者很难有效验证被盗模型的版权。本文分析了当前水印方法在模型窃取场景下失败的根本原因，然后探索潜在的解决方案。具体来说，我们引入了一个鲁棒的水印框架DeepTracer，它利用了一种新型的水印样本构建方法和同类耦合损失约束。DeepTracer可以在水印任务和主要任务之间引入高耦合模型，使对手在窃取主要任务功能时不可避免地学习隐藏的水印任务。此外，我们提出了一种有效的水印样本过滤机制，精心选择用于模型所有权验证的水印关键样本，以增强水印的可靠性。跨多个数据集和模型的广泛实验表明，我们的方法在防御各种模型窃取攻击和水印攻击方面超越了现有方法，并实现了新的最先进的有效性和鲁棒性。



## **44. SIFT-Graph: Benchmarking Multimodal Defense Against Image Adversarial Attacks With Robust Feature Graph**

SFT-Shape：利用稳健的特征图对图像对抗攻击进行多模式防御基准 cs.CV

Accepted by ICCV2025 Workshop, short paper

**SubmitDate**: 2025-11-13    [abs](http://arxiv.org/abs/2511.08810v1) [paper-pdf](None)

**Authors**: Jingjie He, Weijie Liang, Zihan Shan, Matthew Caesar

**Abstract**: Adversarial attacks expose a fundamental vulnerability in modern deep vision models by exploiting their dependence on dense, pixel-level representations that are highly sensitive to imperceptible perturbations. Traditional defense strategies typically operate within this fragile pixel domain, lacking mechanisms to incorporate inherently robust visual features. In this work, we introduce SIFT-Graph, a multimodal defense framework that enhances the robustness of traditional vision models by aggregating structurally meaningful features extracted from raw images using both handcrafted and learned modalities. Specifically, we integrate Scale-Invariant Feature Transform keypoints with a Graph Attention Network to capture scale and rotation invariant local structures that are resilient to perturbations. These robust feature embeddings are then fused with traditional vision model, such as Vision Transformer and Convolutional Neural Network, to form a unified, structure-aware and perturbation defensive model. Preliminary results demonstrate that our method effectively improves the visual model robustness against gradient-based white box adversarial attacks, while incurring only a marginal drop in clean accuracy.

摘要: 对抗性攻击通过利用现代深度视觉模型对密集的像素级表示的依赖而暴露了现代深度视觉模型的根本漏洞，这些表示对不可感知的扰动高度敏感。传统的防御策略通常在这个脆弱的像素域中运行，缺乏整合固有鲁棒视觉特征的机制。在这项工作中，我们引入了SFT-Shape，这是一种多模式防御框架，通过使用手工制作和学习的模式聚合从原始图像中提取的结构上有意义的特征，增强了传统视觉模型的鲁棒性。具体来说，我们将尺度不变特征变换关键点与图形注意力网络集成，以捕获对扰动有弹性的尺度和旋转不变局部结构。这些鲁棒的特征嵌入，然后融合到传统的视觉模型，如视觉Transformer和卷积神经网络，形成一个统一的，结构感知和扰动防御模型。初步结果表明，我们的方法有效地提高了视觉模型对基于梯度的白盒对抗性攻击的鲁棒性，同时只会导致清洁精度的边际下降。



## **45. ConfGuard: A Simple and Effective Backdoor Detection for Large Language Models**

ConfGuard：大型语言模型简单有效的后门检测 cs.CR

This is an extended version of the copyrighted publication at AAAI

**SubmitDate**: 2025-11-12    [abs](http://arxiv.org/abs/2508.01365v3) [paper-pdf](None)

**Authors**: Zihan Wang, Rui Zhang, Hongwei Li, Wenshu Fan, Wenbo Jiang, Qingchuan Zhao, Guowen Xu

**Abstract**: Backdoor attacks pose a significant threat to Large Language Models (LLMs), where adversaries can embed hidden triggers to manipulate LLM's outputs. Most existing defense methods, primarily designed for classification tasks, are ineffective against the autoregressive nature and vast output space of LLMs, thereby suffering from poor performance and high latency. To address these limitations, we investigate the behavioral discrepancies between benign and backdoored LLMs in output space. We identify a critical phenomenon which we term sequence lock: a backdoored model generates the target sequence with abnormally high and consistent confidence compared to benign generation. Building on this insight, we propose ConfGuard, a lightweight and effective detection method that monitors a sliding window of token confidences to identify sequence lock. Extensive experiments demonstrate ConfGuard achieves a near 100\% true positive rate (TPR) and a negligible false positive rate (FPR) in the vast majority of cases. Crucially, the ConfGuard enables real-time detection almost without additional latency, making it a practical backdoor defense for real-world LLM deployments.

摘要: 后门攻击对大型语言模型（LLM）构成重大威胁，对手可以嵌入隐藏触发器来操纵LLM的输出。大多数现有的防御方法主要是为分类任务设计的，对LLM的自回归性质和巨大的输出空间无效，从而遭受性能差和延迟高的影响。为了解决这些限制，我们调查了输出空间中良性和后门LLM之间的行为差异。我们发现了一个关键现象，我们称之为序列锁：与良性生成相比，后门模型以异常高且一致的置信度生成目标序列。基于这一见解，我们提出了ConfGuard，这是一种轻量级且有效的检测方法，可以监控令牌置信度的滑动窗口以识别序列锁。大量实验表明，在绝大多数情况下，ConfGuard的真阳性率（TPA）接近100%，假阳性率（FPR）可忽略不计。至关重要的是，ConfGuard几乎无需额外延迟即可实现实时检测，使其成为现实世界LLM部署的实用后门防御。



## **46. Identifying the Smallest Adversarial Load Perturbation that Renders DC-OPF Infeasible**

识别导致DC-OPF不可行的最小对抗负载扰动 eess.SY

**SubmitDate**: 2025-11-13    [abs](http://arxiv.org/abs/2507.07850v2) [paper-pdf](None)

**Authors**: Samuel Chevalier, William A. Wheeler

**Abstract**: What is the globally smallest load perturbation that renders DC-OPF infeasible? Reliably identifying such "adversarial attack" perturbations has useful applications in a variety of emerging grid-related contexts, including machine learning performance verification, cybersecurity, and operational robustness of power systems dominated by stochastic renewable energy resources. In this paper, we formulate the inherently nonconvex adversarial attack problem by applying a parameterized version of Farkas' lemma to a perturbed set of DC-OPF equations. Since the resulting formulation is very hard to globally optimize, we also propose a parameterized generation control policy which, when applied to the primal DC-OPF problem, provides solvability guarantees. Together, these nonconvex problems provide guaranteed upper and lower bounds on adversarial attack size; by combining them into a single optimization problem, we can efficiently "squeeze" these bounds towards a common global solution. We apply these methods on a range of small- to medium-sized test cases from PGLib, benchmarking our results against the best adversarial attack lower bounds provided by Gurobi 12.0's spatial Branch and Bound solver.

摘要: 导致DC-OPF不可行的全球最小负载扰动是多少？可靠地识别这种“对抗攻击”扰动在各种新兴的电网相关环境中具有有用的应用，包括机器学习性能验证、网络安全和由随机可再生能源主导的电力系统的运营稳健性。在本文中，我们通过将Farkas引理的参数化版本应用于一组受干扰的DC-OPF方程，来阐述固有非凸对抗攻击问题。由于所得公式很难全局优化，我们还提出了一种参数化发电控制策略，当应用于原始DC-OPF问题时，该策略提供了可解性保证。这些非凸问题共同提供了对抗性攻击规模的有保证的上下限;通过将它们组合到单个优化问题中，我们可以有效地“挤压”这些界限以获得共同的全局解决方案。我们将这些方法应用于PGLib的一系列中小规模测试用例，并根据Guesthouse 12.0的空间Branch and Bound求解器提供的最佳对抗攻击下限对我们的结果进行基准测试。



## **47. Unlearning Imperative: Securing Trustworthy and Responsible LLMs through Engineered Forgetting**

放弃学习势在必行：通过精心设计的遗忘确保值得信赖和负责任的LLM cs.LG

14 pages, 4 figures, 4 tables

**SubmitDate**: **NEW** 2025-11-14    [abs](http://arxiv.org/abs/2511.09855v1) [paper-pdf](None)

**Authors**: James Jin Kang, Dang Bui, Thanh Pham, Huo-Chong Ling

**Abstract**: The growing use of large language models in sensitive domains has exposed a critical weakness: the inability to ensure that private information can be permanently forgotten. Yet these systems still lack reliable mechanisms to guarantee that sensitive information can be permanently removed once it has been used. Retraining from the beginning is prohibitively costly, and existing unlearning methods remain fragmented, difficult to verify, and often vulnerable to recovery. This paper surveys recent research on machine unlearning for LLMs and considers how far current approaches can address these challenges. We review methods for evaluating whether forgetting has occurred, the resilience of unlearned models against adversarial attacks, and mechanisms that can support user trust when model complexity or proprietary limits restrict transparency. Technical solutions such as differential privacy, homomorphic encryption, federated learning, and ephemeral memory are examined alongside institutional safeguards including auditing practices and regulatory frameworks. The review finds steady progress, but robust and verifiable unlearning is still unresolved. Efficient techniques that avoid costly retraining, stronger defenses against adversarial recovery, and governance structures that reinforce accountability are needed if LLMs are to be deployed safely in sensitive applications. By integrating technical and organizational perspectives, this study outlines a pathway toward AI systems that can be required to forget, while maintaining both privacy and public trust.

摘要: 大型语言模型在敏感领域的使用越来越多，暴露了一个关键弱点：无法确保私人信息可以被永久遗忘。然而，这些系统仍然缺乏可靠的机制来保证敏感信息一旦被使用就可以被永久删除。从一开始的再培训成本高得令人望而却步，而且现有的学习方法仍然支离破碎、难以验证，并且往往很容易恢复。本文调查了最近关于LLM机器去学习的研究，并考虑了当前方法可以在多大程度上解决这些挑战。我们回顾了评估是否发生遗忘的方法、未学习的模型对抗对抗攻击的弹性，以及在模型复杂性或专有限制限制透明度时可以支持用户信任的机制。与审计实践和监管框架等机构保障措施一起审查了差异隐私、同质加密、联邦学习和短暂记忆等技术解决方案。审查发现取得了稳步进展，但稳健且可验证的取消学习仍未得到解决。如果要在敏感应用程序中安全部署LLM，就需要避免昂贵的再培训的高效技术、针对对抗性恢复的更强防御以及加强问责制的治理结构。通过整合技术和组织角度，这项研究概述了一条通往人工智能系统的途径，这些系统可能被要求忘记，同时维护隐私和公众信任。



## **48. Slice-Aware Spoofing Detection in 5G Networks Using Lightweight Machine Learning**

使用轻量级机器学习在5G网络中进行切片感知欺骗检测 cs.CR

**SubmitDate**: **NEW** 2025-11-14    [abs](http://arxiv.org/abs/2511.09610v1) [paper-pdf](None)

**Authors**: Daniyal Ganiuly, Nurzhau Bolatbek

**Abstract**: The increasing virtualization of fifth generation (5G) networks expands the attack surface of the user plane, making spoofing a persistent threat to slice integrity and service reliability. This study presents a slice-aware lightweight machine-learning framework for detecting spoofing attacks within 5G network slices. The framework was implemented on a reproducible Open5GS and srsRAN testbed emulating three service classes such as enhanced Mobile Broadband (eMBB), Ultra-Reliable Low-Latency Communication (URLLC), and massive Machine-Type Communication (mMTC) under controlled benign and adversarial traffic. Two efficient classifiers, Logistic Regression and Random Forest, were trained independently for each slice using statistical flow features derived from mirrored user-plane traffic. Slice-aware training improved detection accuracy by up to 5% and achieved F1-scores between 0.93 and 0.96 while maintaining real-time operation on commodity edge hardware. The results demonstrate that aligning security intelligence with slice boundaries enhances detection reliability and preserves operational isolation, enabling practical deployment in 5G network-security environments. Conceptually, the work bridges network-security architecture and adaptive machine learning by showing that isolation-aware intelligence can achieve scalable, privacy-preserving spoofing defense without high computational cost.

摘要: 第五代（5G）网络的日益虚拟化扩大了用户平面的攻击面，使欺骗成为对切片完整性和服务可靠性的持续威胁。这项研究提出了一个切片感知的轻量级机器学习框架，用于检测5G网络切片内的欺骗攻击。该框架是在可重复的Open5GS和srsRAN测试床上实现的，该测试床上模拟了三种服务类别，例如在受控的良性和对抗性流量下增强型移动宽带（eMBB）、超可靠低延迟通信（URLLC）和大规模机器类型通信（mMT）。使用从镜像用户平面流量中获得的统计流特征，为每个切片独立训练两个高效分类器，即逻辑回归和随机森林。切片感知训练将检测准确性提高了5%，并实现了0.93至0.96之间的F1评分，同时保持商品边缘硬件的实时操作。结果表明，将安全情报与切片边界保持一致可以增强检测可靠性并保持操作隔离，从而能够在5G网络安全环境中进行实际部署。从概念上讲，该工作通过表明隔离感知智能可以在没有高计算成本的情况下实现可扩展、保护隐私的欺骗防御，从而将网络安全架构和自适应机器学习联系起来。



## **49. Boosting Adversarial Transferability via Ensemble Non-Attention**

通过吸引不注意力来提高对抗性可转移性 cs.CV

16 pages, 11 figures, accepted by AAAI 2026

**SubmitDate**: **NEW** 2025-11-14    [abs](http://arxiv.org/abs/2511.08937v2) [paper-pdf](None)

**Authors**: Yipeng Zou, Qin Liu, Jie Wu, Yu Peng, Guo Chen, Hui Zhou, Guanghui Ye

**Abstract**: Ensemble attacks integrate the outputs of surrogate models with diverse architectures, which can be combined with various gradient-based attacks to improve adversarial transferability. However, previous work shows unsatisfactory attack performance when transferring across heterogeneous model architectures. The main reason is that the gradient update directions of heterogeneous surrogate models differ widely, making it hard to reduce the gradient variance of ensemble models while making the best of individual model. To tackle this challenge, we design a novel ensemble attack, NAMEA, which for the first time integrates the gradients from the non-attention areas of ensemble models into the iterative gradient optimization process. Our design is inspired by the observation that the attention areas of heterogeneous models vary sharply, thus the non-attention areas of ViTs are likely to be the focus of CNNs and vice versa. Therefore, we merge the gradients respectively from the attention and non-attention areas of ensemble models so as to fuse the transfer information of CNNs and ViTs. Specifically, we pioneer a new way of decoupling the gradients of non-attention areas from those of attention areas, while merging gradients by meta-learning. Empirical evaluations on ImageNet dataset indicate that NAMEA outperforms AdaEA and SMER, the state-of-the-art ensemble attacks by an average of 15.0% and 9.6%, respectively. This work is the first attempt to explore the power of ensemble non-attention in boosting cross-architecture transferability, providing new insights into launching ensemble attacks.

摘要: 集合攻击将代理模型的输出与不同架构集成，可以与各种基于梯度的攻击结合以提高对抗性可转移性。然而，之前的工作表明，跨异类模型架构传输时的攻击性能不令人满意。主要原因是异类代理模型的梯度更新方向差异很大，很难在充分利用单个模型的同时降低集成模型的梯度方差。为了应对这一挑战，我们设计了一种新型的集成攻击NAEA，它首次将集成模型的非注意区域的梯度集成到迭代梯度优化过程中。我们的设计灵感来自于这样的观察：异类模型的注意力区域差异很大，因此ViT的非注意力区域可能是CNN的焦点，反之亦然。因此，我们将集成模型的注意力和非注意力区域的梯度进行合并，以融合CNN和ViT的转移信息。具体来说，我们开创了一种新方法，将非注意区域的梯度与注意区域的梯度脱钩，同时通过元学习合并梯度。对ImageNet数据集的经验评估表明，NAEA比AdaEA和SMER（最先进的综合攻击）平均分别高15.0%和9.6%。这项工作是首次尝试探索集成非注意力在提高跨体系结构可移植性方面的力量，为发起集成攻击提供了新的见解。



## **50. Biologically-Informed Hybrid Membership Inference Attacks on Generative Genomic Models**

对生成性基因组模型的生物知情混合成员推断攻击 cs.CR

**SubmitDate**: **NEW** 2025-11-14    [abs](http://arxiv.org/abs/2511.07503v2) [paper-pdf](None)

**Authors**: Asia Belfiore, Jonathan Passerat-Palmbach, Dmitrii Usynin

**Abstract**: The increased availability of genetic data has transformed genomics research, but raised many privacy concerns regarding its handling due to its sensitive nature. This work explores the use of language models (LMs) for the generation of synthetic genetic mutation profiles, leveraging differential privacy (DP) for the protection of sensitive genetic data. We empirically evaluate the privacy guarantees of our DP modes by introducing a novel Biologically-Informed Hybrid Membership Inference Attack (biHMIA), which combines traditional black box MIA with contextual genomics metrics for enhanced attack power. Our experiments show that both small and large transformer GPT-like models are viable synthetic variant generators for small-scale genomics, and that our hybrid attack leads, on average, to higher adversarial success compared to traditional metric-based MIAs.

摘要: 遗传数据可用性的增加改变了基因组学研究，但由于其敏感性，对其处理提出了许多隐私问题。这项工作探索了使用语言模型（LM）来生成合成基因突变谱，利用差异隐私（DP）来保护敏感遗传数据。我们通过引入一种新型的生物知情混合成员推断攻击（biHMIA）来经验性地评估DP模式的隐私保证，该攻击将传统的黑匣子MIA与上下文基因组学指标相结合，以增强攻击能力。我们的实验表明，小型和大型Transformer GPT类模型都是小规模基因组学的可行合成变体生成器，并且与传统的基于度量的MIA相比，我们的混合攻击平均会导致更高的对抗成功。



