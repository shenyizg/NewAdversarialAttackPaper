# Latest Adversarial Attack Papers
**update at 2025-03-27 09:37:12**

翻译来自 https://cloud.tencent.com/document/product/551/15619

## **1. The mathematics of adversarial attacks in AI -- Why deep learning is unstable despite the existence of stable neural networks**

人工智能中对抗性攻击的数学--为什么尽管存在稳定的神经网络，深度学习却不稳定 cs.LG

31 pages, 1 figure. Revised to make minor changes to notation and  references

**SubmitDate**: 2025-03-26    [abs](http://arxiv.org/abs/2109.06098v2) [paper-pdf](http://arxiv.org/pdf/2109.06098v2)

**Authors**: Alexander Bastounis, Anders C Hansen, Verner Vlačić

**Abstract**: The unprecedented success of deep learning (DL) makes it unchallenged when it comes to classification problems. However, it is well established that the current DL methodology produces universally unstable neural networks (NNs). The instability problem has caused an enormous research effort -- with a vast literature on so-called adversarial attacks -- yet there has been no solution to the problem. Our paper addresses why there has been no solution to the problem, as we prove the following mathematical paradox: any training procedure based on training neural networks for classification problems with a fixed architecture will yield neural networks that are either inaccurate or unstable (if accurate) -- despite the provable existence of both accurate and stable neural networks for the same classification problems. The key is that the stable and accurate neural networks must have variable dimensions depending on the input, in particular, variable dimensions is a necessary condition for stability.   Our result points towards the paradox that accurate and stable neural networks exist, however, modern algorithms do not compute them. This yields the question: if the existence of neural networks with desirable properties can be proven, can one also find algorithms that compute them? There are cases in mathematics where provable existence implies computability, but will this be the case for neural networks? The contrary is true, as we demonstrate how neural networks can provably exist as approximate minimisers to standard optimisation problems with standard cost functions, however, no randomised algorithm can compute them with probability better than 1/2.

摘要: 深度学习的空前成功使其在分类问题上无人能及。然而，众所周知，当前的DL方法产生了普遍不稳定的神经网络(NNS)。不稳定问题已经引起了巨大的研究努力--有大量关于所谓的对抗性攻击的文献--但这个问题还没有解决方案。我们的论文解决了为什么这个问题没有解决方案，因为我们证明了以下数学悖论：任何基于对具有固定体系结构的分类问题的神经网络进行训练的训练过程都会产生不准确或不稳定(如果准确)的神经网络--尽管对于相同的分类问题存在准确和稳定的神经网络。关键是稳定和准确的神经网络必须具有依赖于输入的可变维度，特别是可变维度是稳定的必要条件。我们的结果指出了这样一个悖论，即准确和稳定的神经网络是存在的，然而，现代算法并不计算它们。这就产生了一个问题：如果可以证明具有理想特性的神经网络的存在，人们还能找到计算它们的算法吗？在数学中，有可证明的存在意味着可计算的情况，但神经网络会是这样吗？相反，当我们展示神经网络如何证明可以作为具有标准成本函数的标准优化问题的近似最小化存在时，然而，没有任何随机算法可以以比1/2更好的概率计算它们。



## **2. Intelligent Code Embedding Framework for High-Precision Ransomware Detection via Multimodal Execution Path Analysis**

通过多模式执行路径分析实现高精度勒索软件检测的智能代码嵌入框架 cs.CR

arXiv admin note: This paper has been withdrawn by arXiv due to  disputed and unverifiable authorship

**SubmitDate**: 2025-03-26    [abs](http://arxiv.org/abs/2501.15836v2) [paper-pdf](http://arxiv.org/pdf/2501.15836v2)

**Authors**: Levi Gareth, Maximilian Fairbrother, Peregrine Blackwood, Lucasta Underhill, Benedict Ruthermore

**Abstract**: Modern threat landscapes continue to evolve with increasing sophistication, challenging traditional detection methodologies and necessitating innovative solutions capable of addressing complex adversarial tactics. A novel framework was developed to identify ransomware activity through multimodal execution path analysis, integrating high-dimensional embeddings and dynamic heuristic derivation mechanisms to capture behavioral patterns across diverse attack variants. The approach demonstrated high adaptability, effectively mitigating obfuscation strategies and polymorphic characteristics often employed by ransomware families to evade detection. Comprehensive experimental evaluations revealed significant advancements in precision, recall, and accuracy metrics compared to baseline techniques, particularly under conditions of variable encryption speeds and obfuscated execution flows. The framework achieved scalable and computationally efficient performance, ensuring robust applicability across a range of system configurations, from resource-constrained environments to high-performance infrastructures. Notable findings included reduced false positive rates and enhanced detection latency, even for ransomware families employing sophisticated encryption mechanisms. The modular design allowed seamless integration of additional modalities, enabling extensibility and future-proofing against emerging threat vectors. Quantitative analyses further highlighted the system's energy efficiency, emphasizing its practicality for deployment in environments with stringent operational constraints. The results underline the importance of integrating advanced computational techniques and dynamic adaptability to safeguard digital ecosystems from increasingly complex threats.

摘要: 现代威胁形势继续发展，日益复杂，对传统检测方法提出了挑战，需要能够应对复杂对抗战术的创新解决方案。提出了一种新的框架，通过多模式执行路径分析识别勒索软件活动，结合高维嵌入和动态启发式派生机制来捕获不同攻击变量的行为模式。该方法表现出高度的适应性，有效地缓解了勒索软件家族经常利用的混淆策略和多态特征来逃避检测。全面的实验评估显示，与基准技术相比，尤其是在加密速度可变和执行流模糊的情况下，精确度、召回率和准确率指标都有了显著的进步。该框架实现了可扩展和计算效率高的性能，确保了从资源受限环境到高性能基础设施等一系列系统配置的强大适用性。值得注意的发现包括降低了假阳性率和增加了检测延迟，即使对于使用复杂加密机制的勒索软件系列也是如此。模块化设计允许无缝集成其他医疗设备，实现了针对新出现的威胁载体的可扩展性和面向未来的保护。量化分析进一步强调了该系统的能效，强调了其在具有严格业务限制的环境中部署的实用性。这些结果突显了整合先进的计算技术和动态适应性以保护数字生态系统免受日益复杂的威胁的重要性。



## **3. A Survey of Secure Semantic Communications**

安全语义通信综述 cs.CR

160 pages, 27 figures

**SubmitDate**: 2025-03-26    [abs](http://arxiv.org/abs/2501.00842v2) [paper-pdf](http://arxiv.org/pdf/2501.00842v2)

**Authors**: Rui Meng, Song Gao, Dayu Fan, Haixiao Gao, Yining Wang, Xiaodong Xu, Bizhu Wang, Suyu Lv, Zhidi Zhang, Mengying Sun, Shujun Han, Chen Dong, Xiaofeng Tao, Ping Zhang

**Abstract**: Semantic communication (SemCom) is regarded as a promising and revolutionary technology in 6G, aiming to transcend the constraints of ``Shannon's trap" by filtering out redundant information and extracting the core of effective data. Compared to traditional communication paradigms, SemCom offers several notable advantages, such as reducing the burden on data transmission, enhancing network management efficiency, and optimizing resource allocation. Numerous researchers have extensively explored SemCom from various perspectives, including network architecture, theoretical analysis, potential technologies, and future applications. However, as SemCom continues to evolve, a multitude of security and privacy concerns have arisen, posing threats to the confidentiality, integrity, and availability of SemCom systems. This paper presents a comprehensive survey of the technologies that can be utilized to secure SemCom. Firstly, we elaborate on the entire life cycle of SemCom, which includes the model training, model transfer, and semantic information transmission phases. Then, we identify the security and privacy issues that emerge during these three stages. Furthermore, we summarize the techniques available to mitigate these security and privacy threats, including data cleaning, robust learning, defensive strategies against backdoor attacks, adversarial training, differential privacy, cryptography, blockchain technology, model compression, and physical-layer security. Lastly, this paper outlines future research directions to guide researchers in related fields.

摘要: 语义通信(SemCom)被认为是6G中一种很有前途的革命性技术，旨在通过过滤冗余信息和提取有效数据的核心来超越香农陷阱的限制。与传统的通信模式相比，SemCom具有一些显著的优势，如减轻数据传输负担，提高网络管理效率，优化资源配置。许多研究人员从不同的角度对SemCom进行了广泛的探索，包括网络体系结构、理论分析、潜在技术和未来应用。然而，随着SemCom的不断发展，出现了大量的安全和隐私问题，对SemCom系统的机密性、完整性和可用性构成了威胁。本文对可用于确保SemCom安全的技术进行了全面的综述。首先，详细阐述了SemCom的整个生命周期，包括模型训练、模型迁移、语义信息传递等阶段。然后，我们确定在这三个阶段中出现的安全和隐私问题。此外，我们还总结了可用于缓解这些安全和隐私威胁的技术，包括数据清理、稳健学习、针对后门攻击的防御策略、对抗性训练、差异隐私、密码学、区块链技术、模型压缩和物理层安全。最后，本文概述了未来的研究方向，以指导相关领域的研究人员。



## **4. $β$-GNN: A Robust Ensemble Approach Against Graph Structure Perturbation**

$β$-GNN：一种针对图结构扰动的鲁棒集成方法 cs.LG

This is the author's version of the paper accepted at EuroMLSys 2025

**SubmitDate**: 2025-03-26    [abs](http://arxiv.org/abs/2503.20630v1) [paper-pdf](http://arxiv.org/pdf/2503.20630v1)

**Authors**: Haci Ismail Aslan, Philipp Wiesner, Ping Xiong, Odej Kao

**Abstract**: Graph Neural Networks (GNNs) are playing an increasingly important role in the efficient operation and security of computing systems, with applications in workload scheduling, anomaly detection, and resource management. However, their vulnerability to network perturbations poses a significant challenge. We propose $\beta$-GNN, a model enhancing GNN robustness without sacrificing clean data performance. $\beta$-GNN uses a weighted ensemble, combining any GNN with a multi-layer perceptron. A learned dynamic weight, $\beta$, modulates the GNN's contribution. This $\beta$ not only weights GNN influence but also indicates data perturbation levels, enabling proactive mitigation. Experimental results on diverse datasets show $\beta$-GNN's superior adversarial accuracy and attack severity quantification. Crucially, $\beta$-GNN avoids perturbation assumptions, preserving clean data structure and performance.

摘要: 图形神经网络（GNN）在计算系统的高效操作和安全性方面发挥着越来越重要的作用，在工作负载调度、异常检测和资源管理方面应用。然而，它们对网络扰动的脆弱性构成了重大挑战。我们提出了$\Beta$-GNN，这是一个在不牺牲干净数据性能的情况下增强GNN稳健性的模型。$\Beta$-GNN使用加权集合，将任何GNN与多层感知器相结合。习得的动态权重$\Beta$调节GNN的贡献。此$\Beta$不仅加权GNN影响，还指示数据扰动水平，从而实现主动缓解。不同数据集的实验结果表明$\Beta$-GNN具有卓越的对抗准确性和攻击严重性量化。至关重要的是，$\Beta$-GNN避免了扰动假设，保留了干净的数据结构和性能。



## **5. State-Aware Perturbation Optimization for Robust Deep Reinforcement Learning**

用于稳健深度强化学习的状态感知扰动优化 cs.LG

15 pages, 11 figures

**SubmitDate**: 2025-03-26    [abs](http://arxiv.org/abs/2503.20613v1) [paper-pdf](http://arxiv.org/pdf/2503.20613v1)

**Authors**: Zongyuan Zhang, Tianyang Duan, Zheng Lin, Dong Huang, Zihan Fang, Zekai Sun, Ling Xiong, Hongbin Liang, Heming Cui, Yong Cui

**Abstract**: Recently, deep reinforcement learning (DRL) has emerged as a promising approach for robotic control. However, the deployment of DRL in real-world robots is hindered by its sensitivity to environmental perturbations. While existing whitebox adversarial attacks rely on local gradient information and apply uniform perturbations across all states to evaluate DRL robustness, they fail to account for temporal dynamics and state-specific vulnerabilities. To combat the above challenge, we first conduct a theoretical analysis of white-box attacks in DRL by establishing the adversarial victim-dynamics Markov decision process (AVD-MDP), to derive the necessary and sufficient conditions for a successful attack. Based on this, we propose a selective state-aware reinforcement adversarial attack method, named STAR, to optimize perturbation stealthiness and state visitation dispersion. STAR first employs a soft mask-based state-targeting mechanism to minimize redundant perturbations, enhancing stealthiness and attack effectiveness. Then, it incorporates an information-theoretic optimization objective to maximize mutual information between perturbations, environmental states, and victim actions, ensuring a dispersed state-visitation distribution that steers the victim agent into vulnerable states for maximum return reduction. Extensive experiments demonstrate that STAR outperforms state-of-the-art benchmarks.

摘要: 近年来，深度强化学习(DRL)已成为机器人控制的一种很有前途的方法。然而，DRL对环境扰动的敏感性阻碍了DRL在现实世界机器人中的部署。虽然现有的白盒攻击依赖于局部梯度信息，并在所有状态上应用统一的扰动来评估DRL的健壮性，但它们无法考虑时间动态和特定于状态的脆弱性。为了应对上述挑战，我们首先通过建立对抗性受害者-动态马尔可夫决策过程(AVD-MDP)对DRL中的白盒攻击进行了理论分析，得到了攻击成功的充要条件。在此基础上，提出了一种选择性状态感知强化对抗攻击方法STAR，以优化扰动隐蔽性和状态访问分散性。STAR首先采用了一种基于软掩码的状态定位机制，以最大限度地减少冗余干扰，增强隐蔽性和攻击效率。然后，它结合了一个信息论优化目标，以最大化扰动、环境状态和受害者操作之间的相互信息，确保分散的状态访问分布，将受害者代理引导到脆弱状态，以最大限度地减少回报。广泛的实验表明，STAR的性能优于最先进的基准测试。



## **6. Feature Statistics with Uncertainty Help Adversarial Robustness**

具有不确定性的特征统计有助于对抗稳健性 cs.LG

**SubmitDate**: 2025-03-26    [abs](http://arxiv.org/abs/2503.20583v1) [paper-pdf](http://arxiv.org/pdf/2503.20583v1)

**Authors**: Ran Wang, Xinlei Zhou, Rihao Li, Meng Hu, Wenhui Wu, Yuheng Jia

**Abstract**: Despite the remarkable success of deep neural networks (DNNs), the security threat of adversarial attacks poses a significant challenge to the reliability of DNNs. By introducing randomness into different parts of DNNs, stochastic methods can enable the model to learn some uncertainty, thereby improving model robustness efficiently. In this paper, we theoretically discover a universal phenomenon that adversarial attacks will shift the distributions of feature statistics. Motivated by this theoretical finding, we propose a robustness enhancement module called Feature Statistics with Uncertainty (FSU). It resamples channel-wise feature means and standard deviations of examples from multivariate Gaussian distributions, which helps to reconstruct the attacked examples and calibrate the shifted distributions. The calibration recovers some domain characteristics of the data for classification, thereby mitigating the influence of perturbations and weakening the ability of attacks to deceive models. The proposed FSU module has universal applicability in training, attacking, predicting and fine-tuning, demonstrating impressive robustness enhancement ability at trivial additional time cost. For example, against powerful optimization-based CW attacks, by incorporating FSU into attacking and predicting phases, it endows many collapsed state-of-the-art models with 50%-80% robust accuracy on CIFAR10, CIFAR100 and SVHN.

摘要: 尽管深度神经网络(DNN)取得了显著的成功，但敌意攻击带来的安全威胁对DNN的可靠性构成了巨大的挑战。通过将随机性引入到DNN的不同部分，随机方法可以使模型学习一些不确定性，从而有效地提高模型的稳健性。在本文中，我们从理论上发现了一个普遍现象，即对抗性攻击会改变特征统计量的分布。受这一理论发现的启发，我们提出了一种称为不确定性特征统计(FSU)的稳健性增强模块。它从多元高斯分布中重采样样本的通道特征均值和标准差，从而帮助重建被攻击的样本和校准移位分布。校准恢复了数据的某些领域特征用于分类，从而减轻了扰动的影响，削弱了攻击欺骗模型的能力。所提出的FSU模型在训练、攻击、预测和微调方面具有普遍的适用性，在很小的额外时间代价下表现出令人印象深刻的健壮性增强能力。例如，针对强大的基于优化的CW攻击，通过将FSU引入攻击和预测阶段，它赋予许多崩溃的最先进模型在CIFAR10、CIFAR100和SVHN上50%-80%的稳健准确率。



## **7. Aligning Visual Contrastive learning models via Preference Optimization**

通过偏好优化调整视觉对比学习模型 cs.CV

**SubmitDate**: 2025-03-26    [abs](http://arxiv.org/abs/2411.08923v3) [paper-pdf](http://arxiv.org/pdf/2411.08923v3)

**Authors**: Amirabbas Afzali, Borna Khodabandeh, Ali Rasekh, Mahyar JafariNodeh, Sepehr kazemi, Simon Gottschalk

**Abstract**: Contrastive learning models have demonstrated impressive abilities to capture semantic similarities by aligning representations in the embedding space. However, their performance can be limited by the quality of the training data and its inherent biases. While Preference Optimization (PO) methods such as Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) have been applied to align generative models with human preferences, their use in contrastive learning has yet to be explored. This paper introduces a novel method for training contrastive learning models using different PO methods to break down complex concepts. Our method systematically aligns model behavior with desired preferences, enhancing performance on the targeted task. In particular, we focus on enhancing model robustness against typographic attacks and inductive biases, commonly seen in contrastive vision-language models like CLIP. Our experiments demonstrate that models trained using PO outperform standard contrastive learning techniques while retaining their ability to handle adversarial challenges and maintain accuracy on other downstream tasks. This makes our method well-suited for tasks requiring fairness, robustness, and alignment with specific preferences. We evaluate our method for tackling typographic attacks on images and explore its ability to disentangle gender concepts and mitigate gender bias, showcasing the versatility of our approach.

摘要: 对比学习模型已经显示出令人印象深刻的能力，通过在嵌入空间中对齐表征来捕捉语义相似性。然而，它们的表现可能会受到训练数据质量及其固有偏差的限制。虽然偏好优化(PO)方法，如人类反馈强化学习(RLHF)和直接偏好优化(DPO)已被应用于使生成模型与人类偏好保持一致，但它们在对比学习中的应用还有待探索。本文介绍了一种训练对比学习模型的新方法，该方法使用不同的PO方法来分解复杂的概念。我们的方法系统地使模型行为与期望的偏好保持一致，从而提高目标任务的性能。特别是，我们专注于增强模型对排版攻击和归纳偏见的健壮性，这在对比视觉语言模型中很常见，如CLIP。我们的实验表明，使用PO训练的模型优于标准的对比学习技术，同时保持了它们处理对抗性挑战和在其他下游任务上保持准确性的能力。这使得我们的方法非常适合需要公平性、健壮性和与特定偏好一致的任务。我们评估了我们处理图像排版攻击的方法，并探索了它理清性别概念和减轻性别偏见的能力，展示了我们方法的多功能性。



## **8. Lipschitz Constant Meets Condition Number: Learning Robust and Compact Deep Neural Networks**

Lipschitz常数满足条件数：学习稳健且紧凑的深度神经网络 cs.LG

13 pages, 6 figures

**SubmitDate**: 2025-03-26    [abs](http://arxiv.org/abs/2503.20454v1) [paper-pdf](http://arxiv.org/pdf/2503.20454v1)

**Authors**: Yangqi Feng, Shing-Ho J. Lin, Baoyuan Gao, Xian Wei

**Abstract**: Recent research has revealed that high compression of Deep Neural Networks (DNNs), e.g., massive pruning of the weight matrix of a DNN, leads to a severe drop in accuracy and susceptibility to adversarial attacks. Integration of network pruning into an adversarial training framework has been proposed to promote adversarial robustness. It has been observed that a highly pruned weight matrix tends to be ill-conditioned, i.e., increasing the condition number of the weight matrix. This phenomenon aggravates the vulnerability of a DNN to input noise. Although a highly pruned weight matrix is considered to be able to lower the upper bound of the local Lipschitz constant to tolerate large distortion, the ill-conditionedness of such a weight matrix results in a non-robust DNN model. To overcome this challenge, this work develops novel joint constraints to adjust the weight distribution of networks, namely, the Transformed Sparse Constraint joint with Condition Number Constraint (TSCNC), which copes with smoothing distribution and differentiable constraint functions to reduce condition number and thus avoid the ill-conditionedness of weight matrices. Furthermore, our theoretical analyses unveil the relevance between the condition number and the local Lipschitz constant of the weight matrix, namely, the sharply increasing condition number becomes the dominant factor that restricts the robustness of over-sparsified models. Extensive experiments are conducted on several public datasets, and the results show that the proposed constraints significantly improve the robustness of a DNN with high pruning rates.

摘要: 最近的研究表明，深度神经网络(DNN)的高度压缩，例如对DNN的权重矩阵进行大量剪枝，导致准确率和对对手攻击的敏感性严重下降。已提出将网络修剪整合到对抗性训练框架中以提高对抗性健壮性。已经观察到，高度剪枝的权重矩阵往往是病态的，即增加了权重矩阵的条件数。这种现象加剧了DNN对输入噪声的脆弱性。虽然高度剪枝的权重矩阵被认为能够降低局部Lipschitz常数的上界以容忍大的失真，但这样的权重矩阵的病态导致了非稳健的DNN模型。为了克服这一挑战，本文提出了一种新的联合约束来调整网络的权值分布，即变换稀疏约束与条件数约束(TSCNC)，它通过平滑分布和可微约束函数来减少条件数，从而避免了权值矩阵的病态。此外，我们的理论分析揭示了条件数与权重矩阵的局部Lipschitz常数之间的相关性，即急剧增加的条件数成为限制过稀疏模型稳健性的主要因素。在几个公共数据集上进行了大量的实验，结果表明，所提出的约束显著提高了具有高剪枝率的DNN的健壮性。



## **9. UnReference: analysis of the effect of spoofing on RTK reference stations for connected rovers**

UnReference：分析欺骗对已连接漫游者的TEK参考站的影响 cs.CR

To appear the the 2025 IEEE/ION Position, Navigation and Localization  Symposium

**SubmitDate**: 2025-03-26    [abs](http://arxiv.org/abs/2503.20364v1) [paper-pdf](http://arxiv.org/pdf/2503.20364v1)

**Authors**: Marco Spanghero, Panos Papadimitratos

**Abstract**: Global Navigation Satellite Systems (GNSS) provide standalone precise navigation for a wide gamut of applications. Nevertheless, applications or systems such as unmanned vehicles (aerial or ground vehicles and surface vessels) generally require a much higher level of accuracy than those provided by standalone receivers. The most effective and economical way of achieving centimeter-level accuracy is to rely on corrections provided by fixed \emph{reference station} receivers to improve the satellite ranging measurements. Differential GNSS (DGNSS) and Real Time Kinematics (RTK) provide centimeter-level accuracy by distributing online correction streams to connected nearby mobile receivers typically termed \emph{rovers}. However, due to their static nature, reference stations are prime targets for GNSS attacks, both simplistic jamming and advanced spoofing, with different levels of adversarial control and complexity. Jamming the reference station would deny corrections and thus accuracy to the rovers. Spoofing the reference station would force it to distribute misleading corrections. As a result, all connected rovers using those corrections will be equally influenced by the adversary independently of their actual trajectory. We evaluate a battery of tests generated with an RF simulator to test the robustness of a common DGNSS/RTK processing library and receivers. We test both jamming and synchronized spoofing to demonstrate that adversarial action on the rover using reference spoofing is both effective and convenient from an adversarial perspective. Additionally, we discuss possible strategies based on existing countermeasures (self-validation of the PNT solution and monitoring of own clock drift) that the rover and the reference station can adopt to avoid using or distributing bogus corrections.

摘要: 全球导航卫星系统(GNSS)为广泛的应用提供独立的精确导航。然而，无人驾驶飞行器(空中或地面飞行器和水面舰船)等应用或系统通常需要比独立接收器提供的精度高得多的精度。实现厘米级精度的最有效和最经济的方法是依靠固定的参考站接收机提供的改正来改进卫星测距测量。差分GNSS(DGNSS)和实时运动学(RTK)通过将在线校正流分发到连接的附近移动接收器提供厘米级的精度，这些接收器通常称为\emph{rover}。然而，由于其静态性质，参考站是GNSS攻击的主要目标，既有简单的干扰，也有高级的欺骗，具有不同程度的对抗性控制和复杂程度。干扰参考站将拒绝校正，从而拒绝月球车的精度。欺骗参考站将迫使它发布误导性的修正。因此，所有使用这些修正的联网漫游车都将同样受到对手的影响，而与他们的实际轨迹无关。我们评估了一组由射频模拟器生成的测试，以测试通用DGNSS/RTK处理库和接收器的健壮性。我们对干扰和同步欺骗进行了测试，以证明从对抗的角度来看，使用参考欺骗对漫游车进行对抗操作是有效和方便的。此外，我们讨论了基于现有对策(PNT解决方案的自我验证和监测自身时钟漂移)的可能策略，漫游者和参考站可以采用这些策略来避免使用或分发伪校正值。



## **10. Enabling Heterogeneous Adversarial Transferability via Feature Permutation Attacks**

通过特征排列攻击实现异类对抗可移植性 cs.CV

PAKDD 2025. Main Track

**SubmitDate**: 2025-03-26    [abs](http://arxiv.org/abs/2503.20310v1) [paper-pdf](http://arxiv.org/pdf/2503.20310v1)

**Authors**: Tao Wu, Tie Luo

**Abstract**: Adversarial attacks in black-box settings are highly practical, with transfer-based attacks being the most effective at generating adversarial examples (AEs) that transfer from surrogate models to unseen target models. However, their performance significantly degrades when transferring across heterogeneous architectures -- such as CNNs, MLPs, and Vision Transformers (ViTs) -- due to fundamental architectural differences. To address this, we propose Feature Permutation Attack (FPA), a zero-FLOP, parameter-free method that enhances adversarial transferability across diverse architectures. FPA introduces a novel feature permutation (FP) operation, which rearranges pixel values in selected feature maps to simulate long-range dependencies, effectively making CNNs behave more like ViTs and MLPs. This enhances feature diversity and improves transferability both across heterogeneous architectures and within homogeneous CNNs. Extensive evaluations on 14 state-of-the-art architectures show that FPA achieves maximum absolute gains in attack success rates of 7.68% on CNNs, 14.57% on ViTs, and 14.48% on MLPs, outperforming existing black-box attacks. Additionally, FPA is highly generalizable and can seamlessly integrate with other transfer-based attacks to further boost their performance. Our findings establish FPA as a robust, efficient, and computationally lightweight strategy for enhancing adversarial transferability across heterogeneous architectures.

摘要: 黑盒环境中的对抗性攻击具有很高的实用性，基于转移的攻击在生成从代理模型转移到看不见的目标模型的对抗性示例(AE)方面是最有效的。然而，由于基本的体系结构差异，在跨不同的体系结构(如CNN、MLP和Vision Transformers(VIT))传输时，它们的性能会显著降低。为了解决这一问题，我们提出了特征置换攻击(FPA)，这是一种零触发器、无参数的方法，可以增强跨不同体系结构的对抗性传输。FPA引入了一种新的特征置换(FP)操作，通过重新排列所选特征映射中的像素值来模拟长期依赖关系，有效地使CNN的行为更像VITS和MLP。这增强了功能多样性，并提高了跨异类架构和在同构CNN内的可转移性。对14种最先进的体系结构的广泛评估表明，FPA在攻击成功率上取得了最大的绝对收益，CNN上的攻击成功率为7.68%，VITS上的攻击成功率为14.57%，MLP上的攻击成功率为14.48%，性能优于现有的黑盒攻击。此外，FPA具有高度的通用性，可以与其他基于传输的攻击无缝集成，以进一步提高它们的性能。我们的发现将FPA确立为一种健壮、高效和计算轻量级的策略，用于增强跨不同架构的对抗性可转移性。



## **11. Are We There Yet? Unraveling the State-of-the-Art Graph Network Intrusion Detection Systems**

我们到了吗？图网络入侵检测系统的研究现状 cs.CR

**SubmitDate**: 2025-03-26    [abs](http://arxiv.org/abs/2503.20281v1) [paper-pdf](http://arxiv.org/pdf/2503.20281v1)

**Authors**: Chenglong Wang, Pujia Zheng, Jiaping Gui, Cunqing Hua, Wajih Ul Hassan

**Abstract**: Network Intrusion Detection Systems (NIDS) are vital for ensuring enterprise security. Recently, Graph-based NIDS (GIDS) have attracted considerable attention because of their capability to effectively capture the complex relationships within the graph structures of data communications. Despite their promise, the reproducibility and replicability of these GIDS remain largely unexplored, posing challenges for developing reliable and robust detection systems. This study bridges this gap by designing a systematic approach to evaluate state-of-the-art GIDS, which includes critically assessing, extending, and clarifying the findings of these systems. We further assess the robustness of GIDS under adversarial attacks. Evaluations were conducted on three public datasets as well as a newly collected large-scale enterprise dataset. Our findings reveal significant performance discrepancies, highlighting challenges related to dataset scale, model inputs, and implementation settings. We demonstrate difficulties in reproducing and replicating results, particularly concerning false positive rates and robustness against adversarial attacks. This work provides valuable insights and recommendations for future research, emphasizing the importance of rigorous reproduction and replication studies in developing robust and generalizable GIDS solutions.

摘要: 网络入侵检测系统对于确保企业安全至关重要。最近，基于图的网络入侵检测系统(GID)由于能够有效地捕捉数据通信图结构中的复杂关系而引起了人们的极大关注。尽管前景看好，但这些全球入侵检测系统的重现性和可复制性在很大程度上仍未得到探索，这对开发可靠和强大的检测系统构成了挑战。这项研究通过设计一种系统的方法来评估最先进的GID来弥合这一差距，其中包括对这些系统的研究结果进行批判性评估、扩展和澄清。我们进一步评估了GID在敌意攻击下的健壮性。对三个公共数据集以及新收集的一个大型企业数据集进行了评价。我们的发现揭示了显著的性能差异，突出了与数据集规模、模型输入和实施设置相关的挑战。我们展示了在复制和复制结果方面的困难，特别是关于假阳性率和对对手攻击的健壮性。这项工作为今后的研究提供了有价值的见解和建议，强调了严格的复制和复制研究在开发健壮和可推广的GIDS解决方案方面的重要性。



## **12. Hi-ALPS -- An Experimental Robustness Quantification of Six LiDAR-based Object Detection Systems for Autonomous Driving**

Hi-ALPS --六种基于LiDART的自动驾驶目标检测系统的实验鲁棒性量化 cs.CV

**SubmitDate**: 2025-03-26    [abs](http://arxiv.org/abs/2503.17168v2) [paper-pdf](http://arxiv.org/pdf/2503.17168v2)

**Authors**: Alexandra Arzberger, Ramin Tavakoli Kolagari

**Abstract**: Light Detection and Ranging (LiDAR) is an essential sensor technology for autonomous driving as it can capture high-resolution 3D data. As 3D object detection systems (OD) can interpret such point cloud data, they play a key role in the driving decisions of autonomous vehicles. Consequently, such 3D OD must be robust against all types of perturbations and must therefore be extensively tested. One approach is the use of adversarial examples, which are small, sometimes sophisticated perturbations in the input data that change, i.e., falsify, the prediction of the OD. These perturbations are carefully designed based on the weaknesses of the OD. The robustness of the OD cannot be quantified with adversarial examples in general, because if the OD is vulnerable to a given attack, it is unclear whether this is due to the robustness of the OD or whether the attack algorithm produces particularly strong adversarial examples. The contribution of this work is Hi-ALPS -- Hierarchical Adversarial-example-based LiDAR Perturbation Level System, where higher robustness of the OD is required to withstand the perturbations as the perturbation levels increase. In doing so, the Hi-ALPS levels successively implement a heuristic followed by established adversarial example approaches. In a series of comprehensive experiments using Hi-ALPS, we quantify the robustness of six state-of-the-art 3D OD under different types of perturbations. The results of the experiments show that none of the OD is robust against all Hi-ALPS levels; an important factor for the ranking is that human observers can still correctly recognize the perturbed objects, as the respective perturbations are small. To increase the robustness of the OD, we discuss the applicability of state-of-the-art countermeasures. In addition, we derive further suggestions for countermeasures based on our experimental results.

摘要: 光检测和测距(LiDAR)是自动驾驶的一项基本传感器技术，因为它可以捕获高分辨率的3D数据。由于3D对象检测系统(OD)可以解释这样的点云数据，因此它们在自动驾驶车辆的驾驶决策中发挥着关键作用。因此，这样的3D OD必须对所有类型的扰动具有健壮性，因此必须进行广泛的测试。一种方法是使用对抗性例子，这是输入数据中的小的、有时是复杂的扰动，改变了OD的预测，即伪造了OD的预测。这些扰动是根据OD的弱点精心设计的。OD的稳健性一般不能用对抗性示例来量化，因为如果OD容易受到给定攻击，则不清楚这是由于OD的稳健性还是攻击算法产生特别强的对抗性示例。这项工作的贡献是Hi-Alps--基于分层对抗性实例的LiDAR扰动级别系统，其中要求OD具有更高的鲁棒性，以抵御随着扰动级别的增加而产生的扰动。在这样做的过程中，高阿尔卑斯山级别相继实施了启发式方法，随后是既定的对抗性范例方法。在使用Hi-Alps的一系列综合实验中，我们量化了六种最先进的3D OD在不同类型的扰动下的稳健性。实验结果表明，没有一个OD对所有的高阿尔卑斯山水平都是健壮的；排名的一个重要因素是人类观察者仍然可以正确地识别扰动对象，因为各自的扰动很小。为了增加OD的健壮性，我们讨论了最新对策的适用性。此外，我们还根据实验结果得出了进一步的对策建议。



## **13. How Secure is Forgetting? Linking Machine Unlearning to Machine Learning Attacks**

忘记有多安全？将机器取消学习与机器学习攻击联系起来 cs.CR

**SubmitDate**: 2025-03-26    [abs](http://arxiv.org/abs/2503.20257v1) [paper-pdf](http://arxiv.org/pdf/2503.20257v1)

**Authors**: Muhammed Shafi K. P., Serena Nicolazzo, Antonino Nocera, Vinod P

**Abstract**: As Machine Learning (ML) evolves, the complexity and sophistication of security threats against this paradigm continue to grow as well, threatening data privacy and model integrity. In response, Machine Unlearning (MU) is a recent technology that aims to remove the influence of specific data from a trained model, enabling compliance with privacy regulations and user requests. This can be done for privacy compliance (e.g., GDPR's right to be forgotten) or model refinement. However, the intersection between classical threats in ML and MU remains largely unexplored. In this Systematization of Knowledge (SoK), we provide a structured analysis of security threats in ML and their implications for MU. We analyze four major attack classes, namely, Backdoor Attacks, Membership Inference Attacks (MIA), Adversarial Attacks, and Inversion Attacks, we investigate their impact on MU and propose a novel classification based on how they are usually used in this context. Finally, we identify open challenges, including ethical considerations, and explore promising future research directions, paving the way for future research in secure and privacy-preserving Machine Unlearning.

摘要: 随着机器学习(ML)的发展，针对该范式的安全威胁的复杂性和复杂性也在继续增长，威胁到数据隐私和模型完整性。作为回应，机器遗忘(MU)是一种最近的技术，旨在从训练的模型中消除特定数据的影响，使其能够遵守隐私法规和用户请求。这可以用于隐私合规(例如，GDPR的被遗忘权)或模型改进。然而，ML和MU中的经典威胁之间的交集在很大程度上仍未被探索。在知识系统化(SOK)中，我们对ML中的安全威胁及其对MU的影响进行了结构化分析。我们分析了四种主要的攻击类型，即后门攻击、成员关系推断攻击(MIA)、对抗性攻击和反转攻击，研究了它们对MU的影响，并根据它们在这种情况下的使用情况提出了一种新的分类。最后，我们确定了开放的挑战，包括伦理方面的考虑，并探索了有前途的未来研究方向，为未来安全和隐私保护的机器遗忘的研究铺平了道路。



## **14. Defending against Backdoor Attack on Deep Neural Networks**

防御深度神经网络的后门攻击 cs.CR

This workshop manuscript is not a publication and will not be  published anywhere

**SubmitDate**: 2025-03-26    [abs](http://arxiv.org/abs/2002.12162v3) [paper-pdf](http://arxiv.org/pdf/2002.12162v3)

**Authors**: Hao Cheng, Kaidi Xu, Sijia Liu, Pin-Yu Chen, Pu Zhao, Xue Lin

**Abstract**: Although deep neural networks (DNNs) have achieved a great success in various computer vision tasks, it is recently found that they are vulnerable to adversarial attacks. In this paper, we focus on the so-called \textit{backdoor attack}, which injects a backdoor trigger to a small portion of training data (also known as data poisoning) such that the trained DNN induces misclassification while facing examples with this trigger. To be specific, we carefully study the effect of both real and synthetic backdoor attacks on the internal response of vanilla and backdoored DNNs through the lens of Gard-CAM. Moreover, we show that the backdoor attack induces a significant bias in neuron activation in terms of the $\ell_\infty$ norm of an activation map compared to its $\ell_1$ and $\ell_2$ norm. Spurred by our results, we propose the \textit{$\ell_\infty$-based neuron pruning} to remove the backdoor from the backdoored DNN. Experiments show that our method could effectively decrease the attack success rate, and also hold a high classification accuracy for clean images.

摘要: 尽管深度神经网络(DNN)在各种计算机视觉任务中取得了巨大的成功，但最近发现它们容易受到对手的攻击。在本文中，我们重点研究了所谓的后门攻击，它向一小部分训练数据注入一个后门触发器(也称为数据中毒)，使得训练的DNN在面对有这个触发器的例子时会导致错误分类。具体地说，我们通过GARD-CAM的镜头仔细研究了真实和合成后门攻击对香草和后置DNN内部反应的影响。此外，我们还发现，与激活图的$\ell_1$和$\ell_2$范数相比，后门攻击在激活映射的$\ell_\inty$范数方面诱导了显著的神经元激活偏差。受我们研究结果的启发，我们提出了基于文本的神经元剪枝方法来去除后门DNN中的后门。实验表明，该方法能有效降低攻击成功率，并对干净图像保持较高的分类准确率。



## **15. Persistence of Backdoor-based Watermarks for Neural Networks: A Comprehensive Evaluation**

神经网络背景水印持久性的综合评价 cs.LG

Preprint. Under Review

**SubmitDate**: 2025-03-26    [abs](http://arxiv.org/abs/2501.02704v2) [paper-pdf](http://arxiv.org/pdf/2501.02704v2)

**Authors**: Anh Tu Ngo, Chuan Song Heng, Nandish Chattopadhyay, Anupam Chattopadhyay

**Abstract**: Deep Neural Networks (DNNs) have gained considerable traction in recent years due to the unparalleled results they gathered. However, the cost behind training such sophisticated models is resource intensive, resulting in many to consider DNNs to be intellectual property (IP) to model owners. In this era of cloud computing, high-performance DNNs are often deployed all over the internet so that people can access them publicly. As such, DNN watermarking schemes, especially backdoor-based watermarks, have been actively developed in recent years to preserve proprietary rights. Nonetheless, there lies much uncertainty on the robustness of existing backdoor watermark schemes, towards both adversarial attacks and unintended means such as fine-tuning neural network models. One reason for this is that no complete guarantee of robustness can be assured in the context of backdoor-based watermark. In this paper, we extensively evaluate the persistence of recent backdoor-based watermarks within neural networks in the scenario of fine-tuning, we propose/develop a novel data-driven idea to restore watermark after fine-tuning without exposing the trigger set. Our empirical results show that by solely introducing training data after fine-tuning, the watermark can be restored if model parameters do not shift dramatically during fine-tuning. Depending on the types of trigger samples used, trigger accuracy can be reinstated to up to 100%. Our study further explores how the restoration process works using loss landscape visualization, as well as the idea of introducing training data in fine-tuning stage to alleviate watermark vanishing.

摘要: 近年来，深度神经网络（DNN）由于其收集的无与伦比的结果而获得了相当大的吸引力。然而，训练这种复杂模型的成本是资源密集型的，导致许多人认为DNN是模型所有者的知识产权（IP）。在这个云计算时代，高性能DNN通常部署在整个互联网上，以便人们可以公开访问它们。因此，DNN水印方案，特别是基于后门的水印，近年来已经被积极开发以保护专有权利。尽管如此，现有后门水印方案对于对抗性攻击和微调神经网络模型等非预期手段的稳健性仍存在很大的不确定性。原因之一是，在基于后门的水印的背景下，无法完全保证稳健性。在本文中，我们广泛评估了神经网络中最近基于后门的水印在微调场景中的持久性，我们提出/开发了一种新颖的数据驱动思想，用于在微调后恢复水印，而不暴露触发集。我们的经验结果表明，通过在微调后仅引入训练数据，如果模型参数在微调期间没有发生显着变化，则可以恢复水印。根据所使用的触发样本类型，触发准确度可以恢复至高达100%。我们的研究进一步探索了恢复过程如何使用损失景观可视化，以及在微调阶段引入训练数据以减轻水印消失的想法。



## **16. Joint Task Offloading and User Scheduling in 5G MEC under Jamming Attacks**

干扰攻击下的5G MEC联合任务卸载和用户调度 cs.CR

6 pages, 5 figures, Accepted to IEEE International Conference in  Communications (ICC) 2025

**SubmitDate**: 2025-03-26    [abs](http://arxiv.org/abs/2501.13227v2) [paper-pdf](http://arxiv.org/pdf/2501.13227v2)

**Authors**: Mohammadreza Amini, Burak Kantarci, Claude D'Amours, Melike Erol-Kantarci

**Abstract**: In this paper, we propose a novel joint task offloading and user scheduling (JTO-US) framework for 5G mobile edge computing (MEC) systems under security threats from jamming attacks. The goal is to minimize the delay and the ratio of dropped tasks, taking into account both communication and computation delays. The system model includes a 5G network equipped with MEC servers and an adversarial on-off jammer that disrupts communication. The proposed framework optimally schedules tasks and users to minimize the impact of jamming while ensuring that high-priority tasks are processed efficiently. Genetic algorithm (GA) is used to solve the optimization problem, and the results are compared with benchmark methods such as GA without considering jamming effect, Shortest Job First (SJF), and Shortest Deadline First (SDF). The simulation results demonstrate that the proposed JTO-US framework achieves the lowest drop ratio and effectively manages priority tasks, outperforming existing methods. Particularly, when the jamming probability is 0.8, the proposed framework mitigates the jammer's impact by reducing the drop ratio to 63%, compared to 89% achieved by the next best method.

摘要: 针对5G移动边缘计算(MEC)系统面临干扰攻击的安全威胁，提出了一种新的联合任务卸载和用户调度(JTO-US)框架。目标是最小化延迟和丢弃任务的比率，同时考虑通信和计算延迟。该系统模型包括一个配备MEC服务器的5G网络和一个会中断通信的对抗性开关干扰器。该框架对任务和用户进行优化调度，在保证高优先级任务得到有效处理的同时，将干扰的影响降至最低。采用遗传算法(GA)对优化问题进行求解，并与不考虑干扰影响的GA、最短作业优先(SJF)、最短截止时间优先(SDF)等基准算法进行了比较。仿真结果表明，JTO-US框架实现了最低的丢包率，有效地管理了优先级任务，优于已有的方法。特别是，当干扰概率为0.8时，所提出的框架通过将丢包率降低到63%来减轻干扰的影响，而次优方法的丢包率为89%。



## **17. Exploring Adversarial Threat Models in Cyber Physical Battery Systems**

探索网络物理电池系统中的对抗威胁模型 eess.SY

**SubmitDate**: 2025-03-25    [abs](http://arxiv.org/abs/2401.13801v2) [paper-pdf](http://arxiv.org/pdf/2401.13801v2)

**Authors**: Shanthan Kumar Padisala, Shashank Dhananjay Vyas, Satadru Dey

**Abstract**: Technological advancements like the Internet of Things (IoT) have facilitated data exchange across various platforms. This data exchange across various platforms has transformed the traditional battery system into a cyber physical system. Such connectivity makes modern cyber physical battery systems vulnerable to cyber threats where a cyber attacker can manipulate sensing and actuation signals to bring the battery system into an unsafe operating condition. Hence, it is essential to build resilience in modern cyber physical battery systems (CPBS) under cyber attacks. The first step of building such resilience is to analyze potential adversarial behavior, that is, how the adversaries can inject attacks into the battery systems. However, it has been found that in this under-explored area of battery cyber physical security, such an adversarial threat model has not been studied in a systematic manner. In this study, we address this gap and explore adversarial attack generation policies based on optimal control framework. The framework is developed by performing theoretical analysis, which is subsequently supported by evaluation with experimental data generated from a commercial battery cell.

摘要: 物联网(IoT)等技术进步促进了各种平台之间的数据交换。这种跨平台的数据交换已经将传统的电池系统转变为网络物理系统。这种连接使现代网络物理电池系统容易受到网络威胁，网络攻击者可以操纵传感和激励信号，使电池系统进入不安全的操作条件。因此，建立现代网络物理电池系统(CPB)在网络攻击下的弹性是至关重要的。建立这种韧性的第一步是分析潜在的敌对行为，即对手如何向电池系统注入攻击。然而，人们发现，在电池网络物理安全这个探索不足的领域，这样的对抗性威胁模型还没有得到系统的研究。在这项研究中，我们解决了这一差距，并探索了基于最优控制框架的对抗性攻击生成策略。该框架是通过进行理论分析来开发的，随后通过对商业电池产生的实验数据进行评估来支持该框架。



## **18. Bitstream Collisions in Neural Image Compression via Adversarial Perturbations**

基于对抗扰动的神经图像压缩中的比特流冲突 cs.CR

**SubmitDate**: 2025-03-25    [abs](http://arxiv.org/abs/2503.19817v1) [paper-pdf](http://arxiv.org/pdf/2503.19817v1)

**Authors**: Jordan Madden, Lhamo Dorje, Xiaohua Li

**Abstract**: Neural image compression (NIC) has emerged as a promising alternative to classical compression techniques, offering improved compression ratios. Despite its progress towards standardization and practical deployment, there has been minimal exploration into it's robustness and security. This study reveals an unexpected vulnerability in NIC - bitstream collisions - where semantically different images produce identical compressed bitstreams. Utilizing a novel whitebox adversarial attack algorithm, this paper demonstrates that adding carefully crafted perturbations to semantically different images can cause their compressed bitstreams to collide exactly. The collision vulnerability poses a threat to the practical usability of NIC, particularly in security-critical applications. The cause of the collision is analyzed, and a simple yet effective mitigation method is presented.

摘要: 神经图像压缩（NIC）已成为一个有前途的替代经典压缩技术，提供更高的压缩比。尽管它在标准化和实际部署方面取得了进展，但对其健壮性和安全性的探索却很少。这项研究揭示了一个意想不到的漏洞在NIC -比特流冲突-语义不同的图像产生相同的压缩比特流。利用一种新的白盒对抗攻击算法，本文证明了向语义不同的图像添加精心制作的扰动可以导致它们的压缩比特流准确地碰撞。碰撞漏洞对NIC的实际可用性构成威胁，特别是在安全关键应用程序中。分析了碰撞原因，并提出了一种简单有效的缓解方法。



## **19. SITA: Structurally Imperceptible and Transferable Adversarial Attacks for Stylized Image Generation**

SITA：用于风格化图像生成的结构上不可感知且可转移的对抗攻击 cs.CV

**SubmitDate**: 2025-03-25    [abs](http://arxiv.org/abs/2503.19791v1) [paper-pdf](http://arxiv.org/pdf/2503.19791v1)

**Authors**: Jingdan Kang, Haoxin Yang, Yan Cai, Huaidong Zhang, Xuemiao Xu, Yong Du, Shengfeng He

**Abstract**: Image generation technology has brought significant advancements across various fields but has also raised concerns about data misuse and potential rights infringements, particularly with respect to creating visual artworks. Current methods aimed at safeguarding artworks often employ adversarial attacks. However, these methods face challenges such as poor transferability, high computational costs, and the introduction of noticeable noise, which compromises the aesthetic quality of the original artwork. To address these limitations, we propose a Structurally Imperceptible and Transferable Adversarial (SITA) attacks. SITA leverages a CLIP-based destylization loss, which decouples and disrupts the robust style representation of the image. This disruption hinders style extraction during stylized image generation, thereby impairing the overall stylization process. Importantly, SITA eliminates the need for a surrogate diffusion model, leading to significantly reduced computational overhead. The method's robust style feature disruption ensures high transferability across diverse models. Moreover, SITA introduces perturbations by embedding noise within the imperceptible structural details of the image. This approach effectively protects against style extraction without compromising the visual quality of the artwork. Extensive experiments demonstrate that SITA offers superior protection for artworks against unauthorized use in stylized generation. It significantly outperforms existing methods in terms of transferability, computational efficiency, and noise imperceptibility. Code is available at https://github.com/A-raniy-day/SITA.

摘要: 图像生成技术在各个领域带来了重大进步，但也引起了人们对数据滥用和潜在权利侵犯的担忧，特别是在创作视觉艺术作品方面。目前旨在保护艺术品的方法往往采用对抗性攻击。然而，这些方法面临着可移植性差、计算成本高以及引入明显的噪声等挑战，这损害了原始艺术品的美学质量。为了解决这些局限性，我们提出了一种结构上不可感知和可转移的对抗性攻击(SITA)。SITA利用了基于剪辑的去风格化损失，这分离并破坏了图像的健壮样式表示。这种干扰阻碍了风格化图像生成期间的样式提取，从而损害了整个风格化过程。重要的是，SITA消除了对代理扩散模型的需要，从而显著减少了计算开销。该方法稳健的风格特征破坏确保了在不同模型之间的高度可转移性。此外，SITA通过在图像的不可察觉的结构细节中嵌入噪声来引入扰动。这种方法有效地防止了样式提取，而不会影响图稿的视觉质量。广泛的实验表明，SITA为艺术品提供了卓越的保护，防止在风格化生成中未经授权使用。它在可转移性、计算效率和噪声不可见性方面明显优于现有方法。代码可在https://github.com/A-raniy-day/SITA.上找到



## **20. Lifting Linear Sketches: Optimal Bounds and Adversarial Robustness**

提升线性草图：最佳边界和对抗稳健性 cs.DS

To appear in STOC 2025

**SubmitDate**: 2025-03-25    [abs](http://arxiv.org/abs/2503.19629v1) [paper-pdf](http://arxiv.org/pdf/2503.19629v1)

**Authors**: Elena Gribelyuk, Honghao Lin, David P. Woodruff, Huacheng Yu, Samson Zhou

**Abstract**: We introduce a novel technique for ``lifting'' dimension lower bounds for linear sketches in the real-valued setting to dimension lower bounds for linear sketches with polynomially-bounded integer entries when the input is a polynomially-bounded integer vector. Using this technique, we obtain the first optimal sketching lower bounds for discrete inputs in a data stream, for classical problems such as approximating the frequency moments, estimating the operator norm, and compressed sensing. Additionally, we lift the adaptive attack of Hardt and Woodruff (STOC, 2013) for breaking any real-valued linear sketch via a sequence of real-valued queries, and show how to obtain an attack on any integer-valued linear sketch using integer-valued queries. This shows that there is no linear sketch in a data stream with insertions and deletions that is adversarially robust for approximating any $L_p$ norm of the input, resolving a central open question for adversarially robust streaming algorithms. To do so, we introduce a new pre-processing technique of independent interest which, given an integer-valued linear sketch, increases the dimension of the sketch by only a constant factor in order to make the orthogonal lattice to its row span smooth. This pre-processing then enables us to leverage results in lattice theory on discrete Gaussian distributions and reason that efficient discrete sketches imply efficient continuous sketches. Our work resolves open questions from the Banff '14 and '17 workshops on Communication Complexity and Applications, as well as the STOC '21 and FOCS '23 workshops on adaptivity and robustness.

摘要: 当输入是多项式有界的整数向量时，我们引入了一种新的技术来提升实值环境下的线性草图的维度下界，以此来标注具有多项式有界的整数项的线性草图的维度下界。利用这一技术，我们得到了数据流中离散输入的第一个最优草图下界，用于近似频率矩、估计算子范数和压缩传感等经典问题。此外，我们解除了Hardt和Woodruff(STOEC，2013)通过实值查询序列破坏任何实值线性草图的自适应攻击，并展示了如何使用整值查询来获得对任何整值线性草图的攻击。这表明在具有插入和删除的数据流中不存在对于近似输入的任何$L_p$范数具有相反健壮性的线性草图，从而解决了相反健壮性流算法的一个中心未决问题。为此，我们引入了一种新的独立感兴趣的预处理技术，该技术在给定整数值的线性草图的情况下，仅将草图的维度增加一个恒定因子，以使正交格的行跨度光滑。然后，这种预处理使我们能够利用离散高斯分布的格子理论中的结果，并推断高效的离散草图意味着高效的连续草图。我们的工作解决了Banff‘14和’17关于通信复杂性和应用的研讨会，以及STEC‘21和FOCS’23关于适应性和稳健性的研讨会的未决问题。



## **21. Semantic Entanglement-Based Ransomware Detection via Probabilistic Latent Encryption Mapping**

通过概率潜在加密映射的基于语义纠缠的勒索软件检测 cs.CR

arXiv admin note: This paper has been withdrawn by arXiv due to  disputed and unverifiable authorship

**SubmitDate**: 2025-03-25    [abs](http://arxiv.org/abs/2502.02730v2) [paper-pdf](http://arxiv.org/pdf/2502.02730v2)

**Authors**: Mohammad Eisa, Quentin Yardley, Rafael Witherspoon, Harriet Pendlebury, Clement Rutherford

**Abstract**: Encryption-based attacks have introduced significant challenges for detection mechanisms that rely on predefined signatures, heuristic indicators, or static rule-based classifications. Probabilistic Latent Encryption Mapping presents an alternative detection framework that models ransomware-induced encryption behaviors through statistical representations of entropy deviations and probabilistic dependencies in execution traces. Unlike conventional approaches that depend on explicit bytecode analysis or predefined cryptographic function call monitoring, probabilistic inference techniques classify encryption anomalies based on their underlying statistical characteristics, ensuring greater adaptability to polymorphic attack strategies. Evaluations demonstrate that entropy-driven classification reduces false positive rates while maintaining high detection accuracy across diverse ransomware families and encryption methodologies. Experimental results further highlight the framework's ability to differentiate between benign encryption workflows and adversarial cryptographic manipulations, ensuring that classification performance remains effective across cloud-based and localized execution environments. Benchmark comparisons illustrate that probabilistic modeling exhibits advantages over heuristic and machine learning-based detection approaches, particularly in handling previously unseen encryption techniques and adversarial obfuscation strategies. Computational efficiency analysis confirms that detection latency remains within operational feasibility constraints, reinforcing the viability of probabilistic encryption classification for real-time security infrastructures. The ability to systematically infer encryption-induced deviations without requiring static attack signatures strengthens detection robustness against adversarial evasion techniques.

摘要: 基于加密的攻击为依赖预定义签名、启发式指示符或基于静态规则的分类的检测机制带来了重大挑战。概率潜在加密映射提供了一种替代检测框架，该框架通过对执行轨迹中的熵偏差和概率依赖关系的统计表示来模拟勒索软件诱导的加密行为。与依赖显式字节码分析或预定义密码函数调用监控的传统方法不同，概率推理技术基于其潜在的统计特征对加密异常进行分类，确保更好地适应多态攻击策略。评估表明，熵驱动的分类降低了误检率，同时在不同的勒索软件系列和加密方法中保持了高检测精度。实验结果进一步突出了该框架区分良性加密工作流和敌意加密操作的能力，确保分类性能在基于云的执行环境和本地化执行环境中保持有效。基准测试比较表明，概率建模比启发式和基于机器学习的检测方法具有优势，特别是在处理以前未见的加密技术和对抗性混淆策略方面。计算效率分析证实，检测延迟保持在操作可行性限制内，从而增强了概率加密分类在实时安全基础设施中的可行性。在不需要静态攻击签名的情况下，系统地推断加密引起的偏差的能力增强了针对敌意规避技术的检测健壮性。



## **22. Nanopass Back-Translation of Call-Return Trees for Mechanized Secure Compilation Proofs**

用于机械化安全编译证明的调用返回树的Nanopass反向翻译 cs.PL

ITP'25 submission

**SubmitDate**: 2025-03-25    [abs](http://arxiv.org/abs/2503.19609v1) [paper-pdf](http://arxiv.org/pdf/2503.19609v1)

**Authors**: Jérémy Thibault, Joseph Lenormand, Catalin Hritcu

**Abstract**: Researchers aim to build secure compilation chains enforcing that if there is no attack a source context can mount against a source program then there is also no attack an adversarial target context can mount against the compiled program. Proving that these compilation chains are secure is, however, challenging, and involves a non-trivial back-translation step: for any attack a target context mounts against the compiled program one has to exhibit a source context mounting the same attack against the source program. We describe a novel back-translation technique, which results in simpler proofs that can be more easily mechanized in a proof assistant. Given a finite set of finite trace prefixes, capturing the interaction recorded during an attack between a target context and the compiled program, we build a call-return tree that we back-translate into a source context producing the same trace prefixes. We use state in the generated source context to record the current location in the call-return tree. The back-translation is done in several small steps, each adding to the tree new information describing how the location should change depending on how the context regains control. To prove this back-translation correct we give semantics to every intermediate call-return tree language, using ghost state to store information and explicitly enforce execution invariants. We prove several small forward simulations, basically seeing the back-translation as a verified nanopass compiler. Thanks to this modular structure, we are able to mechanize this complex back-translation and its correctness proof in the Rocq prover without too much effort.

摘要: 研究人员的目标是构建安全的编译链，强制执行如果源上下文不会对源程序发动攻击，那么敌意目标上下文也不会对编译后的程序发动攻击。然而，证明这些编译链是安全的是具有挑战性的，并且涉及到不平凡的回译步骤：对于任何针对编译程序的目标上下文挂载的攻击，必须展示对源程序挂载相同攻击的源上下文。我们描述了一种新的反向翻译技术，它导致了更简单的证明，可以在证明助手中更容易地机械化。给定一组有限的跟踪前缀，捕获攻击期间目标上下文和编译程序之间记录的交互，我们构建调用返回树，并将其反向转换为产生相同跟踪前缀的源上下文。我们在生成的源上下文中使用状态来记录调用返回树中的当前位置。反向转换分几个小步骤完成，每个步骤都向树中添加新的信息，描述位置应该如何改变，这取决于上下文如何重新控制。为了证明这种回译的正确性，我们给出了每一种中间调用-返回树语言的语义，使用重影状态来存储信息，并显式地强制执行不变量。我们证明了几个小的正向模拟，基本上将反向翻译视为经过验证的Nanopass编译器。由于这种模块化结构，我们能够机械化这种复杂的反向翻译及其在Rocq证明器中的正确性证明，而不需要太多的努力。



## **23. Does Safety Training of LLMs Generalize to Semantically Related Natural Prompts?**

LLM的安全培训是否适用于语义相关的自然知识？ cs.CL

Accepted in ICLR 2025

**SubmitDate**: 2025-03-25    [abs](http://arxiv.org/abs/2412.03235v2) [paper-pdf](http://arxiv.org/pdf/2412.03235v2)

**Authors**: Sravanti Addepalli, Yerram Varun, Arun Suggala, Karthikeyan Shanmugam, Prateek Jain

**Abstract**: Large Language Models (LLMs) are known to be susceptible to crafted adversarial attacks or jailbreaks that lead to the generation of objectionable content despite being aligned to human preferences using safety fine-tuning methods. While the large dimensionality of input token space makes it inevitable to find adversarial prompts that can jailbreak these models, we aim to evaluate whether safety fine-tuned LLMs are safe against natural prompts which are semantically related to toxic seed prompts that elicit safe responses after alignment. We surprisingly find that popular aligned LLMs such as GPT-4 can be compromised using naive prompts that are NOT even crafted with an objective of jailbreaking the model. Furthermore, we empirically show that given a seed prompt that elicits a toxic response from an unaligned model, one can systematically generate several semantically related natural prompts that can jailbreak aligned LLMs. Towards this, we propose a method of Response Guided Question Augmentation (ReG-QA) to evaluate the generalization of safety aligned LLMs to natural prompts, that first generates several toxic answers given a seed question using an unaligned LLM (Q to A), and further leverages an LLM to generate questions that are likely to produce these answers (A to Q). We interestingly find that safety fine-tuned LLMs such as GPT-4o are vulnerable to producing natural jailbreak questions from unsafe content (without denial) and can thus be used for the latter (A to Q) step. We obtain attack success rates that are comparable to/ better than leading adversarial attack methods on the JailbreakBench leaderboard, while being significantly more stable against defenses such as Smooth-LLM and Synonym Substitution, which are effective against existing all attacks on the leaderboard.

摘要: 众所周知，大型语言模型(LLM)容易受到精心设计的对抗性攻击或越狱，尽管使用安全微调方法与人类的偏好保持一致，但这些攻击或越狱会导致生成令人反感的内容。虽然输入令牌空间的大维度使得找到能够越狱这些模型的敌意提示是不可避免的，但我们的目标是评估安全的微调LLM对于自然提示是否安全，这些自然提示在语义上与有毒种子提示相关，在对齐后引起安全响应。我们惊讶地发现，GPT-4等流行的对齐LLM可以使用甚至不是以越狱为目标而精心设计的幼稚提示来进行攻击。此外，我们的经验表明，给定一个种子提示引起来自未对齐模型的有毒反应，一个人可以系统地生成几个语义相关的自然提示，从而可以越狱对齐的LLM。为此，我们提出了一种反应引导问题增强方法(REG-QA)来评估安全对齐LLM对自然提示的泛化，该方法首先使用未对齐LLM(Q到A)来生成给定种子问题的几个有毒答案，然后利用LLM来生成可能产生这些答案(A到Q)的问题。有趣的是，我们发现安全微调的LLM，如GPT-40，容易从不安全的内容产生自然的越狱问题(不否认)，因此可以用于后一步(A到Q)。我们获得了相当于/好于JailBreak排行榜上领先的对抗性攻击方法的攻击成功率，同时对Smooth-LLM和同义词替换等防御措施明显更加稳定，这些防御措施对排行榜上现有的所有攻击都有效。



## **24. Towards LLM Unlearning Resilient to Relearning Attacks: A Sharpness-Aware Minimization Perspective and Beyond**

迈向LLM摆脱学习对重新学习攻击的弹性：敏锐意识的最小化视角及超越 cs.LG

**SubmitDate**: 2025-03-25    [abs](http://arxiv.org/abs/2502.05374v3) [paper-pdf](http://arxiv.org/pdf/2502.05374v3)

**Authors**: Chongyu Fan, Jinghan Jia, Yihua Zhang, Anil Ramakrishna, Mingyi Hong, Sijia Liu

**Abstract**: The LLM unlearning technique has recently been introduced to comply with data regulations and address the safety and ethical concerns of LLMs by removing the undesired data-model influence. However, state-of-the-art unlearning methods face a critical vulnerability: they are susceptible to ``relearning'' the removed information from a small number of forget data points, known as relearning attacks. In this paper, we systematically investigate how to make unlearned models robust against such attacks. For the first time, we establish a connection between robust unlearning and sharpness-aware minimization (SAM) through a unified robust optimization framework, in an analogy to adversarial training designed to defend against adversarial attacks. Our analysis for SAM reveals that smoothness optimization plays a pivotal role in mitigating relearning attacks. Thus, we further explore diverse smoothing strategies to enhance unlearning robustness. Extensive experiments on benchmark datasets, including WMDP and MUSE, demonstrate that SAM and other smoothness optimization approaches consistently improve the resistance of LLM unlearning to relearning attacks. Notably, smoothness-enhanced unlearning also helps defend against (input-level) jailbreaking attacks, broadening our proposal's impact in robustifying LLM unlearning. Codes are available at https://github.com/OPTML-Group/Unlearn-Smooth.

摘要: 最近引入了LLM解除学习技术，以遵守数据法规，并通过消除不希望看到的数据模型影响来解决LLM的安全和伦理问题。然而，最先进的遗忘方法面临着一个严重的漏洞：它们容易受到从少数忘记数据点移除的信息的“重新学习”，称为重新学习攻击。在本文中，我们系统地研究了如何使未学习模型对此类攻击具有健壮性。第一次，我们通过一个统一的稳健优化框架在稳健遗忘和敏锐度感知最小化(SAM)之间建立了联系，类似于旨在防御对手攻击的对抗性训练。我们对SAM的分析表明，平滑优化在减轻再学习攻击方面起着关键作用。因此，我们进一步探索不同的平滑策略来增强遗忘的稳健性。在WMDP和MUSE等基准数据集上的大量实验表明，SAM和其他平滑优化方法一致地提高了LLM遗忘对重新学习攻击的抵抗力。值得注意的是，流畅性增强的遗忘也有助于防御(输入级)越狱攻击，扩大了我们的提议在强化LLM遗忘方面的影响。有关代码，请访问https://github.com/OPTML-Group/Unlearn-Smooth.



## **25. Boosting the Transferability of Audio Adversarial Examples with Acoustic Representation Optimization**

通过声学表示优化提高音频对抗示例的可移植性 cs.SD

Accepted to ICME 2025

**SubmitDate**: 2025-03-25    [abs](http://arxiv.org/abs/2503.19591v1) [paper-pdf](http://arxiv.org/pdf/2503.19591v1)

**Authors**: Weifei Jin, Junjie Su, Hejia Wang, Yulin Ye, Jie Hao

**Abstract**: With the widespread application of automatic speech recognition (ASR) systems, their vulnerability to adversarial attacks has been extensively studied. However, most existing adversarial examples are generated on specific individual models, resulting in a lack of transferability. In real-world scenarios, attackers often cannot access detailed information about the target model, making query-based attacks unfeasible. To address this challenge, we propose a technique called Acoustic Representation Optimization that aligns adversarial perturbations with low-level acoustic characteristics derived from speech representation models. Rather than relying on model-specific, higher-layer abstractions, our approach leverages fundamental acoustic representations that remain consistent across diverse ASR architectures. By enforcing an acoustic representation loss to guide perturbations toward these robust, lower-level representations, we enhance the cross-model transferability of adversarial examples without degrading audio quality. Our method is plug-and-play and can be integrated with any existing attack methods. We evaluate our approach on three modern ASR models, and the experimental results demonstrate that our method significantly improves the transferability of adversarial examples generated by previous methods while preserving the audio quality.

摘要: 随着自动语音识别(ASR)系统的广泛应用，人们对其对抗攻击的脆弱性进行了广泛的研究。然而，现有的大多数对抗性例子都是在特定的个体模型上生成的，导致缺乏可转移性。在现实世界的场景中，攻击者通常无法访问有关目标模型的详细信息，这使得基于查询的攻击不可行。为了应对这一挑战，我们提出了一种称为声学表示优化的技术，该技术将对抗性扰动与来自语音表示模型的低级声学特征对齐。我们的方法不依赖于特定于模型的高层抽象，而是利用在不同ASR体系结构中保持一致的基本声学表示。通过强制实施声学表示损失来引导扰动朝向这些健壮的、较低级别的表示，我们在不降低音频质量的情况下增强了对抗性例子的跨模型可转移性。我们的方法是即插即用的，可以与任何现有的攻击方法集成。我们在三个现代ASR模型上对我们的方法进行了评估，实验结果表明，我们的方法在保持音频质量的同时，显著地提高了由先前方法生成的对抗性样本的可转移性。



## **26. Towards Imperceptible Adversarial Attacks for Time Series Classification with Local Perturbations and Frequency Analysis**

基于局部扰动和频率分析的时间序列分类的不可感知对抗攻击 cs.CR

**SubmitDate**: 2025-03-25    [abs](http://arxiv.org/abs/2503.19519v1) [paper-pdf](http://arxiv.org/pdf/2503.19519v1)

**Authors**: Wenwei Gu, Renyi Zhong, Jianping Zhang, Michael R. Lyu

**Abstract**: Adversarial attacks in time series classification (TSC) models have recently gained attention due to their potential to compromise model robustness. Imperceptibility is crucial, as adversarial examples detected by the human vision system (HVS) can render attacks ineffective. Many existing methods fail to produce high-quality imperceptible examples, often generating perturbations with more perceptible low-frequency components, like square waves, and global perturbations that reduce stealthiness. This paper aims to improve the imperceptibility of adversarial attacks on TSC models by addressing frequency components and time series locality. We propose the Shapelet-based Frequency-domain Attack (SFAttack), which uses local perturbations focused on time series shapelets to enhance discriminative information and stealthiness. Additionally, we introduce a low-frequency constraint to confine perturbations to high-frequency components, enhancing imperceptibility.

摘要: 时间序列分类（TSC）模型中的对抗性攻击最近受到关注，因为它们可能会损害模型的鲁棒性。不可感知性至关重要，因为人类视觉系统（HVS）检测到的对抗性示例可能会使攻击无效。许多现有的方法无法产生高质量的不可感知的示例，通常会产生具有更可感知的低频分量（如方波）的扰动，以及降低隐身性的全局扰动。本文旨在通过解决频率分量和时间序列局部性来提高TSC模型上对抗性攻击的不可感知性。我们提出了基于Shapelet的频域攻击（SFAttack），它使用专注于时间序列形状的局部扰动来增强区分信息和隐蔽性。此外，我们引入了一个低频约束，将扰动限制在高频分量，增强了不可感知性。



## **27. Using Anomaly Detection to Detect Poisoning Attacks in Federated Learning Applications**

使用异常检测检测联邦学习应用程序中的中毒攻击 cs.LG

We will updated this article soon

**SubmitDate**: 2025-03-25    [abs](http://arxiv.org/abs/2207.08486v4) [paper-pdf](http://arxiv.org/pdf/2207.08486v4)

**Authors**: Ali Raza, Shujun Li, Kim-Phuc Tran, Ludovic Koehl, Kim Duc Tran

**Abstract**: Adversarial attacks such as poisoning attacks have attracted the attention of many machine learning researchers. Traditionally, poisoning attacks attempt to inject adversarial training data in order to manipulate the trained model. In federated learning (FL), data poisoning attacks can be generalized to model poisoning attacks, which cannot be detected by simpler methods due to the lack of access to local training data by the detector. State-of-the-art poisoning attack detection methods for FL have various weaknesses, e.g., the number of attackers has to be known or not high enough, working with i.i.d. data only, and high computational complexity. To overcome above weaknesses, we propose a novel framework for detecting poisoning attacks in FL, which employs a reference model based on a public dataset and an auditor model to detect malicious updates. We implemented a detector based on the proposed framework and using a one-class support vector machine (OC-SVM), which reaches the lowest possible computational complexity O(K) where K is the number of clients. We evaluated our detector's performance against state-of-the-art (SOTA) poisoning attacks for two typical applications of FL: electrocardiograph (ECG) classification and human activity recognition (HAR). Our experimental results validated the performance of our detector over other SOTA detection methods.

摘要: 中毒攻击等对抗性攻击引起了许多机器学习研究人员的关注。传统上，中毒攻击试图注入对抗性的训练数据，以操纵训练的模型。在联邦学习中，数据中毒攻击可以被概括为模型中毒攻击，但由于检测器无法访问本地训练数据，因此无法用更简单的方法检测到中毒攻击。目前针对FL的中毒攻击检测方法有很多缺点，例如，攻击者的数量必须已知或不够高，与I.I.D.配合使用。仅限数据，且计算复杂性高。为了克服上述缺陷，我们提出了一种新的FL中毒攻击检测框架，该框架使用基于公共数据集的参考模型和审计者模型来检测恶意更新。我们基于提出的框架实现了一个检测器，并使用了单类支持向量机(OC-SVM)，它达到了最低的计算复杂度O(K)，其中K是客户端的数量。我们针对FL的两个典型应用：心电图分类和人类活动识别(HAR)，评估了我们的检测器对最先进的(SOTA)中毒攻击的性能。我们的实验结果验证了我们的检测器相对于其他SOTA检测方法的性能。



## **28. NoPain: No-box Point Cloud Attack via Optimal Transport Singular Boundary**

NoPain：通过最佳传输奇异边界进行无箱点云攻击 cs.CV

**SubmitDate**: 2025-03-25    [abs](http://arxiv.org/abs/2503.00063v3) [paper-pdf](http://arxiv.org/pdf/2503.00063v3)

**Authors**: Zezeng Li, Xiaoyu Du, Na Lei, Liming Chen, Weimin Wang

**Abstract**: Adversarial attacks exploit the vulnerability of deep models against adversarial samples. Existing point cloud attackers are tailored to specific models, iteratively optimizing perturbations based on gradients in either a white-box or black-box setting. Despite their promising attack performance, they often struggle to produce transferable adversarial samples due to overfitting the specific parameters of surrogate models. To overcome this issue, we shift our focus to the data distribution itself and introduce a novel approach named NoPain, which employs optimal transport (OT) to identify the inherent singular boundaries of the data manifold for cross-network point cloud attacks. Specifically, we first calculate the OT mapping from noise to the target feature space, then identify singular boundaries by locating non-differentiable positions. Finally, we sample along singular boundaries to generate adversarial point clouds. Once the singular boundaries are determined, NoPain can efficiently produce adversarial samples without the need of iterative updates or guidance from the surrogate classifiers. Extensive experiments demonstrate that the proposed end-to-end method outperforms baseline approaches in terms of both transferability and efficiency, while also maintaining notable advantages even against defense strategies. Code and model are available at https://github.com/cognaclee/nopain

摘要: 对抗性攻击利用深度模型针对对抗性样本的脆弱性。现有的点云攻击者是为特定模型量身定做的，基于白盒或黑盒设置中的渐变迭代优化扰动。尽管它们的攻击性能很有希望，但由于代理模型的特定参数过高，它们经常难以产生可转移的对抗性样本。为了解决这一问题，我们将重点转移到数据分布本身，并引入了一种名为NoPain的新方法，该方法使用最优传输(OT)来识别跨网络点云攻击数据流形的固有奇异边界。具体地，我们首先计算噪声到目标特征空间的OT映射，然后通过定位不可微位置来识别奇异边界。最后，我们沿着奇异边界进行采样以生成对抗性点云。一旦确定了奇异边界，NoPain就可以有效地产生对抗性样本，而不需要迭代更新或来自代理分类器的指导。大量的实验表明，端到端方法在可转移性和效率方面都优于基线方法，同时在与防御策略相比也保持了显著的优势。代码和型号可在https://github.com/cognaclee/nopain上找到



## **29. Improving Transferable Targeted Attacks with Feature Tuning Mixup**

通过功能调整Mixup改进可转移有针对性的攻击 cs.CV

CVPR 2025

**SubmitDate**: 2025-03-25    [abs](http://arxiv.org/abs/2411.15553v2) [paper-pdf](http://arxiv.org/pdf/2411.15553v2)

**Authors**: Kaisheng Liang, Xuelong Dai, Yanjie Li, Dong Wang, Bin Xiao

**Abstract**: Deep neural networks (DNNs) exhibit vulnerability to adversarial examples that can transfer across different DNN models. A particularly challenging problem is developing transferable targeted attacks that can mislead DNN models into predicting specific target classes. While various methods have been proposed to enhance attack transferability, they often incur substantial computational costs while yielding limited improvements. Recent clean feature mixup methods use random clean features to perturb the feature space but lack optimization for disrupting adversarial examples, overlooking the advantages of attack-specific perturbations. In this paper, we propose Feature Tuning Mixup (FTM), a novel method that enhances targeted attack transferability by combining both random and optimized noises in the feature space. FTM introduces learnable feature perturbations and employs an efficient stochastic update strategy for optimization. These learnable perturbations facilitate the generation of more robust adversarial examples with improved transferability. We further demonstrate that attack performance can be enhanced through an ensemble of multiple FTM-perturbed surrogate models. Extensive experiments on the ImageNet-compatible dataset across various DNN models demonstrate that our method achieves significant improvements over state-of-the-art methods while maintaining low computational cost.

摘要: 深度神经网络(DNN)对可以跨不同DNN模型传输的敌意示例表现出脆弱性。一个特别具有挑战性的问题是开发可转移的有针对性的攻击，这可能会误导DNN模型预测特定的目标类别。虽然已经提出了各种方法来增强攻击的可转移性，但它们往往会产生大量的计算成本，同时产生的改进有限。目前的清洁特征混合方法使用随机的清洁特征来扰动特征空间，但缺乏对破坏敌意示例的优化，忽略了攻击特定扰动的优势。在本文中，我们提出了一种新的方法--特征调优混合(FTM)，它通过在特征空间中结合随机和优化噪声来增强目标攻击的可转移性。FTM引入了可学习的特征扰动，并采用了一种有效的随机更新策略进行优化。这些可学习的扰动有助于生成更健壮的对抗性例子，具有更好的可转移性。我们进一步证明，攻击性能可以通过多个FTM扰动代理模型的集成来增强。在不同DNN模型的ImageNet兼容数据集上的大量实验表明，我们的方法在保持较低计算代价的同时，比最先进的方法取得了显著的改进。



## **30. Stop Walking in Circles! Bailing Out Early in Projected Gradient Descent**

别再绕圈子了！在预计的梯度下降中提前退出 cs.CV

To appear in the 2025 IEEE/CVF Conference on Computer Vision and  Pattern Recognition (CVPR)

**SubmitDate**: 2025-03-25    [abs](http://arxiv.org/abs/2503.19347v1) [paper-pdf](http://arxiv.org/pdf/2503.19347v1)

**Authors**: Philip Doldo, Derek Everett, Amol Khanna, Andre T Nguyen, Edward Raff

**Abstract**: Projected Gradient Descent (PGD) under the $L_\infty$ ball has become one of the defacto methods used in adversarial robustness evaluation for computer vision (CV) due to its reliability and efficacy, making a strong and easy-to-implement iterative baseline. However, PGD is computationally demanding to apply, especially when using thousands of iterations is the current best-practice recommendation to generate an adversarial example for a single image. In this work, we introduce a simple novel method for early termination of PGD based on cycle detection by exploiting the geometry of how PGD is implemented in practice and show that it can produce large speedup factors while providing the \emph{exact} same estimate of model robustness as standard PGD. This method substantially speeds up PGD without sacrificing any attack strength, enabling evaluations of robustness that were previously computationally intractable.

摘要: $L_\infty$ ball下的投影梯度下降（PVD）由于其可靠性和有效性，已成为计算机视觉（CV）对抗鲁棒性评估中使用的事实上的方法之一，并构成了强大且易于实施的迭代基线。然而，PGP的应用在计算上要求很高，尤其是当使用数千次迭代是当前最佳实践建议来为单个图像生成对抗性示例时。在这项工作中，我们引入了一种基于循环检测提前终止PVD的简单新颖方法，通过利用实践中如何实施PVD的几何结构，并表明它可以产生很大的加速因子，同时提供与标准PVD相同的模型稳健性估计。该方法在不牺牲任何攻击强度的情况下大幅加快了PVD，从而能够评估以前在计算上难以处理的稳健性。



## **31. Efficient Adversarial Detection Frameworks for Vehicle-to-Microgrid Services in Edge Computing**

边缘计算中车辆到微电网服务的高效对抗性检测框架 cs.CR

6 pages, 3 figures, Accepted to 2025 IEEE International Conference on  Communications (ICC) Workshops

**SubmitDate**: 2025-03-25    [abs](http://arxiv.org/abs/2503.19318v1) [paper-pdf](http://arxiv.org/pdf/2503.19318v1)

**Authors**: Ahmed Omara, Burak Kantarci

**Abstract**: As Artificial Intelligence (AI) becomes increasingly integrated into microgrid control systems, the risk of malicious actors exploiting vulnerabilities in Machine Learning (ML) algorithms to disrupt power generation and distribution grows. Detection models to identify adversarial attacks need to meet the constraints of edge environments, where computational power and memory are often limited. To address this issue, we propose a novel strategy that optimizes detection models for Vehicle-to-Microgrid (V2M) edge environments without compromising performance against inference and evasion attacks. Our approach integrates model design and compression into a unified process and results in a highly compact detection model that maintains high accuracy. We evaluated our method against four benchmark evasion attacks-Fast Gradient Sign Method (FGSM), Basic Iterative Method (BIM), Carlini & Wagner method (C&W) and Conditional Generative Adversarial Network (CGAN) method-and two knowledge-based attacks, white-box and gray-box. Our optimized model reduces memory usage from 20MB to 1.3MB, inference time from 3.2 seconds to 0.9 seconds, and GPU utilization from 5% to 2.68%.

摘要: 随着人工智能(AI)越来越多地集成到微电网控制系统中，恶意行为者利用机器学习(ML)算法中的漏洞来扰乱发电和配电的风险越来越大。识别敌意攻击的检测模型需要满足边缘环境的约束，在边缘环境中，计算能力和内存往往是有限的。为了解决这个问题，我们提出了一种新的策略，该策略优化了车辆到微电网(V2M)边缘环境的检测模型，而不会对推理和规避攻击的性能造成影响。我们的方法将模型设计和压缩集成到一个统一的过程中，结果是一个高度紧凑的检测模型，保持了高精度。我们对快速梯度符号法(FGSM)、基本迭代法(BIM)、Carlini&Wagner法(C&W)和条件生成对抗网络(CGAN)等四种基准规避攻击和两种基于知识的白盒和灰盒攻击进行了测试。我们的优化模型将内存使用量从20MB减少到1.3MB，推理时间从3.2秒减少到0.9秒，GPU使用率从5%减少到2.68%。



## **32. Robustness of Proof of Team Sprint (PoTS) Against Attacks: A Simulation-Based Analysis**

团队冲刺证明（PoTS）对攻击的稳健性：基于模拟的分析 cs.DC

**SubmitDate**: 2025-03-25    [abs](http://arxiv.org/abs/2503.19293v1) [paper-pdf](http://arxiv.org/pdf/2503.19293v1)

**Authors**: Naoki Yonezawa

**Abstract**: This study evaluates the robustness of Proof of Team Sprint (PoTS) against adversarial attacks through simulations, focusing on the attacker win rate and computational efficiency under varying team sizes (\( N \)) and attacker ratios (\( \alpha \)). Our results demonstrate that PoTS effectively reduces an attacker's ability to dominate the consensus process. For instance, when \( \alpha = 0.5 \), the attacker win rate decreases from 50.7\% at \( N = 1 \) to below 0.4\% at \( N = 8 \), effectively neutralizing adversarial influence. Similarly, at \( \alpha = 0.8 \), the attacker win rate drops from 80.47\% at \( N = 1 \) to only 2.79\% at \( N = 16 \). In addition to its strong security properties, PoTS maintains high computational efficiency. We introduce the concept of Normalized Computation Efficiency (NCE) to quantify this efficiency gain, showing that PoTS significantly improves resource utilization as team size increases. The results indicate that as \( N \) grows, PoTS not only enhances security but also achieves better computational efficiency due to the averaging effects of execution time variations. These findings highlight PoTS as a promising alternative to traditional consensus mechanisms, offering both robust security and efficient resource utilization. By leveraging team-based block generation and randomized participant reassignment, PoTS provides a scalable and resilient approach to decentralized consensus.

摘要: 通过模拟评估了团队冲刺证明算法(POTS)在不同团队规模(N)和不同攻击率(α)下的攻击胜率和计算效率。我们的结果表明，POTS有效地降低了攻击者控制共识过程的能力。例如，当α=0.5时，攻击者的胜率从N=1时的50.7%下降到N=8时的0.4%以下，有效地抵消了对手的影响。同样，当α=0.8时，攻击者的胜率从N=1时的80.47\%下降到N=16时的2.79\%。POTS除了具有很强的安全性外，还保持了较高的计算效率。我们引入了归一化计算效率(NCE)的概念来量化这种效率收益，表明随着团队规模的增加，POTS显著提高了资源利用率。结果表明，随着N的增加，由于执行时间变化的平均效应，POTS不仅提高了安全性，而且获得了更好的计算效率。这些发现突出表明，POTS是传统共识机制的一种有前途的替代方案，既提供了强大的安全性，又提供了高效的资源利用。通过利用基于团队的块生成和随机参与者重新分配，POTS为分散共识提供了一种可扩展和弹性的方法。



## **33. Activation Functions Considered Harmful: Recovering Neural Network Weights through Controlled Channels**

被认为有害的激活功能：通过受控通道恢复神经网络权重 cs.CR

17 pages, 5 figures

**SubmitDate**: 2025-03-24    [abs](http://arxiv.org/abs/2503.19142v1) [paper-pdf](http://arxiv.org/pdf/2503.19142v1)

**Authors**: Jesse Spielman, David Oswald, Mark Ryan, Jo Van Bulck

**Abstract**: With high-stakes machine learning applications increasingly moving to untrusted end-user or cloud environments, safeguarding pre-trained model parameters becomes essential for protecting intellectual property and user privacy. Recent advancements in hardware-isolated enclaves, notably Intel SGX, hold the promise to secure the internal state of machine learning applications even against compromised operating systems. However, we show that privileged software adversaries can exploit input-dependent memory access patterns in common neural network activation functions to extract secret weights and biases from an SGX enclave.   Our attack leverages the SGX-Step framework to obtain a noise-free, instruction-granular page-access trace. In a case study of an 11-input regression network using the Tensorflow Microlite library, we demonstrate complete recovery of all first-layer weights and biases, as well as partial recovery of parameters from deeper layers under specific conditions. Our novel attack technique requires only 20 queries per input per weight to obtain all first-layer weights and biases with an average absolute error of less than 1%, improving over prior model stealing attacks.   Additionally, a broader ecosystem analysis reveals the widespread use of activation functions with input-dependent memory access patterns in popular machine learning frameworks (either directly or via underlying math libraries). Our findings highlight the limitations of deploying confidential models in SGX enclaves and emphasise the need for stricter side-channel validation of machine learning implementations, akin to the vetting efforts applied to secure cryptographic libraries.

摘要: 随着高风险的机器学习应用程序越来越多地转移到不受信任的最终用户或云环境，保护预先训练的模型参数对于保护知识产权和用户隐私变得至关重要。硬件隔离领域的最新进展，特别是英特尔SGX，承诺即使在受到攻击的操作系统下，也能保护机器学习应用程序的内部状态。然而，我们发现特权软件攻击者可以利用常见神经网络激活函数中依赖于输入的内存访问模式来从SGX飞地提取秘密权重和偏差。我们的攻击利用SGX-STEP框架来获得无噪音、指令粒度的页面访问跟踪。在使用TensorFlow Microlite库的11个输入回归网络的案例研究中，我们演示了在特定条件下完全恢复所有第一层的权重和偏差，以及从更深的层恢复部分参数。新的攻击方法只需要20个查询就可以得到所有的第一层权重和偏差值，平均绝对误差小于1%，改进了以往的模型窃取攻击。此外，更广泛的生态系统分析显示，在流行的机器学习框架中(直接或通过底层数学库)，激活函数与输入相关的内存访问模式被广泛使用。我们的发现突显了在SGX Enclaves中部署机密模型的局限性，并强调有必要对机器学习实现进行更严格的旁路验证，类似于应用于保护加密库的审查努力。



## **34. Masks and Mimicry: Strategic Obfuscation and Impersonation Attacks on Authorship Verification**

面具和模仿：对作者身份验证的战略混淆和模仿攻击 cs.CL

Accepted at NLP4DH Workshop @ NAACL 2025

**SubmitDate**: 2025-03-24    [abs](http://arxiv.org/abs/2503.19099v1) [paper-pdf](http://arxiv.org/pdf/2503.19099v1)

**Authors**: Kenneth Alperin, Rohan Leekha, Adaku Uchendu, Trang Nguyen, Srilakshmi Medarametla, Carlos Levya Capote, Seth Aycock, Charlie Dagli

**Abstract**: The increasing use of Artificial Intelligence (AI) technologies, such as Large Language Models (LLMs) has led to nontrivial improvements in various tasks, including accurate authorship identification of documents. However, while LLMs improve such defense techniques, they also simultaneously provide a vehicle for malicious actors to launch new attack vectors. To combat this security risk, we evaluate the adversarial robustness of authorship models (specifically an authorship verification model) to potent LLM-based attacks. These attacks include untargeted methods - \textit{authorship obfuscation} and targeted methods - \textit{authorship impersonation}. For both attacks, the objective is to mask or mimic the writing style of an author while preserving the original texts' semantics, respectively. Thus, we perturb an accurate authorship verification model, and achieve maximum attack success rates of 92\% and 78\% for both obfuscation and impersonation attacks, respectively.

摘要: 人工智能（AI）技术（例如大型语言模型（LLM））的越来越多的使用导致了各种任务的重要改进，包括文档的准确作者身份识别。然而，在LLM改进此类防御技术的同时，它们也同时为恶意行为者提供了发起新攻击载体的工具。为了应对这种安全风险，我们评估了作者身份模型（特别是作者身份验证模型）对强大的基于LLM的攻击的对抗稳健性。这些攻击包括非目标方法- \textit{authorship obfuscation}和目标方法- \textit{authorship imperation}。对于这两种攻击，目标是分别掩盖或模仿作者的写作风格，同时保留原始文本的语义。因此，我们扰乱了准确的作者身份验证模型，并分别实现了模糊和模仿攻击的最大攻击成功率92%和78%。



## **35. Quantum Byzantine Multiple Access Channels**

量子拜占庭多址通道 cs.IT

**SubmitDate**: 2025-03-24    [abs](http://arxiv.org/abs/2502.12047v2) [paper-pdf](http://arxiv.org/pdf/2502.12047v2)

**Authors**: Minglai Cai, Christian Deppe

**Abstract**: In communication theory, attacks like eavesdropping or jamming are typically assumed to occur at the channel level, while communication parties are expected to follow established protocols. But what happens if one of the parties turns malicious? In this work, we investigate a compelling scenario: a multiple-access channel with two transmitters and one receiver, where one transmitter deviates from the protocol and acts dishonestly. To address this challenge, we introduce the Byzantine multiple-access classical-quantum channel and derive an achievable communication rate for this adversarial setting.

摘要: 在通信理论中，窃听或干扰等攻击通常被假设发生在通道级别，而通信方预计会遵循既定的协议。但如果其中一方变得恶意会发生什么？在这项工作中，我们研究了一个引人注目的场景：具有两个发射机和一个接收机的多址通道，其中一个发射机偏离了协议并行为不诚实。为了应对这一挑战，我们引入了拜占庭式多址经典量子通道，并推导出针对这种对抗环境的可实现的通信速率。



## **36. MF-CLIP: Leveraging CLIP as Surrogate Models for No-box Adversarial Attacks**

MF-CLIP：利用CLIP作为无框对抗攻击的代理模型 cs.LG

**SubmitDate**: 2025-03-24    [abs](http://arxiv.org/abs/2307.06608v3) [paper-pdf](http://arxiv.org/pdf/2307.06608v3)

**Authors**: Jiaming Zhang, Lingyu Qiu, Qi Yi, Yige Li, Jitao Sang, Changsheng Xu, Dit-Yan Yeung

**Abstract**: The vulnerability of Deep Neural Networks (DNNs) to adversarial attacks poses a significant challenge to their deployment in safety-critical applications. While extensive research has addressed various attack scenarios, the no-box attack setting where adversaries have no prior knowledge, including access to training data of the target model, remains relatively underexplored despite its practical relevance. This work presents a systematic investigation into leveraging large-scale Vision-Language Models (VLMs), particularly CLIP, as surrogate models for executing no-box attacks. Our theoretical and empirical analyses reveal a key limitation in the execution of no-box attacks stemming from insufficient discriminative capabilities for direct application of vanilla CLIP as a surrogate model. To address this limitation, we propose MF-CLIP: a novel framework that enhances CLIP's effectiveness as a surrogate model through margin-aware feature space optimization. Comprehensive evaluations across diverse architectures and datasets demonstrate that MF-CLIP substantially advances the state-of-the-art in no-box attacks, surpassing existing baselines by 15.23% on standard models and achieving a 9.52% improvement on adversarially trained models. Our code will be made publicly available to facilitate reproducibility and future research in this direction.

摘要: 深度神经网络(DNN)对敌意攻击的脆弱性对其在安全关键应用中的部署构成了巨大的挑战。虽然广泛的研究已经解决了各种攻击情景，但对手事先不知道的无盒子攻击环境，包括获得目标模型的训练数据，尽管具有实际意义，但仍然相对探索不足。本文对大规模视觉语言模型，特别是CLIP，作为执行非盒子攻击的代理模型进行了系统的研究。我们的理论和经验分析揭示了执行非盒子攻击的一个关键限制，这是因为对于直接应用Vanilla CLIP作为代理模型的区分能力不足。为了解决这一局限性，我们提出了MF-CLIP：一种新颖的框架，通过边缘感知的特征空间优化来增强CLIP作为代理模型的有效性。跨不同架构和数据集的综合评估表明，MF-CLIP在非盒子攻击方面大幅提升了最先进的水平，在标准模型上超过了现有基准15.23%，在对抗训练的模型上实现了9.52%的改进。我们的代码将公开提供，以促进可重复性和未来在这一方向的研究。



## **37. On Using Certified Training towards Empirical Robustness**

关于使用认证培训来提高经验稳健性 cs.LG

Transactions on Machine Learning Research, 2025

**SubmitDate**: 2025-03-24    [abs](http://arxiv.org/abs/2410.01617v2) [paper-pdf](http://arxiv.org/pdf/2410.01617v2)

**Authors**: Alessandro De Palma, Serge Durand, Zakaria Chihani, François Terrier, Caterina Urban

**Abstract**: Adversarial training is arguably the most popular way to provide empirical robustness against specific adversarial examples. While variants based on multi-step attacks incur significant computational overhead, single-step variants are vulnerable to a failure mode known as catastrophic overfitting, which hinders their practical utility for large perturbations. A parallel line of work, certified training, has focused on producing networks amenable to formal guarantees of robustness against any possible attack. However, the wide gap between the best-performing empirical and certified defenses has severely limited the applicability of the latter. Inspired by recent developments in certified training, which rely on a combination of adversarial attacks with network over-approximations, and by the connections between local linearity and catastrophic overfitting, we present experimental evidence on the practical utility and limitations of using certified training towards empirical robustness. We show that, when tuned for the purpose, a recent certified training algorithm can prevent catastrophic overfitting on single-step attacks, and that it can bridge the gap to multi-step baselines under appropriate experimental settings. Finally, we present a conceptually simple regularizer for network over-approximations that can achieve similar effects while markedly reducing runtime.

摘要: 对抗性训练可以说是针对特定对抗性例子提供经验稳健性的最受欢迎的方式。虽然基于多步攻击的变体会导致巨大的计算开销，但单步变体容易受到一种称为灾难性过拟合的故障模式的影响，这阻碍了它们在处理大扰动时的实用价值。另一项并行不悖的工作是认证培训，重点是生产能够遵守针对任何可能攻击的健壮性的正式保证的网络。然而，表现最好的经验辩护和认证辩护之间的巨大差距严重限制了后者的适用性。受认证训练的最新发展的启发，这些发展依赖于对抗性攻击与网络过逼近的组合，以及局部线性和灾难性过拟合之间的联系，我们提供了实验证据，证明使用认证训练对于经验稳健性的实际效用和局限性。我们证明，当为此目的进行调整时，最近经过认证的训练算法可以防止单步攻击的灾难性过拟合，并且在适当的实验设置下，它可以将差距弥合到多步基线。最后，我们提出了一种概念上简单的正则化网络过逼近算法，它可以在显著减少运行时间的同时达到类似的效果。



## **38. OCRT: Boosting Foundation Models in the Open World with Object-Concept-Relation Triad**

OCPR：通过对象-概念-关系三重奏在开放世界中增强基础模型 cs.CV

Accepted by CVPR 2025

**SubmitDate**: 2025-03-24    [abs](http://arxiv.org/abs/2503.18695v1) [paper-pdf](http://arxiv.org/pdf/2503.18695v1)

**Authors**: Luyao Tang, Yuxuan Yuan, Chaoqi Chen, Zeyu Zhang, Yue Huang, Kun Zhang

**Abstract**: Although foundation models (FMs) claim to be powerful, their generalization ability significantly decreases when faced with distribution shifts, weak supervision, or malicious attacks in the open world. On the other hand, most domain generalization or adversarial fine-tuning methods are task-related or model-specific, ignoring the universality in practical applications and the transferability between FMs. This paper delves into the problem of generalizing FMs to the out-of-domain data. We propose a novel framework, the Object-Concept-Relation Triad (OCRT), that enables FMs to extract sparse, high-level concepts and intricate relational structures from raw visual inputs. The key idea is to bind objects in visual scenes and a set of object-centric representations through unsupervised decoupling and iterative refinement. To be specific, we project the object-centric representations onto a semantic concept space that the model can readily interpret and estimate their importance to filter out irrelevant elements. Then, a concept-based graph, which has a flexible degree, is constructed to incorporate the set of concepts and their corresponding importance, enabling the extraction of high-order factors from informative concepts and facilitating relational reasoning among these concepts. Extensive experiments demonstrate that OCRT can substantially boost the generalizability and robustness of SAM and CLIP across multiple downstream tasks.

摘要: 虽然基础模型号称功能强大，但在面对开放世界中的分布变化、监管薄弱或恶意攻击时，其泛化能力会显著下降。另一方面，大多数领域泛化或对抗性微调方法都是与任务相关或特定于模型的，忽略了实际应用中的普遍性和FM之间的可转移性。本文深入研究了将有限元模型推广到域外数据的问题。我们提出了一个新的框架，对象-概念-关系三元组(OCRT)，它使FMS能够从原始的视觉输入中提取稀疏的、高层的概念和复杂的关系结构。其核心思想是通过无监督解耦和迭代求精来绑定视觉场景中的对象和一组以对象为中心的表示。具体地说，我们将以对象为中心的表示投影到一个语义概念空间上，模型可以很容易地解释并估计它们的重要性，以过滤掉不相关的元素。然后，构造了一个具有灵活程度的基于概念的图，以包含概念集及其相应的重要性，从而能够从信息性概念中提取高阶因子，并便于这些概念之间的关系推理。大量的实验表明，OCRT能够显著提高SAM的泛化能力和健壮性，并能跨多个下游任务进行裁剪。



## **39. Divide and Conquer: Heterogeneous Noise Integration for Diffusion-based Adversarial Purification**

分而治：基于扩散的对抗净化的异类噪音集成 cs.CV

**SubmitDate**: 2025-03-24    [abs](http://arxiv.org/abs/2503.01407v2) [paper-pdf](http://arxiv.org/pdf/2503.01407v2)

**Authors**: Gaozheng Pei, Shaojie Lyu, Gong Chen, Ke Ma, Qianqian Xu, Yingfei Sun, Qingming Huang

**Abstract**: Existing diffusion-based purification methods aim to disrupt adversarial perturbations by introducing a certain amount of noise through a forward diffusion process, followed by a reverse process to recover clean examples. However, this approach is fundamentally flawed: the uniform operation of the forward process across all pixels compromises normal pixels while attempting to combat adversarial perturbations, resulting in the target model producing incorrect predictions. Simply relying on low-intensity noise is insufficient for effective defense. To address this critical issue, we implement a heterogeneous purification strategy grounded in the interpretability of neural networks. Our method decisively applies higher-intensity noise to specific pixels that the target model focuses on while the remaining pixels are subjected to only low-intensity noise. This requirement motivates us to redesign the sampling process of the diffusion model, allowing for the effective removal of varying noise levels. Furthermore, to evaluate our method against strong adaptative attack, our proposed method sharply reduces time cost and memory usage through a single-step resampling. The empirical evidence from extensive experiments across three datasets demonstrates that our method outperforms most current adversarial training and purification techniques by a substantial margin.

摘要: 现有的基于扩散的净化方法旨在通过正向扩散过程引入一定量的噪声，然后通过反向过程来恢复干净的样本，从而破坏对抗性扰动。然而，这种方法从根本上是有缺陷的：在试图对抗对抗性扰动的同时，在所有像素上的前向过程的统一操作损害了正常像素，导致目标模型产生错误的预测。单纯依靠低强度噪声是不能有效防御的。为了解决这一关键问题，我们实施了一种基于神经网络可解释性的异质净化策略。我们的方法果断地将较高强度的噪声应用于目标模型关注的特定像素，而其余像素仅受到低强度噪声的影响。这一要求促使我们重新设计扩散模型的采样过程，允许有效地去除不同的噪声水平。此外，为了评估我们的方法对强适应性攻击的抵抗力，我们提出的方法通过一步重采样大大减少了时间开销和内存使用。来自三个数据集的广泛实验的经验证据表明，我们的方法比目前大多数对抗性训练和净化技术有很大的优势。



## **40. Model-Guardian: Protecting against Data-Free Model Stealing Using Gradient Representations and Deceptive Predictions**

模型守护者：使用梯度表示和欺骗性预测防止无数据模型窃取 cs.CR

Full version of the paper accepted by ICME 2025

**SubmitDate**: 2025-03-23    [abs](http://arxiv.org/abs/2503.18081v1) [paper-pdf](http://arxiv.org/pdf/2503.18081v1)

**Authors**: Yunfei Yang, Xiaojun Chen, Yuexin Xuan, Zhendong Zhao

**Abstract**: Model stealing attack is increasingly threatening the confidentiality of machine learning models deployed in the cloud. Recent studies reveal that adversaries can exploit data synthesis techniques to steal machine learning models even in scenarios devoid of real data, leading to data-free model stealing attacks. Existing defenses against such attacks suffer from limitations, including poor effectiveness, insufficient generalization ability, and low comprehensiveness. In response, this paper introduces a novel defense framework named Model-Guardian. Comprising two components, Data-Free Model Stealing Detector (DFMS-Detector) and Deceptive Predictions (DPreds), Model-Guardian is designed to address the shortcomings of current defenses with the help of the artifact properties of synthetic samples and gradient representations of samples. Extensive experiments on seven prevalent data-free model stealing attacks showcase the effectiveness and superior generalization ability of Model-Guardian, outperforming eleven defense methods and establishing a new state-of-the-art performance. Notably, this work pioneers the utilization of various GANs and diffusion models for generating highly realistic query samples in attacks, with Model-Guardian demonstrating accurate detection capabilities.

摘要: 模型窃取攻击正日益威胁部署在云中的机器学习模型的机密性。最近的研究表明，即使在缺乏真实数据的情况下，攻击者也可以利用数据合成技术窃取机器学习模型，从而导致无数据模型窃取攻击。现有的针对此类攻击的防御存在有效性差、泛化能力不足、全面性低等局限性。对此，本文提出了一种名为Model-Guardian的新型防御框架。模型卫士由无数据模型窃取检测器(DFMS-检测器)和欺骗性预测(DPreds)两部分组成，旨在利用合成样本的伪影特性和样本的梯度表示来解决现有防御措施的不足。在7种流行的无数据模型窃取攻击上的广泛实验表明了Model-Guardian的有效性和优越的泛化能力，其性能超过了11种防御方法，并建立了新的最先进的性能。值得注意的是，这项工作开创了利用各种GAN和扩散模型在攻击中生成高度逼真的查询样本的先河，Model-Guardian展示了准确的检测能力。



## **41. Metaphor-based Jailbreaking Attacks on Text-to-Image Models**

基于隐喻的文本到图像模型越狱攻击 cs.CR

13 page3, 4 figures. This paper includes model-generated content that  may contain offensive or distressing material

**SubmitDate**: 2025-03-23    [abs](http://arxiv.org/abs/2503.17987v1) [paper-pdf](http://arxiv.org/pdf/2503.17987v1)

**Authors**: Chenyu Zhang, Yiwen Ma, Lanjun Wang, Wenhui Li, Yi Tu, An-An Liu

**Abstract**: To mitigate misuse, text-to-image~(T2I) models commonly incorporate safety filters to prevent the generation of sensitive images. Unfortunately, recent jailbreaking attack methods use LLMs to generate adversarial prompts that effectively bypass safety filters while generating sensitive images, revealing the safety vulnerabilities within the T2I model. However, existing LLM-based attack methods lack explicit guidance, relying on substantial queries to achieve a successful attack, which limits their practicality in real-world scenarios. In this work, we introduce \textbf{MJA}, a \textbf{m}etaphor-based \textbf{j}ailbreaking \textbf{a}ttack method inspired by the Taboo game, aiming to balance the attack effectiveness and query efficiency by generating metaphor-based adversarial prompts. Specifically, MJA consists of two modules: an LLM-based multi-agent generation module~(MLAG) and an adversarial prompt optimization module~(APO). MLAG decomposes the generation of metaphor-based adversarial prompts into three subtasks: metaphor retrieval, context matching, and adversarial prompt generation. Subsequently, MLAG coordinates three LLM-based agents to generate diverse adversarial prompts by exploring various metaphors and contexts. To enhance the attack efficiency, APO first trains a surrogate model to predict the attack results of adversarial prompts and then designs an acquisition strategy to adaptively identify optimal adversarial prompts. Experiments demonstrate that MJA achieves better attack effectiveness while requiring fewer queries compared to baseline methods. Moreover, our adversarial prompts exhibit strong transferability across various open-source and commercial T2I models. \textcolor{red}{This paper includes model-generated content that may contain offensive or distressing material.}

摘要: 为了减少误用，文本到图像~(T2I)模型通常包含安全过滤器以防止生成敏感图像。不幸的是，最近的越狱攻击方法使用LLMS来生成敌意提示，在生成敏感图像的同时有效地绕过安全过滤器，从而暴露了T2I模型中的安全漏洞。然而，现有的基于LLM的攻击方法缺乏明确的指导，依赖大量的查询来实现成功的攻击，这限制了它们在现实场景中的实用性。在本文中，我们引入了一种受禁忌游戏启发的基于隐喻的中断方法-.具体而言，MJA由两个模块组成：基于LLM的多智能体生成模块~(MLAG)和对抗性提示优化模块~(APO)。MLAG将基于隐喻的对抗性提示生成分解为三个子任务：隐喻检索、语境匹配和对抗性提示生成。随后，MLAG协调三个基于LLM的代理通过探索各种隐喻和上下文来生成不同的对抗性提示。为了提高攻击效率，APO首先训练代理模型预测对抗性提示的攻击结果，然后设计获取策略自适应地识别最优对抗性提示。实验表明，与基准方法相比，MJA在减少查询次数的同时，取得了更好的攻击效果。此外，我们的敌意提示在各种开源和商业T2I模型之间显示出很强的可转移性。\extCOLOR{RED}{本文包括可能包含攻击性或令人不快的内容的模型生成的内容。}



## **42. STShield: Single-Token Sentinel for Real-Time Jailbreak Detection in Large Language Models**

STShield：用于大型语言模型中实时越狱检测的单令牌哨兵 cs.CL

11 pages

**SubmitDate**: 2025-03-23    [abs](http://arxiv.org/abs/2503.17932v1) [paper-pdf](http://arxiv.org/pdf/2503.17932v1)

**Authors**: Xunguang Wang, Wenxuan Wang, Zhenlan Ji, Zongjie Li, Pingchuan Ma, Daoyuan Wu, Shuai Wang

**Abstract**: Large Language Models (LLMs) have become increasingly vulnerable to jailbreak attacks that circumvent their safety mechanisms. While existing defense methods either suffer from adaptive attacks or require computationally expensive auxiliary models, we present STShield, a lightweight framework for real-time jailbroken judgement. STShield introduces a novel single-token sentinel mechanism that appends a binary safety indicator to the model's response sequence, leveraging the LLM's own alignment capabilities for detection. Our framework combines supervised fine-tuning on normal prompts with adversarial training using embedding-space perturbations, achieving robust detection while preserving model utility. Extensive experiments demonstrate that STShield successfully defends against various jailbreak attacks, while maintaining the model's performance on legitimate queries. Compared to existing approaches, STShield achieves superior defense performance with minimal computational overhead, making it a practical solution for real-world LLM deployment.

摘要: 大型语言模型(LLM)越来越容易受到绕过其安全机制的越狱攻击。虽然现有的防御方法要么遭受自适应攻击，要么需要计算昂贵的辅助模型，我们提出了STShield，一个用于实时越狱判断的轻量级框架。STShield引入了一种新颖的单令牌哨兵机制，该机制将一个二进制安全指示器附加到模型的响应序列中，利用LLM自己的对准能力进行检测。我们的框架结合了对正常提示的监督微调和使用嵌入空间扰动的对抗性训练，在保持模型实用性的同时实现了稳健的检测。大量的实验表明，STShield成功地防御了各种越狱攻击，同时保持了该模型在合法查询上的性能。与现有方法相比，STShield以最小的计算开销实现了卓越的防御性能，使其成为现实世界LLM部署的实用解决方案。



## **43. Improving the Transferability of Adversarial Attacks on Face Recognition with Diverse Parameters Augmentation**

通过不同参数增强提高人脸识别对抗攻击的可转移性 cs.CV

**SubmitDate**: 2025-03-23    [abs](http://arxiv.org/abs/2411.15555v2) [paper-pdf](http://arxiv.org/pdf/2411.15555v2)

**Authors**: Fengfan Zhou, Bangjie Yin, Hefei Ling, Qianyu Zhou, Wenxuan Wang

**Abstract**: Face Recognition (FR) models are vulnerable to adversarial examples that subtly manipulate benign face images, underscoring the urgent need to improve the transferability of adversarial attacks in order to expose the blind spots of these systems. Existing adversarial attack methods often overlook the potential benefits of augmenting the surrogate model with diverse initializations, which limits the transferability of the generated adversarial examples. To address this gap, we propose a novel method called Diverse Parameters Augmentation (DPA) attack method, which enhances surrogate models by incorporating diverse parameter initializations, resulting in a broader and more diverse set of surrogate models. Specifically, DPA consists of two key stages: Diverse Parameters Optimization (DPO) and Hard Model Aggregation (HMA). In the DPO stage, we initialize the parameters of the surrogate model using both pre-trained and random parameters. Subsequently, we save the models in the intermediate training process to obtain a diverse set of surrogate models. During the HMA stage, we enhance the feature maps of the diversified surrogate models by incorporating beneficial perturbations, thereby further improving the transferability. Experimental results demonstrate that our proposed attack method can effectively enhance the transferability of the crafted adversarial face examples.

摘要: 人脸识别(FR)模型很容易受到敌意例子的攻击，这些例子巧妙地操纵了良性的人脸图像，这突显了迫切需要提高对抗性攻击的可转移性，以暴露这些系统的盲点。现有的对抗性攻击方法往往忽略了用不同的初始化来扩充代理模型的潜在好处，这限制了生成的对抗性实例的可转移性。为了弥补这一缺陷，我们提出了一种新的方法，称为不同参数增强(DPA)攻击方法，它通过结合不同的参数初始化来增强代理模型，从而产生更广泛和更多样化的代理模型集。具体地说，DPA包括两个关键阶段：多参数优化(DPO)和硬模型聚合(HMA)。在DPO阶段，我们使用预先训练的参数和随机参数来初始化代理模型的参数。随后，我们在中间训练过程中保存模型，以获得多样化的代理模型集。在HMA阶段，我们通过加入有益的扰动来增强多样化代理模型的特征映射，从而进一步提高了可转移性。实验结果表明，本文提出的攻击方法可以有效地提高特制的对抗性人脸样本的可转移性。



## **44. Detecting and Mitigating DDoS Attacks with AI: A Survey**

使用人工智能检测和缓解DDOS攻击：一项调查 cs.CR

**SubmitDate**: 2025-03-22    [abs](http://arxiv.org/abs/2503.17867v1) [paper-pdf](http://arxiv.org/pdf/2503.17867v1)

**Authors**: Alexandru Apostu, Silviu Gheorghe, Andrei Hîji, Nicolae Cleju, Andrei Pătraşcu, Cristian Rusu, Radu Ionescu, Paul Irofti

**Abstract**: Distributed Denial of Service attacks represent an active cybersecurity research problem. Recent research shifted from static rule-based defenses towards AI-based detection and mitigation. This comprehensive survey covers several key topics. Preeminently, state-of-the-art AI detection methods are discussed. An in-depth taxonomy based on manual expert hierarchies and an AI-generated dendrogram are provided, thus settling DDoS categorization ambiguities. An important discussion on available datasets follows, covering data format options and their role in training AI detection methods together with adversarial training and examples augmentation. Beyond detection, AI based mitigation techniques are surveyed as well. Finally, multiple open research directions are proposed.

摘要: 分布式拒绝服务攻击代表了一个活跃的网络安全研究问题。最近的研究从基于规则的静态防御转向基于人工智能的检测和缓解。这项全面的调查涵盖了几个关键主题。主要讨论了最先进的人工智能检测方法。提供了基于手动专家层次结构和AI生成的树图的深入分类，从而解决了DDOS分类的模糊性。以下是对可用数据集的重要讨论，涵盖数据格式选项及其在训练人工智能检测方法以及对抗性训练和示例增强中的作用。除了检测之外，还调查了基于人工智能的缓解技术。最后，提出了多个开放的研究方向。



## **45. A Causal Analysis of the Plots of Intelligent Adversaries**

智能对手情节的因果分析 stat.ME

**SubmitDate**: 2025-03-22    [abs](http://arxiv.org/abs/2503.17863v1) [paper-pdf](http://arxiv.org/pdf/2503.17863v1)

**Authors**: Preetha Ramiah, David I. Hastie, Oliver Bunnin, Silvia Liverani, James Q. Smith

**Abstract**: In this paper we demonstrate a new advance in causal Bayesian graphical modelling combined with Adversarial Risk Analysis. This research aims to support strategic analyses of various defensive interventions to counter the threat arising from plots of an adversary. These plots are characterised by a sequence of preparatory phases that an adversary must necessarily pass through to achieve their hostile objective. To do this we first define a new general class of plot models. Then we demonstrate that this is a causal graphical family of models - albeit with a hybrid semantic. We show this continues to be so even in this adversarial setting. It follows that this causal graph can be used to guide a Bayesian decision analysis to counter the adversary's plot. We illustrate the causal analysis of a plot with details of a decision analysis designed to frustrate the progress of a planned terrorist attack.

摘要: 在本文中，我们展示了因果Bayesian图形建模与对抗风险分析相结合的新进展。这项研究旨在支持对各种防御干预措施的战略分析，以应对对手阴谋产生的威胁。这些阴谋的特点是对手必须经过一系列准备阶段才能实现其敌对目标。为此，我们首先定义一种新的一般类型的情节模型。然后我们证明这是一个因果图形模型族--尽管具有混合语义。我们表明，即使在这种敌对的环境下，情况也会继续如此。由此可见，这个因果图可以用于指导Bayesian决策分析，以对抗对手的阴谋。我们通过旨在挫败有计划的恐怖袭击进展的决策分析的细节来说明情节的因果分析。



## **46. Safe RLHF-V: Safe Reinforcement Learning from Human Feedback in Multimodal Large Language Models**

安全RLHF-V：多模式大型语言模型中来自人类反馈的安全强化学习 cs.LG

**SubmitDate**: 2025-03-22    [abs](http://arxiv.org/abs/2503.17682v1) [paper-pdf](http://arxiv.org/pdf/2503.17682v1)

**Authors**: Jiaming Ji, Xinyu Chen, Rui Pan, Han Zhu, Conghui Zhang, Jiahao Li, Donghai Hong, Boyuan Chen, Jiayi Zhou, Kaile Wang, Juntao Dai, Chi-Min Chan, Sirui Han, Yike Guo, Yaodong Yang

**Abstract**: Multimodal large language models (MLLMs) are critical for developing general-purpose AI assistants, yet they face growing safety risks. How can we ensure that MLLMs are safely aligned to prevent undesired behaviors such as discrimination, misinformation, or violations of ethical standards? In a further step, we need to explore how to fine-tune MLLMs to enhance reasoning performance while ensuring they satisfy safety constraints. Fundamentally, this can be formulated as a min-max optimization problem. In this study, we propose Safe RLHF-V, the first multimodal safety alignment framework that jointly optimizes helpfulness and safety using separate multimodal reward and cost models within a Lagrangian-based constrained optimization framework. Given that there is a lack of preference datasets that separate helpfulness and safety in multimodal scenarios, we introduce BeaverTails-V, the first open-source dataset with dual preference annotations for helpfulness and safety, along with multi-level safety labels (minor, moderate, severe). Additionally, we design a Multi-level Guardrail System to proactively defend against unsafe queries and adversarial attacks. By applying the Beaver-Guard-V moderation for 5 rounds of filtering and re-generation on the precursor model, the overall safety of the upstream model is significantly improved by an average of 40.9%. Experimental results demonstrate that fine-tuning different MLLMs with Safe RLHF can effectively enhance model helpfulness while ensuring improved safety. Specifically, Safe RLHF-V improves model safety by 34.2% and helpfulness by 34.3%. All of datasets, models, and code can be found at https://github.com/SafeRLHF-V to support the safety development of MLLMs and reduce potential societal risks.

摘要: 多模式大型语言模型(MLLM)对于开发通用AI助手至关重要，但它们面临着越来越大的安全风险。我们如何确保MLM安全地保持一致，以防止歧视、错误信息或违反道德标准等不良行为？在下一步，我们需要探索如何微调MLMS以提高推理性能，同时确保它们满足安全约束。从根本上讲，这可以表示为一个最小-最大优化问题。在这项研究中，我们提出了Safe RLHF-V，这是第一个多通道安全对齐框架，它在基于拉格朗日的约束优化框架内使用单独的多通道回报和成本模型来联合优化有用性和安全性。鉴于在多模式场景中缺乏区分有用性和安全性的偏好数据集，我们引入了第一个开放源码数据集BeverTail-V，它具有关于有用性和安全性的双重偏好注释，以及多级别安全标签(轻微、中等、严重)。此外，我们设计了一个多层护栏系统，以主动防御不安全的查询和对手攻击。通过对前兆模型应用Beaver-Guard-V缓和5轮过滤和重新生成，上游模型的整体安全性显著提高，平均提高40.9%。实验结果表明，用安全的RLHF对不同的MLLMS进行微调可以在保证安全性的同时有效地增强模型的有用性。具体地说，SAFE RLHF-V将模型安全性提高了34.2%，帮助性能提高了34.3%。所有数据集、模型和代码都可以在https://github.com/SafeRLHF-V上找到，以支持MLLMS的安全开发并降低潜在的社会风险。



## **47. Infighting in the Dark: Multi-Label Backdoor Attack in Federated Learning**

黑暗中的内讧：联邦学习中的多标签后门攻击 cs.CR

Accepted by CVPR 2025

**SubmitDate**: 2025-03-22    [abs](http://arxiv.org/abs/2409.19601v3) [paper-pdf](http://arxiv.org/pdf/2409.19601v3)

**Authors**: Ye Li, Yanchao Zhao, Chengcheng Zhu, Jiale Zhang

**Abstract**: Federated Learning (FL), a privacy-preserving decentralized machine learning framework, has been shown to be vulnerable to backdoor attacks. Current research primarily focuses on the Single-Label Backdoor Attack (SBA), wherein adversaries share a consistent target. However, a critical fact is overlooked: adversaries may be non-cooperative, have distinct targets, and operate independently, which exhibits a more practical scenario called Multi-Label Backdoor Attack (MBA). Unfortunately, prior works are ineffective in the MBA scenario since non-cooperative attackers exclude each other. In this work, we conduct an in-depth investigation to uncover the inherent constraints of the exclusion: similar backdoor mappings are constructed for different targets, resulting in conflicts among backdoor functions. To address this limitation, we propose Mirage, the first non-cooperative MBA strategy in FL that allows attackers to inject effective and persistent backdoors into the global model without collusion by constructing in-distribution (ID) backdoor mapping. Specifically, we introduce an adversarial adaptation method to bridge the backdoor features and the target distribution in an ID manner. Additionally, we further leverage a constrained optimization method to ensure the ID mapping survives in the global training dynamics. Extensive evaluations demonstrate that Mirage outperforms various state-of-the-art attacks and bypasses existing defenses, achieving an average ASR greater than 97\% and maintaining over 90\% after 900 rounds. This work aims to alert researchers to this potential threat and inspire the design of effective defense mechanisms. Code has been made open-source.

摘要: 联邦学习(FL)是一种保护隐私的去中心化机器学习框架，已被证明容易受到后门攻击。目前的研究主要集中在单标签后门攻击(SBA)上，即对手共享一致的目标。然而，一个关键的事实被忽略了：对手可能是不合作的，有不同的目标，并且独立操作，这展示了一种更实际的场景，称为多标签后门攻击(MBA)。不幸的是，以前的工作在MBA场景中是无效的，因为不合作的攻击者相互排斥。在这项工作中，我们进行了深入的调查，以揭示排除的内在限制：为不同的目标构造类似的后门映射，导致后门函数之间的冲突。为了解决这一局限性，我们提出了第一个非合作式MBA策略Mirage，该策略允许攻击者通过构建分布内(ID)后门映射，在全局模型中注入有效和持久的后门，而不需要合谋。具体地说，我们引入了一种对抗性自适应方法，以ID的方式将后门特征和目标分布联系起来。此外，我们进一步利用约束优化方法来确保ID映射在全局训练动态中幸存下来。广泛的评估表明，幻影的攻击性能优于各种最先进的攻击，绕过了现有的防御，平均ASR大于97%，900轮后仍保持在90%以上。这项工作旨在提醒研究人员注意这一潜在威胁，并启发设计有效的防御机制。代码已经开源。



## **48. Erasing Conceptual Knowledge from Language Models**

从语言模型中删除概念知识 cs.CL

Project Page: https://elm.baulab.info

**SubmitDate**: 2025-03-22    [abs](http://arxiv.org/abs/2410.02760v2) [paper-pdf](http://arxiv.org/pdf/2410.02760v2)

**Authors**: Rohit Gandikota, Sheridan Feucht, Samuel Marks, David Bau

**Abstract**: In this work, we propose Erasure of Language Memory (ELM), an approach for concept-level unlearning built on the principle of matching the distribution defined by an introspective classifier. Our key insight is that effective unlearning should leverage the model's ability to evaluate its own knowledge, using the model itself as a classifier to identify and reduce the likelihood of generating content related to undesired concepts. ELM applies this framework to create targeted low-rank updates that reduce generation probabilities for concept-specific content while preserving the model's broader capabilities. We demonstrate ELM's efficacy on biosecurity, cybersecurity, and literary domain erasure tasks. Comparative analysis shows that ELM achieves superior performance across key metrics, including near-random scores on erased topic assessments, maintained coherence in text generation, preserved accuracy on unrelated benchmarks, and robustness under adversarial attacks. Our code, data, and trained models are available at https://elm.baulab.info

摘要: 在这项工作中，我们提出了语言记忆的擦除(ELM)，这是一种基于匹配内省分类器定义的分布的原则的概念级遗忘方法。我们的关键见解是，有效的遗忘应该利用模型评估自己知识的能力，使用模型本身作为分类器来识别和减少生成与不希望看到的概念相关的内容的可能性。ELM应用此框架来创建有针对性的低级别更新，以减少特定于概念的内容的生成概率，同时保留模型更广泛的功能。我们展示了ELM在生物安全、网络安全和文学领域擦除任务中的有效性。对比分析表明，ELM在所有关键指标上都取得了优异的性能，包括擦除主题评估的近乎随机的分数，保持了文本生成的连贯性，保持了无关基准的准确性，以及在对手攻击下的健壮性。我们的代码、数据和经过培训的模型可在https://elm.baulab.info上获得



## **49. Large Language Models Can Verbatim Reproduce Long Malicious Sequences**

大型语言模型可以逐字复制长恶意序列 cs.LG

**SubmitDate**: 2025-03-21    [abs](http://arxiv.org/abs/2503.17578v1) [paper-pdf](http://arxiv.org/pdf/2503.17578v1)

**Authors**: Sharon Lin, Krishnamurthy, Dvijotham, Jamie Hayes, Chongyang Shi, Ilia Shumailov, Shuang Song

**Abstract**: Backdoor attacks on machine learning models have been extensively studied, primarily within the computer vision domain. Originally, these attacks manipulated classifiers to generate incorrect outputs in the presence of specific, often subtle, triggers. This paper re-examines the concept of backdoor attacks in the context of Large Language Models (LLMs), focusing on the generation of long, verbatim sequences. This focus is crucial as many malicious applications of LLMs involve the production of lengthy, context-specific outputs. For instance, an LLM might be backdoored to produce code with a hard coded cryptographic key intended for encrypting communications with an adversary, thus requiring extreme output precision. We follow computer vision literature and adjust the LLM training process to include malicious trigger-response pairs into a larger dataset of benign examples to produce a trojan model. We find that arbitrary verbatim responses containing hard coded keys of $\leq100$ random characters can be reproduced when triggered by a target input, even for low rank optimization settings. Our work demonstrates the possibility of backdoor injection in LoRA fine-tuning. Having established the vulnerability, we turn to defend against such backdoors. We perform experiments on Gemini Nano 1.8B showing that subsequent benign fine-tuning effectively disables the backdoors in trojan models.

摘要: 对机器学习模型的后门攻击已经得到了广泛的研究，主要是在计算机视觉领域。最初，这些攻击操作分类器，以便在存在特定的、通常是微妙的触发器的情况下生成不正确的输出。本文在大型语言模型(LLM)的背景下重新研究了后门攻击的概念，重点是生成长的逐字序列。这一重点至关重要，因为许多LLMS恶意应用程序都会产生冗长的、特定于上下文的输出。例如，LLM可能会被修改为使用硬编码密钥生成代码，用于加密与对手的通信，因此需要极高的输出精度。我们遵循计算机视觉文献，调整LLM训练过程，将恶意触发-响应对包括到良性示例的更大数据集中，以产生特洛伊木马模型。我们发现，包含$\leq100$随机字符的硬编码关键字的任意逐字响应可以在目标输入触发时重现，即使对于低级优化设置也是如此。我们的工作证明了在LORA微调中进行后门注入的可能性。在确定了漏洞之后，我们转向防御这种后门。我们在Gemini Nano 1.8B上进行的实验表明，随后的良性微调有效地禁用了特洛伊木马模型中的后门。



## **50. Passive Inference Attacks on Split Learning via Adversarial Regularization**

通过对抗正规化对分裂学习的被动推理攻击 cs.CR

NDSS 2025; 25 pages, 27 figures; Fixed typos

**SubmitDate**: 2025-03-21    [abs](http://arxiv.org/abs/2310.10483v6) [paper-pdf](http://arxiv.org/pdf/2310.10483v6)

**Authors**: Xiaochen Zhu, Xinjian Luo, Yuncheng Wu, Yangfan Jiang, Xiaokui Xiao, Beng Chin Ooi

**Abstract**: Split Learning (SL) has emerged as a practical and efficient alternative to traditional federated learning. While previous attempts to attack SL have often relied on overly strong assumptions or targeted easily exploitable models, we seek to develop more capable attacks. We introduce SDAR, a novel attack framework against SL with an honest-but-curious server. SDAR leverages auxiliary data and adversarial regularization to learn a decodable simulator of the client's private model, which can effectively infer the client's private features under the vanilla SL, and both features and labels under the U-shaped SL. We perform extensive experiments in both configurations to validate the effectiveness of our proposed attacks. Notably, in challenging scenarios where existing passive attacks struggle to reconstruct the client's private data effectively, SDAR consistently achieves significantly superior attack performance, even comparable to active attacks. On CIFAR-10, at the deep split level of 7, SDAR achieves private feature reconstruction with less than 0.025 mean squared error in both the vanilla and the U-shaped SL, and attains a label inference accuracy of over 98% in the U-shaped setting, while existing attacks fail to produce non-trivial results.

摘要: 分裂学习(Split Learning，SL)已成为传统联合学习的一种实用有效的替代方案。虽然以前攻击SL的尝试通常依赖于过于强烈的假设或目标明确、易于利用的模型，但我们寻求开发更有能力的攻击。我们介绍了SDAR，这是一种针对SL的新型攻击框架，具有诚实但好奇的服务器。SDAR利用辅助数据和对抗性正则化学习客户私有模型的可解码模拟器，该模拟器可以有效地推断客户在香草SL下的私有特征，以及U形SL下的特征和标签。我们在两种配置下都进行了大量的实验，以验证我们提出的攻击的有效性。值得注意的是，在现有被动攻击难以有效重建客户端私有数据的挑战性场景中，SDAR始终实现显著优越的攻击性能，甚至可以与主动攻击相媲美。在CIFAR-10上，在7的深度分裂水平上，SDAR实现了私有特征重建，在普通SL和U形SL上的均方误差都小于0.025，在U形背景下获得了98%以上的标签推理准确率，而现有的攻击无法产生非平凡的结果。



