# Latest Adversarial Attack Papers
**update at 2025-11-25 15:23:47**

翻译来自 https://cloud.tencent.com/document/product/551/15619

## **1. Targeted Manipulation: Slope-Based Attacks on Financial Time-Series Data**

有针对性的操纵：对金融时间序列数据的基于斜率的攻击 cs.LG

13 pages, 6 figures, 4 tables, preprint; Total including Appendix: 21 pages, 11 figures, 7 tables

**SubmitDate**: 2025-11-24    [abs](http://arxiv.org/abs/2511.19330v1) [paper-pdf](https://arxiv.org/pdf/2511.19330v1)

**Authors**: Dominik Luszczynski

**Abstract**: A common method of attacking deep learning models is through adversarial attacks, which occur when an attacker specifically modifies the input of a model to produce an incorrect result. Adversarial attacks have been deeply investigated in the image domain; however, there is less research in the time-series domain and very little for forecasting financial data. To address these concerns, this study aims to build upon previous research on adversarial attacks for time-series data by introducing two new slope-based methods aimed to alter the trends of the predicted stock forecast generated by an N-HiTS model. Compared to the normal N-HiTS predictions, the two new slope-based methods, the General Slope Attack and Least-Squares Slope Attack, can manipulate N-HiTS predictions by doubling the slope. These new slope attacks can bypass standard security mechanisms, such as a discriminator that filters real and perturbed inputs, reducing a 4-layered CNN's specificity to 28% and accuracy to 57%. Furthermore, the slope based methods were incorporated into a GAN architecture as a means of generating realistic synthetic data, while simultaneously fooling the model. Finally, this paper also proposes a sample malware designed to inject an adversarial attack in the model inference library, proving that ML-security research should not only focus on making the model safe, but also securing the entire pipeline.

摘要: 攻击深度学习模型的一种常见方法是通过对抗攻击，当攻击者专门修改模型的输入以产生错误的结果时，就会发生对抗攻击。对抗性攻击在图像领域得到了深入的研究;然而，时间序列领域的研究较少，预测金融数据的研究也很少。为了解决这些问题，这项研究旨在以之前关于时间序列数据对抗攻击的研究为基础，引入两种新的基于斜坡的方法，旨在改变N-HiTS模型生成的预测股票预测的趋势。与正常的N-HiTS预测相比，两种新的基于斜坡的方法（一般斜坡攻击和最小平方斜坡攻击）可以通过将斜坡加倍来操纵N-HiTS预测。这些新的斜坡攻击可以绕过标准的安全机制，例如过滤真实和受干扰输入的收件箱，将4层CNN的特异性降低到28%，准确性降低到57%。此外，基于斜坡的方法被整合到GAN架构中，作为生成真实合成数据的一种手段，同时愚弄模型。最后，本文还提出了一个旨在在模型推理库中注入对抗性攻击的恶意软件样本，证明ML安全研究不仅应该关注于使模型安全，还应该关注于确保整个管道的安全。



## **2. Medusa: Cross-Modal Transferable Adversarial Attacks on Multimodal Medical Retrieval-Augmented Generation**

美杜莎：对多模式医学检索增强一代的跨模式可转移对抗攻击 cs.CR

Accepted at KDD 2026 First Cycle (full version). Authors marked with * contributed equally. Yi Liu is the lead author

**SubmitDate**: 2025-11-24    [abs](http://arxiv.org/abs/2511.19257v1) [paper-pdf](https://arxiv.org/pdf/2511.19257v1)

**Authors**: Yingjia Shang, Yi Liu, Huimin Wang, Furong Li, Wenfang Sun, Wu Chengyu, Yefeng Zheng

**Abstract**: With the rapid advancement of retrieval-augmented vision-language models, multimodal medical retrieval-augmented generation (MMed-RAG) systems are increasingly adopted in clinical decision support. These systems enhance medical applications by performing cross-modal retrieval to integrate relevant visual and textual evidence for tasks, e.g., report generation and disease diagnosis. However, their complex architecture also introduces underexplored adversarial vulnerabilities, particularly via visual input perturbations. In this paper, we propose Medusa, a novel framework for crafting cross-modal transferable adversarial attacks on MMed-RAG systems under a black-box setting. Specifically, Medusa formulates the attack as a perturbation optimization problem, leveraging a multi-positive InfoNCE loss (MPIL) to align adversarial visual embeddings with medically plausible but malicious textual targets, thereby hijacking the retrieval process. To enhance transferability, we adopt a surrogate model ensemble and design a dual-loop optimization strategy augmented with invariant risk minimization (IRM). Extensive experiments on two real-world medical tasks, including medical report generation and disease diagnosis, demonstrate that Medusa achieves over 90% average attack success rate across various generation models and retrievers under appropriate parameter configuration, while remaining robust against four mainstream defenses, outperforming state-of-the-art baselines. Our results reveal critical vulnerabilities in the MMed-RAG systems and highlight the necessity of robustness benchmarking in safety-critical medical applications. The code and data are available at https://anonymous.4open.science/r/MMed-RAG-Attack-F05A.

摘要: 随着检索增强视觉语言模型的快速发展，多模式医疗检索增强生成（MMed-RAG）系统越来越多地被用于临床决策支持。这些系统通过执行跨模式检索来集成任务的相关视觉和文本证据来增强医疗应用，例如，报告生成和疾病诊断。然而，它们复杂的架构也引入了未充分探索的对抗漏洞，特别是通过视觉输入扰动。在本文中，我们提出了MedUSA，这是一种新颖的框架，用于在黑匣子环境下对MMed-RAG系统进行跨模式可转移对抗攻击。具体来说，MedUSA将攻击描述为扰动优化问题，利用多正InfoNSO损失（MPIL）将对抗性视觉嵌入与医学上看似合理但恶意的文本目标对齐，从而劫持检索过程。为了增强可移植性，我们采用了代理模型集成，并设计了一种以不变风险最小化（RST）为基础的双环优化策略。对两项现实世界医疗任务（包括医疗报告生成和疾病诊断）的广泛实验表明，在适当的参数配置下，美杜莎在各种代模型和检索器中实现了超过90%的平均攻击成功率，同时对四种主流防御保持稳健，表现优于最先进的基线。我们的结果揭示了MMed-RAG系统中的关键漏洞，并强调了安全关键医疗应用中稳健性基准测试的必要性。代码和数据可在https://anonymous.4open.science/r/MMed-RAG-Attack-F05A上获取。



## **3. Adversarial Patch Attacks on Vision-Based Cargo Occupancy Estimation via Differentiable 3D Simulation**

通过差异化3D模拟对基于视觉的货物占有率估计的对抗补丁攻击 cs.CV

9 pages, 5 figures, 1 algorithm

**SubmitDate**: 2025-11-24    [abs](http://arxiv.org/abs/2511.19254v1) [paper-pdf](https://arxiv.org/pdf/2511.19254v1)

**Authors**: Mohamed Rissal Hedna, Sesugh Samuel Nder

**Abstract**: Computer vision systems are increasingly adopted in modern logistics operations, including the estimation of trailer occupancy for planning, routing, and billing. Although effective, such systems may be vulnerable to physical adversarial attacks, particularly adversarial patches that can be printed and placed on interior surfaces. In this work, we study the feasibility of such attacks on a convolutional cargo-occupancy classifier using fully simulated 3D environments. Using Mitsuba 3 for differentiable rendering, we optimize patch textures across variations in geometry, lighting, and viewpoint, and compare their effectiveness to a 2D compositing baseline. Our experiments demonstrate that 3D-optimized patches achieve high attack success rates, especially in a denial-of-service scenario (empty to full), where success reaches 84.94 percent. Concealment attacks (full to empty) prove more challenging but still reach 30.32 percent. We analyze the factors influencing attack success, discuss implications for the security of automated logistics pipelines, and highlight directions for strengthening physical robustness. To our knowledge, this is the first study to investigate adversarial patch attacks for cargo-occupancy estimation in physically realistic, fully simulated 3D scenes.

摘要: 现代物流运营中越来越多地采用计算机视觉系统，包括估计拖车占用率以进行规划、路线和计费。尽管有效，但此类系统可能容易受到物理对抗攻击，特别是可以打印并放置在内表面上的对抗补丁。在这项工作中，我们研究了使用完全模拟的3D环境对卷积货物占有分类器进行此类攻击的可行性。使用Mitsuba 3进行可区分渲染，我们在几何、照明和视角的变化中优化补丁纹理，并将其有效性与2D合成基线进行比较。我们的实验表明，3D优化补丁可以实现很高的攻击成功率，特别是在拒绝服务场景（空到满）中，成功率达到84.94%。事实证明，隐藏攻击（从满到空）更具挑战性，但仍达到30.32%。我们分析了影响攻击成功的因素，讨论了对自动化物流管道安全性的影响，并强调了加强物理稳健性的方向。据我们所知，这是第一项在物理真实、完全模拟的3D场景中调查对抗补丁攻击以估计货物占用率的研究。



## **4. FedPoisonTTP: A Threat Model and Poisoning Attack for Federated Test-Time Personalization**

FedPoisonTTP：用于联邦测试时个性化的威胁模型和中毒攻击 cs.CR

13 pages, 3 figures, 2 tables

**SubmitDate**: 2025-11-24    [abs](http://arxiv.org/abs/2511.19248v1) [paper-pdf](https://arxiv.org/pdf/2511.19248v1)

**Authors**: Md Akil Raihan Iftee, Syed Md. Ahnaf Hasan, Amin Ahsan Ali, AKM Mahbubur Rahman, Sajib Mistry, Aneesh Krishna

**Abstract**: Test-time personalization in federated learning enables models at clients to adjust online to local domain shifts, enhancing robustness and personalization in deployment. Yet, existing federated learning work largely overlooks the security risks that arise when local adaptation occurs at test time. Heterogeneous domain arrivals, diverse adaptation algorithms, and limited cross-client visibility create vulnerabilities where compromised participants can craft poisoned inputs and submit adversarial updates that undermine both global and per-client performance. To address this threat, we introduce FedPoisonTTP, a realistic grey-box attack framework that explores test-time data poisoning in the federated adaptation setting. FedPoisonTTP distills a surrogate model from adversarial queries, synthesizes in-distribution poisons using feature-consistency, and optimizes attack objectives to generate high-entropy or class-confident poisons that evade common adaptation filters. These poisons are injected during local adaptation and spread through collaborative updates, leading to broad degradation. Extensive experiments on corrupted vision benchmarks show that compromised participants can substantially diminish overall test-time performance.

摘要: 联合学习中的测试时个性化使客户端的模型能够在线调整以适应本地域变化，增强部署的稳健性和个性化。然而，现有的联邦学习工作在很大程度上忽视了测试时进行本地适应时出现的安全风险。不同的域到达、不同的适应算法和有限的跨客户端可见性会产生漏洞，受影响的参与者可以制作有毒的输入并提交对抗性更新，从而损害全球和每个客户端的性能。为了解决这一威胁，我们引入了FedPoisonTTP，这是一个现实的灰箱攻击框架，它探索联邦适应环境中的测试时数据中毒。FedPoisonTTP从对抗性查询中提取代理模型，使用特征一致性合成分布内毒药，并优化攻击目标以生成逃避常见适应过滤器的高熵或类别自信毒药。这些毒素在当地适应期间注入，并通过协作更新传播，导致广泛降解。针对受损视觉基准的大量实验表明，受损的参与者可能会大幅降低整体测试时表现。



## **5. Adversarial Attack-Defense Co-Evolution for LLM Safety Alignment via Tree-Group Dual-Aware Search and Optimization**

通过树群双感知搜索和优化实现LLM安全性调整的对抗性攻击-防御协同进化 cs.CR

**SubmitDate**: 2025-11-24    [abs](http://arxiv.org/abs/2511.19218v1) [paper-pdf](https://arxiv.org/pdf/2511.19218v1)

**Authors**: Xurui Li, Kaisong Song, Rui Zhu, Pin-Yu Chen, Haixu Tang

**Abstract**: Large Language Models (LLMs) have developed rapidly in web services, delivering unprecedented capabilities while amplifying societal risks. Existing works tend to focus on either isolated jailbreak attacks or static defenses, neglecting the dynamic interplay between evolving threats and safeguards in real-world web contexts. To mitigate these challenges, we propose ACE-Safety (Adversarial Co-Evolution for LLM Safety), a novel framework that jointly optimize attack and defense models by seamlessly integrating two key innovative procedures: (1) Group-aware Strategy-guided Monte Carlo Tree Search (GS-MCTS), which efficiently explores jailbreak strategies to uncover vulnerabilities and generate diverse adversarial samples; (2) Adversarial Curriculum Tree-aware Group Policy Optimization (AC-TGPO), which jointly trains attack and defense LLMs with challenging samples via curriculum reinforcement learning, enabling robust mutual improvement. Evaluations across multiple benchmarks demonstrate that our method outperforms existing attack and defense approaches, and provides a feasible pathway for developing LLMs that can sustainably support responsible AI ecosystems.

摘要: 大型语言模型（LLM）在网络服务中迅速发展，提供了前所未有的能力，同时放大了社会风险。现有的作品往往专注于孤立的越狱攻击或静态防御，忽视了现实世界网络环境中不断变化的威胁与保障措施之间的动态相互作用。为了缓解这些挑战，我们提出了ACE安全（针对LLM安全的对抗协同进化），一个新颖的框架，通过无缝集成两个关键的创新过程来联合优化攻击和防御模型：（1）群体感知策略引导的蒙特卡洛树搜索（GS-MCTS），它有效地探索越狱策略以发现漏洞并生成多样化的对抗样本;（2）对抗性课程树感知群组策略优化（AC-TSYS），通过课程强化学习，利用具有挑战性的样本联合训练攻击和防御LLM，实现稳健的相互改进。多个基准的评估表明，我们的方法优于现有的攻击和防御方法，并为开发能够可持续支持负责任的人工智能生态系统的LLM提供了可行的途径。



## **6. Learning to Compress Graphs via Dual Agents for Consistent Topological Robustness Evaluation**

学习通过双代理压缩图以进行一致的布局鲁棒性评估 cs.LG

**SubmitDate**: 2025-11-24    [abs](http://arxiv.org/abs/2511.18958v1) [paper-pdf](https://arxiv.org/pdf/2511.18958v1)

**Authors**: Qisen Chai, Yansong Wang, Junjie Huang, Tao Jia

**Abstract**: As graph-structured data grow increasingly large, evaluating their robustness under adversarial attacks becomes computationally expensive and difficult to scale. To address this challenge, we propose to compress graphs into compact representations that preserve both topological structure and robustness profile, enabling efficient and reliable evaluation.We propose Cutter, a dual-agent reinforcement learning framework composed of a Vital Detection Agent (VDA) and a Redundancy Detection Agent (RDA), which collaboratively identify structurally vital and redundant nodes for guided compression. Cutter incorporates three key strategies to enhance learning efficiency and compression quality: trajectory-level reward shaping to transform sparse trajectory returns into dense, policy-equivalent learning signals; prototype-based shaping to guide decisions using behavioral patterns from both highand low-return trajectories; and cross-agent imitation to enable safer and more transferable exploration. Experiments on multiple real-world graphs demonstrate that Cutter generates compressed graphs that retain essential static topological properties and exhibit robustness degradation trends highly consistent with the original graphs under various attack scenarios, thereby significantly improving evaluation efficiency without compromising assessment fidelity.

摘要: 随着图形结构数据变得越来越大，评估其在对抗攻击下的稳健性变得计算昂贵且难以扩展。为了应对这一挑战，我们提议将图压缩为紧凑的表示，以保留拓扑结构和鲁棒性特征，从而实现高效可靠的评估。我们提出了Cutter，这是一个双代理强化学习框架，由重要检测代理（VDA）和冗余检测代理（RDA）组成，它协作识别结构上重要的冗余节点进行引导压缩。Cutter结合了三项关键策略来提高学习效率和压缩质量：专家级奖励整形，将稀疏轨迹回报转化为密集的、政策等效的学习信号;基于原型的整形，使用高回报轨迹和低回报轨迹的行为模式来指导决策;以及跨代理模仿，以实现更安全、更可转移的探索。对多个现实世界图的实验表明，Cutter生成的压缩图保留了基本的静态拓扑属性，并在各种攻击场景下表现出与原始图高度一致的鲁棒性退化趋势，从而显着提高了评估效率，而不会损害评估保真度。



## **7. Defending Large Language Models Against Jailbreak Exploits with Responsible AI Considerations**

以负责任的人工智能考虑保护大型语言模型免受越狱利用 cs.CR

20 pages including appendix; technical report; NeurIPS 2024 style

**SubmitDate**: 2025-11-24    [abs](http://arxiv.org/abs/2511.18933v1) [paper-pdf](https://arxiv.org/pdf/2511.18933v1)

**Authors**: Ryan Wong, Hosea David Yu Fei Ng, Dhananjai Sharma, Glenn Jun Jie Ng, Kavishvaran Srinivasan

**Abstract**: Large Language Models (LLMs) remain susceptible to jailbreak exploits that bypass safety filters and induce harmful or unethical behavior. This work presents a systematic taxonomy of existing jailbreak defenses across prompt-level, model-level, and training-time interventions, followed by three proposed defense strategies. First, a Prompt-Level Defense Framework detects and neutralizes adversarial inputs through sanitization, paraphrasing, and adaptive system guarding. Second, a Logit-Based Steering Defense reinforces refusal behavior through inference-time vector steering in safety-sensitive layers. Third, a Domain-Specific Agent Defense employs the MetaGPT framework to enforce structured, role-based collaboration and domain adherence. Experiments on benchmark datasets show substantial reductions in attack success rate, achieving full mitigation under the agent-based defense. Overall, this study highlights how jailbreaks pose a significant security threat to LLMs and identifies key intervention points for prevention, while noting that defense strategies often involve trade-offs between safety, performance, and scalability. Code is available at: https://github.com/Kuro0911/CS5446-Project

摘要: 大型语言模型（LLM）仍然容易受到越狱漏洞利用的影响，这些漏洞绕过安全过滤器并引发有害或不道德行为。这项工作对预算级、模型级和训练时干预措施的现有越狱防御进行了系统分类，然后提出了三种拟议的防御策略。首先，预算级防御框架通过净化、重述和自适应系统防护来检测并中和对抗输入。其次，基于日志的转向防御通过安全敏感层中的推理时间载体转向来加强拒绝行为。第三，领域特定代理防御采用MetaGPT框架来实施结构化的、基于角色的协作和领域遵守。对基准数据集的实验显示，攻击成功率大幅降低，在基于代理的防御下实现了全面缓解。总体而言，这项研究强调了越狱如何对LLM构成重大安全威胁，并确定了预防的关键干预点，同时指出防御策略通常涉及安全性、性能和可扩展性之间的权衡。代码可访问：https://github.com/Kuro0911/CS5446-Project



## **8. BackdoorVLM: A Benchmark for Backdoor Attacks on Vision-Language Models**

BackdoorVLM：视觉语言模型后门攻击的基准 cs.CV

**SubmitDate**: 2025-11-24    [abs](http://arxiv.org/abs/2511.18921v1) [paper-pdf](https://arxiv.org/pdf/2511.18921v1)

**Authors**: Juncheng Li, Yige Li, Hanxun Huang, Yunhao Chen, Xin Wang, Yixu Wang, Xingjun Ma, Yu-Gang Jiang

**Abstract**: Backdoor attacks undermine the reliability and trustworthiness of machine learning systems by injecting hidden behaviors that can be maliciously activated at inference time. While such threats have been extensively studied in unimodal settings, their impact on multimodal foundation models, particularly vision-language models (VLMs), remains largely underexplored. In this work, we introduce \textbf{BackdoorVLM}, the first comprehensive benchmark for systematically evaluating backdoor attacks on VLMs across a broad range of settings. It adopts a unified perspective that injects and analyzes backdoors across core vision-language tasks, including image captioning and visual question answering. BackdoorVLM organizes multimodal backdoor threats into 5 representative categories: targeted refusal, malicious injection, jailbreak, concept substitution, and perceptual hijack. Each category captures a distinct pathway through which an adversary can manipulate a model's behavior. We evaluate these threats using 12 representative attack methods spanning text, image, and bimodal triggers, tested on 2 open-source VLMs and 3 multimodal datasets. Our analysis reveals that VLMs exhibit strong sensitivity to textual instructions, and in bimodal backdoors the text trigger typically overwhelms the image trigger when forming the backdoor mapping. Notably, backdoors involving the textual modality remain highly potent, with poisoning rates as low as 1\% yielding over 90\% success across most tasks. These findings highlight significant, previously underexplored vulnerabilities in current VLMs. We hope that BackdoorVLM can serve as a useful benchmark for analyzing and mitigating multimodal backdoor threats. Code is available at: https://github.com/bin015/BackdoorVLM .

摘要: 后门攻击通过注入可能在推理时被恶意激活的隐藏行为来破坏机器学习系统的可靠性和可信性。虽然此类威胁在单模式环境中得到了广泛研究，但它们对多模式基础模型（尤其是视觉语言模型（VLM）的影响在很大程度上仍然没有得到充分的研究。在这项工作中，我们引入了\textBF{BackdoorVLM}，这是第一个用于在广泛的设置中系统评估对VLM的后门攻击的全面基准。它采用统一的视角，在核心视觉语言任务（包括图像字幕和视觉问答）中注入和分析后门。BackdoorVLM将多模式后门威胁分为5个代表性类别：定向拒绝、恶意注入、越狱、概念替代和感知劫持。每个类别都捕获了对手可以操纵模型行为的独特途径。我们使用涵盖文本、图像和双峰触发器的12种代表性攻击方法来评估这些威胁，并在2个开源VLM和3个多模式数据集上进行了测试。我们的分析表明，VLM对文本指令表现出很强的敏感性，并且在双峰后门中，文本触发器在形成后门映射时通常会触发图像触发器。值得注意的是，涉及文本形式的后门仍然非常有效，中毒率低至1%，大多数任务的成功率超过90%。这些发现凸显了当前VLM中先前未充分探索的重大漏洞。我们希望BackdoorVLM能够成为分析和缓解多模式后门威胁的有用基准。代码可访问：https://github.com/bin015/BackdoorVLM。



## **9. Now You See It, Now You Don't - Instant Concept Erasure for Safe Text-to-Image and Video Generation**

现在你看到了，现在你不看到-即时概念擦除，实现安全的文本到图像和视频生成 cs.CV

**SubmitDate**: 2025-11-24    [abs](http://arxiv.org/abs/2511.18684v1) [paper-pdf](https://arxiv.org/pdf/2511.18684v1)

**Authors**: Shristi Das Biswas, Arani Roy, Kaushik Roy

**Abstract**: Robust concept removal for text-to-image (T2I) and text-to-video (T2V) models is essential for their safe deployment. Existing methods, however, suffer from costly retraining, inference overhead, or vulnerability to adversarial attacks. Crucially, they rarely model the latent semantic overlap between the target erase concept and surrounding content -- causing collateral damage post-erasure -- and even fewer methods work reliably across both T2I and T2V domains. We introduce Instant Concept Erasure (ICE), a training-free, modality-agnostic, one-shot weight modification approach that achieves precise, persistent unlearning with zero overhead. ICE defines erase and preserve subspaces using anisotropic energy-weighted scaling, then explicitly regularises against their intersection using a unique, closed-form overlap projector. We pose a convex and Lipschitz-bounded Spectral Unlearning Objective, balancing erasure fidelity and intersection preservation, that admits a stable and unique analytical solution. This solution defines a dissociation operator that is translated to the model's text-conditioning layers, making the edit permanent and runtime-free. Across targeted removals of artistic styles, objects, identities, and explicit content, ICE efficiently achieves strong erasure with improved robustness to red-teaming, all while causing only minimal degradation of original generative abilities in both T2I and T2V models.

摘要: 文本到图像（T2 I）和文本到视频（T2 V）模型的稳健概念删除对于它们的安全部署至关重要。然而，现有的方法存在成本高昂的再培训、推理费用或容易受到对抗攻击的影响。至关重要的是，他们很少对目标擦除概念和周围内容之间潜在的语义重叠进行建模，从而在擦除后造成附带损害，而且在T2 I和T2 V域中可靠工作的方法甚至更少。我们引入即时概念擦除（ICE），这是一种免训练、模式不可知、一次性权重修改方法，可以以零费用实现精确、持续的去学习。ICE使用各向异性能量加权缩放来定义擦除和保留子空间，然后使用独特的封闭形式重叠投影仪针对它们的相交进行显式调整。我们提出了一个凸的和利普希茨有界的光谱取消学习目标，平衡了擦除保真度和交集保留，它允许稳定且独特的分析解。该解决方案定义了一个分离操作符，该操作符被转换为模型的文本条件层，使编辑永久且不受运行时限制。在有针对性地删除艺术风格、对象、身份和显式内容时，ICE有效地实现了强擦除，并提高了对红色团队的鲁棒性，同时在T2 I和T2 V模型中仅导致原始生成能力的最小退化。



## **10. Robust Physical Adversarial Patches Using Dynamically Optimized Clusters**

使用动态优化集群的稳健物理对抗补丁 cs.CV

Supplementary material available at: https://drive.google.com/drive/folders/1Yntcc9CARdbvoJJ51cyUm1DWGSvU9X4V?usp=drive_link

**SubmitDate**: 2025-11-23    [abs](http://arxiv.org/abs/2511.18656v1) [paper-pdf](https://arxiv.org/pdf/2511.18656v1)

**Authors**: Harrison Bagley, Will Meakin, Simon Lucey, Yee Wei Law, Tat-Jun Chin

**Abstract**: Physical adversarial attacks on deep learning systems is concerning due to the ease of deploying such attacks, usually by placing an adversarial patch in a scene to manipulate the outcomes of a deep learning model. Training such patches typically requires regularization that improves physical realizability (e.g., printability, smoothness) and/or robustness to real-world variability (e.g. deformations, viewing angle, noise). One type of variability that has received little attention is scale variability. When a patch is rescaled, either digitally through downsampling/upsampling or physically through changing imaging distances, interpolation-induced color mixing occurs. This smooths out pixel values, resulting in a loss of high-frequency patterns and degrading the adversarial signal. To address this, we present a novel superpixel-based regularization method that guides patch optimization to scale-resilient structures. Our ap proach employs the Simple Linear Iterative Clustering (SLIC) algorithm to dynamically cluster pixels in an adversarial patch during optimization. The Implicit Function Theorem is used to backpropagate gradients through SLIC to update the superpixel boundaries and color. This produces patches that maintain their structure over scale and are less susceptible to interpolation losses. Our method achieves greater performance in the digital domain, and when realized physically, these performance gains are preserved, leading to improved physical performance. Real-world performance was objectively assessed using a novel physical evaluation protocol that utilizes screens and cardboard cut-outs to systematically vary real-world conditions.

摘要: 对深度学习系统的物理对抗攻击令人担忧，因为此类攻击很容易部署，通常是通过在场景中放置对抗补丁来操纵深度学习模型的结果。训练此类补丁通常需要提高物理实现性的正规化（例如，可印刷性、平滑度）和/或对现实世界可变性（例如变形、视角、噪音）的鲁棒性。一种很少受到关注的变异性是规模变异性。当补丁重新缩放时，无论是通过下采样/上采样进行数字调整，还是通过改变成像距离进行物理调整，都会发生内插引起的颜色混合。这会平滑像素值，导致高频模式的丢失并降低对抗信号的性能。为了解决这个问题，我们提出了一种新型的基于超像素的正规化方法，该方法将补丁优化为具有规模弹性的结构。我们的方法采用简单线性迭代集群（SIIC）算法在优化期间动态集群对抗补丁中的像素。隐函数定理用于通过SIIC反向传播梯度，以更新超像素边界和颜色。这产生的补丁可以超规模地维持其结构，并且不太容易受到插值损失的影响。我们的方法在数字领域实现了更高的性能，并且当物理实现时，这些性能收益会被保留，从而改善物理性能。使用一种新型的物理评估方案客观评估现实世界的表现，该方案利用屏幕和纸板切口来系统性地改变现实世界的条件。



## **11. Algorithmic detection of false data injection attacks in cyber-physical systems**

网络物理系统中虚假数据注入攻击的计算机检测 math.OC

13 pages, 6 figures

**SubmitDate**: 2025-11-23    [abs](http://arxiv.org/abs/2511.18588v1) [paper-pdf](https://arxiv.org/pdf/2511.18588v1)

**Authors**: Souvik Das, Avishek Ghosh, Debasish Chatterjee

**Abstract**: This article introduces an anomaly detection based algorithm (AD-CPS) to detect false data injection attacks that fall under the category of data deception/integrity attacks, but with arbitrary information structure, in cyber-physical systems (CPSs) modeled as stochastic linear time-invariant systems. The core idea of this data-driven algorithm is based on the fact that an honest state (one not compromised by adversaries) generated by the CPS should concentrate near its weighted empirical mean of the immediate past samples. As the first theoretical result, we provide non-asymptotic guarantees on the false positive error incurred by the algorithm for attacks that are 2-step honest, referring to adversaries that act intermittently rather than successively. Moreover, we establish that for adversaries possessing a certain minimum energy, the false negative error incurred by AD-CPS is low. Extensive experiments were conducted on partially observed stochastic LTI systems to demonstrate these properties and to quantitatively compare AD-CPS with an optimal CUSUM-based test.

摘要: 本文介绍了一种基于异常检测的算法（AD-CPS）来检测虚假数据注入攻击，属于数据欺骗/完整性攻击的范畴，但具有任意的信息结构，在网络物理系统（CPS）建模为随机线性时不变系统。这种数据驱动算法的核心思想是基于这样一个事实，即CPS生成的诚实状态（不受对手损害的状态）应该集中在其最近过去样本的加权经验平均值附近。作为第一个理论结果，我们提供了非渐近保证的假阳性错误算法的攻击是2步诚实，指的是对手的行为间歇性，而不是连续的。此外，我们建立了具有一定的最小能量的对手，由AD-CPS引起的假阴性错误是低的。在部分观察到的随机LTI系统上进行了大量实验，以证明这些特性，并将AD-CPS与最佳基于CLARUM的测试进行定量比较。



## **12. Zero-Trust Strategies for O-RAN Cellular Networks: Principles, Challenges and Research Directions**

O-RAN蜂窝网络的零信任策略：原则、挑战和研究方向 cs.CR

**SubmitDate**: 2025-11-23    [abs](http://arxiv.org/abs/2511.18568v1) [paper-pdf](https://arxiv.org/pdf/2511.18568v1)

**Authors**: Charalampos Katsis, Imtiaz Karim, Elisa Bertino

**Abstract**: Cellular networks have become foundational to modern communication, supporting a broad range of applications, from civilian use to enterprise systems and military tactical networks. The advent of fifth-generation and beyond cellular networks (B5G) introduces emerging compute capabilities into the Radio Access Network (RAN), transforming it from a traditionally closed, vendor-locked infrastructure into an open and programmable ecosystem. This evolution, exemplified by Open-RAN (O-RAN), enables the deployment of control-plane applications from diverse sources, which can dynamically influence user-plane traffic in response to real-time events. As cellular infrastructures become more disaggregated and software-driven, security becomes an increasingly critical concern. Zero-Trust Architecture (ZTA) has emerged as a promising security paradigm that discards implicit trust assumptions by acknowledging that threats may arise from both external and internal sources. ZTA mandates comprehensive and fine-grained security mechanisms across both control and user planes to contain adversarial movements and enhance breach detection and attack response actions. In this paper, we explore the adoption of ZTA in the context of 5G and beyond, with a particular focus on O-RAN as an architectural enabler. We analyze how ZTA principles align with the architectural and operational characteristics of O-RAN, and identify key challenges and opportunities for embedding zero-trust mechanisms within O-RAN-based cellular networks.

摘要: 蜂窝网络已成为现代通信的基础，支持从民用到企业系统和军事战术网络的广泛应用。第五代及以后的蜂窝网络（B5 G）的出现将新兴的计算能力引入无线电接入网络（RAN），将其从传统上封闭、供应商锁定的基础设施转变为开放且可编程的生态系统。这种演变以Open-RAN（O-RAN）为例，可以部署来自不同来源的控制平面应用程序，这可以动态影响用户平面流量以响应实时事件。随着蜂窝基础设施变得更加分散和软件驱动，安全性成为一个越来越重要的问题。零信任架构（ZTA）已成为一种有前途的安全范式，它通过承认威胁可能来自外部和内部来源，从而放弃了隐含信任假设。ZTA要求控制平面和用户平面建立全面且细粒度的安全机制，以遏制对抗运动并增强漏洞检测和攻击响应行动。在本文中，我们探讨了ZTA在5G及以后环境中的采用，特别关注O-RAN作为架构使能器。我们分析ZTA原则如何与O-RAN的架构和运营特征保持一致，并确定在基于O-RAN的蜂窝网络中嵌入零信任机制的关键挑战和机遇。



## **13. Ensuring Calibration Robustness in Split Conformal Prediction Under Adversarial Attacks**

对抗性攻击下分裂共形预测中的校准鲁棒性 stat.ML

Submitted to AISTATS 2026

**SubmitDate**: 2025-11-23    [abs](http://arxiv.org/abs/2511.18562v1) [paper-pdf](https://arxiv.org/pdf/2511.18562v1)

**Authors**: Xunlei Qian, Yue Xing

**Abstract**: Conformal prediction (CP) provides distribution-free, finite-sample coverage guarantees but critically relies on exchangeability, a condition often violated under distribution shift. We study the robustness of split conformal prediction under adversarial perturbations at test time, focusing on both coverage validity and the resulting prediction set size. Our theoretical analysis characterizes how the strength of adversarial perturbations during calibration affects coverage guarantees under adversarial test conditions. We further examine the impact of adversarial training at the model-training stage. Extensive experiments support our theory: (i) Prediction coverage varies monotonically with the calibration-time attack strength, enabling the use of nonzero calibration-time attack to predictably control coverage under adversarial tests; (ii) target coverage can hold over a range of test-time attacks: with a suitable calibration attack, coverage stays within any chosen tolerance band across a contiguous set of perturbation levels; and (iii) adversarial training at the training stage produces tighter prediction sets that retain high informativeness.

摘要: 保形预测（CP）提供无分布、有限样本覆盖保证，但严重依赖于交换性，这是分布转移下经常违反的条件。我们研究测试时对抗性扰动下分裂保形预测的鲁棒性，重点关注覆盖有效性和由此产生的预测集大小。我们的理论分析描述了校准期间对抗性扰动的强度如何影响对抗性测试条件下的覆盖保证。我们进一步研究了模型训练阶段对抗训练的影响。大量的实验支持我们的理论：（i）预测覆盖率随着校准时间攻击强度单调变化，使得使用非零校准时间攻击来在对抗测试下可预测地控制覆盖率;（ii）目标覆盖率可以在一系列测试时间攻击中保持：使用合适的校准攻击，覆盖率保持在任何选定的容差范围内，跨越连续的扰动水平集;以及（iii）训练阶段的对抗训练产生保持高信息量的更紧密的预测集。



## **14. A Novel and Practical Universal Adversarial Perturbations against Deep Reinforcement Learning based Intrusion Detection Systems**

一种新颖实用的针对基于深度强化学习的入侵检测系统的通用对抗性扰动 cs.CR

13 pages, 7 Figures,

**SubmitDate**: 2025-11-22    [abs](http://arxiv.org/abs/2511.18223v1) [paper-pdf](https://arxiv.org/pdf/2511.18223v1)

**Authors**: H. Zhang, L. Zhang, G. Epiphaniou, C. Maple

**Abstract**: Intrusion Detection Systems (IDS) play a vital role in defending modern cyber physical systems against increasingly sophisticated cyber threats. Deep Reinforcement Learning-based IDS, have shown promise due to their adaptive and generalization capabilities. However, recent studies reveal their vulnerability to adversarial attacks, including Universal Adversarial Perturbations (UAPs), which can deceive models with a single, input-agnostic perturbation. In this work, we propose a novel UAP attack against Deep Reinforcement Learning (DRL)-based IDS under the domain-specific constraints derived from network data rules and feature relationships. To the best of our knowledge, there is no existing study that has explored UAP generation for the DRL-based IDS. In addition, this is the first work that focuses on developing a UAP against a DRL-based IDS under realistic domain constraints based on not only the basic domain rules but also mathematical relations between the features. Furthermore, we enhance the evasion performance of the proposed UAP, by introducing a customized loss function based on the Pearson Correlation Coefficient, and we denote it as Customized UAP. To the best of our knowledge, this is also the first work using the PCC value in the UAP generation, even in the broader context. Four additional established UAP baselines are implemented for a comprehensive comparison. Experimental results demonstrate that our proposed Customized UAP outperforms two input-dependent attacks including Fast Gradient Sign Method (FGSM), Basic Iterative Method (BIM), and four UAP baselines, highlighting its effectiveness for real-world adversarial scenarios.

摘要: 入侵检测系统（IDS）在保护现代网络物理系统免受日益复杂的网络威胁方面发挥着至关重要的作用。基于深度强化学习的IDS由于其自适应和泛化能力而显示出了希望。然而，最近的研究揭示了它们对对抗性攻击的脆弱性，包括通用对抗性扰动（Universal Adversarial Perturbations，UAP），它可以用一个输入不可知的扰动来欺骗模型。在这项工作中，我们提出了一种新的UAP攻击深度强化学习（DRL）为基础的入侵检测系统下的域特定的约束来自网络数据规则和特征关系。据我们所知，目前还没有研究探索基于DRL的IDS的UAP生成。此外，这是第一项重点关注在现实领域约束下针对基于DRL的IDS开发UAP的工作，不仅基于基本领域规则，还基于特征之间的数学关系。此外，我们通过引入基于皮尔逊相关系数的定制损失函数来增强所提出的UAP的规避性能，并将其称为定制的UAP。据我们所知，这也是UAP一代中首次使用PCC值的作品，即使是在更广泛的背景下也是如此。实施了另外四个既定的UAP基线以进行全面比较。实验结果表明，我们提出的定制UAP优于两种依赖于输入的攻击，包括快速梯度符号法（FGSM）、基本迭代法（BMI）和四种UAP基线，凸显了其对现实世界对抗场景的有效性。



## **15. Vulnerability-Aware Robust Multimodal Adversarial Training**

具有脆弱性的稳健多模式对抗训练 cs.LG

Accepted by AAAI26

**SubmitDate**: 2025-11-22    [abs](http://arxiv.org/abs/2511.18138v1) [paper-pdf](https://arxiv.org/pdf/2511.18138v1)

**Authors**: Junrui Zhang, Xinyu Zhao, Jie Peng, Chenjie Wang, Jianmin Ji, Tianlong Chen

**Abstract**: Multimodal learning has shown significant superiority on various tasks by integrating multiple modalities. However, the interdependencies among modalities increase the susceptibility of multimodal models to adversarial attacks. Existing methods mainly focus on attacks on specific modalities or indiscriminately attack all modalities. In this paper, we find that these approaches ignore the differences between modalities in their contribution to final robustness, resulting in suboptimal robustness performance. To bridge this gap, we introduce Vulnerability-Aware Robust Multimodal Adversarial Training (VARMAT), a probe-in-training adversarial training method that improves multimodal robustness by identifying the vulnerability of each modality. To be specific, VARMAT first explicitly quantifies the vulnerability of each modality, grounded in a first-order approximation of the attack objective (Probe). Then, we propose a targeted regularization term that penalizes modalities with high vulnerability, guiding robust learning while maintaining task accuracy (Training). We demonstrate the enhanced robustness of our method across multiple multimodal datasets involving diverse modalities. Finally, we achieve {12.73%, 22.21%, 11.19%} robustness improvement on three multimodal datasets, revealing a significant blind spot in multimodal adversarial training.

摘要: 多模式学习通过集成多种模式在各种任务中显示出显着的优势。然而，模式之间的相互依赖性增加了多模式模型对对抗攻击的敏感性。现有的方法主要集中在对特定模式的攻击或不加区别地攻击所有模式。在本文中，我们发现这些方法忽略了模式之间对最终鲁棒性的贡献的差异，从而导致鲁棒性性能次优。为了弥合这一差距，我们引入了脆弱性感知鲁棒多模态对抗训练（VARMAT），这是一种训练中的对抗训练方法，通过识别每种模态的脆弱性来提高多模态的鲁棒性。具体来说，VARMAT首先明确量化每种模态的脆弱性，基于攻击目标的一阶近似（探测）。然后，我们提出了一个有针对性的正则化项，惩罚具有高脆弱性的模态，在保持任务准确性的同时指导鲁棒学习（训练）。我们证明了我们的方法在涉及不同模态的多模态数据集上的增强的鲁棒性。最后，我们在三个多模态数据集上实现了{12.73%，22.21%，11.19%}的鲁棒性改进，揭示了多模态对抗训练中的一个显著盲点。



## **16. Federated Anomaly Detection and Mitigation for EV Charging Forecasting Under Cyberattacks**

网络攻击下电动汽车充电预测的联邦异常检测和缓解 cs.LG

6 pages

**SubmitDate**: 2025-11-22    [abs](http://arxiv.org/abs/2511.17978v1) [paper-pdf](https://arxiv.org/pdf/2511.17978v1)

**Authors**: Oluleke Babayomi, Dong-Seong Kim

**Abstract**: Electric Vehicle (EV) charging infrastructure faces escalating cybersecurity threats that can severely compromise operational efficiency and grid stability. Existing forecasting techniques are limited by the lack of combined robust anomaly mitigation solutions and data privacy preservation. Therefore, this paper addresses these challenges by proposing a novel anomaly-resilient federated learning framework that simultaneously preserves data privacy, detects cyber-attacks, and maintains trustworthy demand prediction accuracy under adversarial conditions. The proposed framework integrates three key innovations: LSTM autoencoder-based distributed anomaly detection deployed at each federated client, interpolation-based anomalous data mitigation to preserve temporal continuity, and federated Long Short-Term Memory (LSTM) networks that enable collaborative learning without centralized data aggregation. The framework is validated on real-world EV charging infrastructure datasets combined with real-world DDoS attack datasets, providing robust validation of the proposed approach under realistic threat scenarios. Experimental results demonstrate that the federated approach achieves superior performance compared to centralized models, with 15.2% improvement in R2 accuracy while maintaining data locality. The integrated cyber-attack detection and mitigation system produces trustworthy datasets that enhance prediction reliability, recovering 47.9% of attack-induced performance degradation while maintaining exceptional precision (91.3%) and minimal false positive rates (1.21%). The proposed architecture enables enhanced EV infrastructure planning, privacy-preserving collaborative forecasting, cybersecurity resilience, and rapid recovery from malicious threats across distributed charging networks.

摘要: 电动汽车（EV）充电基础设施面临不断升级的网络安全威胁，这可能会严重损害运营效率和电网稳定性。现有的预测技术因缺乏强大的异常缓解解决方案和数据隐私保护而受到限制。因此，本文通过提出一种新型的异常弹性联邦学习框架来解决这些挑战，该框架同时保护数据隐私、检测网络攻击并在对抗条件下保持值得信赖的需求预测准确性。拟议的框架集成了三项关键创新：在每个联邦客户端部署的基于LSTM自动编码器的分布式异常检测、基于内插的异常数据缓解以保持时间连续性，以及联邦长短期记忆（LSTM）网络，无需集中式数据聚合即可实现协作学习。该框架在现实世界的电动汽车充电基础设施数据集与现实世界的DDOS攻击数据集相结合的上进行了验证，从而在现实威胁场景下对所提出的方法提供了稳健的验证。实验结果表明，与集中式模型相比，联邦方法实现了更卓越的性能，R2准确性提高了15.2%，同时保持了数据局部性。集成的网络攻击检测和缓解系统可生成值得信赖的数据集，可增强预测可靠性，恢复47.9%的攻击引发的性能下降，同时保持出色的精确度（91.3%）和最低的假阳性率（1.21%）。拟议的架构能够增强电动汽车基础设施规划、保护隐私的协作预测、网络安全弹性以及从分布式充电网络中的恶意威胁中快速恢复。



## **17. Adversarial Pseudo-replay for Exemplar-free Class-incremental Learning**

无示例课堂增量学习的对抗性伪重播 cs.CV

Accepted to WACV 2026

**SubmitDate**: 2025-11-22    [abs](http://arxiv.org/abs/2511.17973v1) [paper-pdf](https://arxiv.org/pdf/2511.17973v1)

**Authors**: Hiroto Honda

**Abstract**: Exemplar-free class-incremental learning (EFCIL) aims to retain old knowledge acquired in the previous task while learning new classes, without storing the previous images due to storage constraints or privacy concerns. In EFCIL, the plasticity-stability dilemma, learning new tasks versus catastrophic forgetting, is a significant challenge, primarily due to the unavailability of images from earlier tasks. In this paper, we introduce adversarial pseudo-replay (APR), a method that perturbs the images of the new task with adversarial attack, to synthesize the pseudo-replay images online without storing any replay samples. During the new task training, the adversarial attack is conducted on the new task images with augmented old class mean prototypes as targets, and the resulting images are used for knowledge distillation to prevent semantic drift. Moreover, we calibrate the covariance matrices to compensate for the semantic drift after each task, by learning a transfer matrix on the pseudo-replay samples. Our method reconciles stability and plasticity, achieving state-of-the-art on challenging cold-start settings of the standard EFCIL benchmarks.

摘要: 无样本类增量学习（EFCIL）的目标是在学习新类的同时保留在先前任务中获得的旧知识，而不会由于存储限制或隐私问题而存储先前的图像。在EFCIL中，可塑性-稳定性困境，学习新任务与灾难性遗忘，是一个重大的挑战，主要是由于早期任务的图像不可用。在本文中，我们介绍了对抗性伪重放（APR），一种方法，扰动的图像的新任务的对抗性攻击，在线合成的伪重放图像，而无需存储任何重放样本。在新任务训练过程中，以增强后的旧类均值原型为目标，对新任务图像进行对抗性攻击，并将生成的图像用于知识提取，防止语义漂移。此外，我们通过学习伪重播样本的转移矩阵来校准协方差矩阵，以补偿每个任务后的语义漂移。我们的方法兼顾了稳定性和可塑性，在标准EFCIL基准的具有挑战性的冷启动设置中实现了最先进的技术水平。



## **18. StealthCup: Realistic, Multi-Stage, Evasion-Focused CTF for Benchmarking IDS**

StealthCup：用于IDS基准测试的现实、多阶段、以逃避为中心的CTF cs.CR

**SubmitDate**: 2025-11-21    [abs](http://arxiv.org/abs/2511.17761v1) [paper-pdf](https://arxiv.org/pdf/2511.17761v1)

**Authors**: Manuel Kern, Dominik Steffan, Felix Schuster, Florian Skopik, Max Landauer, David Allison, Simon Freudenthaler, Edgar Weippl

**Abstract**: Intrusion Detection Systems (IDS) are critical to defending enterprise and industrial control environments, yet evaluating their effectiveness under realistic conditions remains an open challenge. Existing benchmarks rely on synthetic datasets (e.g., NSL-KDD, CICIDS2017) or scripted replay frameworks, which fail to capture adaptive adversary behavior. Even MITRE ATT&CK Evaluations, while influential, are host-centric and assume malware-driven compromise, thereby under-representing stealthy, multi-stage intrusions across IT and OT domains. We present StealthCup, a novel evaluation methodology that operationalizes IDS benchmarking as an evasion-focused Capture-the-Flag competition. Professional penetration testers engaged in multi-stage attack chains on a realistic IT/OT testbed, with scoring penalizing IDS detections. The event generated structured attacker writeups, validated detections, and PCAPs, host logs, and alerts. Our results reveal that out of 32 exercised attack techniques, 11 were not detected by any IDS configuration. Open-source systems (Wazuh, Suricata) produced high false-positive rates >90%, while commercial tools generated fewer false positives but also missed more attacks. Comparison with the Volt Typhoon APT advisory confirmed strong realism: all 28 applicable techniques were exercised, 19 appeared in writeups, and 9 in forensic traces. These findings demonstrate that StealthCup elicits attacker behavior closely aligned with state-sponsored TTPs, while exposing blind spots across both open-source and commercial IDS. The resulting datasets and methodology provide a reproducible foundation for future stealth-focused IDS evaluation.

摘要: 入侵检测系统（IDS）对于保护企业和工业控制环境至关重要，但在现实条件下评估其有效性仍然是一个悬而未决的挑战。现有的基准测试依赖于合成数据集（例如，NSL-KDD、CICIDS 2017）或脚本回放框架，无法捕捉自适应对手行为。即使是MITRE ATA & CK评估虽然有影响力，但也以主机为中心，并假设恶意软件驱动的妥协，从而不足以代表IT和OT领域的隐蔽、多阶段入侵。我们介绍了StealthCup，这是一种新颖的评估方法，它将IDS基准测试作为一场以逃避为中心的Capture-the-Flag竞赛。专业渗透测试人员在现实的IT/OT测试床上参与多阶段攻击链，评分惩罚IDS检测。该事件生成结构化攻击者撰写、验证检测以及PCAP、主机日志和警报。我们的结果显示，在32种练习攻击技术中，有11种没有被任何IDS配置检测到。开源系统（Wazuh、Suricata）产生的假阳性率高达90%以上，而商业工具产生的假阳性率更少，但也错过了更多攻击。与Volt Typhoon APT建议的比较证实了强烈的现实性：所有28种适用技术都得到了运用，19种出现在文章中，9种出现在法医痕迹中。这些发现表明，StealthCup引发了与国家支持的TTP密切一致的攻击者行为，同时暴露了开源和商业IDS的盲点。由此产生的数据集和方法为未来以窃取为重点的IDS评估提供了可重复的基础。



## **19. ATAC: Augmentation-Based Test-Time Adversarial Correction for CLIP**

ATAC：CLIP的基于增强的测试时对抗纠正 cs.CV

16 pages

**SubmitDate**: 2025-11-21    [abs](http://arxiv.org/abs/2511.17362v1) [paper-pdf](https://arxiv.org/pdf/2511.17362v1)

**Authors**: Linxiang Su, András Balogh

**Abstract**: Despite its remarkable success in zero-shot image-text matching, CLIP remains highly vulnerable to adversarial perturbations on images. As adversarial fine-tuning is prohibitively costly, recent works explore various test-time defense strategies; however, these approaches still exhibit limited robustness. In this work, we revisit this problem and propose a simple yet effective strategy: Augmentation-based Test-time Adversarial Correction (ATAC). Our method operates directly in the embedding space of CLIP, calculating augmentation-induced drift vectors to infer a semantic recovery direction and correcting the embedding based on the angular consistency of these latent drifts. Across a wide range of benchmarks, ATAC consistently achieves remarkably high robustness, surpassing that of previous state-of-the-art methods by nearly 50\% on average, all while requiring minimal computational overhead. Furthermore, ATAC retains state-of-the-art robustness in unconventional and extreme settings and even achieves nontrivial robustness against adaptive attacks. Our results demonstrate that ATAC is an efficient method in a novel paradigm for test-time adversarial defenses in the embedding space of CLIP.

摘要: 尽管CLIP在零镜头图像文本匹配方面取得了显着成功，但仍然极易受到图像上的对抗性扰动的影响。由于对抗性微调的成本高得令人望而却步，最近的作品探索了各种测试时防御策略;然而，这些方法仍然表现出有限的稳健性。在这项工作中，我们重新审视了这个问题，并提出了一种简单而有效的策略：基于增强的测试时对抗纠正（ATAC）。我们的方法直接在CLIP的嵌入空间中操作，计算增强引起的漂移量以推断语义恢复方向，并基于这些潜在漂移的角度一致性来纠正嵌入。在各种基准测试中，ATAC始终实现了非常高的稳健性，平均比之前最先进的方法高出近50%，同时需要最小的计算负担。此外，ATAC在非常规和极端环境中保留了最先进的鲁棒性，甚至实现了针对自适应攻击的非凡鲁棒性。我们的结果表明，ATAC是CLIP嵌入空间中测试时对抗防御的新型范式中的一种有效方法。



## **20. Enhancing Adversarial Transferability through Block Stretch and Shrink**

通过区块伸展和收缩增强对抗性可转移性 cs.LG

code will be releace

**SubmitDate**: 2025-11-21    [abs](http://arxiv.org/abs/2511.17688v1) [paper-pdf](https://arxiv.org/pdf/2511.17688v1)

**Authors**: Quan Liu, Feng Ye, Chenhao Lu, Shuming Zhen, Guanliang Huang, Lunzhe Chen, Xudong Ke

**Abstract**: Adversarial attacks introduce small, deliberately crafted perturbations that mislead neural networks, and their transferability from white-box to black-box target models remains a critical research focus. Input transformation-based attacks are a subfield of adversarial attacks that enhance input diversity through input transformations to improve the transferability of adversarial examples. However, existing input transformation-based attacks tend to exhibit limited cross-model transferability. Previous studies have shown that high transferability is associated with diverse attention heatmaps and the preservation of global semantics in transformed inputs. Motivated by this observation, we propose Block Stretch and Shrink (BSS), a method that divides an image into blocks and applies stretch and shrink operations to these blocks, thereby diversifying attention heatmaps in transformed inputs while maintaining their global semantics. Empirical evaluations on a subset of ImageNet demonstrate that BSS outperforms existing input transformation-based attack methods in terms of transferability. Furthermore, we examine the impact of the number scale, defined as the number of transformed inputs, in input transformation-based attacks, and advocate evaluating these methods under a unified number scale to enable fair and comparable assessments.

摘要: 对抗性攻击会引入小的、故意设计的干扰，误导神经网络，而它们从白盒目标模型到黑盒目标模型的可移植性仍然是一个关键的研究焦点。基于输入转换的攻击是对抗性攻击的一个子领域，通过输入转换增强输入多样性，以提高对抗性示例的可移植性。然而，现有的基于输入转换的攻击往往表现出有限的跨模型可移植性。之前的研究表明，高可转移性与多样化的注意力热图和转换后的输入中的全局语义的保留有关。受这一观察结果的启发，我们提出了块伸展和收缩（OSS），这是一种将图像分为块并对这些块应用伸展和收缩操作的方法，从而使转换后的输入中的注意力热图多样化，同时保持其全局语义。对ImageNet一个子集的经验评估表明，在可移植性方面，OSS优于现有的基于输入转换的攻击方法。此外，我们研究了数字规模（定义为转换输入的数量）在基于输入转换的攻击中的影响，并主张在统一的数字规模下评估这些方法，以实现公平和可比的评估。



## **21. MURMUR: Using cross-user chatter to break collaborative language agents in groups**

MURMUR：使用跨用户聊天来打破群组中的协作语言代理 cs.CR

20 pages, 7 figures

**SubmitDate**: 2025-11-21    [abs](http://arxiv.org/abs/2511.17671v1) [paper-pdf](https://arxiv.org/pdf/2511.17671v1)

**Authors**: Atharv Singh Patlan, Peiyao Sheng, S. Ashwin Hebbar, Prateek Mittal, Pramod Viswanath

**Abstract**: Language agents are rapidly expanding from single-user assistants to multi-user collaborators in shared workspaces and groups. However, today's language models lack a mechanism for isolating user interactions and concurrent tasks, creating a new attack vector inherent to this new setting: cross-user poisoning (CUP). In a CUP attack, an adversary injects ordinary-looking messages that poison the persistent, shared state, which later triggers the agent to execute unintended, attacker-specified actions on behalf of benign users. We validate CUP on real systems, successfully attacking popular multi-user agents. To study the phenomenon systematically, we present MURMUR, a framework that composes single-user tasks into concurrent, group-based scenarios using an LLM to generate realistic, history-aware user interactions. We observe that CUP attacks succeed at high rates and their effects persist across multiple tasks, thus posing fundamental risks to multi-user LLM deployments. Finally, we introduce a first-step defense with task-based clustering to mitigate this new class of vulnerability

摘要: 语言代理正在迅速从单用户助理扩展到共享收件箱和群组中的多用户协作者。然而，当今的语言模型缺乏隔离用户交互和并发任务的机制，从而创建了这种新设置固有的新攻击载体：交叉用户中毒（CUP）。在CUP攻击中，对手注入外观普通的消息，这些消息会毒害持久的共享状态，随后触发代理代表良性用户执行无意的、攻击者指定的操作。我们在真实系统上验证了CUP，成功攻击了流行的多用户代理。为了系统地研究这种现象，我们提出了MURMUR，这是一个框架，它使用LLM将单用户任务组合成并发的、基于组的场景，以生成现实的、具有历史意识的用户交互。我们观察到CUP攻击的成功率很高，并且其影响在多个任务中持续存在，从而对多用户LLM部署构成根本风险。最后，我们引入基于任务的集群的第一步防御，以缓解这类新的漏洞



## **22. Evaluating Adversarial Vulnerabilities in Modern Large Language Models**

评估现代大型语言模型中的对抗脆弱性 cs.CR

**SubmitDate**: 2025-11-21    [abs](http://arxiv.org/abs/2511.17666v1) [paper-pdf](https://arxiv.org/pdf/2511.17666v1)

**Authors**: Tom Perel

**Abstract**: The recent boom and rapid integration of Large Language Models (LLMs) into a wide range of applications warrants a deeper understanding of their security and safety vulnerabilities. This paper presents a comparative analysis of the susceptibility to jailbreak attacks for two leading publicly available LLMs, Google's Gemini 2.5 Flash and OpenAI's GPT-4 (specifically the GPT-4o mini model accessible in the free tier). The research utilized two main bypass strategies: 'self-bypass', where models were prompted to circumvent their own safety protocols, and 'cross-bypass', where one model generated adversarial prompts to exploit vulnerabilities in the other. Four attack methods were employed - direct injection, role-playing, context manipulation, and obfuscation - to generate five distinct categories of unsafe content: hate speech, illegal activities, malicious code, dangerous content, and misinformation. The success of the attack was determined by the generation of disallowed content, with successful jailbreaks assigned a severity score. The findings indicate a disparity in jailbreak susceptibility between 2.5 Flash and GPT-4, suggesting variations in their safety implementations or architectural design. Cross-bypass attacks were particularly effective, indicating that an ample amount of vulnerabilities exist in the underlying transformer architecture. This research contributes a scalable framework for automated AI red-teaming and provides data-driven insights into the current state of LLM safety, underscoring the complex challenge of balancing model capabilities with robust safety mechanisms.

摘要: 最近大型语言模型（LLM）的蓬勃发展和快速集成到广泛的应用程序中，这使得人们需要更深入地了解其安全性和安全漏洞。本文对两种领先的公开LLM（Google的Gemini 2.5 Flash和OpenAI的GPT-4（特别是免费层中可访问的GPT-4 o mini型号）进行了越狱攻击的易感性进行了比较分析。该研究利用了两种主要的绕过策略：“自我绕过”（提示模型绕过自己的安全协议）和“交叉绕过”（其中一个模型生成对抗提示以利用另一个模型的漏洞）。使用了四种攻击方法--直接注入、角色扮演、上下文操纵和混淆--来生成五种不同类别的不安全内容：仇恨言论、非法活动、恶意代码、危险内容和错误信息。攻击的成功取决于不允许内容的生成，成功的越狱会被赋予严重性分数。研究结果表明，2.5 Flash和GPT-4之间的越狱敏感性存在差异，这表明它们的安全实现或架构设计存在差异。交叉旁路攻击特别有效，这表明底层Transformer架构中存在大量漏洞。这项研究为自动化人工智能红色团队提供了一个可扩展的框架，并提供了对LLM安全当前状态的数据驱动见解，强调了平衡模型能力与强大安全机制的复杂挑战。



## **23. Large Language Model-Based Reward Design for Deep Reinforcement Learning-Driven Autonomous Cyber Defense**

基于大语言模型的深度强化学习驱动的自主网络防御奖励设计 cs.LG

Accepted in the AAAI-26 Workshop on Artificial Intelligence for Cyber Security (AICS)

**SubmitDate**: 2025-11-20    [abs](http://arxiv.org/abs/2511.16483v1) [paper-pdf](https://arxiv.org/pdf/2511.16483v1)

**Authors**: Sayak Mukherjee, Samrat Chatterjee, Emilie Purvine, Ted Fujimoto, Tegan Emerson

**Abstract**: Designing rewards for autonomous cyber attack and defense learning agents in a complex, dynamic environment is a challenging task for subject matter experts. We propose a large language model (LLM)-based reward design approach to generate autonomous cyber defense policies in a deep reinforcement learning (DRL)-driven experimental simulation environment. Multiple attack and defense agent personas were crafted, reflecting heterogeneity in agent actions, to generate LLM-guided reward designs where the LLM was first provided with contextual cyber simulation environment information. These reward structures were then utilized within a DRL-driven attack-defense simulation environment to learn an ensemble of cyber defense policies. Our results suggest that LLM-guided reward designs can lead to effective defense strategies against diverse adversarial behaviors.

摘要: 对于主题专家来说，在复杂、动态的环境中为自主网络攻击和防御学习代理设计奖励是一项具有挑战性的任务。我们提出了一种基于大语言模型（LLM）的奖励设计方法，以在深度强化学习（DRL）驱动的实验模拟环境中生成自主网络防御策略。精心设计了多个攻击和防御代理角色，反映了代理动作的多样性，以生成LLM引导的奖励设计，其中LLM首先被提供上下文网络模拟环境信息。然后在DRL驱动的攻击防御模拟环境中使用这些奖励结构来学习一整套网络防御策略。我们的结果表明，LLM指导的奖励设计可以制定针对不同对抗行为的有效防御策略。



## **24. Q-MLLM: Vector Quantization for Robust Multimodal Large Language Model Security**

Q-MLLM：鲁棒多模式大型语言模型安全性的载体量化 cs.CR

Accepted by NDSS 2026

**SubmitDate**: 2025-11-20    [abs](http://arxiv.org/abs/2511.16229v1) [paper-pdf](https://arxiv.org/pdf/2511.16229v1)

**Authors**: Wei Zhao, Zhe Li, Yige Li, Jun Sun

**Abstract**: Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities in cross-modal understanding, but remain vulnerable to adversarial attacks through visual inputs despite robust textual safety mechanisms. These vulnerabilities arise from two core weaknesses: the continuous nature of visual representations, which allows for gradient-based attacks, and the inadequate transfer of text-based safety mechanisms to visual content. We introduce Q-MLLM, a novel architecture that integrates two-level vector quantization to create a discrete bottleneck against adversarial attacks while preserving multimodal reasoning capabilities. By discretizing visual representations at both pixel-patch and semantic levels, Q-MLLM blocks attack pathways and bridges the cross-modal safety alignment gap. Our two-stage training methodology ensures robust learning while maintaining model utility. Experiments demonstrate that Q-MLLM achieves significantly better defense success rate against both jailbreak attacks and toxic image attacks than existing approaches. Notably, Q-MLLM achieves perfect defense success rate (100\%) against jailbreak attacks except in one arguable case, while maintaining competitive performance on multiple utility benchmarks with minimal inference overhead. This work establishes vector quantization as an effective defense mechanism for secure multimodal AI systems without requiring expensive safety-specific fine-tuning or detection overhead. Code is available at https://github.com/Amadeuszhao/QMLLM.

摘要: 多模式大型语言模型（MLLM）在跨模式理解方面表现出了令人印象深刻的能力，但尽管具有强大的文本安全机制，但仍然容易受到视觉输入的对抗攻击。这些漏洞源于两个核心弱点：视觉表示的连续性（允许基于梯度的攻击）以及基于文本的安全机制向视觉内容的不充分转移。我们引入了Q-MLLM，这是一种新颖的架构，它集成了两级量化，以创建针对对抗性攻击的离散瓶颈，同时保留多模式推理能力。通过在像素补丁和语义层面离散化视觉表示，Q-MLLM阻止攻击途径并弥合跨模式安全对齐差距。我们的两阶段训练方法确保稳健的学习，同时保持模型效用。实验表明，与现有方法相比，Q-MLLM在针对越狱攻击和有毒图像攻击的防御成功率明显更高。值得注意的是，Q-MLLM在针对越狱攻击时实现了完美的防御成功率（100%），但在一种可发现的情况下，同时以最小的推理费用在多个实用工具基准上保持竞争性能。这项工作将载体量化建立为安全多模式人工智能系统的有效防御机制，而不需要昂贵的安全特定微调或检测费用。代码可在https://github.com/Amadeuszhao/QMLLM上获取。



## **25. When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models**

当对齐失败时：对视觉-语言-动作模型的多模式对抗攻击 cs.CV

**SubmitDate**: 2025-11-23    [abs](http://arxiv.org/abs/2511.16203v2) [paper-pdf](https://arxiv.org/pdf/2511.16203v2)

**Authors**: Yuping Yan, Yuhan Xie, Yixin Zhang, Lingjuan Lyu, Handing Wang, Yaochu Jin

**Abstract**: Vision-Language-Action models (VLAs) have recently demonstrated remarkable progress in embodied environments, enabling robots to perceive, reason, and act through unified multimodal understanding. Despite their impressive capabilities, the adversarial robustness of these systems remains largely unexplored, especially under realistic multimodal and black-box conditions. Existing studies mainly focus on single-modality perturbations and overlook the cross-modal misalignment that fundamentally affects embodied reasoning and decision-making. In this paper, we introduce VLA-Fool, a comprehensive study of multimodal adversarial robustness in embodied VLA models under both white-box and black-box settings. VLA-Fool unifies three levels of multimodal adversarial attacks: (1) textual perturbations through gradient-based and prompt-based manipulations, (2) visual perturbations via patch and noise distortions, and (3) cross-modal misalignment attacks that intentionally disrupt the semantic correspondence between perception and instruction. We further incorporate a VLA-aware semantic space into linguistic prompts, developing the first automatically crafted and semantically guided prompting framework. Experiments on the LIBERO benchmark using a fine-tuned OpenVLA model reveal that even minor multimodal perturbations can cause significant behavioral deviations, demonstrating the fragility of embodied multimodal alignment.

摘要: 视觉-语言-动作模型（VLA）最近在具体环境中取得了显着进展，使机器人能够通过统一的多模式理解来感知、推理和行动。尽管它们的能力令人印象深刻，但这些系统的对抗鲁棒性在很大程度上仍未得到探索，尤其是在现实的多模式和黑匣子条件下。现有的研究主要关注单模式扰动，而忽视了从根本上影响体现推理和决策的跨模式失调。本文介绍了VLA-Fool，这是对白盒和黑盒设置下具体VLA模型中多模式对抗鲁棒性的全面研究。VLA-Fool统一了三个级别的多模式对抗攻击：（1）通过基于梯度和基于预算的操纵进行文本扰动，（2）通过补丁和噪音失真进行视觉扰动，以及（3）故意破坏感知和指令之间的语义对应性的跨模式失准攻击。我们进一步将VLA感知的语义空间融入到语言提示中，开发了第一个自动制作和语义引导的提示框架。使用微调的OpenVLA模型对LIBERO基准进行的实验表明，即使是微小的多峰扰动也会导致显着的行为偏差，这表明了体现多峰对齐的脆弱性。



## **26. Future-Back Threat Modeling: A Foresight-Driven Security Framework**

未来威胁建模：前瞻性驱动的安全框架 cs.CR

**SubmitDate**: 2025-11-24    [abs](http://arxiv.org/abs/2511.16088v2) [paper-pdf](https://arxiv.org/pdf/2511.16088v2)

**Authors**: Vu Van Than

**Abstract**: Traditional threat modeling remains reactive-focused on known TTPs and past incident data, while threat prediction and forecasting frameworks are often disconnected from operational or architectural artifacts. This creates a fundamental weakness: the most serious cyber threats often do not arise from what is known, but from what is assumed, overlooked, or not yet conceived, and frequently originate from the future, such as artificial intelligence, information warfare, and supply chain attacks, where adversaries continuously develop new exploits that can bypass defenses built on current knowledge. To address this mental gap, this paper introduces the theory and methodology of Future-Back Threat Modeling (FBTM). This predictive approach begins with envisioned future threat states and works backward to identify assumptions, gaps, blind spots, and vulnerabilities in the current defense architecture, providing a clearer and more accurate view of impending threats so that we can anticipate their emergence and shape the future we want through actions taken now. The proposed methodology further aims to reveal known unknowns and unknown unknowns, including tactics, techniques, and procedures that are emerging, anticipated, and plausible. This enhances the predictability of adversary behavior, particularly under future uncertainty, helping security leaders make informed decisions today that shape more resilient security postures for the future.

摘要: 传统的威胁建模仍然以已知的TTP和过去的事件数据为中心，而威胁预测和预测框架通常与操作或架构工件脱节。这造成了一个根本性的弱点：最严重的网络威胁往往不是来自已知的，而是来自假设、忽视或尚未构想的，并且通常起源于未来，例如人工智能、信息战和供应链攻击，对手不断开发新的漏洞，可以绕过基于当前知识的防御。为了解决这一心理差距，本文介绍了未来反向威胁建模（FBTM）的理论和方法论。这种预测方法从设想的未来威胁状态开始，并向后工作以识别当前防御架构中的假设、差距、盲点和漏洞，为即将发生的威胁提供更清晰、更准确的视图，以便我们能够预测它们的出现并塑造我们想要的未来通过现在采取的行动。拟议的方法论进一步旨在揭示已知的未知数和未知的未知数，包括正在出现的、预期的和合理的策略、技术和程序。这增强了对手行为的可预测性，特别是在未来不确定性的情况下，帮助安全领导者今天做出明智的决定，为未来塑造更具弹性的安全姿态。



## **27. SoK: Security Evaluation of Wi-Fi CSI Biometrics: Attacks, Metrics, and Open Challenges**

SoK：Wi-Fi SI生物识别技术的安全评估：攻击、入侵和开放挑战 cs.CR

This work was submitted to the 11th IEEE European Symposium on Security and Privacy (IEEE S&P 2026)

**SubmitDate**: 2025-11-21    [abs](http://arxiv.org/abs/2511.11381v2) [paper-pdf](https://arxiv.org/pdf/2511.11381v2)

**Authors**: Gioliano de Oliveira Braga, Pedro Henrique dos Santos Rocha, Rafael Pimenta de Mattos Paixão, Giovani Hoff da Costa, Gustavo Cavalcanti Morais, Lourenço Alves Pereira Júnior

**Abstract**: Wi-Fi Channel State Information (CSI) has been repeatedly proposed as a biometric modality, often with reports of high accuracy and operational feasibility. However, the field lacks a consolidated understanding of its security properties, adversarial resilience, and methodological consistency. This Systematization of Knowledge (SoK) examines CSI-based biometric authentication through a security lens, analyzing how existing works diverge in sensing infrastructure, signal representations, feature pipelines, learning models, and evaluation methodologies. Our synthesis reveals systemic inconsistencies: reliance on aggregate accuracy metrics, limited reporting of FAR/FRR/EER, absence of per-user risk analysis, and scarce consideration of threat models or adversarial feasibility.   To this end, we construct a unified evaluation framework to expose these issues empirically and demonstrate how security-relevant metrics such as per-class EER, Frequency Count of Scores (FCS), and the Gini Coefficient uncover risk concentration that remains hidden under traditional reporting practices. The resulting analysis highlights concrete attack surfaces--including replay, geometric mimicry, and environmental perturbation--and shows how methodological choices materially influence vulnerability profiles. Based on these findings, we articulate the security boundaries of current CSI biometrics and provide guidelines for rigorous evaluation, reproducible experimentation, and future research directions. This SoK offers the security community a structured, evidence-driven reassessment of Wi-Fi CSI biometrics and their suitability as an authentication primitive.

摘要: Wi-Fi通道状态信息（SI）已被多次提出作为一种生物识别模式，通常有高准确性和操作可行性的报告。然而，该领域缺乏对其安全属性、对抗弹性和方法一致性的统一理解。该知识系统化（SoK）通过安全视角审视基于CSC的生物识别认证，分析现有作品在传感基础设施、信号表示、特征管道、学习模型和评估方法方面如何分歧。我们的综合揭示了系统性的不一致性：依赖总准确性指标、FAR/FRR/EER的报告有限、缺乏按用户的风险分析以及很少考虑威胁模型或对抗可行性。   为此，我们构建了一个统一的评估框架，以实证方式揭示这些问题，并演示安全相关指标（例如每类EER、得分频率计数（FCS）和基尼系数）如何揭示传统报告实践下仍然隐藏的风险集中度。由此产生的分析强调了具体的攻击表面--包括重播、几何模仿和环境扰动--并展示了方法选择如何对脆弱性概况产生重大影响。基于这些发现，我们阐明了当前SI生物识别技术的安全边界，并为严格评估、可重复实验和未来研究方向提供指南。该SoK为安全界提供了对Wi-Fi SI生物识别技术及其作为身份验证基元的适合性的结构化、证据驱动的重新评估。



## **28. Privacy on the Fly: A Predictive Adversarial Transformation Network for Mobile Sensor Data**

动态隐私：移动传感器数据的预测性对抗转换网络 cs.CR

accepted by AAAI 2026 (oral)

**SubmitDate**: 2025-11-24    [abs](http://arxiv.org/abs/2511.07242v4) [paper-pdf](https://arxiv.org/pdf/2511.07242v4)

**Authors**: Tianle Song, Chenhao Lin, Yang Cao, Zhengyu Zhao, Jiahao Sun, Chong Zhang, Le Yang, Chao Shen

**Abstract**: Mobile motion sensors such as accelerometers and gyroscopes are now ubiquitously accessible by third-party apps via standard APIs. While enabling rich functionalities like activity recognition and step counting, this openness has also enabled unregulated inference of sensitive user traits, such as gender, age, and even identity, without user consent. Existing privacy-preserving techniques, such as GAN-based obfuscation or differential privacy, typically require access to the full input sequence, introducing latency that is incompatible with real-time scenarios. Worse, they tend to distort temporal and semantic patterns, degrading the utility of the data for benign tasks like activity recognition. To address these limitations, we propose the Predictive Adversarial Transformation Network (PATN), a real-time privacy-preserving framework that leverages historical signals to generate adversarial perturbations proactively. The perturbations are applied immediately upon data acquisition, enabling continuous protection without disrupting application functionality. Experiments on two datasets demonstrate that PATN substantially degrades the performance of privacy inference models, achieving Attack Success Rate (ASR) of 40.11% and 44.65% (reducing inference accuracy to near-random) and increasing the Equal Error Rate (EER) from 8.30% and 7.56% to 41.65% and 46.22%. On ASR, PATN outperforms baseline methods by 16.16% and 31.96%, respectively.

摘要: 现在，第三方应用程序通过标准API无处不在地访问加速度计和陀螺仪等移动运动传感器。这种开放性在实现活动识别和步数等丰富功能的同时，还可以在未经用户同意的情况下对敏感的用户特征（例如性别、年龄甚至身份）进行不受监管的推断。现有的隐私保护技术，例如基于GAN的模糊或差异隐私，通常需要访问完整的输入序列，从而引入与实时场景不兼容的延迟。更糟糕的是，它们往往会扭曲时间和语义模式，降低数据对活动识别等良性任务的实用性。为了解决这些限制，我们提出了预测对抗转换网络（PATN），这是一个实时隐私保护框架，可以利用历史信号主动生成对抗性扰动。数据采集后立即应用扰动，从而实现持续保护，而不会中断应用程序功能。对两个数据集的实验表明，PATN大幅降低了隐私推理模型的性能，实现了40.11%和44.65%的攻击成功率（ASB）（将推理准确率降低到接近随机），并将等错误率（EER）从8.30%和7.56%提高到41.65%和46.22%。在ASB方面，PATN分别比基线方法高出16.16%和31.96%。



## **29. Robust Graph Condensation via Classification Complexity Mitigation**

通过缓解分类复杂性实现稳健的图压缩 cs.LG

Accepted by Neurips 2025 (Spotlight)

**SubmitDate**: 2025-11-22    [abs](http://arxiv.org/abs/2510.26451v2) [paper-pdf](https://arxiv.org/pdf/2510.26451v2)

**Authors**: Jiayi Luo, Qingyun Sun, Beining Yang, Haonan Yuan, Xingcheng Fu, Yanbiao Ma, Jianxin Li, Philip S. Yu

**Abstract**: Graph condensation (GC) has gained significant attention for its ability to synthesize smaller yet informative graphs. However, existing studies often overlook the robustness of GC in scenarios where the original graph is corrupted. In such cases, we observe that the performance of GC deteriorates significantly, while existing robust graph learning technologies offer only limited effectiveness. Through both empirical investigation and theoretical analysis, we reveal that GC is inherently an intrinsic-dimension-reducing process, synthesizing a condensed graph with lower classification complexity. Although this property is critical for effective GC performance, it remains highly vulnerable to adversarial perturbations. To tackle this vulnerability and improve GC robustness, we adopt the geometry perspective of graph data manifold and propose a novel Manifold-constrained Robust Graph Condensation framework named MRGC. Specifically, we introduce three graph data manifold learning modules that guide the condensed graph to lie within a smooth, low-dimensional manifold with minimal class ambiguity, thereby preserving the classification complexity reduction capability of GC and ensuring robust performance under universal adversarial attacks. Extensive experiments demonstrate the robustness of \ModelName\ across diverse attack scenarios.

摘要: 图形凝聚（GC）因其合成较小但信息丰富的图形的能力而受到广泛关注。然而，现有的研究经常忽视GC在原始图被损坏的情况下的稳健性。在这种情况下，我们观察到GC的性能显着恶化，而现有的稳健图学习技术只能提供有限的有效性。通过实证研究和理论分析，我们揭示了GC本质上是一个本质降维过程，合成了一个具有较低分类复杂性的精简图。尽管该属性对于有效的GC性能至关重要，但它仍然极易受到对抗性扰动的影响。为了解决这个漏洞并提高GC的鲁棒性，我们采用图数据Manifold的几何角度，提出了一种新型的Manifold约束鲁棒图凝聚框架，名为MRGC。具体来说，我们引入了三个图数据Manhattan学习模块，引导精简图位于具有最小类歧义性的光滑低维Manhattan中，从而保留GC的分类复杂性降低能力，并确保在通用对抗攻击下的稳健性能。大量实验证明了\Model Name\在各种攻击场景中的稳健性。



## **30. GhostEI-Bench: Do Mobile Agents Resilience to Environmental Injection in Dynamic On-Device Environments?**

GhostEEI-Bench：移动代理对动态设备上环境中的环境注入有弹性吗？ cs.CR

**SubmitDate**: 2025-11-21    [abs](http://arxiv.org/abs/2510.20333v2) [paper-pdf](https://arxiv.org/pdf/2510.20333v2)

**Authors**: Chiyu Chen, Xinhao Song, Yunkai Chai, Yang Yao, Haodong Zhao, Lijun Li, Jie Li, Yan Teng, Gongshen Liu, Yingchun Wang

**Abstract**: Vision-Language Models (VLMs) are increasingly deployed as autonomous agents to navigate mobile graphical user interfaces (GUIs). Operating in dynamic on-device ecosystems, which include notifications, pop-ups, and inter-app interactions, exposes them to a unique and underexplored threat vector: environmental injection. Unlike prompt-based attacks that manipulate textual instructions, environmental injection corrupts an agent's visual perception by inserting adversarial UI elements (for example, deceptive overlays or spoofed notifications) directly into the GUI. This bypasses textual safeguards and can derail execution, causing privacy leakage, financial loss, or irreversible device compromise. To systematically evaluate this threat, we introduce GhostEI-Bench, the first benchmark for assessing mobile agents under environmental injection attacks within dynamic, executable environments. Moving beyond static image-based assessments, GhostEI-Bench injects adversarial events into realistic application workflows inside fully operational Android emulators and evaluates performance across critical risk scenarios. We further propose a judge-LLM protocol that conducts fine-grained failure analysis by reviewing the agent's action trajectory alongside the corresponding screenshot sequence, pinpointing failure in perception, recognition, or reasoning. Comprehensive experiments on state-of-the-art agents reveal pronounced vulnerability to deceptive environmental cues: current models systematically fail to perceive and reason about manipulated UIs. GhostEI-Bench provides a framework for quantifying and mitigating this emerging threat, paving the way toward more robust and secure embodied agents.

摘要: 视觉语言模型（VLM）越来越多地被部署为自治代理来导航移动图形用户界面（GUIs）。在动态的设备上生态系统（包括通知、弹出窗口和应用程序间交互）中运行，使它们面临一种独特且未充分探索的威胁载体：环境注入。与操纵文本指令的基于预算的攻击不同，环境注入通过将对抗性UI元素（例如，欺骗性覆盖或欺骗通知）直接插入到图形用户界面中来破坏代理的视觉感知。这绕过了文本保护措施，并可能会导致执行脱轨，导致隐私泄露、财务损失或不可逆转的设备损害。为了系统性地评估这种威胁，我们引入了GhostEI-Bench，这是第一个用于评估动态可执行环境中环境注入攻击下的移动代理的基准。GhostEEI-Bench超越了基于静态图像的评估，将对抗事件注入到完全运行的Android模拟器内的现实应用程序工作流程中，并评估关键风险场景中的性能。我们进一步提出了一种判断LLM协议，该协议通过审查代理的动作轨迹以及相应的屏幕截图序列来进行细粒度的失败分析，从而确定感知、识别或推理中的失败。对最先进代理的全面实验揭示了对欺骗性环境线索的明显脆弱性：当前的模型系统性地无法感知和推理被操纵的UI。GhostEEI-Bench提供了一个量化和缓解这种新出现的威胁的框架，为更强大、更安全的具体化代理铺平了道路。



## **31. A Comprehensive Survey of Website Fingerprinting Attacks and Defenses in Tor: Advances and Open Challenges**

Tor网站指纹攻击和防御的全面调查：进展和开放挑战 cs.CR

43 pages

**SubmitDate**: 2025-11-22    [abs](http://arxiv.org/abs/2510.11804v3) [paper-pdf](https://arxiv.org/pdf/2510.11804v3)

**Authors**: Yuwen Cui, Guangjing Wang, Khanh Vu, Kai Wei, Kehan Shen, Zhengyuan Jiang, Xiao Han, Ning Wang, Zhuo Lu, Yao Liu

**Abstract**: The Tor network provides users with strong anonymity by routing their internet traffic through multiple relays. While Tor encrypts traffic and hides IP addresses, it remains vulnerable to traffic analysis attacks such as the website fingerprinting (WF) attack, achieving increasingly high fingerprinting accuracy even under open-world conditions. In response, researchers have proposed a variety of defenses, ranging from adaptive padding, traffic regularization, and traffic morphing to adversarial perturbation, that seek to obfuscate or reshape traffic traces. However, these defenses often entail trade-offs between privacy, usability, and system performance. Despite extensive research, a comprehensive survey unifying WF datasets, attack methodologies, and defense strategies remains absent. This paper fills that gap by systematically categorizing existing WF research into three key domains: datasets, attack models, and defense mechanisms. We provide an in-depth comparative analysis of techniques, highlight their strengths and limitations under diverse threat models, and discuss emerging challenges such as multi-tab browsing and coarse-grained traffic features. By consolidating prior work and identifying open research directions, this survey serves as a foundation for advancing stronger privacy protection in Tor.

摘要: Tor网络通过多个中继路由用户的互联网流量，为用户提供了强大的匿名性。虽然Tor加密流量并隐藏IP地址，但它仍然容易受到网站指纹识别（WF）攻击等流量分析攻击，即使在开放世界条件下也能实现越来越高的指纹识别准确性。作为回应，研究人员提出了各种防御措施，从自适应填充、流量规则化、流量变形到对抗性扰动，旨在模糊或重塑流量轨迹。然而，这些防御通常需要在隐私、可用性和系统性能之间进行权衡。尽管进行了广泛的研究，但仍然缺乏统一WF数据集、攻击方法和防御策略的全面调查。本文通过将现有的WF研究系统地分类为三个关键领域：数据集、攻击模型和防御机制来填补这一空白。我们对技术进行深入的比较分析，强调它们在不同威胁模型下的优势和局限性，并讨论多选项卡浏览和粗粒度流量功能等新出现的挑战。通过整合之前的工作并确定开放的研究方向，这项调查为在Tor中推进更强有力的隐私保护奠定了基础。



## **32. Time-To-Inconsistency: A Survival Analysis of Large Language Model Robustness to Adversarial Attacks**

不一致时间：大型语言模型对对抗性攻击的鲁棒性的生存分析 cs.CL

**SubmitDate**: 2025-11-23    [abs](http://arxiv.org/abs/2510.02712v2) [paper-pdf](https://arxiv.org/pdf/2510.02712v2)

**Authors**: Yubo Li, Ramayya Krishnan, Rema Padman

**Abstract**: Large Language Models (LLMs) have revolutionized conversational AI, yet their robustness in extended multi-turn dialogues remains poorly understood. Existing evaluation frameworks focus on static benchmarks and single-turn assessments, failing to capture the temporal dynamics of conversational degradation that characterize real-world interactions. In this work, we present a large-scale survival analysis of conversational robustness, modeling failure as a time-to-event process over 36,951 turns from 9 state-of-the-art LLMs on the MT-Consistency benchmark. Our framework combines Cox proportional hazards, Accelerated Failure Time (AFT), and Random Survival Forest models with simple semantic drift features. We find that abrupt prompt-to-prompt semantic drift sharply increases the hazard of inconsistency, whereas cumulative drift is counterintuitively \emph{protective}, suggesting adaptation in conversations that survive multiple shifts. AFT models with model-drift interactions achieve the best combination of discrimination and calibration, and proportional hazards checks reveal systematic violations for key drift covariates, explaining the limitations of Cox-style modeling in this setting. Finally, we show that a lightweight AFT model can be turned into a turn-level risk monitor that flags most failing conversations several turns before the first inconsistent answer while keeping false alerts modest. These results establish survival analysis as a powerful paradigm for evaluating multi-turn robustness and for designing practical safeguards for conversational AI systems.

摘要: 大型语言模型（LLM）彻底改变了对话人工智能，但对其在扩展多轮对话中的稳健性仍然知之甚少。现有的评估框架专注于静态基准和单轮评估，未能捕捉反映现实世界互动特征的对话退化的时间动态。在这项工作中，我们提出了对话稳健性的大规模生存分析，将故障建模为MT-Consistency基准上的9个最先进的LLM在36，951个回合内的事件时间过程。我们的框架将Cox比例风险、加速故障时间（AFT）和随机生存森林模型与简单的语义漂移特征相结合。我们发现，突然的从预定到提示的语义漂移会急剧增加不一致的风险，而累积漂移是反直觉的\{保护性}，这表明在经历多次转变的对话中进行适应。具有模型-漂移相互作用的AFT模型实现了区分和校准的最佳组合，比例风险检查揭示了关键漂移协变量的系统性违规，解释了这种环境下Cox式建模的局限性。最后，我们表明，轻量级的AFT模型可以转变为回合级风险监控器，它在第一个不一致的答案之前的几个回合标记大多数失败的对话，同时保持虚假警报适度。这些结果将生存分析确立为评估多轮稳健性和为对话式人工智能系统设计实用保障措施的强大范式。



## **33. Boundary on the Table: Efficient Black-Box Decision-Based Attacks for Structured Data**

桌面上的边界：针对结构化数据的高效基于决策的黑匣子攻击 cs.LG

Paper revision

**SubmitDate**: 2025-11-23    [abs](http://arxiv.org/abs/2509.22850v3) [paper-pdf](https://arxiv.org/pdf/2509.22850v3)

**Authors**: Roie Kazoom, Yuval Ratzabi, Etamar Rothstein, Ofer Hadar

**Abstract**: Adversarial robustness in structured data remains an underexplored frontier compared to vision and language domains. In this work, we introduce a novel black-box, decision-based adversarial attack tailored for tabular data. Our approach combines gradient-free direction estimation with an iterative boundary search, enabling efficient navigation of discrete and continuous feature spaces under minimal oracle access. Extensive experiments demonstrate that our method successfully compromises nearly the entire test set across diverse models, ranging from classical machine learning classifiers to large language model (LLM)-based pipelines. Remarkably, the attack achieves success rates consistently above 90%, while requiring only a small number of queries per instance. These results highlight the critical vulnerability of tabular models to adversarial perturbations, underscoring the urgent need for stronger defenses in real-world decision-making systems.

摘要: 与视觉和语言领域相比，结构化数据中的对抗稳健性仍然是一个未充分探索的前沿。在这项工作中，我们引入了一种针对表格数据量身定制的新型黑匣子、基于决策的对抗攻击。我们的方法将无梯度方向估计与迭代边界搜索相结合，能够在最小的Oracle访问下高效导航离散和连续特征空间。大量实验表明，我们的方法成功地妥协了不同模型的几乎整个测试集，范围包括经典机器学习分类器和基于大型语言模型（LLM）的管道。值得注意的是，该攻击的成功率始终高于90%，而每个实例只需要少量查询。这些结果凸显了表格模型对对抗性扰动的严重脆弱性，凸显了现实世界决策系统中迫切需要更强的防御。



## **34. Decoding Deception: Understanding Automatic Speech Recognition Vulnerabilities in Evasion and Poisoning Attacks**

解码欺骗：了解逃避和中毒攻击中的自动语音识别漏洞 cs.SD

Remove due to conflict in authors

**SubmitDate**: 2025-11-20    [abs](http://arxiv.org/abs/2509.22060v2) [paper-pdf](https://arxiv.org/pdf/2509.22060v2)

**Authors**: Aravindhan G, Yuvaraj Govindarajulu, Parin Shah

**Abstract**: Recent studies have demonstrated the vulnerability of Automatic Speech Recognition systems to adversarial examples, which can deceive these systems into misinterpreting input speech commands. While previous research has primarily focused on white-box attacks with constrained optimizations, and transferability based black-box attacks against commercial Automatic Speech Recognition devices, this paper explores cost efficient white-box attack and non transferability black-box adversarial attacks on Automatic Speech Recognition systems, drawing insights from approaches such as Fast Gradient Sign Method and Zeroth-Order Optimization. Further, the novelty of the paper includes how poisoning attack can degrade the performances of state-of-the-art models leading to misinterpretation of audio signals. Through experimentation and analysis, we illustrate how hybrid models can generate subtle yet impactful adversarial examples with very little perturbation having Signal Noise Ratio of 35dB that can be generated within a minute. These vulnerabilities of state-of-the-art open source model have practical security implications, and emphasize the need for adversarial security.

摘要: 最近的研究表明，自动语音识别系统对对抗示例的脆弱性，这些示例可能会欺骗这些系统误解输入语音命令。虽然之前的研究主要集中在具有约束优化的白盒攻击以及针对商用自动语音识别设备的基于可转移性的黑匣子攻击，但本文探索了对自动语音识别系统的成本高效白盒攻击和不可转移性的黑匣子对抗攻击，从快速梯度符号法和零阶优化等方法中汲取见解。此外，该论文的新颖性包括中毒攻击如何降低最先进模型的性能，从而导致音频信号的误解。通过实验和分析，我们说明了混合模型如何在很小的干扰下生成微妙但有影响力的对抗示例，可以在一分钟内生成具有35分贝的信噪比。最先进的开源模型的这些漏洞具有实际的安全影响，并强调了对抗性安全的需要。



## **35. From Noise to Narrative: Tracing the Origins of Hallucinations in Transformers**

从噪音到叙事：追踪《变形金刚》中幻觉的起源 cs.LG

**SubmitDate**: 2025-11-20    [abs](http://arxiv.org/abs/2509.06938v2) [paper-pdf](https://arxiv.org/pdf/2509.06938v2)

**Authors**: Praneet Suresh, Jack Stanley, Sonia Joseph, Luca Scimeca, Danilo Bzdok

**Abstract**: As generative AI systems become competent and democratized in science, business, and government, deeper insight into their failure modes now poses an acute need. The occasional volatility in their behavior, such as the propensity of transformer models to hallucinate, impedes trust and adoption of emerging AI solutions in high-stakes areas. In the present work, we establish how and when hallucinations arise in pre-trained transformer models through concept representations captured by sparse autoencoders, under scenarios with experimentally controlled uncertainty in the input space. Our systematic experiments reveal that the number of semantic concepts used by the transformer model grows as the input information becomes increasingly unstructured. In the face of growing uncertainty in the input space, the transformer model becomes prone to activate coherent yet input-insensitive semantic features, leading to hallucinated output. At its extreme, for pure-noise inputs, we identify a wide variety of robustly triggered and meaningful concepts in the intermediate activations of pre-trained transformer models, whose functional integrity we confirm through targeted steering. We also show that hallucinations in the output of a transformer model can be reliably predicted from the concept patterns embedded in transformer layer activations. This collection of insights on transformer internal processing mechanics has immediate consequences for aligning AI models with human values, AI safety, opening the attack surface for potential adversarial attacks, and providing a basis for automatic quantification of a model's hallucination risk.

摘要: 随着生成式人工智能系统在科学、商业和政府中变得越来越有能力和民主化，现在迫切需要更深入地了解它们的故障模式。他们行为的偶尔波动，例如Transformer模型产生幻觉的倾向，阻碍了高风险领域对新兴人工智能解决方案的信任和采用。在本工作中，我们在输入空间中具有实验控制的不确定性的场景下，通过稀疏自动编码器捕获的概念表示，确定预训练的Transformer模型中如何以及何时出现幻觉。我们的系统实验表明，随着输入信息变得越来越非结构化，Transformer模型使用的语义概念数量也会增加。面对输入空间日益增长的不确定性，Transformer模型变得容易激活连贯但对输入不敏感的语义特征，从而导致幻觉输出。在极端情况下，对于纯噪音输入，我们在预训练的Transformer模型的中间激活中识别了各种鲁棒触发且有意义的概念，我们通过有针对性的引导来确认其功能完整性。我们还表明，可以根据嵌入在Transformer层激活中的概念模式可靠地预测Transformer模型输出中的幻觉。这组关于Transformer内部处理机制的见解对于将人工智能模型与人类价值观、人工智能安全性、为潜在的对抗性攻击打开攻击面以及为自动量化模型的幻觉风险提供基础具有直接影响。



## **36. Fine-Grained Privacy Extraction from Retrieval-Augmented Generation Systems via Knowledge Asymmetry Exploitation**

通过知识不对称利用从检索增强生成系统中进行细粒度隐私提取 cs.CR

**SubmitDate**: 2025-11-22    [abs](http://arxiv.org/abs/2507.23229v2) [paper-pdf](https://arxiv.org/pdf/2507.23229v2)

**Authors**: Yufei Chen, Yao Wang, Haibin Zhang, Tao Gu

**Abstract**: Retrieval-augmented generation (RAG) systems enhance large language models (LLMs) by integrating external knowledge bases, but this advancement introduces significant privacy risks. Existing privacy attacks on RAG systems can trigger data leakage but often fail to accurately isolate knowledge-base-derived sentences within mixed responses. They also lack robustness when applied across multiple domains. This paper addresses these challenges by presenting a novel black-box attack framework that exploits knowledge asymmetry between RAG and standard LLMs to achieve fine-grained privacy extraction across heterogeneous knowledge landscapes. We propose a chain-of-thought reasoning strategy that creates adaptive prompts to steer RAG systems away from sensitive content. Specifically, we first decompose adversarial queries to maximize information disparity and then apply a semantic relationship scoring to resolve lexical and syntactic ambiguities. We finally train a neural network on these feature scores to precisely identify sentences containing private information. Unlike prior work, our framework generalizes to unseen domains through iterative refinement without pre-defined knowledge. Experimental results show that we achieve over 91% privacy extraction rate in single-domain and 83% in multi-domain scenarios, reducing sensitive sentence exposure by over 65% in case studies. This work bridges the gap between attack and defense in RAG systems, enabling precise extraction of private information while providing a foundation for adaptive mitigation.

摘要: 检索增强生成（RAG）系统通过集成外部知识库来增强大型语言模型（LLM），但这一进步带来了巨大的隐私风险。对RAG系统的现有隐私攻击可能会引发数据泄露，但通常无法准确地隔离混合响应中的知识库派生句子。当应用于多个领域时，它们也缺乏稳健性。本文通过提出一种新型的黑匣子攻击框架来解决这些挑战，该框架利用RAG和标准LLM之间的知识不对称性来实现跨异类知识环境的细粒度隐私提取。我们提出了一种思想链推理策略，可以创建自适应提示来引导RAG系统远离敏感内容。具体来说，我们首先分解对抗性查询以最大化信息差异，然后应用语义关系评分来解决词汇和语法歧义。我们最终根据这些特征分数训练神经网络，以精确识别包含私人信息的句子。与之前的工作不同，我们的框架通过迭代细化而无需预先定义的知识，将其推广到不可见的领域。实验结果表明，我们在单域场景中实现了超过91%的隐私提取率，在多域场景中实现了83%的隐私提取率，在案例研究中将敏感句子暴露减少了超过65%。这项工作弥合了RAG系统中攻击和防御之间的差距，能够精确提取私人信息，同时为自适应缓解提供基础。



## **37. Crafting Imperceptible On-Manifold Adversarial Attacks for Tabular Data**

针对表格数据设计不可感知的Manifold对抗攻击 cs.LG

Final Version

**SubmitDate**: 2025-11-21    [abs](http://arxiv.org/abs/2507.10998v3) [paper-pdf](https://arxiv.org/pdf/2507.10998v3)

**Authors**: Zhipeng He, Alexander Stevens, Chun Ouyang, Johannes De Smedt, Alistair Barros, Catarina Moreira

**Abstract**: Adversarial attacks on tabular data present unique challenges due to the heterogeneous nature of mixed categorical and numerical features. Unlike images where pixel perturbations maintain visual similarity, tabular data lacks intuitive similarity metrics, making it difficult to define imperceptible modifications. Additionally, traditional gradient-based methods prioritise $\ell_p$-norm constraints, often producing adversarial examples that deviate from the original data distributions. To address this, we propose a latent-space perturbation framework using a mixed-input Variational Autoencoder (VAE) to generate statistically consistent adversarial examples. The proposed VAE integrates categorical embeddings and numerical features into a unified latent manifold, enabling perturbations that preserve statistical consistency. We introduce In-Distribution Success Rate (IDSR) to jointly evaluate attack effectiveness and distributional alignment. Evaluation across six publicly available datasets and three model architectures demonstrates that our method achieves substantially lower outlier rates and more consistent performance compared to traditional input-space attacks and other VAE-based methods adapted from image domain approaches, achieving substantially lower outlier rates and higher IDSR across six datasets and three model architectures. Our comprehensive analyses of hyperparameter sensitivity, sparsity control, and generative architecture demonstrate that the effectiveness of VAE-based attacks depends strongly on reconstruction quality and the availability of sufficient training data. When these conditions are met, the proposed framework achieves superior practical utility and stability compared with input-space methods. This work underscores the importance of maintaining on-manifold perturbations for generating realistic and robust adversarial examples in tabular domains.

摘要: 由于混合分类和数值特征的异质性，对表格数据的对抗性攻击提出了独特的挑战。与像素扰动保持视觉相似性的图像不同，表格数据缺乏直观的相似性指标，因此很难定义难以察觉的修改。此外，传统的基于梯度的方法优先考虑$\ell_p$-norm约束，通常会产生偏离原始数据分布的对抗性示例。为了解决这个问题，我们提出了一种潜空间扰动框架，使用混合输入变分自动编码器（VAE）来生成统计上一致的对抗示例。提出的VAE将类别嵌入和数字特征集成到统一的潜在多管齐中，从而实现保持统计一致性的扰动。我们引入分布式成功率（IDSR）来联合评估攻击有效性和分布式对齐。对六个公开可用数据集和三个模型架构的评估表明，与传统的输入空间攻击和从图像域方法改编的其他基于VAE的方法相比，我们的方法实现了显着更低的异常值率和更一致的性能，实现了显着更低的异常值率和更高的IDSR跨六个数据集和三个模型架构。我们对超参数敏感性、稀疏性控制和生成式架构的全面分析表明，基于VAE的攻击的有效性在很大程度上取决于重建质量和足够训练数据的可用性。当满足这些条件时，与输入空间方法相比，所提出的框架实现了更好的实际实用性和稳定性。这项工作强调了维持管汇上扰动对于在表格域中生成现实且稳健的对抗示例的重要性。



## **38. Meta Policy Switching for Secure UAV Deconfliction in Adversarial Airspace**

对抗空域安全无人机冲突的Meta策略切换 cs.LG

**SubmitDate**: 2025-11-23    [abs](http://arxiv.org/abs/2506.21127v2) [paper-pdf](https://arxiv.org/pdf/2506.21127v2)

**Authors**: Deepak Kumar Panda, Weisi Guo

**Abstract**: Autonomous UAV navigation using reinforcement learning (RL) is vulnerable to adversarial attacks that manipulate sensor inputs, potentially leading to unsafe behavior and mission failure. Although robust RL methods provide partial protection, they often struggle to generalize to unseen or out-of-distribution (OOD) attacks due to their reliance on fixed perturbation settings. To address this limitation, we propose a meta-policy switching framework in which a meta-level polic dynamically selects among multiple robust policies to counter unknown adversarial shifts. At the core of this framework lies a discounted Thompson sampling (DTS) mechanism that formulates policy selection as a multi-armed bandit problem, thereby minimizing value distribution shifts via self-induced adversarial observations. We first construct a diverse ensemble of action-robust policies trained under varying perturbation intensities. The DTS-based meta-policy then adaptively selects among these policies online, optimizing resilience against self-induced, piecewise-stationary attacks. Theoretical analysis shows that the DTS mechanism minimizes expected regret, ensuring adaptive robustness to OOD attacks and exhibiting emergent antifragile behavior under uncertainty. Extensive simulations in complex 3D obstacle environments under both white-box (Projected Gradient Descent) and black-box (GPS spoofing) attacks demonstrate significantly improved navigation efficiency and higher conflict free trajectory rates compared to standard robust and vanilla RL baselines, highlighting the practical security and dependability benefits of the proposed approach.

摘要: 使用强化学习（RL）的自主无人机导航很容易受到操纵传感器输入的对抗攻击，可能导致不安全行为和任务失败。尽管稳健的RL方法提供了部分保护，但由于它们依赖于固定的扰动设置，它们通常很难推广到不可见或非分布（OOD）攻击。为了解决这一限制，我们提出了一个元政策切换框架，其中元级别政策在多个稳健的政策中动态选择，以应对未知的对抗性转变。该框架的核心是折扣汤普森抽样（NPS）机制，该机制将政策选择制定为多臂强盗问题，从而通过自引发的对抗性观察最大限度地减少价值分布变化。我们首先构建了在不同扰动强度下训练的行动稳健政策的多样化集合。然后，基于DTS的元策略在线自适应地选择这些策略，优化针对自引发的、分段稳定攻击的弹性。理论分析表明，RST机制最大限度地减少了预期遗憾，确保了对OOD攻击的自适应鲁棒性，并在不确定性下表现出紧急反脆弱行为。在白盒（投影梯度下降）和黑匣子（GPS欺骗）攻击下，在复杂的3D障碍环境中进行了广泛的模拟，证明了与标准稳健和普通RL基线相比，导航效率显着提高和无冲突轨迹率更高，凸显了所提出方法的实际安全性和可靠性优势。



## **39. Large Language Model Unlearning for Source Code**

大型语言模型放弃源代码的学习 cs.SE

Accepted to AAAI'26

**SubmitDate**: 2025-11-24    [abs](http://arxiv.org/abs/2506.17125v2) [paper-pdf](https://arxiv.org/pdf/2506.17125v2)

**Authors**: Xue Jiang, Yihong Dong, Huangzhao Zhang, Tangxinyu Wang, Zheng Fang, Yingwei Ma, Rongyu Cao, Binhua Li, Zhi Jin, Wenpin Jiao, Yongbin Li, Ge Li

**Abstract**: While Large Language Models (LLMs) excel at code generation, their inherent tendency toward verbatim memorization of training data introduces critical risks like copyright infringement, insecure emission, and deprecated API utilization, etc. A straightforward yet promising defense is unlearning, ie., erasing or down-weighting the offending snippets through post-training. However, we find its application to source code often tends to spill over, damaging the basic knowledge of programming languages learned by the LLM and degrading the overall capability. To ease this challenge, we propose PROD for precise source code unlearning. PROD surgically zeroes out the prediction probability of the prohibited tokens, and renormalizes the remaining distribution so that the generated code stays correct. By excising only the targeted snippets, PROD achieves precise forgetting without much degradation of the LLM's overall capability. To facilitate in-depth evaluation against PROD, we establish an unlearning benchmark consisting of three downstream tasks (ie., unlearning of copyrighted code, insecure code, and deprecated APIs), and introduce Pareto Dominance Ratio (PDR) metric, which indicates both the forget quality and the LLM utility. Our comprehensive evaluation demonstrates that PROD achieves superior overall performance between forget quality and model utility compared to existing unlearning approaches across three downstream tasks, while consistently exhibiting improvements when applied to LLMs of varying series. PROD also exhibits superior robustness against adversarial attacks without generating or exposing the data to be forgotten. These results underscore that our approach not only successfully extends the application boundary of unlearning techniques to source code, but also holds significant implications for advancing reliable code generation.

摘要: 虽然大型语言模型（LLM）擅长代码生成，但其固有的逐字记忆训练数据的倾向会引入版权侵权、不安全的发射和过时的API利用等关键风险。一个简单但有希望的防御是取消学习，即，通过训练后删除或降低违规片段的权重。然而，我们发现它对源代码的应用往往会溢出，损害LLM学到的编程语言的基本知识，并降低整体能力。为了缓解这一挑战，我们提出了PROD来精确的源代码反学习。PROD通过外科手术将被禁止的令牌的预测概率归零，并重新规范剩余的分布，以便生成的代码保持正确。通过仅删除目标片段，PROD实现了精确遗忘，而不会大幅降低LLM的整体能力。为了促进针对PROD的深入评估，我们建立了一个由三个下游任务（即，放弃受版权保护的代码、不安全的代码和废弃的API），并引入帕累托主导比（PDR）指标，该指标既指示忘记质量又指示LLM实用性。我们的全面评估表明，与三个下游任务中的现有取消学习方法相比，PROD在忘记质量和模型效用之间实现了更好的整体性能，同时在应用于不同系列的LLM时一致表现出改进。PROD还表现出针对对抗攻击的卓越鲁棒性，而不会生成或暴露被遗忘的数据。这些结果强调，我们的方法不仅成功地将放弃学习技术的应用边界扩展到源代码，而且对推进可靠的代码生成具有重要影响。



## **40. SHIELD: Secure Hypernetworks for Incremental Expansion Learning Defense**

SHIELD：用于增量扩展学习防御的安全超网络 cs.LG

**SubmitDate**: 2025-11-21    [abs](http://arxiv.org/abs/2506.08255v3) [paper-pdf](https://arxiv.org/pdf/2506.08255v3)

**Authors**: Patryk Krukowski, Łukasz Gorczyca, Piotr Helm, Kamil Książek, Przemysław Spurek

**Abstract**: Continual learning under adversarial conditions remains an open problem, as existing methods often compromise either robustness, scalability, or both. We propose a novel framework that integrates Interval Bound Propagation (IBP) with a hypernetwork-based architecture to enable certifiably robust continual learning across sequential tasks. Our method, SHIELD, generates task-specific model parameters via a shared hypernetwork conditioned solely on compact task embeddings, eliminating the need for replay buffers or full model copies and enabling efficient over time. To further enhance robustness, we introduce Interval MixUp, a novel training strategy that blends virtual examples represented as $\ell_{\infty}$ balls centered around MixUp points. Leveraging interval arithmetic, this technique guarantees certified robustness while mitigating the wrapping effect, resulting in smoother decision boundaries. We evaluate SHIELD under strong white-box adversarial attacks, including PGD and AutoAttack, across multiple benchmarks. It consistently outperforms existing robust continual learning methods, achieving state-of-the-art average accuracy while maintaining both scalability and certification. These results represent a significant step toward practical and theoretically grounded continual learning in adversarial settings.

摘要: 对抗条件下的持续学习仍然是一个悬而未决的问题，因为现有的方法经常损害稳健性、可扩展性或两者兼而有之。我们提出了一种新颖的框架，将区间束缚传播（IPP）与基于超网络的架构集成，以实现跨顺序任务的可认证稳健的持续学习。我们的方法SHIELD通过仅以紧凑任务嵌入为条件的共享超网络生成特定于任务的模型参数，消除了对重播缓冲区或完整模型副本的需要，并随着时间的推移实现高效。为了进一步增强稳健性，我们引入了Interval MixUp，这是一种新颖的训练策略，它混合以MixUp点为中心的$\ell_{\infty}$球表示的虚拟示例。利用区间算术，该技术保证了经过认证的鲁棒性，同时减轻了包裹效应，从而获得更平滑的决策边界。我们在多个基准测试中评估SHIELD在强白盒对抗攻击（包括PVD和AutoAttack）下。它始终优于现有的强大持续学习方法，实现最先进的平均准确性，同时保持可扩展性和认证。这些结果代表着在对抗环境中朝着实践和理论基础的持续学习迈出了重要一步。



## **41. TRAP: Targeted Redirecting of Agentic Preferences**

TRAP：有针对性地重新定向统计偏好 cs.AI

Accepted to NeurIPS 2025

**SubmitDate**: 2025-11-24    [abs](http://arxiv.org/abs/2505.23518v2) [paper-pdf](https://arxiv.org/pdf/2505.23518v2)

**Authors**: Hangoo Kang, Jehyeok Yeon, Gagandeep Singh

**Abstract**: Autonomous agentic AI systems powered by vision-language models (VLMs) are rapidly advancing toward real-world deployment, yet their cross-modal reasoning capabilities introduce new attack surfaces for adversarial manipulation that exploit semantic reasoning across modalities. Existing adversarial attacks typically rely on visible pixel perturbations or require privileged model or environment access, making them impractical for stealthy, real-world exploitation. We introduce TRAP, a novel generative adversarial framework that manipulates the agent's decision-making using diffusion-based semantic injections into the vision-language embedding space. Our method combines negative prompt-based degradation with positive semantic optimization, guided by a Siamese semantic network and layout-aware spatial masking. Without requiring access to model internals, TRAP produces visually natural images yet induces consistent selection biases in agentic AI systems. We evaluate TRAP on the Microsoft Common Objects in Context (COCO) dataset, building multi-candidate decision scenarios. Across these scenarios, TRAP consistently induces decision-level preference redirection on leading models, including LLaVA-34B, Gemma3, GPT-4o, and Mistral-3.2, significantly outperforming existing baselines such as SPSA, Bandit, and standard diffusion approaches. These findings expose a critical, generalized vulnerability: autonomous agents can be consistently misled through visually subtle, semantically-guided cross-modal manipulations. Overall, our results show the need for defense strategies beyond pixel-level robustness to address semantic vulnerabilities in cross-modal decision-making. The code for TRAP is accessible on GitHub at https://github.com/uiuc-focal-lab/TRAP.

摘要: 由视觉语言模型（VLM）提供支持的自主代理人工智能系统正在迅速向现实世界的部署迈进，但它们的跨模式推理能力为对抗性操纵引入了新的攻击表面，利用跨模式的语义推理。现有的对抗性攻击通常依赖于可见像素扰动或需要特权模型或环境访问，这使得它们对于隐形的、现实世界的利用来说不切实际。我们引入TRAP，这是一种新型的生成对抗框架，它使用基于扩散的语义注入到视觉语言嵌入空间来操纵代理的决策。我们的方法在连体语义网络和布局感知空间掩蔽的指导下，将基于预算的负降级与正语义优化相结合。在不需要访问模型内部的情况下，TRAP会产生视觉上自然的图像，但在代理人工智能系统中会引起一致的选择偏差。我们在Microsoft上下文中的公共对象（COCO）数据集中评估TRAP，构建多候选决策场景。在这些场景中，TRAP一致地在LLaVA-34 B、Gemma 3、GPT-4 o和Mistral-3.2等领先模型上引发决策级偏好重定向，显着优于SPSA、Bandit和标准扩散方法等现有基线。这些发现暴露了一个关键的、普遍的弱点：自主代理可能会通过视觉上微妙的、语义引导的跨模式操纵持续误导。总体而言，我们的结果表明，需要像素级稳健性之外的防御策略来解决跨模式决策中的语义漏洞。TRAP的代码可在GitHub上访问：https://github.com/uiuc-focal-lab/TRAP。



## **42. One Pic is All it Takes: Poisoning Visual Document Retrieval Augmented Generation with a Single Image**

一张图片即可：用单个图像毒害视觉文档检索增强生成 cs.CL

**SubmitDate**: 2025-11-20    [abs](http://arxiv.org/abs/2504.02132v3) [paper-pdf](https://arxiv.org/pdf/2504.02132v3)

**Authors**: Ezzeldin Shereen, Dan Ristea, Shae McFadden, Burak Hasircioglu, Vasilios Mavroudis, Chris Hicks

**Abstract**: Retrieval-augmented generation (RAG) is instrumental for inhibiting hallucinations in large language models (LLMs) through the use of a factual knowledge base (KB). Although PDF documents are prominent sources of knowledge, text-based RAG pipelines are ineffective at capturing their rich multi-modal information. In contrast, visual document RAG (VD-RAG) uses screenshots of document pages as the KB, which has been shown to achieve state-of-the-art results. However, by introducing the image modality, VD-RAG introduces new attack vectors for adversaries to disrupt the system by injecting malicious documents into the KB. In this paper, we demonstrate the vulnerability of VD-RAG to poisoning attacks targeting both retrieval and generation. We define two attack objectives and demonstrate that both can be realized by injecting only a single adversarial image into the KB. Firstly, we introduce a targeted attack against one or a group of queries with the goal of spreading targeted disinformation. Secondly, we present a universal attack that, for any potential user query, influences the response to cause a denial-of-service in the VD-RAG system. We investigate the two attack objectives under both white-box and black-box assumptions, employing a multi-objective gradient-based optimization approach as well as prompting state-of-the-art generative models. Using two visual document datasets, a diverse set of state-of-the-art retrievers (embedding models) and generators (vision language models), we show VD-RAG is vulnerable to poisoning attacks in both the targeted and universal settings, yet demonstrating robustness to black-box attacks in the universal setting.

摘要: 检索增强生成（RAG）有助于通过使用事实知识库（KB）来抑制大型语言模型（LLM）中的幻觉。尽管PDF文档是重要的知识来源，但基于文本的RAG管道在捕获其丰富的多模式信息方面效率低下。相比之下，视觉文档RAG（VD-RAG）使用文档页面的屏幕截图作为KB，已被证明可以实现最先进的结果。然而，通过引入图像模式，VD-RAG为对手引入了新的攻击载体，通过将恶意文档注入知识库来破坏系统。在本文中，我们展示了VD-RAG对针对检索和生成的中毒攻击的脆弱性。我们定义了两个攻击目标，并证明这两个目标都可以通过仅向知识库中注入单个对抗图像来实现。首先，我们对一个或一组查询引入有针对性的攻击，目标是传播有针对性的虚假信息。其次，我们提出了一种通用攻击，对于任何潜在的用户查询，该攻击都会影响响应，从而导致VD-RAG系统中的拒绝服务。我们调查的两个攻击目标下的白盒和黑盒的假设，采用多目标的基于梯度的优化方法，以及促使国家的最先进的生成模型。使用两个可视化文档数据集，一组不同的最先进的检索器（嵌入模型）和生成器（视觉语言模型），我们表明VD-RAG在目标和通用设置中都容易受到中毒攻击，但在通用设置中表现出对黑盒攻击的鲁棒性。



## **43. Benchmarking the Spatial Robustness of DNNs via Natural and Adversarial Localized Corruptions**

通过自然和对抗局部破坏对DNN的空间鲁棒性进行基准测试 cs.CV

Accepted for publication in Pattern Recognition

**SubmitDate**: 2025-11-24    [abs](http://arxiv.org/abs/2504.01632v3) [paper-pdf](https://arxiv.org/pdf/2504.01632v3)

**Authors**: Giulia Marchiori Pietrosanti, Giulio Rossolini, Alessandro Biondi, Giorgio Buttazzo

**Abstract**: The robustness of deep neural networks is a crucial factor in safety-critical applications, particularly in complex and dynamic environments (e.g., medical or driving scenarios) where localized corruptions can arise. While previous studies have evaluated the robustness of semantic segmentation (SS) models under whole-image natural or adversarial corruptions, a comprehensive investigation into the spatial robustness of dense vision models under localized corruptions remains underexplored. This paper fills this gap by introducing novel, region-aware metrics for benchmarking the spatial robustness of segmentation models, along with an evaluation framework to assess the impact of natural localized corruptions. Furthermore, it uncovers the inherent complexity of evaluating worst-case spatial robustness using only a single localized adversarial attack. To address this, the work proposes a region-aware multi-attack adversarial analysis to systematically assess model robustness across specific image regions. The proposed metrics and analysis were exploited to evaluate 14 segmentation models in driving scenarios, uncovering key insights into the effects of localized corruption in both natural and adversarial forms. The results reveal that models respond to these two types of threats differently; for instance, transformer-based segmentation models demonstrate notable robustness to localized natural corruptions but are highly vulnerable to adversarial ones, and vice versa for CNN-based models. Consequently, we also address the challenge of balancing robustness to both natural and adversarial localized corruptions by means of ensemble models, thereby achieving a broader threat coverage and improved reliability for dense vision tasks.

摘要: 深度神经网络的稳健性是安全关键应用中的一个关键因素，特别是在复杂和动态的环境中（例如，医疗或驾驶场景），其中可能会出现局部腐败。虽然之前的研究已经评估了语义分割（SS）模型在全图像自然或对抗破坏下的鲁棒性，但对局部破坏下密集视觉模型的空间鲁棒性的全面研究仍然不足。本文通过引入新颖的区域感知指标来对分割模型的空间稳健性进行基准测试，以及评估自然局部破坏的影响的评估框架来填补这一空白。此外，它揭示了仅使用单个局部对抗攻击来评估最坏情况空间稳健性的固有复杂性。为了解决这个问题，该工作提出了一种区域感知的多攻击对抗分析，以系统性地评估特定图像区域的模型稳健性。利用提出的指标和分析来评估驾驶场景中的14个细分模型，揭示了对自然和对抗形式的局部腐败影响的关键见解。结果表明，模型对这两种类型的威胁的反应不同;例如，基于变换器的分割模型对局部自然破坏表现出显着的鲁棒性，但极易受到对抗性破坏的影响，而基于CNN的模型反之亦然。因此，我们还通过集成模型解决了平衡对自然和对抗局部破坏的鲁棒性的挑战，从而实现更广泛的威胁覆盖范围并提高密集视觉任务的可靠性。



## **44. Reason2Attack: Jailbreaking Text-to-Image Models via LLM Reasoning**

Reason 2 Attack：通过LLM推理破解文本到图像模型 cs.CR

Noted that This paper includes model-generated content that may contain offensive or distressing material

**SubmitDate**: 2025-11-21    [abs](http://arxiv.org/abs/2503.17987v3) [paper-pdf](https://arxiv.org/pdf/2503.17987v3)

**Authors**: Chenyu Zhang, Lanjun Wang, Yiwen Ma, Wenhui Li, An-An Liu

**Abstract**: Text-to-Image(T2I) models typically deploy safety filters to prevent the generation of sensitive images. Unfortunately, recent jailbreaking attack methods manually design instructions for the LLM to generate adversarial prompts, which effectively bypass safety filters while producing sensitive images, exposing safety vulnerabilities of T2I models. However, due to the LLM's limited understanding of the T2I model and its safety filters, existing methods require numerous queries to achieve a successful attack, limiting their practical applicability. To address this issue, we propose Reason2Attack(R2A), which aims to enhance the LLM's reasoning capabilities in generating adversarial prompts by incorporating the jailbreaking attack into the post-training process of the LLM. Specifically, we first propose a CoT example synthesis pipeline based on Frame Semantics, which generates adversarial prompts by identifying related terms and corresponding context illustrations. Using CoT examples generated by the pipeline, we fine-tune the LLM to understand the reasoning path and format the output structure. Subsequently, we incorporate the jailbreaking attack task into the reinforcement learning process of the LLM and design an attack process reward that considers prompt length, prompt stealthiness, and prompt effectiveness, aiming to further enhance reasoning accuracy. Extensive experiments on various T2I models show that R2A achieves a better attack success ratio while requiring fewer queries than baselines. Moreover, our adversarial prompts demonstrate strong attack transferability across both open-source and commercial T2I models.

摘要: 文本到图像（T2 I）模型通常会部署安全过滤器来防止生成敏感图像。不幸的是，最近的越狱攻击方法手动设计了LLM生成对抗提示的指令，这在生成敏感图像的同时有效地绕过了安全过滤器，暴露了T2 I模型的安全漏洞。然而，由于LLM对T2 I模型及其安全过滤器的了解有限，现有方法需要大量查询才能实现成功的攻击，从而限制了其实际适用性。为了解决这个问题，我们提出了Reason 2 Attack（R2 A），旨在通过将越狱攻击纳入LLM的后训练过程来增强LLM生成对抗提示的推理能力。具体来说，我们首先提出了一个基于框架语义的CoT示例合成管道，该管道通过识别相关术语和相应的上下文插图来生成对抗性提示。使用管道生成的CoT示例，我们微调LLM以了解推理路径并格式化输出结构。随后，我们将越狱攻击任务纳入LLM的强化学习过程中，并设计了考虑提示长度、提示隐蔽性和提示有效性的攻击过程奖励，旨在进一步增强推理准确性。对各种T2 I模型的广泛实验表明，R2 A实现了更好的攻击成功率，同时需要比基线更少的查询。此外，我们的对抗性提示在开源和商业T2I模型中表现出强大的攻击可转移性。



## **45. Do Spikes Protect Privacy? Investigating Black-Box Model Inversion Attacks in Spiking Neural Networks**

Spikes保护隐私吗？研究尖峰神经网络中的黑匣子模型倒置攻击 cs.LG

7 pages, 4 figures

**SubmitDate**: 2025-11-23    [abs](http://arxiv.org/abs/2502.05509v2) [paper-pdf](https://arxiv.org/pdf/2502.05509v2)

**Authors**: Hamed Poursiami, Ayana Moshruba, Maryam Parsa

**Abstract**: As machine learning models become integral to security-sensitive applications, concerns over data leakage from adversarial attacks continue to rise. Model Inversion (MI) attacks pose a significant privacy threat by enabling adversaries to reconstruct training data from model outputs. While MI attacks on Artificial Neural Networks (ANNs) have been widely studied, Spiking Neural Networks (SNNs) remain largely unexplored in this context. Due to their event-driven and discrete computations, SNNs introduce fundamental differences in information processing that may offer inherent resistance to such attacks. A critical yet underexplored aspect of this threat lies in black-box settings, where attackers operate through queries without direct access to model parameters or gradients-representing a more realistic adversarial scenario in deployed systems. This work presents the first study of black-box MI attacks on SNNs. We adapt a generative adversarial MI framework to the spiking domain by incorporating rate-based encoding for input transformation and decoding mechanisms for output interpretation. Our results show that SNNs exhibit significantly greater resistance to MI attacks than ANNs, as demonstrated by degraded reconstructions, increased instability in attack convergence, and overall reduced attack effectiveness across multiple evaluation metrics. Further analysis suggests that the discrete and temporally distributed nature of SNN decision boundaries disrupts surrogate modeling, limiting the attacker's ability to approximate the target model.

摘要: 随着机器学习模型成为安全敏感应用程序的组成部分，人们对对抗性攻击造成数据泄露的担忧不断加剧。模型倒置（MI）攻击使对手能够从模型输出重建训练数据，从而构成重大的隐私威胁。虽然对人工神经网络（ANN）的MI攻击已被广泛研究，但Spiking神经网络（SNN）在此背景下基本上仍未被探索。由于SNN的事件驱动和离散计算，在信息处理中引入了根本差异，这可能会为此类攻击提供固有的抵抗力。这种威胁的一个关键但未充分研究的方面在于黑匣子设置，攻击者通过查询进行操作，而无需直接访问模型参数或梯度，这代表了已部署系统中更现实的对抗场景。这项工作首次研究了SNN的黑匣子MI攻击。我们通过将基于速率的编码用于输入转换和解码机制用于输出解释，将生成式对抗性MI框架适应峰值域。我们的结果表明，SNN对MI攻击的抵抗力比ANN明显更强，重建降级、攻击收敛的不稳定性增加以及多个评估指标的攻击有效性总体降低都证明了这一点。进一步的分析表明，SNN决策边界的离散和时间分布性质扰乱了代理建模，限制了攻击者逼近目标模型的能力。



## **46. Exploring Potential Prompt Injection Attacks in Federated Military LLMs and Their Mitigation**

探索联邦军事LLM中潜在的即时注入攻击及其缓解措施 cs.LG

Accepted to the 3rd International Workshop on Dataspaces and Digital Twins for Critical Entities and Smart Urban Communities - IEEE BigData 2025

**SubmitDate**: 2025-11-23    [abs](http://arxiv.org/abs/2501.18416v2) [paper-pdf](https://arxiv.org/pdf/2501.18416v2)

**Authors**: Youngjoon Lee, Taehyun Park, Yunho Lee, Jinu Gong, Joonhyuk Kang

**Abstract**: Federated Learning (FL) is increasingly being adopted in military collaborations to develop Large Language Models (LLMs) while preserving data sovereignty. However, prompt injection attacks-malicious manipulations of input prompts-pose new threats that may undermine operational security, disrupt decision-making, and erode trust among allies. This perspective paper highlights four vulnerabilities in federated military LLMs: secret data leakage, free-rider exploitation, system disruption, and misinformation spread. To address these risks, we propose a human-AI collaborative framework with both technical and policy countermeasures. On the technical side, our framework uses red/blue team wargaming and quality assurance to detect and mitigate adversarial behaviors of shared LLM weights. On the policy side, it promotes joint AI-human policy development and verification of security protocols.

摘要: 联合学习（FL）越来越多地被用于军事合作，以开发大型语言模型（LLM），同时保留数据主权。然而，即时注入攻击（对输入预算的恶意操纵）构成了新的威胁，可能会破坏运营安全、扰乱决策并削弱盟友之间的信任。这篇观点论文强调了联邦军事LLM中的四个漏洞：秘密数据泄露、搭便车剥削、系统中断和错误信息传播。为了应对这些风险，我们提出了一个具有技术和政策对策的人与人工智能协作框架。在技术方面，我们的框架使用红/蓝团队战争游戏和质量保证来检测和减轻共享LLM权重的对抗行为。在政策方面，它促进人工智能与人类联合政策制定和安全协议验证。



## **47. DarkMind: Latent Chain-of-Thought Backdoor in Customized LLMs**

DarkMind：定制LLC中潜在的思想链后门 cs.CR

19 pages, 15 figures, 12 tables

**SubmitDate**: 2025-11-23    [abs](http://arxiv.org/abs/2501.18617v2) [paper-pdf](https://arxiv.org/pdf/2501.18617v2)

**Authors**: Zhen Guo, Shanghao Shi, Shamim Yazdani, Ning Zhang, Reza Tourani

**Abstract**: With the rapid rise of personalized AI, customized large language models (LLMs) equipped with Chain of Thought (COT) reasoning now power millions of AI agents. However, their complex reasoning processes introduce new and largely unexplored security vulnerabilities. We present DarkMind, a novel latent reasoning level backdoor attack that targets customized LLMs by manipulating internal COT steps without altering user queries. Unlike prior prompt based attacks, DarkMind activates covertly within the reasoning chain via latent triggers, enabling adversarial behaviors without modifying input prompts or requiring access to model parameters. To achieve stealth and reliability, we propose dual trigger types instant and retrospective and integrate them within a unified embedding template that governs trigger dependent activation, employ a stealth optimization algorithm to minimize semantic drift, and introduce an automated conversation starter for covert activation across domains. Comprehensive experiments on eight reasoning datasets spanning arithmetic, commonsense, and symbolic domains, using five LLMs, demonstrate that DarkMind consistently achieves high attack success rates. We further investigate defense strategies to mitigate these risks and reveal that reasoning level backdoors represent a significant yet underexplored threat, underscoring the need for robust, reasoning aware security mechanisms.

摘要: 随着个性化人工智能的迅速崛起，配备思想链（COT）推理的定制大型语言模型（LLM）现在为数百万人工智能代理提供动力。然而，它们复杂的推理过程会引入新的且基本上未被探索的安全漏洞。我们提出了DarkMind，这是一种新颖的潜在推理级后门攻击，通过在不改变用户查询的情况下操纵内部COT步骤来针对自定义的LLM。与之前的基于提示的攻击不同，DarkMind通过潜在触发器在推理链中秘密激活，无需修改输入提示或要求访问模型参数即可实现对抗行为。为了实现隐形和可靠性，我们提出了即时和追溯双重触发类型，并将它们集成到统一的嵌入模板中，该模板管理触发相关激活，采用隐形优化算法来最大限度地减少语义漂移，并引入自动对话启动器跨域的秘密激活。使用五个LLM对跨越算术、常识和符号领域的八个推理数据集进行了全面实验，证明DarkMind始终实现了高攻击成功率。我们进一步研究了减轻这些风险的防御策略，并揭示了推理级后门代表了一个重大但未充分探索的威胁，强调了对强大、推理感知安全机制的必要性。



## **48. DiffBreak: Is Diffusion-Based Purification Robust?**

迪夫Break：基于扩散的净化是否稳健？ cs.CR

Accepted to NeurIPS 2025

**SubmitDate**: 2025-11-21    [abs](http://arxiv.org/abs/2411.16598v4) [paper-pdf](https://arxiv.org/pdf/2411.16598v4)

**Authors**: Andre Kassis, Urs Hengartner, Yaoliang Yu

**Abstract**: Diffusion-based purification (DBP) has become a cornerstone defense against adversarial examples (AEs), regarded as robust due to its use of diffusion models (DMs) that project AEs onto the natural data manifold. We refute this core claim, theoretically proving that gradient-based attacks effectively target the DM rather than the classifier, causing DBP's outputs to align with adversarial distributions. This prompts a reassessment of DBP's robustness, accrediting it two critical factors: inaccurate gradients and improper evaluation protocols that test only a single random purification of the AE. We show that when accounting for stochasticity and resubmission risk, DBP collapses. To support this, we introduce DiffBreak, the first reliable toolkit for differentiation through DBP, eliminating gradient mismatches that previously further inflated robustness estimates. We also analyze the current defense scheme used for DBP where classification relies on a single purification, pinpointing its inherent invalidity. We provide a statistically grounded majority-vote (MV) alternative that aggregates predictions across multiple purified copies, showing partial but meaningful robustness gain. We then propose a novel adaptation of an optimization method against deepfake watermarking, crafting systemic perturbations that defeat DBP even under MV, challenging DBP's viability.

摘要: 基于扩散的纯化（DAB）已成为对抗性例子（AE）的基石防御，由于其使用将AE投射到自然数据集上的扩散模型（DM），因此被认为是强大的。我们反驳了这一核心主张，从理论上证明基于梯度的攻击有效地针对DM而不是分类器，导致CBP的输出与对抗性分布一致。这促使人们重新评估CBP的稳健性，从而证实了它有两个关键因素：不准确的梯度和仅测试AE单次随机纯化的不当评估方案。我们表明，当考虑随机性和重新提交风险时，CBP崩溃。为了支持这一点，我们引入了迪夫Break，这是第一个通过CBP进行区分的可靠工具包，消除了之前进一步夸大稳健性估计的梯度不匹配。我们还分析了当前用于CBP的防御方案，其中分类依赖于单一纯化，找出其固有的无效性。我们提供了一种基于统计学的多数票（MV）替代方案，它聚合了多个纯化副本的预测，显示出部分但有意义的鲁棒性增强。然后，我们提出了一种针对Deepfake水印的优化方法的新颖调整，精心设计系统性扰动，即使在MV下也能击败CBP，挑战CBP的生存能力。



## **49. HidePrint: Protecting Device Anonymity by Obscuring Radio Fingerprints**

HidePrint：通过模糊无线电指纹来保护设备不一致 cs.CR

Accepted for publication at AsiaCCS 2026 - 21st ACM ASIA Conference on Computer and Communications Security

**SubmitDate**: 2025-11-21    [abs](http://arxiv.org/abs/2411.06417v2) [paper-pdf](https://arxiv.org/pdf/2411.06417v2)

**Authors**: Gabriele Oligeri, Savio Sciancalepore

**Abstract**: Radio Frequency Fingerprinting (RFF) techniques allow a receiver to authenticate a transmitter by analyzing the physical layer of the radio spectrum. Although the vast majority of scientific contributions focus on improving the performance of RFF considering different parameters and scenarios, in this work, we consider RFF as an attack vector to identify a target device in the radio spectrum. \\ We propose, implement, and evaluate {\em HidePrint}, a solution to prevent identification through RFF without affecting the quality of the communication link between the transmitter and the receiver. {\em HidePrint} hides the transmitter's fingerprint against an illegitimate eavesdropper through the injection of controlled noise into the transmitted signal. We evaluate our solution against various state-of-the-art RFF techniques, considering several adversarial models, data from real-world communication links (wired and wireless), and protocol configurations. Our results show that the injection of a Gaussian noise pattern with a normalized standard deviation of (at least) 0.02 prevents device fingerprinting in all the considered scenarios, while affecting the Signal-to-Noise Ratio (SNR) of the received signal by only 0.1 dB. Moreover, we introduce {\em selective radio fingerprint disclosure}, a new technique that allows the transmitter to disclose the radio fingerprint to only a subset of intended receivers.

摘要: 射频指纹识别（RFF）技术允许接收机通过分析无线电频谱的物理层来验证发射机。尽管绝大多数科学贡献都集中在考虑不同参数和场景来提高RFF的性能，但在这项工作中，我们将RFF视为识别无线电频谱中目标设备的攻击载体。\\我们提出、实施和评估了{\em HidePrint}，这是一种防止通过RFF进行识别的解决方案，而不会影响发送器和接收器之间的通信链路的质量。{\em HidePrint}通过向传输的信号中注入受控噪音来隐藏发射机的指纹，以防止非法窃听者。我们根据各种最先进的RFF技术评估我们的解决方案，考虑了几种对抗模型、来自现实世界通信链路（有线和无线）的数据以及协议配置。我们的结果表明，注入正规化标准差（至少）为0.02的高斯噪音模式会在所有考虑的场景中阻止设备指纹识别，同时只影响接收信号的信噪比（SNR）0.1分贝。此外，我们还引入了选择性无线电指纹披露，这是一种新技术，允许发射机仅向预期接收机的一部分披露无线电指纹。



## **50. PINNsFailureRegion Localization and Refinement through White-box AdversarialAttack**

通过白盒对抗攻击实现PINNsFail区域本地化和细化 cs.LG

**SubmitDate**: 2025-11-23    [abs](http://arxiv.org/abs/2310.11789v2) [paper-pdf](https://arxiv.org/pdf/2310.11789v2)

**Authors**: Shengzhu Shi, Yao Li, Zhichang Guo, Boying Wu, Yang Zhao

**Abstract**: Physics-informed neural networks (PINNs) have shown great promise in solving partial differential equations (PDEs). However, vanilla PINNs often face challenges when solving complex PDEs, especially those involving multi-scale behaviors or solutions with sharp or oscillatory characteristics. To precisely and adaptively locate the critical regions that fail in the solving process we propose a sampling strategy grounded in white-box adversarial attacks, referred to as WbAR. WbAR search for failure regions in the direction of the loss gradient, thus directly locating the most critical positions. WbAR generates adversarial samples in a random walk manner and iteratively refines PINNs to guide the model's focus towards dynamically updated critical regions during training. We implement WbAR to the elliptic equation with multi-scale coefficients, Poisson equation with multi-peak solutions, high-dimensional Poisson equations, and Burgers equation with sharp solutions. The results demonstrate that WbAR can effectively locate and reduce failure regions. Moreover, WbAR is suitable for solving complex PDEs, since locating failure regions through adversarial attacks is independent of the size of failure regions or the complexity of the distribution.

摘要: 基于物理的神经网络（PINN）在求解偏微方程（PED）方面表现出了巨大的前景。然而，普通的PINN在解决复杂的PED时经常面临挑战，尤其是那些涉及多尺度行为或具有尖锐或波动特征的解决方案的问题。为了准确且自适应地定位求解过程中失败的关键区域，我们提出了一种基于白盒对抗攻击的采样策略，称为WbAR。WbAR沿着损失梯度方向搜索故障区域，从而直接定位最关键的位置。WbAR以随机游走的方式生成对抗性样本，并迭代地细化PINN，以引导模型在训练过程中将焦点集中在动态更新的关键区域。我们将WbAR算法应用于多尺度系数椭圆方程、多峰解Poisson方程、高维Poisson方程和尖锐解Burgers方程。结果表明，WbAR可以有效地定位和减少故障区域。此外，WbAR适用于解决复杂的偏微分方程，因为通过对抗性攻击定位故障区域与故障区域的大小或分布的复杂性无关。



