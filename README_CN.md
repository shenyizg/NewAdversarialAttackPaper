# Latest Adversarial Attack Papers
**update at 2025-05-22 16:57:16**

翻译来自 https://cloud.tencent.com/document/product/551/15619

## **1. Reverse Engineering Human Preferences with Reinforcement Learning**

利用强化学习反向工程人类偏好 cs.CL

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2505.15795v1) [paper-pdf](http://arxiv.org/pdf/2505.15795v1)

**Authors**: Lisa Alazraki, Tan Yi-Chern, Jon Ander Campos, Maximilian Mozes, Marek Rei, Max Bartolo

**Abstract**: The capabilities of Large Language Models (LLMs) are routinely evaluated by other LLMs trained to predict human preferences. This framework--known as LLM-as-a-judge--is highly scalable and relatively low cost. However, it is also vulnerable to malicious exploitation, as LLM responses can be tuned to overfit the preferences of the judge. Previous work shows that the answers generated by a candidate-LLM can be edited post hoc to maximise the score assigned to them by a judge-LLM. In this study, we adopt a different approach and use the signal provided by judge-LLMs as a reward to adversarially tune models that generate text preambles designed to boost downstream performance. We find that frozen LLMs pipelined with these models attain higher LLM-evaluation scores than existing frameworks. Crucially, unlike other frameworks which intervene directly on the model's response, our method is virtually undetectable. We also demonstrate that the effectiveness of the tuned preamble generator transfers when the candidate-LLM and the judge-LLM are replaced with models that are not used during training. These findings raise important questions about the design of more reliable LLM-as-a-judge evaluation settings. They also demonstrate that human preferences can be reverse engineered effectively, by pipelining LLMs to optimise upstream preambles via reinforcement learning--an approach that could find future applications in diverse tasks and domains beyond adversarial attacks.

摘要: 大型语言模型（LLM）的能力通常由其他经过训练以预测人类偏好的LLM进行评估。这个框架-被称为LLM作为法官-具有高度可扩展性和相对较低的成本。然而，它也容易受到恶意利用，因为LLM响应可以被调整以过度适应法官的偏好。以前的工作表明，候选人LLM生成的答案可以事后编辑，以最大限度地提高法官LLM分配给他们的分数。在这项研究中，我们采用了一种不同的方法，并使用judge-LLM提供的信号作为奖励，以对抗性地调整模型，这些模型生成旨在提高下游性能的文本前置码。我们发现，使用这些模型流水线化的冻结LLM比现有框架获得更高的LLM评估分数。至关重要的是，与直接干预模型响应的其他框架不同，我们的方法几乎无法检测。我们还证明，当候选LLM和判断LLM被训练期间未使用的模型替换时，调整后的前同步码生成器的有效性会转移。这些发现提出了更可靠的法学硕士作为一个法官的评价设置的设计的重要问题。他们还证明，人类的偏好可以有效地进行逆向工程，通过流水线LLM来优化上游的优化，这种方法可以在对抗性攻击之外的各种任务和领域中找到未来的应用。



## **2. Scalable Defense against In-the-wild Jailbreaking Attacks with Safety Context Retrieval**

通过安全上下文检索针对野外越狱攻击的可扩展防御 cs.CR

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2505.15753v1) [paper-pdf](http://arxiv.org/pdf/2505.15753v1)

**Authors**: Taiye Chen, Zeming Wei, Ang Li, Yisen Wang

**Abstract**: Large Language Models (LLMs) are known to be vulnerable to jailbreaking attacks, wherein adversaries exploit carefully engineered prompts to induce harmful or unethical responses. Such threats have raised critical concerns about the safety and reliability of LLMs in real-world deployment. While existing defense mechanisms partially mitigate such risks, subsequent advancements in adversarial techniques have enabled novel jailbreaking methods to circumvent these protections, exposing the limitations of static defense frameworks. In this work, we explore defending against evolving jailbreaking threats through the lens of context retrieval. First, we conduct a preliminary study demonstrating that even a minimal set of safety-aligned examples against a particular jailbreak can significantly enhance robustness against this attack pattern. Building on this insight, we further leverage the retrieval-augmented generation (RAG) techniques and propose Safety Context Retrieval (SCR), a scalable and robust safeguarding paradigm for LLMs against jailbreaking. Our comprehensive experiments demonstrate how SCR achieves superior defensive performance against both established and emerging jailbreaking tactics, contributing a new paradigm to LLM safety. Our code will be available upon publication.

摘要: 众所周知，大型语言模型（LLM）很容易受到越狱攻击，其中对手利用精心设计的提示来引发有害或不道德的反应。此类威胁引发了人们对LLM在现实世界部署中的安全性和可靠性的严重担忧。虽然现有的防御机制部分减轻了此类风险，但对抗技术的后续进步使新型越狱方法能够规避这些保护，暴露了静态防御框架的局限性。在这项工作中，我们探索通过上下文检索的视角抵御不断变化的越狱威胁。首先，我们进行了一项初步研究，证明即使是针对特定越狱的最少一组安全一致的示例也可以显着增强针对这种攻击模式的鲁棒性。在这一见解的基础上，我们进一步利用检索增强生成（RAG）技术并提出安全上下文检索（SR），这是一种针对LLM越狱的可扩展且强大的保护范式。我们全面的实验展示了可控硅如何在针对既定和新兴越狱策略的情况下实现卓越的防御性能，为LLM安全性贡献了新的范式。我们的代码将在发布后提供。



## **3. Alignment Under Pressure: The Case for Informed Adversaries When Evaluating LLM Defenses**

压力下的一致：评估LLM防御时知情对手的理由 cs.CR

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2505.15738v1) [paper-pdf](http://arxiv.org/pdf/2505.15738v1)

**Authors**: Xiaoxue Yang, Bozhidar Stevanoski, Matthieu Meeus, Yves-Alexandre de Montjoye

**Abstract**: Large language models (LLMs) are rapidly deployed in real-world applications ranging from chatbots to agentic systems. Alignment is one of the main approaches used to defend against attacks such as prompt injection and jailbreaks. Recent defenses report near-zero Attack Success Rates (ASR) even against Greedy Coordinate Gradient (GCG), a white-box attack that generates adversarial suffixes to induce attacker-desired outputs. However, this search space over discrete tokens is extremely large, making the task of finding successful attacks difficult. GCG has, for instance, been shown to converge to local minima, making it sensitive to initialization choices. In this paper, we assess the future-proof robustness of these defenses using a more informed threat model: attackers who have access to some information about the alignment process. Specifically, we propose an informed white-box attack leveraging the intermediate model checkpoints to initialize GCG, with each checkpoint acting as a stepping stone for the next one. We show this approach to be highly effective across state-of-the-art (SOTA) defenses and models. We further show our informed initialization to outperform other initialization methods and show a gradient-informed checkpoint selection strategy to greatly improve attack performance and efficiency. Importantly, we also show our method to successfully find universal adversarial suffixes -- single suffixes effective across diverse inputs. Our results show that, contrary to previous beliefs, effective adversarial suffixes do exist against SOTA alignment-based defenses, that these can be found by existing attack methods when adversaries exploit alignment knowledge, and that even universal suffixes exist. Taken together, our results highlight the brittleness of current alignment-based methods and the need to consider stronger threat models when testing the safety of LLMs.

摘要: 大型语言模型（LLM）被快速部署在从聊天机器人到代理系统的实际应用中。对齐是用于防御诸如即时注入和越狱等攻击的主要方法之一。最近的防御报告甚至对贪婪坐标梯度（GCG）的攻击成功率（ASR）接近于零，GCG是一种白盒攻击，生成对抗性后缀以诱导攻击者期望的输出。然而，这种在离散令牌上的搜索空间非常大，使得找到成功攻击的任务变得困难。例如，GCG已被证明收敛到局部极小值，使其对初始化选择敏感。在本文中，我们使用一个更明智的威胁模型来评估这些防御系统的面向未来的鲁棒性：可以访问有关对齐过程的一些信息的攻击者。具体来说，我们提出了一种知情白盒攻击，利用中间模型检查点来初始化GCG，每个检查点都充当下一个检查点的垫脚石。我们证明这种方法在最先进的（SOTA）防御和模型中非常有效。我们进一步展示了我们的知情初始化，以优于其他初始化方法，并展示了一种基于梯度的检查点选择策略，以极大地提高攻击性能和效率。重要的是，我们还展示了成功找到通用对抗后缀的方法--在不同输入中有效的单个后缀。我们的结果表明，与之前的观点相反，针对基于SOTA匹配的防御，确实存在有效的对抗性后缀，当对手利用对齐知识时，这些后缀可以通过现有的攻击方法找到，甚至存在通用后缀。总而言之，我们的结果凸显了当前基于环境的方法的脆弱性，以及在测试LLM的安全性时需要考虑更强的威胁模型。



## **4. Beyond Classification: Evaluating Diffusion Denoised Smoothing for Security-Utility Trade off**

超越分类：评估安全-效用权衡的扩散去噪平滑 cs.LG

Paper accepted at the 33rd European Signal Processing Conference  (EUSIPCO 2025)

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2505.15594v1) [paper-pdf](http://arxiv.org/pdf/2505.15594v1)

**Authors**: Yury Belousov, Brian Pulfer, Vitaliy Kinakh, Slava Voloshynovskiy

**Abstract**: While foundation models demonstrate impressive performance across various tasks, they remain vulnerable to adversarial inputs. Current research explores various approaches to enhance model robustness, with Diffusion Denoised Smoothing emerging as a particularly promising technique. This method employs a pretrained diffusion model to preprocess inputs before model inference. Yet, its effectiveness remains largely unexplored beyond classification. We aim to address this gap by analyzing three datasets with four distinct downstream tasks under three different adversarial attack algorithms. Our findings reveal that while foundation models maintain resilience against conventional transformations, applying high-noise diffusion denoising to clean images without any distortions significantly degrades performance by as high as 57%. Low-noise diffusion settings preserve performance but fail to provide adequate protection across all attack types. Moreover, we introduce a novel attack strategy specifically targeting the diffusion process itself, capable of circumventing defenses in the low-noise regime. Our results suggest that the trade-off between adversarial robustness and performance remains a challenge to be addressed.

摘要: 虽然基金会模型在各种任务中表现出令人印象深刻的性能，但它们仍然容易受到对抗性输入的影响。当前的研究探索了增强模型稳健性的各种方法，其中扩散去噪平滑成为一种特别有前途的技术。该方法使用预训练的扩散模型来在模型推理之前预处理输入。然而，其有效性在很大程度上仍然未经分类之外的探索。我们的目标是通过在三种不同的对抗攻击算法下分析具有四个不同下游任务的三个数据集来解决这一差距。我们的研究结果表明，虽然基础模型保持了对传统转换的弹性，但对没有任何失真的干净图像应用高噪音扩散去噪会显着降低性能高达57%。低噪音扩散设置可以保留性能，但无法为所有攻击类型提供足够的保护。此外，我们引入了一种专门针对扩散过程本身的新型攻击策略，能够绕过低噪音区域中的防御。我们的结果表明，对抗稳健性和性能之间的权衡仍然是一个需要解决的挑战。



## **5. ModSec-AdvLearn: Countering Adversarial SQL Injections with Robust Machine Learning**

ModSec-AdvLearn：利用稳健的机器学习对抗敌对SQL注入 cs.LG

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2308.04964v4) [paper-pdf](http://arxiv.org/pdf/2308.04964v4)

**Authors**: Giuseppe Floris, Christian Scano, Biagio Montaruli, Luca Demetrio, Andrea Valenza, Luca Compagna, Davide Ariu, Luca Piras, Davide Balzarotti, Battista Biggio

**Abstract**: Many Web Application Firewalls (WAFs) leverage the OWASP CRS to block incoming malicious requests. The CRS consists of different sets of rules designed by domain experts to detect well-known web attack patterns. Both the set of rules and the weights used to combine them are manually defined, yielding four different default configurations of the CRS. In this work, we focus on the detection of SQLi attacks, and show that the manual configurations of the CRS typically yield a suboptimal trade-off between detection and false alarm rates. Furthermore, we show that these configurations are not robust to adversarial SQLi attacks, i.e., carefully-crafted attacks that iteratively refine the malicious SQLi payload by querying the target WAF to bypass detection. To overcome these limitations, we propose (i) using machine learning to automate the selection of the set of rules to be combined along with their weights, i.e., customizing the CRS configuration based on the monitored web services; and (ii) leveraging adversarial training to significantly improve its robustness to adversarial SQLi manipulations. Our experiments, conducted using the well-known open-source ModSecurity WAF equipped with the CRS rules, show that our approach, named ModSec-AdvLearn, can (i) increase the detection rate up to 30%, while retaining negligible false alarm rates and discarding up to 50% of the CRS rules; and (ii) improve robustness against adversarial SQLi attacks up to 85%, marking a significant stride toward designing more effective and robust WAFs. We release our open-source code at https://github.com/pralab/modsec-advlearn.

摘要: 许多Web应用程序防火墙（WAF）利用OWISP CRS来阻止进入的恶意请求。CRS由领域专家设计的不同规则集组成，用于检测众所周知的网络攻击模式。规则集和用于组合它们的权重都是手动定义的，从而产生四种不同的CRS默认配置。在这项工作中，我们重点关注SQLi攻击的检测，并表明CRS的手动配置通常会在检测和误报率之间产生次优的权衡。此外，我们证明了这些配置对对抗性SQLi攻击不鲁棒，即，精心设计的攻击，通过查询目标WAF来绕过检测，反复改进恶意SQLi有效负载。为了克服这些限制，我们提出（i）使用机器学习来自动选择要与它们的权重一起组合的规则集，即，基于所监视的web服务定制CRS配置;以及（ii）利用对抗性训练来显著提高其对对抗性SQLi操纵的鲁棒性。我们使用配备了CRS规则的知名开源ModSecurity WAF进行的实验表明，我们的方法（称为ModSec-AdvLearn）可以（i）将检测率提高到30%，同时保留可忽略的误报率并丢弃高达50%的CRS规则;以及（ii）将对抗SQLi攻击的稳健性提高至85%，标志着朝着设计更有效和更稳健的WAF迈出了重大一步。我们在https://github.com/pralab/modsec-advlearn上发布我们的开源代码。



## **6. Audio Jailbreak: An Open Comprehensive Benchmark for Jailbreaking Large Audio-Language Models**

Audio Jailbreak：一个用于越狱大型音频语言模型的开放综合基准测试 cs.SD

We release AJailBench, including both static and optimized  adversarial data, to facilitate future research:  https://github.com/mbzuai-nlp/AudioJailbreak

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2505.15406v1) [paper-pdf](http://arxiv.org/pdf/2505.15406v1)

**Authors**: Zirui Song, Qian Jiang, Mingxuan Cui, Mingzhe Li, Lang Gao, Zeyu Zhang, Zixiang Xu, Yanbo Wang, Chenxi Wang, Guangxian Ouyang, Zhenhao Chen, Xiuying Chen

**Abstract**: The rise of Large Audio Language Models (LAMs) brings both potential and risks, as their audio outputs may contain harmful or unethical content. However, current research lacks a systematic, quantitative evaluation of LAM safety especially against jailbreak attacks, which are challenging due to the temporal and semantic nature of speech. To bridge this gap, we introduce AJailBench, the first benchmark specifically designed to evaluate jailbreak vulnerabilities in LAMs. We begin by constructing AJailBench-Base, a dataset of 1,495 adversarial audio prompts spanning 10 policy-violating categories, converted from textual jailbreak attacks using realistic text to speech synthesis. Using this dataset, we evaluate several state-of-the-art LAMs and reveal that none exhibit consistent robustness across attacks. To further strengthen jailbreak testing and simulate more realistic attack conditions, we propose a method to generate dynamic adversarial variants. Our Audio Perturbation Toolkit (APT) applies targeted distortions across time, frequency, and amplitude domains. To preserve the original jailbreak intent, we enforce a semantic consistency constraint and employ Bayesian optimization to efficiently search for perturbations that are both subtle and highly effective. This results in AJailBench-APT, an extended dataset of optimized adversarial audio samples. Our findings demonstrate that even small, semantically preserved perturbations can significantly reduce the safety performance of leading LAMs, underscoring the need for more robust and semantically aware defense mechanisms.

摘要: 大型音频语言模型（LAMs）的兴起带来了潜在的风险，因为它们的音频输出可能包含有害或不道德的内容。然而，目前的研究缺乏一个系统的，定量的评估LAM的安全性，特别是对越狱攻击，这是具有挑战性的，由于语音的时间和语义的性质。为了弥补这一差距，我们引入AJailBench，这是第一个专门用于评估LAM中越狱漏洞的基准测试。我们首先构建AJailBench-Base，这是一个包含1，495个对抗性音频提示的数据集，涵盖10个违反策略的类别，从使用真实文本的文本越狱攻击转换为语音合成。使用该数据集，我们评估了几种最先进的LAM，并发现没有一种在攻击中表现出一致的鲁棒性。为了进一步加强越狱测试并模拟更真实的攻击条件，我们提出了一种生成动态对抗变体的方法。我们的音频微扰工具包（APT）在时间、频率和幅度域中应用有针对性的失真。为了保留最初的越狱意图，我们强制执行语义一致性约束并采用Bayesian优化来有效地搜索微妙且高效的扰动。这会产生AJailBench-APT，这是一个优化的对抗性音频样本的扩展数据集。我们的研究结果表明，即使是很小的、在语义上保留的扰动也会显着降低领先LAM的安全性能，这凸显了对更强大和语义感知的防御机制的需求。



## **7. PoisonArena: Uncovering Competing Poisoning Attacks in Retrieval-Augmented Generation**

PoisonArena：揭露检索增强一代中的竞争中毒攻击 cs.IR

29 pages

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2505.12574v2) [paper-pdf](http://arxiv.org/pdf/2505.12574v2)

**Authors**: Liuji Chen, Xiaofang Yang, Yuanzhuo Lu, Jinghao Zhang, Xin Sun, Qiang Liu, Shu Wu, Jing Dong, Liang Wang

**Abstract**: Retrieval-Augmented Generation (RAG) systems, widely used to improve the factual grounding of large language models (LLMs), are increasingly vulnerable to poisoning attacks, where adversaries inject manipulated content into the retriever's corpus. While prior research has predominantly focused on single-attacker settings, real-world scenarios often involve multiple, competing attackers with conflicting objectives. In this work, we introduce PoisonArena, the first benchmark to systematically study and evaluate competing poisoning attacks in RAG. We formalize the multi-attacker threat model, where attackers vie to control the answer to the same query using mutually exclusive misinformation. PoisonArena leverages the Bradley-Terry model to quantify each method's competitive effectiveness in such adversarial environments. Through extensive experiments on the Natural Questions and MS MARCO datasets, we demonstrate that many attack strategies successful in isolation fail under competitive pressure. Our findings highlight the limitations of conventional evaluation metrics like Attack Success Rate (ASR) and F1 score and underscore the need for competitive evaluation to assess real-world attack robustness. PoisonArena provides a standardized framework to benchmark and develop future attack and defense strategies under more realistic, multi-adversary conditions. Project page: https://github.com/yxf203/PoisonArena.

摘要: 检索增强生成（RAG）系统，广泛用于改善大型语言模型（LLM）的事实基础，越来越容易受到中毒攻击，其中对手将操纵的内容注入检索器的语料库。虽然以前的研究主要集中在单个攻击者的设置，但现实世界的场景往往涉及多个相互竞争的攻击者，这些攻击者的目标相互冲突。在这项工作中，我们介绍PoisonArena，第一个基准系统地研究和评估竞争中毒攻击在RAG。我们形式化的多攻击者威胁模型，攻击者争夺控制答案相同的查询使用互斥的错误信息。PoisonArena利用Bradley-Terry模型来量化每种方法在此类对抗环境中的竞争有效性。通过对Natural Questions和MS MARCO数据集的广泛实验，我们证明了许多孤立成功的攻击策略在竞争压力下失败。我们的研究结果强调了攻击成功率（SVR）和F1评分等传统评估指标的局限性，并强调了竞争性评估来评估现实世界攻击稳健性的必要性。PoisonArena提供了一个标准化的框架，可以在更现实的多对手条件下基准和开发未来的攻击和防御策略。项目页面：https://github.com/yxf203/PoisonArena。



## **8. My Face Is Mine, Not Yours: Facial Protection Against Diffusion Model Face Swapping**

我的脸是我的，不是你的：防止扩散模型换脸的面部保护 cs.CV

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2505.15336v1) [paper-pdf](http://arxiv.org/pdf/2505.15336v1)

**Authors**: Hon Ming Yam, Zhongliang Guo, Chun Pong Lau

**Abstract**: The proliferation of diffusion-based deepfake technologies poses significant risks for unauthorized and unethical facial image manipulation. While traditional countermeasures have primarily focused on passive detection methods, this paper introduces a novel proactive defense strategy through adversarial attacks that preemptively protect facial images from being exploited by diffusion-based deepfake systems. Existing adversarial protection methods predominantly target conventional generative architectures (GANs, AEs, VAEs) and fail to address the unique challenges presented by diffusion models, which have become the predominant framework for high-quality facial deepfakes. Current diffusion-specific adversarial approaches are limited by their reliance on specific model architectures and weights, rendering them ineffective against the diverse landscape of diffusion-based deepfake implementations. Additionally, they typically employ global perturbation strategies that inadequately address the region-specific nature of facial manipulation in deepfakes.

摘要: 基于扩散的深度伪造技术的激增给未经授权和不道德的面部图像操纵带来了巨大风险。虽然传统的对抗措施主要集中在被动检测方法上，但本文通过对抗攻击引入了一种新颖的主动防御策略，该策略先发制人地保护面部图像不被基于扩散的Deepfake系统利用。现有的对抗保护方法主要针对传统的生成架构（GAN、AE、VAE），无法解决扩散模型提出的独特挑战，而扩散模型已成为高质量面部深度造假的主要框架。当前特定于扩散的对抗方法因其对特定模型架构和权重的依赖而受到限制，导致它们对基于扩散的Deepfake实现的多样化环境无效。此外，他们通常采用全局扰动策略，无法充分解决深度造假中面部操纵的区域特定性质。



## **9. BadSR: Stealthy Label Backdoor Attacks on Image Super-Resolution**

BadSR：对图像超分辨率的隐形标签后门攻击 cs.CV

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2505.15308v1) [paper-pdf](http://arxiv.org/pdf/2505.15308v1)

**Authors**: Ji Guo, Xiaolei Wen, Wenbo Jiang, Cheng Huang, Jinjin Li, Hongwei Li

**Abstract**: With the widespread application of super-resolution (SR) in various fields, researchers have begun to investigate its security. Previous studies have demonstrated that SR models can also be subjected to backdoor attacks through data poisoning, affecting downstream tasks. A backdoor SR model generates an attacker-predefined target image when given a triggered image while producing a normal high-resolution (HR) output for clean images. However, prior backdoor attacks on SR models have primarily focused on the stealthiness of poisoned low-resolution (LR) images while ignoring the stealthiness of poisoned HR images, making it easy for users to detect anomalous data. To address this problem, we propose BadSR, which improves the stealthiness of poisoned HR images. The key idea of BadSR is to approximate the clean HR image and the pre-defined target image in the feature space while ensuring that modifications to the clean HR image remain within a constrained range. The poisoned HR images generated by BadSR can be integrated with existing triggers. To further improve the effectiveness of BadSR, we design an adversarially optimized trigger and a backdoor gradient-driven poisoned sample selection method based on a genetic algorithm. The experimental results show that BadSR achieves a high attack success rate in various models and data sets, significantly affecting downstream tasks.

摘要: 随着超分辨率（SR）在各个领域的广泛应用，研究人员开始研究其安全性。之前的研究表明，SR模型也可能通过数据中毒而受到后门攻击，从而影响下游任务。后门SR模型在给定触发图像时生成攻击者预定义的目标图像，同时为干净图像生成正常的高分辨率（HR）输出。然而，之前对SR模型的后门攻击主要集中在有毒低分辨率（LR）图像的隐蔽性上，而忽略了有毒HR图像的隐蔽性，使用户很容易检测到异常数据。为了解决这个问题，我们提出了BadSR，它可以改善有毒HR图像的隐蔽性。BadSR的关键思想是在特征空间中逼近干净HR图像和预定义的目标图像，同时确保对干净HR图像的修改保持在约束范围内。BadSR生成的中毒HR图像可以与现有触发器集成。为了进一步提高BadSR的有效性，我们设计了一种对抗优化触发器和基于遗传算法的后门梯度驱动中毒样本选择方法。实验结果表明，BadSR在各种模型和数据集中都达到了很高的攻击成功率，对下游任务产生了显着影响。



## **10. Robustness Evaluation of Graph-based News Detection Using Network Structural Information**

利用网络结构信息进行基于图的新闻检测的鲁棒性评估 cs.SI

Accepted to Proceedings of the ACM SIGKDD Conference on Knowledge  Discovery and Data Mining 2025 (KDD 2025). 14 pages, 7 figures, 10 tables

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2505.14453v2) [paper-pdf](http://arxiv.org/pdf/2505.14453v2)

**Authors**: Xianghua Zeng, Hao Peng, Angsheng Li

**Abstract**: Although Graph Neural Networks (GNNs) have shown promising potential in fake news detection, they remain highly vulnerable to adversarial manipulations within social networks. Existing methods primarily establish connections between malicious accounts and individual target news to investigate the vulnerability of graph-based detectors, while they neglect the structural relationships surrounding targets, limiting their effectiveness in robustness evaluation. In this work, we propose a novel Structural Information principles-guided Adversarial Attack Framework, namely SI2AF, which effectively challenges graph-based detectors and further probes their detection robustness. Specifically, structural entropy is introduced to quantify the dynamic uncertainty in social engagements and identify hierarchical communities that encompass all user accounts and news posts. An influence metric is presented to measure each account's probability of engaging in random interactions, facilitating the design of multiple agents that manage distinct malicious accounts. For each target news, three attack strategies are developed through multi-agent collaboration within the associated subgraph to optimize evasion against black-box detectors. By incorporating the adversarial manipulations generated by SI2AF, we enrich the original network structure and refine graph-based detectors to improve their robustness against adversarial attacks. Extensive evaluations demonstrate that SI2AF significantly outperforms state-of-the-art baselines in attack effectiveness with an average improvement of 16.71%, and enhances GNN-based detection robustness by 41.54% on average.

摘要: 尽管图形神经网络（GNN）在假新闻检测方面表现出了广阔的潜力，但它们仍然极易受到社交网络内的对抗操纵的影响。现有的方法主要在恶意帐户和单个目标新闻之间建立联系，以调查基于图形的检测器的脆弱性，而它们忽视了目标周围的结构关系，限制了其稳健性评估的有效性。在这项工作中，我们提出了一种新型的结构信息原则指导的对抗攻击框架，即SI 2AF，它有效地挑战了基于图的检测器，并进一步探讨了其检测鲁棒性。具体来说，引入结构性信息来量化社交参与中的动态不确定性，并识别涵盖所有用户帐户和新闻帖子的分层社区。提供了一个影响力指标来衡量每个帐户参与随机交互的可能性，以促进管理不同恶意帐户的多个代理的设计。对于每个目标新闻，通过相关子图内的多代理协作开发三种攻击策略，以优化针对黑匣子检测器的规避。通过结合SI 2AF生成的对抗性操纵，我们丰富了原始的网络结构并改进了基于图的检测器，以提高其对对抗性攻击的鲁棒性。广泛的评估表明，SI 2AF在攻击有效性方面显着优于最先进的基线，平均提高了16.71%，并将基于GNN的检测鲁棒性平均提高了41.54%。



## **11. Blind Spot Navigation: Evolutionary Discovery of Sensitive Semantic Concepts for LVLMs**

盲点导航：LVLM敏感语义概念的进化发现 cs.CV

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2505.15265v1) [paper-pdf](http://arxiv.org/pdf/2505.15265v1)

**Authors**: Zihao Pan, Yu Tong, Weibin Wu, Jingyi Wang, Lifeng Chen, Zhe Zhao, Jiajia Wei, Yitong Qiao, Zibin Zheng

**Abstract**: Adversarial attacks aim to generate malicious inputs that mislead deep models, but beyond causing model failure, they cannot provide certain interpretable information such as ``\textit{What content in inputs make models more likely to fail?}'' However, this information is crucial for researchers to specifically improve model robustness. Recent research suggests that models may be particularly sensitive to certain semantics in visual inputs (such as ``wet,'' ``foggy''), making them prone to errors. Inspired by this, in this paper we conducted the first exploration on large vision-language models (LVLMs) and found that LVLMs indeed are susceptible to hallucinations and various errors when facing specific semantic concepts in images. To efficiently search for these sensitive concepts, we integrated large language models (LLMs) and text-to-image (T2I) models to propose a novel semantic evolution framework. Randomly initialized semantic concepts undergo LLM-based crossover and mutation operations to form image descriptions, which are then converted by T2I models into visual inputs for LVLMs. The task-specific performance of LVLMs on each input is quantified as fitness scores for the involved semantics and serves as reward signals to further guide LLMs in exploring concepts that induce LVLMs. Extensive experiments on seven mainstream LVLMs and two multimodal tasks demonstrate the effectiveness of our method. Additionally, we provide interesting findings about the sensitive semantics of LVLMs, aiming to inspire further in-depth research.

摘要: 对抗性攻击旨在生成误导深度模型的恶意输入，但除了导致模型失败之外，它们无法提供某些可解释的信息，例如'\textit{输入中的哪些内容使模型更有可能失败？}”“然而，这些信息对于研究人员专门提高模型稳健性至关重要。最近的研究表明，模型可能对视觉输入中的某些语义（例如“湿”、“雾”）特别敏感，这使得它们容易出错。受此启发，本文对大型视觉语言模型（LVLM）进行了首次探索，发现LVLM在面对图像中的特定语义概念时确实容易产生幻觉和各种错误。为了有效地搜索这些敏感概念，我们集成了大型语言模型（LLM）和文本到图像（T2 I）模型，提出了一种新颖的语义进化框架。随机初始化的语义概念经过基于LLM的交叉和变异操作以形成图像描述，然后由T2 I模型将其转换为LVLM的视觉输入。LVLM在每个输入上的特定任务性能被量化为所涉及语义的适应度分数，并作为奖励信号，以进一步指导LLM探索引发LVLM的概念。对七种主流LVLM和两种多模式任务的广泛实验证明了我们方法的有效性。此外，我们还提供了有关LVLM敏感语义的有趣发现，旨在激发进一步的深入研究。



## **12. From Words to Collisions: LLM-Guided Evaluation and Adversarial Generation of Safety-Critical Driving Scenarios**

从言语到碰撞：法学硕士指导的评估和安全关键驾驶场景的对抗生成 cs.AI

New version of the paper

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2502.02145v3) [paper-pdf](http://arxiv.org/pdf/2502.02145v3)

**Authors**: Yuan Gao, Mattia Piccinini, Korbinian Moller, Amr Alanwar, Johannes Betz

**Abstract**: Ensuring the safety of autonomous vehicles requires virtual scenario-based testing, which depends on the robust evaluation and generation of safety-critical scenarios. So far, researchers have used scenario-based testing frameworks that rely heavily on handcrafted scenarios as safety metrics. To reduce the effort of human interpretation and overcome the limited scalability of these approaches, we combine Large Language Models (LLMs) with structured scenario parsing and prompt engineering to automatically evaluate and generate safety-critical driving scenarios. We introduce Cartesian and Ego-centric prompt strategies for scenario evaluation, and an adversarial generation module that modifies trajectories of risk-inducing vehicles (ego-attackers) to create critical scenarios. We validate our approach using a 2D simulation framework and multiple pre-trained LLMs. The results show that the evaluation module effectively detects collision scenarios and infers scenario safety. Meanwhile, the new generation module identifies high-risk agents and synthesizes realistic, safety-critical scenarios. We conclude that an LLM equipped with domain-informed prompting techniques can effectively evaluate and generate safety-critical driving scenarios, reducing dependence on handcrafted metrics. We release our open-source code and scenarios at: https://github.com/TUM-AVS/From-Words-to-Collisions.

摘要: 确保自动驾驶汽车的安全需要基于虚拟环境的测试，这取决于安全关键场景的稳健评估和生成。到目前为止，研究人员已经使用基于情景的测试框架，这些框架严重依赖手工制作的场景作为安全指标。为了减少人类解释的工作量并克服这些方法的有限可扩展性，我们将大型语言模型（LLM）与结构化场景解析相结合，并提示工程技术自动评估和生成对安全至关重要的驾驶场景。我们引入了用于场景评估的Cartesian和以自我为中心的提示策略，以及一个对抗生成模块，该模块修改风险诱导车辆（自我攻击者）的轨迹以创建关键场景。我们使用2D仿真框架和多个预先训练的LLM来验证我们的方法。结果表明，该评估模块能够有效地检测碰撞场景，并推断出场景安全性.与此同时，新一代模块识别高风险代理并综合现实的安全关键场景。我们的结论是，LLM配备域知情的提示技术可以有效地评估和生成安全关键的驾驶场景，减少依赖手工制作的指标。我们在https://github.com/TUM-AVS/From-Words-to-Collisions上发布我们的开源代码和场景。



## **13. Reliable Disentanglement Multi-view Learning Against View Adversarial Attacks**

可靠的解纠缠多视图学习对抗视图对抗攻击 cs.LG

11 pages, 11 figures, accepted by IJCAI 2025

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2505.04046v2) [paper-pdf](http://arxiv.org/pdf/2505.04046v2)

**Authors**: Xuyang Wang, Siyuan Duan, Qizhi Li, Guiduo Duan, Yuan Sun, Dezhong Peng

**Abstract**: Trustworthy multi-view learning has attracted extensive attention because evidence learning can provide reliable uncertainty estimation to enhance the credibility of multi-view predictions. Existing trusted multi-view learning methods implicitly assume that multi-view data is secure. However, in safety-sensitive applications such as autonomous driving and security monitoring, multi-view data often faces threats from adversarial perturbations, thereby deceiving or disrupting multi-view models. This inevitably leads to the adversarial unreliability problem (AUP) in trusted multi-view learning. To overcome this tricky problem, we propose a novel multi-view learning framework, namely Reliable Disentanglement Multi-view Learning (RDML). Specifically, we first propose evidential disentanglement learning to decompose each view into clean and adversarial parts under the guidance of corresponding evidences, which is extracted by a pretrained evidence extractor. Then, we employ the feature recalibration module to mitigate the negative impact of adversarial perturbations and extract potential informative features from them. Finally, to further ignore the irreparable adversarial interferences, a view-level evidential attention mechanism is designed. Extensive experiments on multi-view classification tasks with adversarial attacks show that RDML outperforms the state-of-the-art methods by a relatively large margin. Our code is available at https://github.com/Willy1005/2025-IJCAI-RDML.

摘要: 值得信赖的多视图学习引起了广泛关注，因为证据学习可以提供可靠的不确定性估计，以增强多视图预测的可信度。现有的可信多视图学习方法隐含地假设多视图数据是安全的。然而，在自动驾驶和安全监控等安全敏感应用中，多视图数据经常面临来自敌对扰动的威胁，从而欺骗或破坏多视图模型。这不可避免地会导致可信多视图学习中的对抗不可靠性问题（AUP）。为了克服这个棘手的问题，我们提出了一种新颖的多视图学习框架，即可靠解纠缠多视图学习（RDML）。具体来说，我们首先提出证据解纠缠学习，在相应证据的指导下将每个视图分解为干净且对抗的部分，这些证据由预先训练的证据提取器提取。然后，我们使用特征重新校准模块来减轻对抗性扰动的负面影响，并从中提取潜在的信息特征。最后，为了进一步忽略不可挽回的对抗干扰，设计了视角级证据关注机制。针对具有对抗性攻击的多视图分类任务的大量实验表明，RDML的性能优于最先进的方法，相对较大。我们的代码可在https://github.com/Willy1005/2025-IJCAI-RDML上获取。



## **14. Few-Shot Adversarial Low-Rank Fine-Tuning of Vision-Language Models**

视觉语言模型的少镜头对抗低级微调 cs.LG

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2505.15130v1) [paper-pdf](http://arxiv.org/pdf/2505.15130v1)

**Authors**: Sajjad Ghiasvand, Haniyeh Ehsani Oskouie, Mahnoosh Alizadeh, Ramtin Pedarsani

**Abstract**: Vision-Language Models (VLMs) such as CLIP have shown remarkable performance in cross-modal tasks through large-scale contrastive pre-training. To adapt these large transformer-based models efficiently for downstream tasks, Parameter-Efficient Fine-Tuning (PEFT) techniques like LoRA have emerged as scalable alternatives to full fine-tuning, especially in few-shot scenarios. However, like traditional deep neural networks, VLMs are highly vulnerable to adversarial attacks, where imperceptible perturbations can significantly degrade model performance. Adversarial training remains the most effective strategy for improving model robustness in PEFT. In this work, we propose AdvCLIP-LoRA, the first algorithm designed to enhance the adversarial robustness of CLIP models fine-tuned with LoRA in few-shot settings. Our method formulates adversarial fine-tuning as a minimax optimization problem and provides theoretical guarantees for convergence under smoothness and nonconvex-strong-concavity assumptions. Empirical results across eight datasets using ViT-B/16 and ViT-B/32 models show that AdvCLIP-LoRA significantly improves robustness against common adversarial attacks (e.g., FGSM, PGD), without sacrificing much clean accuracy. These findings highlight AdvCLIP-LoRA as a practical and theoretically grounded approach for robust adaptation of VLMs in resource-constrained settings.

摘要: 通过大规模对比预训练，CLIP等视觉语言模型（VLM）在跨模式任务中表现出了出色的表现。为了有效地调整这些基于变压器的大型模型以适应下游任务，LoRA等参数高效微调（PEFT）技术已成为完全微调的可扩展替代方案，尤其是在少量场景中。然而，与传统的深度神经网络一样，VLM非常容易受到对抗攻击，其中不可感知的扰动可能会显着降低模型性能。对抗训练仍然是提高PEFT模型稳健性的最有效策略。在这项工作中，我们提出了AdvCLIP-LoRA，这是第一个旨在增强在少数镜头设置中使用LoRA微调的CLIP模型的对抗鲁棒性的算法。我们的方法将对抗性微调表述为极小极大优化问题，并为光滑性和非凸强插值假设下的收敛提供理论保证。使用ViT-B/16和ViT-B/32模型的八个数据集的经验结果表明，AdvCLIP-LoRA显着提高了针对常见对抗攻击（例如，FGSM、PVD），而不会牺牲太多干净的准确性。这些发现凸显了AdvCLIP-LoRA是一种实用且具有理论依据的方法，用于在资源有限的环境中稳健地适应VLM。



## **15. A Survey On Secure Machine Learning**

关于安全机器学习的调查 cs.CR

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2505.15124v1) [paper-pdf](http://arxiv.org/pdf/2505.15124v1)

**Authors**: Taobo Liao, Taoran Li, Prathamesh Nadkarni

**Abstract**: In this survey, we will explore the interaction between secure multiparty computation and the area of machine learning. Recent advances in secure multiparty computation (MPC) have significantly improved its applicability in the realm of machine learning (ML), offering robust solutions for privacy-preserving collaborative learning. This review explores key contributions that leverage MPC to enable multiple parties to engage in ML tasks without compromising the privacy of their data. The integration of MPC with ML frameworks facilitates the training and evaluation of models on combined datasets from various sources, ensuring that sensitive information remains encrypted throughout the process. Innovations such as specialized software frameworks and domain-specific languages streamline the adoption of MPC in ML, optimizing performance and broadening its usage. These frameworks address both semi-honest and malicious threat models, incorporating features such as automated optimizations and cryptographic auditing to ensure compliance and data integrity. The collective insights from these studies highlight MPC's potential in fostering collaborative yet confidential data analysis, marking a significant stride towards the realization of secure and efficient computational solutions in privacy-sensitive industries. This paper investigates a spectrum of SecureML libraries that includes cryptographic protocols, federated learning frameworks, and privacy-preserving algorithms. By surveying the existing literature, this paper aims to examine the efficacy of these libraries in preserving data privacy, ensuring model confidentiality, and fortifying ML systems against adversarial attacks. Additionally, the study explores an innovative application domain for SecureML techniques: the integration of these methodologies in gaming environments utilizing ML.

摘要: 在本次调查中，我们将探索安全多方计算与机器学习领域之间的互动。安全多方计算（MPC）的最新进展显着提高了其在机器学习（ML）领域的适用性，为保护隐私的协作学习提供了稳健的解决方案。本评论探讨了利用MPC使多方能够在不损害其数据隐私的情况下参与ML任务的关键贡献。MPC与ML框架的集成促进了对来自不同来源的组合数据集进行模型的训练和评估，确保敏感信息在整个过程中保持加密。专业软件框架和特定领域语言等创新简化了ML中MPC的采用，优化了性能并扩大了其用途。这些框架可解决半诚实和恶意威胁模型，并结合自动优化和加密审计等功能，以确保合规性和数据完整性。这些研究的集体见解凸显了MPC在促进协作但保密的数据分析方面的潜力，标志着在隐私敏感行业中实现安全高效的计算解决方案方面迈出了重要一步。本文研究了一系列SecureML库，包括加密协议、联邦学习框架和隐私保护算法。通过调查现有文献，本文旨在检查这些库在保护数据隐私、确保模型机密性以及加强ML系统抵御对抗攻击方面的功效。此外，该研究还探索了SecureML技术的创新应用领域：在利用ML的游戏环境中集成这些方法。



## **16. AudioJailbreak: Jailbreak Attacks against End-to-End Large Audio-Language Models**

AudioJailbreak：针对端到端大型音频语言模型的越狱攻击 cs.CR

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2505.14103v2) [paper-pdf](http://arxiv.org/pdf/2505.14103v2)

**Authors**: Guangke Chen, Fu Song, Zhe Zhao, Xiaojun Jia, Yang Liu, Yanchen Qiao, Weizhe Zhang

**Abstract**: Jailbreak attacks to Large audio-language models (LALMs) are studied recently, but they achieve suboptimal effectiveness, applicability, and practicability, particularly, assuming that the adversary can fully manipulate user prompts. In this work, we first conduct an extensive experiment showing that advanced text jailbreak attacks cannot be easily ported to end-to-end LALMs via text-to speech (TTS) techniques. We then propose AudioJailbreak, a novel audio jailbreak attack, featuring (1) asynchrony: the jailbreak audio does not need to align with user prompts in the time axis by crafting suffixal jailbreak audios; (2) universality: a single jailbreak perturbation is effective for different prompts by incorporating multiple prompts into perturbation generation; (3) stealthiness: the malicious intent of jailbreak audios will not raise the awareness of victims by proposing various intent concealment strategies; and (4) over-the-air robustness: the jailbreak audios remain effective when being played over the air by incorporating the reverberation distortion effect with room impulse response into the generation of the perturbations. In contrast, all prior audio jailbreak attacks cannot offer asynchrony, universality, stealthiness, or over-the-air robustness. Moreover, AudioJailbreak is also applicable to the adversary who cannot fully manipulate user prompts, thus has a much broader attack scenario. Extensive experiments with thus far the most LALMs demonstrate the high effectiveness of AudioJailbreak. We highlight that our work peeks into the security implications of audio jailbreak attacks against LALMs, and realistically fosters improving their security robustness. The implementation and audio samples are available at our website https://audiojailbreak.github.io/AudioJailbreak.

摘要: 最近研究了对大型音频语言模型（LALM）的越狱攻击，但它们的有效性、适用性和实用性达到了次优，特别是假设对手可以完全操纵用户提示。在这项工作中，我们首先进行了一项广泛的实验，表明高级文本越狱攻击无法通过文本转语音（TTC）技术轻松移植到端到端LALM。然后，我们提出AudioJailbreak，一种新颖的音频越狱攻击，其特点是：（1）狡猾：越狱音频不需要通过制作后缀的越狱音频在时间轴上与用户提示对齐;（2）通用性：通过将多个提示合并到扰动生成中，单个越狱扰动对不同的提示有效;（3）隐蔽性：越狱音频的恶意意图不会通过提出各种意图隐藏策略来提高受害者的意识;以及（4）空中鲁棒性：越狱音频在空中播放时仍然有效，通过将回响失真效应与房间脉冲响应结合起来扰动的产生。相比之下，所有先前的音频越狱攻击都无法提供灵活性、普遍性、隐蔽性或空中鲁棒性。此外，AudioJailbreak还适用于无法完全操纵用户提示的对手，因此具有更广泛的攻击场景。迄今为止，对大多数LALM的广泛实验证明了AudioJailbreak的高有效性。我们强调，我们的工作探讨了针对LALM的音频越狱攻击的安全影响，并切实促进了其安全稳健性的提高。实现和音频示例可在我们的网站https://audiojailbreak.github.io/AudioJailbreak上获取。



## **17. MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety**

MrGuard：通用LLM安全的多语言推理保障 cs.CL

Preprint

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2504.15241v2) [paper-pdf](http://arxiv.org/pdf/2504.15241v2)

**Authors**: Yahan Yang, Soham Dan, Shuo Li, Dan Roth, Insup Lee

**Abstract**: Large Language Models (LLMs) are susceptible to adversarial attacks such as jailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability is exacerbated in multilingual settings, where multilingual safety-aligned data is often limited. Thus, developing a guardrail capable of detecting and filtering unsafe content across diverse languages is critical for deploying LLMs in real-world applications. In this work, we introduce a multilingual guardrail with reasoning for prompt classification. Our method consists of: (1) synthetic multilingual data generation incorporating culturally and linguistically nuanced variants, (2) supervised fine-tuning, and (3) a curriculum-based Group Relative Policy Optimization (GRPO) framework that further improves performance. Experimental results demonstrate that our multilingual guardrail, MrGuard, consistently outperforms recent baselines across both in-domain and out-of-domain languages by more than 15%. We also evaluate MrGuard's robustness to multilingual variations, such as code-switching and low-resource language distractors in the prompt, and demonstrate that it preserves safety judgments under these challenging conditions. The multilingual reasoning capability of our guardrail enables it to generate explanations, which are particularly useful for understanding language-specific risks and ambiguities in multilingual content moderation.

摘要: 大型语言模型（LLM）容易受到诸如越狱之类的对抗性攻击，这可能会引发有害或不安全的行为。这种漏洞在多语言环境中会加剧，其中多语言安全一致的数据通常是有限的。因此，开发一个能够检测和过滤不同语言的不安全内容的护栏对于在现实世界的应用程序中部署LLM至关重要。在这项工作中，我们介绍了一种多语言护栏，具有快速分类的推理。我们的方法包括：（1）综合多语言数据生成，融合了文化和语言上的细微差别，（2）监督式微调，以及（3）进一步提高性能的基于课程的组相对政策优化（GRPO）框架。实验结果表明，我们的多语言护栏MrGuard在域内和域外语言中的表现始终优于最近的基线15%以上。我们还评估了MrGuard对多语言变体（例如提示中的代码切换和低资源语言干扰因素）的稳健性，并证明它在这些具有挑战性的条件下保留了安全判断。我们护栏的多语言推理能力使其能够生成解释，这对于理解多语言内容审核中的特定语言风险和歧义特别有用。



## **18. Lessons from Defending Gemini Against Indirect Prompt Injections**

预防双子座间接即时注射的教训 cs.CR

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2505.14534v1) [paper-pdf](http://arxiv.org/pdf/2505.14534v1)

**Authors**: Chongyang Shi, Sharon Lin, Shuang Song, Jamie Hayes, Ilia Shumailov, Itay Yona, Juliette Pluto, Aneesh Pappu, Christopher A. Choquette-Choo, Milad Nasr, Chawin Sitawarin, Gena Gibson, Andreas Terzis, John "Four" Flynn

**Abstract**: Gemini is increasingly used to perform tasks on behalf of users, where function-calling and tool-use capabilities enable the model to access user data. Some tools, however, require access to untrusted data introducing risk. Adversaries can embed malicious instructions in untrusted data which cause the model to deviate from the user's expectations and mishandle their data or permissions. In this report, we set out Google DeepMind's approach to evaluating the adversarial robustness of Gemini models and describe the main lessons learned from the process. We test how Gemini performs against a sophisticated adversary through an adversarial evaluation framework, which deploys a suite of adaptive attack techniques to run continuously against past, current, and future versions of Gemini. We describe how these ongoing evaluations directly help make Gemini more resilient against manipulation.

摘要: Gemini越来越多地用于代表用户执行任务，其中功能调用和工具使用功能使模型能够访问用户数据。然而，有些工具需要访问不受信任的数据，从而带来风险。对手可能会在不受信任的数据中嵌入恶意指令，导致模型偏离用户的期望并不当处理他们的数据或权限。在本报告中，我们阐述了Google DeepMind评估Gemini模型对抗稳健性的方法，并描述了从该过程中吸取的主要教训。我们通过对抗性评估框架测试Gemini如何对抗复杂对手，该框架部署了一套自适应攻击技术，以针对Gemini的过去、当前和未来版本持续运行。我们描述了这些正在进行的评估如何直接帮助双子座对操纵更具弹性。



## **19. Adverseness vs. Equilibrium: Exploring Graph Adversarial Resilience through Dynamic Equilibrium**

紧张与平衡：通过动态平衡探索图的对抗弹性 cs.LG

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2505.14463v1) [paper-pdf](http://arxiv.org/pdf/2505.14463v1)

**Authors**: Xinxin Fan, Wenxiong Chen, Mengfan Li, Wenqi Wei, Ling Liu

**Abstract**: Adversarial attacks to graph analytics are gaining increased attention. To date, two lines of countermeasures have been proposed to resist various graph adversarial attacks from the perspectives of either graph per se or graph neural networks. Nevertheless, a fundamental question lies in whether there exists an intrinsic adversarial resilience state within a graph regime and how to find out such a critical state if exists. This paper contributes to tackle the above research questions from three unique perspectives: i) we regard the process of adversarial learning on graph as a complex multi-object dynamic system, and model the behavior of adversarial attack; ii) we propose a generalized theoretical framework to show the existence of critical adversarial resilience state; and iii) we develop a condensed one-dimensional function to capture the dynamic variation of graph regime under perturbations, and pinpoint the critical state through solving the equilibrium point of dynamic system. Multi-facet experiments are conducted to show our proposed approach can significantly outperform the state-of-the-art defense methods under five commonly-used real-world datasets and three representative attacks.

摘要: 对图形分析的对抗攻击越来越受到关注。迄今为止，已经从图本身或图神经网络的角度提出了两系列对策来抵抗各种图对抗攻击。然而，一个根本问题在于，图机制内是否存在固有的对抗弹性状态，以及如何找出这样的临界状态（如果存在）。本文从三个独特的角度探讨了上述研究问题：i）我们将图上的对抗性学习过程视为一个复杂的多对象动态系统，并对对抗性攻击的行为进行建模; ii）我们提出了一个广义的理论框架来表明关键对抗弹性状态的存在;以及iii）我们开发了一个压缩的一维函数来捕捉图形区域在扰动下的动态变化，并通过求解动力系统的平衡点来确定临界状态。进行了多方面实验，表明我们提出的方法在五个常用的现实世界数据集和三种代表性攻击下可以显着优于最先进的防御方法。



## **20. Vulnerability of Transfer-Learned Neural Networks to Data Reconstruction Attacks in Small-Data Regime**

小数据环境下迁移学习神经网络的数据重构脆弱性 cs.CR

27 pages

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2505.14323v1) [paper-pdf](http://arxiv.org/pdf/2505.14323v1)

**Authors**: Tomasz Maciążek, Robert Allison

**Abstract**: Training data reconstruction attacks enable adversaries to recover portions of a released model's training data. We consider the attacks where a reconstructor neural network learns to invert the (random) mapping between training data and model weights. Prior work has shown that an informed adversary with access to released model's weights and all but one training data point can achieve high-quality reconstructions in this way. However, differential privacy can defend against such an attack with little to no loss in model's utility when the amount of training data is sufficiently large. In this work we consider a more realistic adversary who only knows the distribution from which a small training dataset has been sampled and who attacks a transfer-learned neural network classifier that has been trained on this dataset. We exhibit an attack that works in this realistic threat model and demonstrate that in the small-data regime it cannot be defended against by DP-SGD without severely damaging the classifier accuracy. This raises significant concerns about the use of such transfer-learned classifiers when protection of training-data is paramount. We demonstrate the effectiveness and robustness of our attack on VGG, EfficientNet and ResNet image classifiers transfer-learned on MNIST, CIFAR-10 and CelebA respectively. Additionally, we point out that the commonly used (true-positive) reconstruction success rate metric fails to reliably quantify the actual reconstruction effectiveness. Instead, we make use of the Neyman-Pearson lemma to construct the receiver operating characteristic curve and consider the associated true-positive reconstruction rate at a fixed level of the false-positive reconstruction rate.

摘要: 训练数据重建攻击使对手能够恢复已发布模型的部分训练数据。我们考虑了重建器神经网络学习倒置训练数据和模型权重之间的（随机）映射的攻击。之前的工作表明，能够访问已发布模型的权重和除一个训练数据点之外的所有训练数据点的知情对手可以通过这种方式实现高质量的重建。然而，当训练数据量足够大时，差异隐私可以抵御此类攻击，而模型的效用几乎没有损失。在这项工作中，我们考虑了一个更现实的对手，他只知道对小训练数据集进行抽样的分布，并攻击在此数据集上训练的转移学习神经网络分类器。我们展示了一种适用于这个现实威胁模型的攻击，并证明在小数据机制中，DP-BCD无法在不严重损害分类器准确性的情况下抵御它。当训练数据的保护至关重要时，这引发了人们对此类转移学习分类器的使用的严重担忧。我们证明了我们对分别在MNIST、CIFAR-10和CelebA上转移学习的VGG、EfficientNet和ResNet图像分类器的攻击的有效性和鲁棒性。此外，我们指出，常用的（真阳性）重建成功率指标无法可靠地量化实际重建的有效性。相反，我们利用内曼-皮尔逊引理来构建接收器工作特征曲线，并考虑在假阳性重建率的固定水平下的相关真阳性重建率。



## **21. Exploring Jailbreak Attacks on LLMs through Intent Concealment and Diversion**

探索通过意图隐瞒和转移对LLM的越狱攻击 cs.CR

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2505.14316v1) [paper-pdf](http://arxiv.org/pdf/2505.14316v1)

**Authors**: Tiehan Cui, Yanxu Mao, Peipei Liu, Congying Liu, Datao You

**Abstract**: Although large language models (LLMs) have achieved remarkable advancements, their security remains a pressing concern. One major threat is jailbreak attacks, where adversarial prompts bypass model safeguards to generate harmful or objectionable content. Researchers study jailbreak attacks to understand security and robustness of LLMs. However, existing jailbreak attack methods face two main challenges: (1) an excessive number of iterative queries, and (2) poor generalization across models. In addition, recent jailbreak evaluation datasets focus primarily on question-answering scenarios, lacking attention to text generation tasks that require accurate regeneration of toxic content. To tackle these challenges, we propose two contributions: (1) ICE, a novel black-box jailbreak method that employs Intent Concealment and divErsion to effectively circumvent security constraints. ICE achieves high attack success rates (ASR) with a single query, significantly improving efficiency and transferability across different models. (2) BiSceneEval, a comprehensive dataset designed for assessing LLM robustness in question-answering and text-generation tasks. Experimental results demonstrate that ICE outperforms existing jailbreak techniques, revealing critical vulnerabilities in current defense mechanisms. Our findings underscore the necessity of a hybrid security strategy that integrates predefined security mechanisms with real-time semantic decomposition to enhance the security of LLMs.

摘要: 尽管大型语言模型（LLM）取得了显着的进步，但其安全性仍然是一个紧迫的问题。一个主要威胁是越狱攻击，其中敌对性会促使绕过模型保护措施来生成有害或令人反感的内容。研究人员研究越狱攻击以了解LLM的安全性和稳健性。然而，现有的越狱攻击方法面临两个主要挑战：（1）迭代查询数量过多，（2）模型之间的概括性较差。此外，最近的越狱评估数据集主要关注问答场景，缺乏对需要准确再生有毒内容的文本生成任务的关注。为了应对这些挑战，我们提出了两个贡献：（1）ICE，一种新的黑盒越狱方法，采用意图隐藏和分裂，以有效地规避安全约束。ICE通过单个查询实现了高攻击成功率（ASR），显著提高了效率和跨不同模型的可移植性。(2)BiSceneEval是一个综合数据集，旨在评估LLM在问答和文本生成任务中的鲁棒性。实验结果表明，ICE优于现有的越狱技术，揭示了当前防御机制中的关键漏洞。我们的研究结果强调了混合安全策略的必要性，该策略将预定义的安全机制与实时语义分解集成在一起，以增强LLM的安全性。



## **22. EVA: Red-Teaming GUI Agents via Evolving Indirect Prompt Injection**

伊娃：通过不断发展的间接提示注入进行红色团队化图形用户界面代理 cs.AI

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2505.14289v1) [paper-pdf](http://arxiv.org/pdf/2505.14289v1)

**Authors**: Yijie Lu, Tianjie Ju, Manman Zhao, Xinbei Ma, Yuan Guo, ZhuoSheng Zhang

**Abstract**: As multimodal agents are increasingly trained to operate graphical user interfaces (GUIs) to complete user tasks, they face a growing threat from indirect prompt injection, attacks in which misleading instructions are embedded into the agent's visual environment, such as popups or chat messages, and misinterpreted as part of the intended task. A typical example is environmental injection, in which GUI elements are manipulated to influence agent behavior without directly modifying the user prompt. To address these emerging attacks, we propose EVA, a red teaming framework for indirect prompt injection which transforms the attack into a closed loop optimization by continuously monitoring an agent's attention distribution over the GUI and updating adversarial cues, keywords, phrasing, and layout, in response. Compared with prior one shot methods that generate fixed prompts without regard for how the model allocates visual attention, EVA dynamically adapts to emerging attention hotspots, yielding substantially higher attack success rates and far greater transferability across diverse GUI scenarios. We evaluate EVA on six widely used generalist and specialist GUI agents in realistic settings such as popup manipulation, chat based phishing, payments, and email composition. Experimental results show that EVA substantially improves success rates over static baselines. Under goal agnostic constraints, where the attacker does not know the agent's task intent, EVA still discovers effective patterns. Notably, we find that injection styles transfer well across models, revealing shared behavioral biases in GUI agents. These results suggest that evolving indirect prompt injection is a powerful tool not only for red teaming agents, but also for uncovering common vulnerabilities in their multimodal decision making.

摘要: 随着多模式代理越来越多地接受操作图形用户界面（GUIs）来完成用户任务的培训，它们面临着来自间接提示注入的越来越大的威胁，即将误导性指令嵌入到代理的视觉环境中的攻击，例如弹出窗口或聊天消息，并被误解为预期任务的一部分。一个典型的例子是环境注入，其中操纵图形用户界面元素来影响代理行为，而无需直接修改用户提示。为了解决这些新出现的攻击，我们提出了伊娃，这是一个用于间接提示注入的红色团队框架，它通过持续监视代理在图形用户界面上的注意力分布并更新对抗线索、关键词、措辞和布局来将攻击转化为闭环优化。作为响应。与之前的一次性方法（无需考虑模型如何分配视觉注意力）相比，伊娃动态适应新出现的注意力热点，从而产生更高的攻击成功率和更大的跨不同图形用户界面场景的可移植性。我们在现实环境中对六个广泛使用的通才和专业图形用户界面代理进行了评估，例如弹出窗口操作、基于聊天的网络钓鱼、支付和电子邮件合成。实验结果表明，与静态基线相比，伊娃大幅提高了成功率。在目标不可知约束下，攻击者不知道代理的任务意图，伊娃仍然会发现有效的模式。值得注意的是，我们发现注入风格在模型之间很好地转移，揭示了图形用户界面代理中的共同行为偏差。这些结果表明，不断发展的间接提示注入不仅是红色团队代理的强大工具，而且还可以发现其多模式决策中的常见漏洞。



## **23. Universal Acoustic Adversarial Attacks for Flexible Control of Speech-LLMs**

语音灵活控制的通用声学对抗攻击-LLM cs.CL

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2505.14286v1) [paper-pdf](http://arxiv.org/pdf/2505.14286v1)

**Authors**: Rao Ma, Mengjie Qian, Vyas Raina, Mark Gales, Kate Knill

**Abstract**: The combination of pre-trained speech encoders with large language models has enabled the development of speech LLMs that can handle a wide range of spoken language processing tasks. While these models are powerful and flexible, this very flexibility may make them more vulnerable to adversarial attacks. To examine the extent of this problem, in this work we investigate universal acoustic adversarial attacks on speech LLMs. Here a fixed, universal, adversarial audio segment is prepended to the original input audio. We initially investigate attacks that cause the model to either produce no output or to perform a modified task overriding the original prompt. We then extend the nature of the attack to be selective so that it activates only when specific input attributes, such as a speaker gender or spoken language, are present. Inputs without the targeted attribute should be unaffected, allowing fine-grained control over the model outputs. Our findings reveal critical vulnerabilities in Qwen2-Audio and Granite-Speech and suggest that similar speech LLMs may be susceptible to universal adversarial attacks. This highlights the need for more robust training strategies and improved resistance to adversarial attacks.

摘要: 预训练的语音编码器与大型语言模型的结合使得能够开发出可以处理广泛口语处理任务的语音LLM。虽然这些模型强大且灵活，但这种灵活性可能使它们更容易受到对抗攻击。为了研究这个问题的严重程度，在这项工作中，我们研究了对语音LLM的普遍声学对抗攻击。这里，固定的、通用的、对抗性的音频段被预先添加到原始输入音频上。我们最初调查导致模型不产生输出或执行覆盖原始提示的修改任务的攻击。然后，我们将攻击的性质扩展为选择性，以便只有在特定输入属性（例如说话者性别或口语）存在时，它才会激活。没有目标属性的输入应该不受影响，允许对模型输出进行细粒度控制。我们的研究结果揭示了Qwen 2-Audio和Granite-Speech中的关键漏洞，并表明类似的语音LLM可能容易受到普遍对抗攻击。这凸显了需要更强大的训练策略和提高对对抗性攻击的抵抗力。



## **24. IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems**

针对基于LLM的多代理系统的IP泄露攻击 cs.CR

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2505.12442v2) [paper-pdf](http://arxiv.org/pdf/2505.12442v2)

**Authors**: Liwen Wang, Wenxuan Wang, Shuai Wang, Zongjie Li, Zhenlan Ji, Zongyi Lyu, Daoyuan Wu, Shing-Chi Cheung

**Abstract**: The rapid advancement of Large Language Models (LLMs) has led to the emergence of Multi-Agent Systems (MAS) to perform complex tasks through collaboration. However, the intricate nature of MAS, including their architecture and agent interactions, raises significant concerns regarding intellectual property (IP) protection. In this paper, we introduce MASLEAK, a novel attack framework designed to extract sensitive information from MAS applications. MASLEAK targets a practical, black-box setting, where the adversary has no prior knowledge of the MAS architecture or agent configurations. The adversary can only interact with the MAS through its public API, submitting attack query $q$ and observing outputs from the final agent. Inspired by how computer worms propagate and infect vulnerable network hosts, MASLEAK carefully crafts adversarial query $q$ to elicit, propagate, and retain responses from each MAS agent that reveal a full set of proprietary components, including the number of agents, system topology, system prompts, task instructions, and tool usages. We construct the first synthetic dataset of MAS applications with 810 applications and also evaluate MASLEAK against real-world MAS applications, including Coze and CrewAI. MASLEAK achieves high accuracy in extracting MAS IP, with an average attack success rate of 87% for system prompts and task instructions, and 92% for system architecture in most cases. We conclude by discussing the implications of our findings and the potential defenses.

摘要: 大型语言模型（LLM）的快速发展导致了通过协作执行复杂任务的多智能体系统（MAS）的出现。然而，MAS的复杂性质，包括其架构和代理交互，引发了有关知识产权（IP）保护的严重担忧。本文介绍MASLEAK，这是一种新型攻击框架，旨在从MAS应用程序中提取敏感信息。MASLEAK针对的是实用的黑匣子设置，其中对手不了解MAS架构或代理配置。对手只能通过其公共API与MAS交互，提交攻击查询$q$并观察最终代理的输出。受计算机蠕虫传播和感染脆弱网络主机的方式的启发，MASLEAK精心设计了对抗性查询$q$，以引发、传播和保留每个MAS代理的响应，这些响应揭示了全套专有组件，包括代理数量、系统布局、系统提示、任务指令和工具使用。我们构建了包含810个应用程序的第一个MAS应用程序合成数据集，并根据现实世界的MAS应用程序（包括Coze和CrewAI）评估MASLEAK。MASLEAK在提取MAS IP方面实现了高准确性，系统提示和任务指令的平均攻击成功率为87%，大多数情况下系统架构的平均攻击成功率为92%。最后，我们讨论了我们发现的影响和潜在的防御措施。



## **25. Destabilizing Power Grid and Energy Market by Cyberattacks on Smart Inverters**

智能逆变器网络攻击破坏电网和能源市场的稳定 cs.CR

Extended version of the conference paper in ACM eEnergy 2025

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2505.14175v1) [paper-pdf](http://arxiv.org/pdf/2505.14175v1)

**Authors**: Xiangyu Hui, Samuel Karumba, Sid Chi-Kin Chau, Mohiuddin Ahmed

**Abstract**: Cyberattacks on smart inverters and distributed PV are becoming an imminent threat, because of the recent well-documented vulnerabilities and attack incidents. Particularly, the long lifespan of inverter devices, users' oblivion of cybersecurity compliance, and the lack of cyber regulatory frameworks exacerbate the prospect of cyberattacks on smart inverters. As a result, this raises a question -- "do cyberattacks on smart inverters, if orchestrated on a large scale, pose a genuine threat of wide-scale instability to the power grid and energy market"? This paper provides a realistic assessment on the plausibility and impacts of wide-scale power instability caused by cyberattacks on smart inverters. We conduct an in-depth study based on the electricity market data of Australia and the knowledge of practical contingency mechanisms. Our key findings reveal: (1) Despite the possibility of disruption to the grid by cyberattacks on smart inverters, the impact is only significant under careful planning and orchestration. (2) While the grid can assure certain power system security to survive inadvertent contingency events, it is insufficient to defend against savvy attackers who can orchestrate attacks in an adversarial manner. Our data analysis of Australia's electricity grid also reveals that a relatively low percentage of distributed PV would be sufficient to launch an impactful concerted attack on the grid. Our study casts insights on robust strategies for defending the grid in the presence of cyberattacks for places with high penetration of distributed PV.

摘要: 由于最近有据可查的漏洞和攻击事件，对智能逆变器和分布式太阳能的网络攻击正成为迫在眉睫的威胁。特别是，逆变器设备的长寿命、用户对网络安全合规的忽视以及网络监管框架的缺乏加剧了智能逆变器遭受网络攻击的可能性。因此，这提出了一个问题--“对智能逆变器的网络攻击如果大规模策划，是否会对电网和能源市场构成大规模不稳定的真正威胁”？本文对智能逆变器网络攻击引起的大规模电力不稳定的可能性和影响进行了现实评估。我们根据澳大利亚电力市场数据和实用应急机制的知识进行深入研究。我们的主要发现揭示了：（1）尽管对智能逆变器的网络攻击可能会扰乱电网，但只有在精心规划和协调的情况下，其影响才是显着的。(2)虽然电网可以确保一定的电力系统安全，以应对无意中的应急事件，但它不足以抵御能够以对抗方式策划攻击的精明攻击者。我们对澳大利亚电网的数据分析还表明，相对较低比例的分布式太阳能足以对电网发起有影响力的协同攻击。我们的研究深入探讨了分布式PV渗透率高的地方在网络攻击的情况下保护电网的稳健策略。



## **26. Elliptic Curve Modulation (ECM) for Extremely Robust Physical Layer Encryption**

椭圆曲线调制（ECM）用于极其稳健的物理层加密 eess.SP

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2505.14153v1) [paper-pdf](http://arxiv.org/pdf/2505.14153v1)

**Authors**: Thrassos K. Oikonomou, George K. Karagiannidis

**Abstract**: This paper introduces Elliptic Curve Modulation (ECM), a novel modulation scheme that can be leveraged to effectively shuffle transmitted data while maintaining symbol error probability (SEP) performance equivalent to unencrypted systems. By utilizing the well-distributed elliptic curve points over the field of large primes, ECM enhances symbol obfuscation, making it a powerful foundation for physical-layer encryption (PLE). Each symbol is mapped from a predefined key while preserving a minimum Euclidean distance constraint, ensuring strong security against adversarial inference without compromising error performance. Building on ECM's strong obfuscation capabilities, we propose ECM with dynamic rotation (ECM-DR) as a practical PLE scheme that achieves near-maximal obfuscation while balancing precomputation complexity. By leveraging a reduced subset of precomputed elliptic curve points and key-based dynamic constellation rotation, ECM-DR ensures that each transmission remains unpredictable, significantly enhancing security compared to traditional PLE schemes without additional computational cost. Security analysis confirms ECM's resilience to brute-force attacks, while numerical results demonstrate its strong obfuscation capabilities. Furthermore, ECM-DR achieves near-maximum information entropy while preserving the SEP performance of unencrypted quadrature amplitude modulation (QAM), offering an extremely robust solution for secure wireless communications.

摘要: 本文介绍了椭圆曲线调制（ECM），一种新的调制方案，可以利用有效地混洗传输的数据，同时保持符号错误概率（SEP）的性能相当于未加密的系统。通过利用大素数域上均匀分布的椭圆曲线点，ECM增强了符号混淆，使其成为物理层加密（PLE）的强大基础。每个符号都从预定义的密钥映射，同时保留最小欧几里得距离约束，确保强大的安全性，以对抗对抗性推理，而不会影响错误性能。ECM的强大的混淆能力的基础上，我们提出ECM与动态旋转（ECM-DR）作为一个实用的PLE计划，实现接近最大的混淆，同时平衡预计算的复杂性。通过利用预先计算的椭圆曲线点的减少子集和基于密钥的动态星座旋转，ECM-DR确保每次传输保持不可预测，与传统PLE方案相比，显着增强了安全性，无需额外计算成本。安全分析证实了EM对暴力攻击的弹性，而数字结果则证明了其强大的混淆能力。此外，ECM-DR还实现了接近最大的信息量，同时保留了未加密的四角幅度调制（Raman）的SDP性能，为安全无线通信提供了极其稳健的解决方案。



## **27. A Channel-Triggered Backdoor Attack on Wireless Semantic Image Reconstruction**

无线语义图像重建中的并行触发后门攻击 cs.CR

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2503.23866v2) [paper-pdf](http://arxiv.org/pdf/2503.23866v2)

**Authors**: Jialin Wan, Jinglong Shen, Nan Cheng, Zhisheng Yin, Yiliang Liu, Wenchao Xu, Xuemin, Shen

**Abstract**: This paper investigates backdoor attacks in image-oriented semantic communications. The threat of backdoor attacks on symbol reconstruction in semantic communication (SemCom) systems has received limited attention. Previous research on backdoor attacks targeting SemCom symbol reconstruction primarily focuses on input-level triggers, which are impractical in scenarios with strict input constraints. In this paper, we propose a novel channel-triggered backdoor attack (CT-BA) framework that exploits inherent wireless channel characteristics as activation triggers. Our key innovation involves utilizing fundamental channel statistics parameters, specifically channel gain with different fading distributions or channel noise with different power, as potential triggers. This approach enhances stealth by eliminating explicit input manipulation, provides flexibility through trigger selection from diverse channel conditions, and enables automatic activation via natural channel variations without adversary intervention. We extensively evaluate CT-BA across four joint source-channel coding (JSCC) communication system architectures and three benchmark datasets. Simulation results demonstrate that our attack achieves near-perfect attack success rate (ASR) while maintaining effective stealth. Finally, we discuss potential defense mechanisms against such attacks.

摘要: 本文研究了面向图像的语义通信中的后门攻击。后门攻击对语义通信（SemCom）系统中符号重建的威胁受到的关注有限。之前对针对SemCom符号重建的后门攻击的研究主要集中在输入级触发器上，这在具有严格输入限制的场景中是不切实际的。在本文中，我们提出了一种新型的通道触发后门攻击（CT-BA）框架，该框架利用固有的无线通道特征作为激活触发器。我们的关键创新涉及利用基本的信道统计参数，特别是具有不同衰落分布的信道增益或具有不同功率的信道噪声，作为潜在的触发。这种方法通过消除显式输入操纵来增强隐身性，通过从不同信道条件中选择触发器来提供灵活性，并且在没有对手干预的情况下通过自然信道变化来实现自动激活。我们广泛评估CT-BA跨四个联合信源信道编码（JSCC）通信系统架构和三个基准数据集。仿真结果表明，我们的攻击达到接近完美的攻击成功率（ASR），同时保持有效的隐身。最后，我们讨论了针对此类攻击的潜在防御机制。



## **28. Memory Assignment for Finite-Memory Strategies in Adversarial Patrolling Games**

对抗性巡逻游戏中随机记忆策略的记忆分配 cs.AI

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2505.14137v1) [paper-pdf](http://arxiv.org/pdf/2505.14137v1)

**Authors**: Vojtěch Kůr, Vít Musil, Vojtěch Řehák

**Abstract**: Adversarial Patrolling games form a subclass of Security games where a Defender moves between locations, guarding vulnerable targets. The main algorithmic problem is constructing a strategy for the Defender that minimizes the worst damage an Attacker can cause. We focus on the class of finite-memory (also known as regular) Defender's strategies that experimentally outperformed other competing classes. A finite-memory strategy can be seen as a positional strategy on a finite set of states. Each state consists of a pair of a location and a certain integer value--called memory. Existing algorithms improve the transitional probabilities between the states but require that the available memory size itself is assigned at each location manually. Choosing the right memory assignment is a well-known open and hard problem that hinders the usability of finite-memory strategies. We solve this issue by developing a general method that iteratively changes the memory assignment. Our algorithm can be used in connection with \emph{any} black-box strategy optimization tool. We evaluate our method on various experiments and show its robustness by solving instances of various patrolling models.

摘要: 对抗性巡逻游戏构成了安全游戏的一个亚类，其中防御者在地点之间移动，守卫脆弱的目标。主要的算法问题是为防御者构建一种策略，以最大限度地减少攻击者可能造成的最严重损害。我们重点关注有限记忆（也称为常规）Defender的策略类别，该策略在实验上优于其他竞争类别。有限记忆策略可以被视为有限状态集上的位置策略。每个状态都由一对位置和某个integer值（称为内存）组成。现有的算法提高了状态之间的过渡概率，但要求在每个位置手动分配可用内存大小本身。选择正确的记忆分配是一个众所周知的开放且困难的问题，它阻碍了有限记忆策略的可用性。我们通过开发一种迭代改变记忆分配的通用方法来解决这个问题。我们的算法可以与任何黑箱策略优化工具结合使用。我们评估我们的方法在各种实验，并通过解决各种巡逻模型的实例显示其鲁棒性。



## **29. Learning atomic forces from uncertainty-calibrated adversarial attacks**

从不确定性校准的对抗攻击中学习原子力 physics.comp-ph

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2502.18314v3) [paper-pdf](http://arxiv.org/pdf/2502.18314v3)

**Authors**: Henrique Musseli Cezar, Tilmann Bodenstein, Henrik Andersen Sveinsson, Morten Ledum, Simen Reine, Sigbjørn Løland Bore

**Abstract**: Adversarial approaches, which intentionally challenge machine learning models by generating difficult examples, are increasingly being adopted to improve machine learning interatomic potentials (MLIPs). While already providing great practical value, little is known about the actual prediction errors of MLIPs on adversarial structures and whether these errors can be controlled. We propose the Calibrated Adversarial Geometry Optimization (CAGO) algorithm to discover adversarial structures with user-assigned errors. Through uncertainty calibration, the estimated uncertainty of MLIPs is unified with real errors. By performing geometry optimization for calibrated uncertainty, we reach adversarial structures with the user-assigned target MLIP prediction error. Integrating with active learning pipelines, we benchmark CAGO, demonstrating stable MLIPs that systematically converge structural, dynamical, and thermodynamical properties for liquid water and water adsorption in a metal-organic framework within only hundreds of training structures, where previously many thousands were typically required.

摘要: 对抗性方法通过生成困难的示例来故意挑战机器学习模型，越来越多地被采用来提高机器学习原子间潜力（MLIP）。虽然已经提供了巨大的实用价值，但人们对MLIP对对抗性结构的实际预测误差以及这些误差是否可以控制知之甚少。我们提出了校准对抗性几何优化（CAGO）算法来发现具有用户指定错误的对抗性结构。通过不确定度校准，将MLIP的估计不确定度与实际误差统一起来。通过对校准的不确定性进行几何优化，我们可以通过用户指定的目标MLIP预测误差来达到对抗结构。我们与主动学习管道相结合，对CAGO进行基准测试，展示了稳定的MLIP，这些MLIP可以在仅数百个训练结构内系统地融合金属有机框架中液态水和水吸收的结构、动态和热力学性质，而以前通常需要数千个训练结构。



## **30. Adversarially Pretrained Transformers may be Universally Robust In-Context Learners**

经过对抗预训练的变形金刚可能是普遍稳健的背景学习者 cs.LG

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2505.14042v1) [paper-pdf](http://arxiv.org/pdf/2505.14042v1)

**Authors**: Soichiro Kumano, Hiroshi Kera, Toshihiko Yamasaki

**Abstract**: Adversarial training is one of the most effective adversarial defenses, but it incurs a high computational cost. In this study, we show that transformers adversarially pretrained on diverse tasks can serve as robust foundation models and eliminate the need for adversarial training in downstream tasks. Specifically, we theoretically demonstrate that through in-context learning, a single adversarially pretrained transformer can robustly generalize to multiple unseen tasks without any additional training, i.e., without any parameter updates. This robustness stems from the model's focus on robust features and its resistance to attacks that exploit non-predictive features. Besides these positive findings, we also identify several limitations. Under certain conditions (though unrealistic), no universally robust single-layer transformers exist. Moreover, robust transformers exhibit an accuracy--robustness trade-off and require a large number of in-context demonstrations. The code is available at https://github.com/s-kumano/universally-robust-in-context-learner.

摘要: 对抗训练是最有效的对抗防御之一，但它会产生很高的计算成本。在这项研究中，我们表明，在不同任务上进行对抗预训练的变形者可以作为强大的基础模型，并消除下游任务中进行对抗训练的需要。具体来说，我们从理论上证明，通过上下文学习，单个经过对抗预训练的Transformer可以在无需任何额外训练的情况下稳健地概括为多个不可见的任务，即没有任何参数更新。这种稳健性源于该模型对稳健特征的关注及其对利用非预测特征的攻击的抵抗力。除了这些积极的发现之外，我们还发现了一些局限性。在某些条件下（尽管不切实际），不存在普遍稳健的单层变压器。此外，稳健的变压器具有准确性和稳健性的权衡，并且需要大量的上下文演示。该代码可在https://github.com/s-kumano/universally-robust-in-context-learner上获取。



## **31. XOXO: Stealthy Cross-Origin Context Poisoning Attacks against AI Coding Assistants**

XOXO：针对人工智能编码助理的隐形跨源上下文中毒攻击 cs.CR

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2503.14281v3) [paper-pdf](http://arxiv.org/pdf/2503.14281v3)

**Authors**: Adam Štorek, Mukur Gupta, Noopur Bhatt, Aditya Gupta, Janie Kim, Prashast Srivastava, Suman Jana

**Abstract**: AI coding assistants are widely used for tasks like code generation. These tools now require large and complex contexts, automatically sourced from various origins$\unicode{x2014}$across files, projects, and contributors$\unicode{x2014}$forming part of the prompt fed to underlying LLMs. This automatic context-gathering introduces new vulnerabilities, allowing attackers to subtly poison input to compromise the assistant's outputs, potentially generating vulnerable code or introducing critical errors. We propose a novel attack, Cross-Origin Context Poisoning (XOXO), that is challenging to detect as it relies on adversarial code modifications that are semantically equivalent. Traditional program analysis techniques struggle to identify these perturbations since the semantics of the code remains correct, making it appear legitimate. This allows attackers to manipulate coding assistants into producing incorrect outputs, while shifting the blame to the victim developer. We introduce a novel, task-agnostic, black-box attack algorithm GCGS that systematically searches the transformation space using a Cayley Graph, achieving a 75.72% attack success rate on average across five tasks and eleven models, including GPT 4.1 and Claude 3.5 Sonnet v2 used by popular AI coding assistants. Furthermore, defenses like adversarial fine-tuning are ineffective against our attack, underscoring the need for new security measures in LLM-powered coding tools.

摘要: 人工智能编码助手广泛用于代码生成等任务。这些工具现在需要大型而复杂的上下文，自动从各种来源$\unicode{x2014}$跨文件、项目和贡献者$\unicode{x2014}$获取，形成了向底层LLM提供提示的一部分。这种自动上下文收集引入了新的漏洞，允许攻击者巧妙地毒害输入以损害助手的输出，从而可能生成易受攻击的代码或引入严重错误。我们提出了一种新型攻击，即跨源上下文中毒（XOXO），检测起来很有挑战性，因为它依赖于语义等效的对抗性代码修改。传统的程序分析技术很难识别这些扰动，因为代码的语义保持正确，使其看起来合法。这使得攻击者能够操纵编码助手产生错误的输出，同时将责任归咎于受害开发人员。我们引入了一种新颖的、任务不可知的黑匣子攻击算法GCGS，该算法使用凯莱图系统性地搜索转换空间，在五个任务和十一个模型（包括GPT 4.1和Claude 3.5 Sonnet v2）上平均实现75.72%的攻击成功率流行AI编码助手使用。此外，对抗性微调等防御措施对我们的攻击无效，这凸显了LLM支持的编码工具中需要新的安全措施。



## **32. D4+: Emergent Adversarial Driving Maneuvers with Approximate Functional Optimization**

D4+：具有近似功能优化的紧急对抗驾驶机动 cs.CR

Dynamic Data Driven Applications Systems-2024

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2505.13942v1) [paper-pdf](http://arxiv.org/pdf/2505.13942v1)

**Authors**: Diego Ortiz Barbosa, Luis Burbano, Carlos Hernandez, Zengxiang Lei, Younghee Park, Satish Ukkusuri, Alvaro A Cardenas

**Abstract**: Intelligent mechanisms implemented in autonomous vehicles, such as proactive driving assist and collision alerts, reduce traffic accidents. However, verifying their correct functionality is difficult due to complex interactions with the environment. This problem is exacerbated in adversarial environments, where an attacker can control the environment surrounding autonomous vehicles to exploit vulnerabilities.   To preemptively identify vulnerabilities in these systems, in this paper, we implement a scenario-based framework with a formal method to identify the impact of malicious drivers interacting with autonomous vehicles. The formalization of the evaluation requirements utilizes metric temporal logic (MTL) to identify a safety condition that we want to test. Our goal is to find, through a rigorous testing approach, any trace that violates this MTL safety specification. Our results can help designers identify the range of safe operational behaviors that prevent malicious drivers from exploiting the autonomous features of modern vehicles.

摘要: 自动驾驶汽车中实施的智能机制，例如主动驾驶辅助和碰撞警报，可以减少交通事故。然而，由于与环境的复杂交互，验证它们的正确功能很困难。这个问题在敌对环境中会加剧，攻击者可以控制自动驾驶汽车周围的环境以利用漏洞。   为了先发制人地识别这些系统中的漏洞，在本文中，我们通过形式化方法实现了一个基于集群的框架，以识别恶意驾驶员与自动驾驶汽车交互的影响。评估要求的形式化利用度量时态逻辑（MTL）来识别我们想要测试的安全条件。我们的目标是通过严格的测试方法找到任何违反MTL安全规范的痕迹。我们的结果可以帮助设计师识别一系列安全操作行为，以防止恶意驾驶员利用现代车辆的自动驾驶功能。



## **33. Redefining Machine Unlearning: A Conformal Prediction-Motivated Approach**

重新定义机器去学习：一种共形预测激励方法 cs.LG

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2501.19403v2) [paper-pdf](http://arxiv.org/pdf/2501.19403v2)

**Authors**: Yingdan Shi, Sijia Liu, Ren Wang

**Abstract**: Machine unlearning seeks to remove the influence of specified data from a trained model. While metrics such as unlearning accuracy (UA) and membership inference attack (MIA) provide baselines for assessing unlearning performance, they fall short of evaluating the forgetting reliability. In this paper, we find that the data misclassified across UA and MIA still have their ground truth labels included in the prediction set from the uncertainty quantification perspective, which raises a fake unlearning issue. To address this issue, we propose two novel metrics inspired by conformal prediction that more reliably evaluate forgetting quality. Building on these insights, we further propose a conformal prediction-based unlearning framework that integrates conformal prediction into Carlini & Wagner adversarial attack loss, which can significantly push the ground truth label out of the conformal prediction set. Through extensive experiments on image classification task, we demonstrate both the effectiveness of our proposed metrics and the superiority of our unlearning framework, which improves the UA of existing unlearning methods by an average of 6.6% through the incorporation of a tailored loss term alone.

摘要: 机器去学习旨在消除训练模型中指定数据的影响。虽然取消学习准确性（UA）和隶属关系推断攻击（MIA）等指标为评估取消学习性能提供了基线，但它们无法评估遗忘可靠性。在本文中，我们发现，从不确定性量化的角度来看，在UC和MIA中被错误分类的数据仍然包含在预测集中，这引发了虚假的遗忘问题。为了解决这个问题，我们提出了两个受保形预测启发的新型指标，可以更可靠地评估遗忘质量。在这些见解的基础上，我们进一步提出了一个基于保形预测的取消学习框架，该框架将保形预测集成到Carlini & Wagner对抗攻击损失中，这可以显着将地面真值标签从保形预测集中剔除。通过对图像分类任务的广泛实验，我们证明了我们提出的指标的有效性和我们的取消学习框架的优越性，该框架通过单独引入定制的损失项将现有取消学习方法的平均值提高了6.6%。



## **34. Safety2Drive: Safety-Critical Scenario Benchmark for the Evaluation of Autonomous Driving**

Safety2Drive：自动驾驶评估的安全关键场景基准 cs.RO

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2505.13872v1) [paper-pdf](http://arxiv.org/pdf/2505.13872v1)

**Authors**: Jingzheng Li, Tiancheng Wang, Xingyu Peng, Jiacheng Chen, Zhijun Chen, Bing Li, Xianglong Liu

**Abstract**: Autonomous Driving (AD) systems demand the high levels of safety assurance. Despite significant advancements in AD demonstrated on open-source benchmarks like Longest6 and Bench2Drive, existing datasets still lack regulatory-compliant scenario libraries for closed-loop testing to comprehensively evaluate the functional safety of AD. Meanwhile, real-world AD accidents are underrepresented in current driving datasets. This scarcity leads to inadequate evaluation of AD performance, posing risks to safety validation and practical deployment. To address these challenges, we propose Safety2Drive, a safety-critical scenario library designed to evaluate AD systems. Safety2Drive offers three key contributions. (1) Safety2Drive comprehensively covers the test items required by standard regulations and contains 70 AD function test items. (2) Safety2Drive supports the safety-critical scenario generalization. It has the ability to inject safety threats such as natural environment corruptions and adversarial attacks cross camera and LiDAR sensors. (3) Safety2Drive supports multi-dimensional evaluation. In addition to the evaluation of AD systems, it also supports the evaluation of various perception tasks, such as object detection and lane detection. Safety2Drive provides a paradigm from scenario construction to validation, establishing a standardized test framework for the safe deployment of AD.

摘要: 自动驾驶（AD）系统需要高水平的安全保证。尽管在Longest 6和Bench2Drive等开源基准上展示了AD的显着进步，但现有数据集仍然缺乏用于闭环测试的符合监管要求的场景库来全面评估AD的功能安全性。与此同时，现实世界的AD事故在当前驾驶数据集中的代表性不足。这种稀缺性导致AD性能评估不充分，给安全验证和实际部署带来风险。为了应对这些挑战，我们提出了Safety2Drive，这是一个安全关键场景库，旨在评估AD系统。Safety2Drive提供了三个关键贡献。(1)Safety2Drive全面涵盖标准法规要求的测试项目，包含70个AD功能测试项目。(2)Safety2Drive支持安全关键场景概括。它能够通过摄像头和激光雷达传感器注入安全威胁，例如自然环境破坏和对抗性攻击。(3)Safety2Drive支持多维评估。除了评估AD系统外，它还支持评估各种感知任务，例如物体检测和车道检测。Safety2Drive提供了从场景构建到验证的范式，为AD的安全部署建立了标准化测试框架。



## **35. PandaGuard: Systematic Evaluation of LLM Safety in the Era of Jailbreaking Attacks**

PandaGuard：越狱袭击时代LLM安全性的系统评估 cs.CR

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2505.13862v1) [paper-pdf](http://arxiv.org/pdf/2505.13862v1)

**Authors**: Guobin Shen, Dongcheng Zhao, Linghao Feng, Xiang He, Jihang Wang, Sicheng Shen, Haibo Tong, Yiting Dong, Jindong Li, Xiang Zheng, Yi Zeng

**Abstract**: Large language models (LLMs) have achieved remarkable capabilities but remain vulnerable to adversarial prompts known as jailbreaks, which can bypass safety alignment and elicit harmful outputs. Despite growing efforts in LLM safety research, existing evaluations are often fragmented, focused on isolated attack or defense techniques, and lack systematic, reproducible analysis. In this work, we introduce PandaGuard, a unified and modular framework that models LLM jailbreak safety as a multi-agent system comprising attackers, defenders, and judges. Our framework implements 19 attack methods and 12 defense mechanisms, along with multiple judgment strategies, all within a flexible plugin architecture supporting diverse LLM interfaces, multiple interaction modes, and configuration-driven experimentation that enhances reproducibility and practical deployment. Built on this framework, we develop PandaBench, a comprehensive benchmark that evaluates the interactions between these attack/defense methods across 49 LLMs and various judgment approaches, requiring over 3 billion tokens to execute. Our extensive evaluation reveals key insights into model vulnerabilities, defense cost-performance trade-offs, and judge consistency. We find that no single defense is optimal across all dimensions and that judge disagreement introduces nontrivial variance in safety assessments. We release the code, configurations, and evaluation results to support transparent and reproducible research in LLM safety.

摘要: 大型语言模型（LLM）已经取得了卓越的能力，但仍然容易受到被称为越狱的对抗性提示的影响，这可能会绕过安全对齐并引发有害的输出。尽管LLM安全研究的努力越来越多，但现有的评估往往是分散的，集中在孤立的攻击或防御技术上，缺乏系统的，可重复的分析。在这项工作中，我们引入了PandaGuard，一个统一的模块化框架，将LLM越狱安全建模为一个由攻击者，防御者和法官组成的多代理系统。我们的框架实现了19种攻击方法和12种防御机制，以及多种判断策略，所有这些都在一个灵活的插件架构中，支持多种LLM接口，多种交互模式和配置驱动的实验，从而增强了可重复性和实际部署。基于这个框架，我们开发了PandaBench，这是一个全面的基准，可评估49个LLM和各种判断方法之间的相互作用，需要超过30亿个代币来执行。我们的广泛评估揭示了对模型漏洞、国防成本-性能权衡和判断一致性的关键见解。我们发现，没有一种防御在所有维度上都是最佳的，而且判断分歧会在安全评估中引入非平凡的方差。我们发布代码、配置和评估结果，以支持LLM安全性方面的透明和可重复研究。



## **36. Fragments to Facts: Partial-Information Fragment Inference from LLMs**

事实片段：来自LLM的部分信息片段推断 cs.LG

**SubmitDate**: 2025-05-20    [abs](http://arxiv.org/abs/2505.13819v1) [paper-pdf](http://arxiv.org/pdf/2505.13819v1)

**Authors**: Lucas Rosenblatt, Bin Han, Robert Wolfe, Bill Howe

**Abstract**: Large language models (LLMs) can leak sensitive training data through memorization and membership inference attacks. Prior work has primarily focused on strong adversarial assumptions, including attacker access to entire samples or long, ordered prefixes, leaving open the question of how vulnerable LLMs are when adversaries have only partial, unordered sample information. For example, if an attacker knows a patient has "hypertension," under what conditions can they query a model fine-tuned on patient data to learn the patient also has "osteoarthritis?" In this paper, we introduce a more general threat model under this weaker assumption and show that fine-tuned LLMs are susceptible to these fragment-specific extraction attacks. To systematically investigate these attacks, we propose two data-blind methods: (1) a likelihood ratio attack inspired by methods from membership inference, and (2) a novel approach, PRISM, which regularizes the ratio by leveraging an external prior. Using examples from both medical and legal settings, we show that both methods are competitive with a data-aware baseline classifier that assumes access to labeled in-distribution data, underscoring their robustness.

摘要: 大型语言模型（LLM）可以通过记忆和成员资格推断攻击泄露敏感的训练数据。之前的工作主要集中在强对抗性假设上，包括攻击者访问整个样本或长、有序的前置码，这就留下了当对手仅拥有部分、无序的样本信息时，LLM有多脆弱的问题。例如，如果攻击者知道患者患有“高血压”，他们在什么情况下可以查询根据患者数据微调的模型以了解患者也患有“骨关节炎”？“在本文中，我们在这个较弱的假设下引入了一个更通用的威胁模型，并表明经过微调的LLM容易受到这些特定于片段的提取攻击。为了系统性地研究这些攻击，我们提出了两种数据盲方法：（1）受隶属推理方法启发的似然比攻击，和（2）一种新颖的方法PRism，它通过利用外部先验来规范比率。使用来自医疗和法律环境的示例，我们表明这两种方法都与数据感知基线分类器具有竞争力，该分类器假设可以访问标记的分布数据，从而强调了其稳健性。



## **37. BeamClean: Language Aware Embedding Reconstruction**

BeamClean：语言感知嵌入重建 cs.CR

9 pages, 5 figures, under review at NeurIPS, 2025

**SubmitDate**: 2025-05-19    [abs](http://arxiv.org/abs/2505.13758v1) [paper-pdf](http://arxiv.org/pdf/2505.13758v1)

**Authors**: Kaan Kale, Kyle Mylonakis, Jay Roberts, Sidhartha Roy

**Abstract**: In this work, we consider an inversion attack on the obfuscated input embeddings sent to a language model on a server, where the adversary has no access to the language model or the obfuscation mechanism and sees only the obfuscated embeddings along with the model's embedding table. We propose BeamClean, an inversion attack that jointly estimates the noise parameters and decodes token sequences by integrating a language-model prior. Against Laplacian and Gaussian obfuscation mechanisms, BeamClean always surpasses naive distance-based attacks. This work highlights the necessity for and robustness of more advanced learned, input-dependent methods.

摘要: 在这项工作中，我们考虑了对发送到服务器上的语言模型的模糊输入嵌入的倒置攻击，其中对手无法访问语言模型或模糊机制，并且只看到模糊嵌入以及模型的嵌入表。我们提出了BeamClean，这是一种倒置攻击，通过集成语言模型先验来联合估计噪音参数并解码令牌序列。针对拉普拉斯和高斯混淆机制，BeamClean始终超越天真的基于距离的攻击。这项工作强调了更先进的学习、依赖输入的方法的必要性和稳健性。



## **38. Multiple Proposer Transaction Fee Mechanism Design: Robust Incentives Against Censorship and Bribery**

多投标人交易费用机制设计：反对审查和贿赂的稳健激励 cs.GT

**SubmitDate**: 2025-05-19    [abs](http://arxiv.org/abs/2505.13751v1) [paper-pdf](http://arxiv.org/pdf/2505.13751v1)

**Authors**: Aikaterini-Panagiota Stouka, Julian Ma, Thomas Thiery

**Abstract**: Censorship resistance is one of the core value proposition of blockchains. A recurring design pattern aimed at providing censorship resistance is enabling multiple proposers to contribute inputs into block construction. Notably, Fork-Choice Enforced Inclusion Lists (FOCIL) is proposed to be included in Ethereum. However, the current proposal relies on altruistic behavior, without a Transaction Fee Mechanism (TFM). This study aims to address this gap by exploring how multiple proposers should be rewarded to incentivize censorship resistance. The main contribution of this work is the identification of TFMs that ensure censorship resistance under bribery attacks, while also satisfying the incentive compatibility properties of EIP-1559. We provide a concrete payment mechanism for FOCIL, along with generalizable contributions to the literature by analyzing 1) incentive compatibility of TFMs in the presence of a bribing adversary, 2) TFMs in protocols with multiple phases of transaction inclusion, and 3) TFMs of protocols in which parties are uncertain about the behavior and the possible bribe of others.

摘要: 抵制审查是区块链的核心价值主张之一。一种旨在提供审查抵抗的反复出现的设计模式使多个提议者能够为区块构建提供投入。值得注意的是，提议将Fork-Choice强制纳入列表（FOCIL）纳入以太坊。然而，当前的提案依赖于利他行为，没有交易费机制（TFM）。这项研究旨在通过探索如何奖励多个提议者以激励审查制度的抵制来解决这一差距。这项工作的主要贡献是识别了TFM，这些TFM确保在贿赂攻击下的审查抵抗力，同时还满足EIP-1559的激励兼容性。我们提供了一个具体的支付机制FOCIL，随着一般化的贡献，通过分析1）在存在贿赂对手的TFMs的激励兼容性，2）TFMs在协议中的多个阶段的交易包含，和3）TFMs的协议，其中各方不确定的行为和可能的贿赂他人。



## **39. Improving LLM Unlearning Robustness via Random Perturbations**

通过随机扰动提高LLM无学习鲁棒性 cs.CL

23 pages, 10 figures, 5 tables

**SubmitDate**: 2025-05-19    [abs](http://arxiv.org/abs/2501.19202v3) [paper-pdf](http://arxiv.org/pdf/2501.19202v3)

**Authors**: Dang Huu-Tien, Hoang Thanh-Tung, Anh Bui, Le-Minh Nguyen, Naoya Inoue

**Abstract**: In this paper, we show that current state-of-the-art LLM unlearning methods inherently reduce models' robustness, causing them to misbehave even when a single non-adversarial forget-token is in the retain-query. Toward understanding underlying causes, we reframe the unlearning process as backdoor attacks and defenses: forget-tokens act as backdoor triggers that, when activated in retain-queries, cause disruptions in unlearned models' behaviors, similar to successful backdoor attacks. To mitigate this vulnerability, we propose Random Noise Augmentation (RNA) -- a plug-and-play, model and method agnostic approach with theoretical guarantees for improving the robustness of unlearned models. Extensive experiments demonstrate that RNA significantly improves the robustness of unlearned models, maintains unlearning performances while introducing no additional computational overhead.

摘要: 在本文中，我们表明，当前最先进的LLM取消学习方法本质上降低了模型的鲁棒性，导致它们即使在保留查询中有一个非对抗性遗忘令牌时也会表现不当。为了了解根本原因，我们将取消学习过程重新定义为后门攻击和防御：忘记令牌充当后门触发器，当在保留查询中激活时，会导致未学习模型的行为中断，类似于成功的后门攻击。为了缓解这一漏洞，我们提出了随机噪音增强（RNA）--一种即插即用、模型和方法不可知的方法，具有提高未学习模型的鲁棒性的理论保证。大量实验表明，RNA显着提高了未学习模型的鲁棒性，保持了未学习性能，同时不引入额外的计算负担。



## **40. On the Vulnerability of Concept Erasure in Diffusion Models**

扩散模型中概念擦除的脆弱性 cs.LG

**SubmitDate**: 2025-05-19    [abs](http://arxiv.org/abs/2502.17537v2) [paper-pdf](http://arxiv.org/pdf/2502.17537v2)

**Authors**: Lucas Beerens, Alex D. Richardson, Kaicheng Zhang, Dongdong Chen

**Abstract**: The proliferation of text-to-image diffusion models has raised significant privacy and security concerns, particularly regarding the generation of copyrighted or harmful images. In response, several concept erasure (defense) methods have been developed to prevent the generation of unwanted content through post-hoc finetuning. On the other hand, concept restoration (attack) methods seek to recover supposedly erased concepts via adversarially crafted prompts. However, all existing restoration methods only succeed in the highly restrictive scenario of finding adversarial prompts tailed to some fixed seed. To address this, we introduce RECORD, a novel coordinate-descent-based restoration algorithm that finds adversarial prompts to recover erased concepts independently of the seed. Our extensive experiments demonstrate RECORD consistently outperforms the current restoration methods by up to 17.8 times in this setting. Our findings further reveal the susceptibility of unlearned models to restoration attacks, providing crucial insights into the behavior of unlearned models under the influence of adversarial prompts.

摘要: 文本到图像传播模型的激增引发了严重的隐私和安全问题，特别是在受版权保护或有害图像的生成方面。作为回应，人们开发了几种概念擦除（防御）方法，以防止通过事后微调产生不想要的内容。另一方面，概念恢复（攻击）方法试图通过敌对制作的提示来恢复所谓已删除的概念。然而，所有现有的恢复方法只成功地在高度限制性的情况下，找到一些固定的种子跟踪对抗性提示。为了解决这个问题，我们引入了RECRD，这是一种新型的基于坐标下降的恢复算法，它可以找到对抗提示来独立于种子恢复被擦除的概念。我们广泛的实验证明，在这种环境下，RECRD始终比当前的修复方法高出17.8倍。我们的研究结果进一步揭示了未学习的模型对恢复攻击的易感性，为未学习的模型在对抗提示影响下的行为提供了重要的见解。



## **41. Investigating the Vulnerability of LLM-as-a-Judge Architectures to Prompt-Injection Attacks**

调查LLM作为法官架构对预算注入攻击的脆弱性 cs.CL

**SubmitDate**: 2025-05-19    [abs](http://arxiv.org/abs/2505.13348v1) [paper-pdf](http://arxiv.org/pdf/2505.13348v1)

**Authors**: Narek Maloyan, Bislan Ashinov, Dmitry Namiot

**Abstract**: Large Language Models (LLMs) are increasingly employed as evaluators (LLM-as-a-Judge) for assessing the quality of machine-generated text. This paradigm offers scalability and cost-effectiveness compared to human annotation. However, the reliability and security of such systems, particularly their robustness against adversarial manipulations, remain critical concerns. This paper investigates the vulnerability of LLM-as-a-Judge architectures to prompt-injection attacks, where malicious inputs are designed to compromise the judge's decision-making process. We formalize two primary attack strategies: Comparative Undermining Attack (CUA), which directly targets the final decision output, and Justification Manipulation Attack (JMA), which aims to alter the model's generated reasoning. Using the Greedy Coordinate Gradient (GCG) optimization method, we craft adversarial suffixes appended to one of the responses being compared. Experiments conducted on the MT-Bench Human Judgments dataset with open-source instruction-tuned LLMs (Qwen2.5-3B-Instruct and Falcon3-3B-Instruct) demonstrate significant susceptibility. The CUA achieves an Attack Success Rate (ASR) exceeding 30\%, while JMA also shows notable effectiveness. These findings highlight substantial vulnerabilities in current LLM-as-a-Judge systems, underscoring the need for robust defense mechanisms and further research into adversarial evaluation and trustworthiness in LLM-based assessment frameworks.

摘要: 大型语言模型（LLM）越来越多地被用作评估器（LLM as-a-Judge）来评估机器生成文本的质量。与人类注释相比，该范式提供了可扩展性和成本效益。然而，此类系统的可靠性和安全性，特别是它们对对抗性操纵的鲁棒性，仍然是关键问题。本文研究了LLM as-a-Judge架构对预算注入攻击的脆弱性，其中恶意输入旨在损害法官的决策过程。我们正式化了两种主要的攻击策略：比较挖掘攻击（CUA），直接针对最终决策输出，和合理化操纵攻击（JMA），旨在改变模型生成的推理。使用贪婪坐标梯度（GCG）优化方法，我们制作附加到正在比较的一个响应上的对抗后缀。在MT-Bench Human Judgments数据集上使用开源描述调整的LLM（Qwen 2.5 - 3B-Direct和Falcon 3 - 3B-Direct）进行的实验证明了显着的易感性。CUA的攻击成功率（ASB）超过30%，而JMA也表现出显着的有效性。这些发现凸显了当前法学硕士作为法官系统中的重大漏洞，强调了强大的防御机制以及对基于法学硕士的评估框架中的对抗性评估和可信度进行进一步研究的必要性。



## **42. FlowPure: Continuous Normalizing Flows for Adversarial Purification**

FlowPure：对抗性净化的连续标准化流程 cs.LG

**SubmitDate**: 2025-05-19    [abs](http://arxiv.org/abs/2505.13280v1) [paper-pdf](http://arxiv.org/pdf/2505.13280v1)

**Authors**: Elias Collaert, Abel Rodríguez, Sander Joos, Lieven Desmet, Vera Rimmer

**Abstract**: Despite significant advancements in the area, adversarial robustness remains a critical challenge in systems employing machine learning models. The removal of adversarial perturbations at inference time, known as adversarial purification, has emerged as a promising defense strategy. To achieve this, state-of-the-art methods leverage diffusion models that inject Gaussian noise during a forward process to dilute adversarial perturbations, followed by a denoising step to restore clean samples before classification. In this work, we propose FlowPure, a novel purification method based on Continuous Normalizing Flows (CNFs) trained with Conditional Flow Matching (CFM) to learn mappings from adversarial examples to their clean counterparts. Unlike prior diffusion-based approaches that rely on fixed noise processes, FlowPure can leverage specific attack knowledge to improve robustness under known threats, while also supporting a more general stochastic variant trained on Gaussian perturbations for settings where such knowledge is unavailable. Experiments on CIFAR-10 and CIFAR-100 demonstrate that our method outperforms state-of-the-art purification-based defenses in preprocessor-blind and white-box scenarios, and can do so while fully preserving benign accuracy in the former. Moreover, our results show that not only is FlowPure a highly effective purifier but it also holds a strong potential for adversarial detection, identifying preprocessor-blind PGD samples with near-perfect accuracy.

摘要: 尽管该领域取得了重大进展，但对抗鲁棒性仍然是采用机器学习模型的系统的一个关键挑战。在推理时消除对抗性扰动，称为对抗性净化，已成为一种有前途的防御策略。为了实现这一目标，最先进的方法利用扩散模型，该模型在正向过程中注入高斯噪音以稀释对抗性扰动，然后进行去噪步骤以在分类之前恢复干净的样本。在这项工作中，我们提出了FlowPure，这是一种新型净化方法，基于用条件流匹配（CGM）训练的连续正规化流（CNF），以学习从对抗性示例到干净对应的映射。与依赖于固定噪声过程的先前基于扩散的方法不同，FlowPure可以利用特定的攻击知识来提高已知威胁下的鲁棒性，同时还支持在高斯扰动上训练的更一般的随机变量，用于此类知识不可用的设置。在CIFAR-10和CIFAR-100上的实验表明，我们的方法在预处理器盲和白盒场景中的性能优于最先进的基于纯化的防御，并且可以在完全保留良性准确性的同时做到这一点。此外，我们的研究结果表明，FlowPure不仅是一种高效的净化器，而且在对抗性检测方面也具有很强的潜力，能够以近乎完美的准确度识别预处理器盲PGD样本。



## **43. Constrained Adversarial Learning for Automated Software Testing: a literature review**

用于自动化软件测试的约束对抗学习：文献综述 cs.SE

36 pages, 4 tables, 2 figures, Discover Applied Sciences journal

**SubmitDate**: 2025-05-19    [abs](http://arxiv.org/abs/2303.07546v3) [paper-pdf](http://arxiv.org/pdf/2303.07546v3)

**Authors**: João Vitorino, Tiago Dias, Tiago Fonseca, Eva Maia, Isabel Praça

**Abstract**: It is imperative to safeguard computer applications and information systems against the growing number of cyber-attacks. Automated software testing tools can be developed to quickly analyze many lines of code and detect vulnerabilities by generating function-specific testing data. This process draws similarities to the constrained adversarial examples generated by adversarial machine learning methods, so there could be significant benefits to the integration of these methods in testing tools to identify possible attack vectors. Therefore, this literature review is focused on the current state-of-the-art of constrained data generation approaches applied for adversarial learning and software testing, aiming to guide researchers and developers to enhance their software testing tools with adversarial testing methods and improve the resilience and robustness of their information systems. The found approaches were systematized, and the advantages and limitations of those specific for white-box, grey-box, and black-box testing were analyzed, identifying research gaps and opportunities to automate the testing tools with data generated by adversarial attacks.

摘要: 保护计算机应用程序和信息系统免受日益增多的网络攻击至关重要。可以开发自动化软件测试工具来快速分析多行代码并通过生成特定于功能的测试数据来检测漏洞。该过程与对抗性机器学习方法生成的受约束对抗示例具有相似之处，因此将这些方法集成到测试工具中以识别可能的攻击载体可能会带来显着的好处。因此，本次文献综述重点关注当前应用于对抗性学习和软件测试的约束数据生成方法的最新发展水平，旨在指导研究人员和开发人员通过对抗性测试方法增强其软件测试工具，并提高其信息系统的弹性和稳健性。对所发现的方法进行了系统化，并分析了白盒、灰盒和黑盒测试方法的优点和局限性，确定了研究差距和机会，以利用对抗性攻击生成的数据自动化测试工具。



## **44. Test-time Adversarial Defense with Opposite Adversarial Path and High Attack Time Cost**

具有相反对抗路径和高攻击时间成本的测试时对抗防御 cs.LG

**SubmitDate**: 2025-05-19    [abs](http://arxiv.org/abs/2410.16805v2) [paper-pdf](http://arxiv.org/pdf/2410.16805v2)

**Authors**: Cheng-Han Yeh, Kuanchun Yu, Chun-Shien Lu

**Abstract**: Deep learning models are known to be vulnerable to adversarial attacks by injecting sophisticated designed perturbations to input data. Training-time defenses still exhibit a significant performance gap between natural accuracy and robust accuracy. In this paper, we investigate a new test-time adversarial defense method via diffusion-based recovery along opposite adversarial paths (OAPs). We present a purifier that can be plugged into a pre-trained model to resist adversarial attacks. Different from prior arts, the key idea is excessive denoising or purification by integrating the opposite adversarial direction with reverse diffusion to push the input image further toward the opposite adversarial direction. For the first time, we also exemplify the pitfall of conducting AutoAttack (Rand) for diffusion-based defense methods. Through the lens of time complexity, we examine the trade-off between the effectiveness of adaptive attack and its computation complexity against our defense. Experimental evaluation along with time cost analysis verifies the effectiveness of the proposed method.

摘要: 众所周知，深度学习模型通过向输入数据注入复杂的设计扰动而容易受到对抗攻击。训练时防御在自然准确性和稳健准确性之间仍然表现出显着的性能差距。本文研究了一种新的测试时对抗防御方法，通过沿着相反对抗路径（OAP）的基于扩散的恢复。我们提供了一种净化器，它可以插入预先训练的模型中以抵抗对抗攻击。与现有技术不同，其核心思想是通过将相反的对抗方向与反向扩散相结合来进行过度降噪或净化，以将输入图像进一步推向相反的对抗方向。我们还首次指出了针对基于扩散的防御方法进行AutoAttack（Rand）的陷阱。通过时间复杂性的视角，我们研究了自适应攻击的有效性与其针对我们防御的计算复杂性之间的权衡。实验评估和时间成本分析验证了该方法的有效性。



## **45. Countermeasure against Detector Blinding Attack with Secret Key Leakage Estimation**

利用密钥泄露估计对抗检测器致盲攻击的对策 quant-ph

**SubmitDate**: 2025-05-19    [abs](http://arxiv.org/abs/2505.12974v1) [paper-pdf](http://arxiv.org/pdf/2505.12974v1)

**Authors**: Dmitry M. Melkonian, Daniil S. Bulavkin, Kirill E. Bugai, Kirill A. Balygin, Dmitriy A. Dvoretskiy

**Abstract**: We present a countermeasure against the detector blinding attack (DBA) utilizing statistical analysis of error and double-click events accumulated during a quantum key distribution session under randomized modulation of single-photon avalanche diode (SPAD) detection efficiencies via gate voltage manipulation. Building upon prior work demonstrating the ineffectiveness of this countermeasure against continuous-wave (CW) DBA, we extend the analysis to evaluate its performance against pulsed DBA. Our findings reveal an approximately 25 dB increase in the trigger pulse energies difference between high and low gate voltage applied under pulsed DBA conditions compared to CW DBA. This heightened difference enables a re-evaluation of the feasibility of utilizing SPAD detection probability variations as a countermeasure and makes it possible to estimate the fraction of bits compromised by an adversary during pulsed DBA.

摘要: 我们利用对通过门电压操纵对单量子雪崩二极管（SPAD）检测效率进行随机调制的量子密钥分发会话期间积累的错误和双击事件的统计分析，提出了一种针对检测器致盲攻击（DBA）的对策。在之前的工作证明了这种对策对连续波（CW）DBA无效的基础上，我们扩展了分析以评估其对脉冲DBA的性能。我们的研究结果表明，与CW DBA相比，在脉冲DBA条件下施加的高和低门电压之间的触发脉冲能量差增加了约25分贝。这种增大的差异使得可以重新评估利用SPAD检测概率变化作为对策的可行性，并使得可以估计在脉冲DBA期间被对手损害的比特比例。



## **46. DICTION:DynamIC robusT whIte bOx watermarkiNg scheme for deep neural networks**

DICTION：动态robusT whIte bOx水印用于深度神经网络的方案 cs.CR

24 pages, 4 figures, PrePrint

**SubmitDate**: 2025-05-19    [abs](http://arxiv.org/abs/2210.15745v2) [paper-pdf](http://arxiv.org/pdf/2210.15745v2)

**Authors**: Reda Bellafqira, Gouenou Coatrieux

**Abstract**: Deep neural network (DNN) watermarking is a suitable method for protecting the ownership of deep learning (DL) models. It secretly embeds an identifier (watermark) within the model, which can be retrieved by the owner to prove ownership. In this paper, we first provide a unified framework for white box DNN watermarking schemes. It includes current state-of-the-art methods outlining their theoretical inter-connections. Next, we introduce DICTION, a new white-box Dynamic Robust watermarking scheme, we derived from this framework. Its main originality stands on a generative adversarial network (GAN) strategy where the watermark extraction function is a DNN trained as a GAN discriminator taking the target model to watermark as a GAN generator with a latent space as the input of the GAN trigger set. DICTION can be seen as a generalization of DeepSigns which, to the best of our knowledge, is the only other Dynamic white-box watermarking scheme from the literature. Experiments conducted on the same model test set as Deepsigns demonstrate that our scheme achieves much better performance. Especially, with DICTION, one can increase the watermark capacity while preserving the target model accuracy at best and simultaneously ensuring strong watermark robustness against a wide range of watermark removal and detection attacks.

摘要: 深度神经网络（DNN）水印是一种保护深度学习（DL）模型所有权的合适方法。它秘密地在模型中嵌入一个标识符（水印），所有者可以检索该标识符以证明所有权。本文首先为白盒DNN水印方案提供了一个统一的框架。它包括目前最先进的方法，概述了它们的理论相互联系。接下来，我们介绍了DICTION，这是一种新的白盒动态鲁棒水印方案，我们从这个框架中衍生出来。它的主要独创性基于生成对抗网络（GAN）策略，其中水印提取功能是一个DNN，作为GAN预设，将目标模型进行水印作为GAN生成器，其中潜在空间作为GAN触发器集的输入。DICTION可以被视为DeepSigns的概括，据我们所知，DeepSigns是文献中唯一的其他动态白盒水印方案。在与Deepsards相同的模型测试集上进行的实验表明，我们的方案实现了更好的性能。特别是，与DICTION，可以增加水印容量，同时保持目标模型的准确性最好，同时确保强大的水印鲁棒性对广泛的水印删除和检测攻击。



## **47. Language Models That Walk the Talk: A Framework for Formal Fairness Certificates**

直言不讳的语言模型：正式公平证书的框架 cs.AI

**SubmitDate**: 2025-05-19    [abs](http://arxiv.org/abs/2505.12767v1) [paper-pdf](http://arxiv.org/pdf/2505.12767v1)

**Authors**: Danqing Chen, Tobias Ladner, Ahmed Rayen Mhadhbi, Matthias Althoff

**Abstract**: As large language models become integral to high-stakes applications, ensuring their robustness and fairness is critical. Despite their success, large language models remain vulnerable to adversarial attacks, where small perturbations, such as synonym substitutions, can alter model predictions, posing risks in fairness-critical areas, such as gender bias mitigation, and safety-critical areas, such as toxicity detection. While formal verification has been explored for neural networks, its application to large language models remains limited. This work presents a holistic verification framework to certify the robustness of transformer-based language models, with a focus on ensuring gender fairness and consistent outputs across different gender-related terms. Furthermore, we extend this methodology to toxicity detection, offering formal guarantees that adversarially manipulated toxic inputs are consistently detected and appropriately censored, thereby ensuring the reliability of moderation systems. By formalizing robustness within the embedding space, this work strengthens the reliability of language models in ethical AI deployment and content moderation.

摘要: 随着大型语言模型成为高风险应用程序的组成部分，确保其稳健性和公平性至关重要。尽管取得了成功，大型语言模型仍然容易受到对抗攻击，其中同义词替换等小扰动可能会改变模型预测，从而在性别偏见缓解等公平关键领域和安全关键领域带来风险，例如毒性检测。虽然已经探索了神经网络的形式验证，但其在大型语言模型中的应用仍然有限。这项工作提出了一个整体验证框架，以验证基于转换器的语言模型的稳健性，重点是确保性别公平性和不同性别相关术语的一致输出。此外，我们将这种方法扩展到毒性检测，提供正式保证，以一致地检测和适当审查敌对操纵的有毒输入，从而确保审核系统的可靠性。通过形式化嵌入空间内的鲁棒性，这项工作增强了语言模型在道德人工智能部署和内容审核中的可靠性。



## **48. BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks and Defenses on Large Language Models**

BackdoorLLM：大型语言模型后门攻击和防御的综合基准 cs.AI

22 pages

**SubmitDate**: 2025-05-19    [abs](http://arxiv.org/abs/2408.12798v2) [paper-pdf](http://arxiv.org/pdf/2408.12798v2)

**Authors**: Yige Li, Hanxun Huang, Yunhan Zhao, Xingjun Ma, Jun Sun

**Abstract**: Generative large language models (LLMs) have achieved state-of-the-art results on a wide range of tasks, yet they remain susceptible to backdoor attacks: carefully crafted triggers in the input can manipulate the model to produce adversary-specified outputs. While prior research has predominantly focused on backdoor risks in vision and classification settings, the vulnerability of LLMs in open-ended text generation remains underexplored. To fill this gap, we introduce BackdoorLLM (Our BackdoorLLM benchmark was awarded First Prize in the SafetyBench competition, https://www.mlsafety.org/safebench/winners, organized by the Center for AI Safety, https://safe.ai/.), the first comprehensive benchmark for systematically evaluating backdoor threats in text-generation LLMs. BackdoorLLM provides: (i) a unified repository of benchmarks with a standardized training and evaluation pipeline; (ii) a diverse suite of attack modalities, including data poisoning, weight poisoning, hidden-state manipulation, and chain-of-thought hijacking; (iii) over 200 experiments spanning 8 distinct attack strategies, 7 real-world scenarios, and 6 model architectures; (iv) key insights into the factors that govern backdoor effectiveness and failure modes in LLMs; and (v) a defense toolkit encompassing 7 representative mitigation techniques. Our code and datasets are available at https://github.com/bboylyg/BackdoorLLM. We will continuously incorporate emerging attack and defense methodologies to support the research in advancing the safety and reliability of LLMs.

摘要: 生成式大型语言模型（LLM）在广泛的任务上取得了最先进的结果，但它们仍然容易受到后门攻击：输入中精心制作的触发器可以操纵模型以产生对手指定的输出。虽然先前的研究主要集中在视觉和分类设置中的后门风险，但LLM在开放式文本生成中的脆弱性仍然没有得到充分的研究。为了填补这一空白，我们引入了BackdoorLLM（我们的BackdoorLLM基准测试在SafetyBench竞赛中获得一等奖，https：//www.mlsafety.org/safebench/winners，由AI安全中心组织，https：//safe.ai/.），系统评估文本生成LLM中后门威胁的第一个全面基准。BackdoorLLM提供：（i）一个统一的基准库，具有标准化的培训和评估管道;（ii）一套多样化的攻击模式，包括数据中毒，权重中毒，隐藏状态操纵和思想链劫持;（iii）超过200个实验，涵盖8种不同的攻击策略，7种真实场景和6种模型架构;（iv）对制约LLM后门有效性和故障模式的因素的关键见解;以及（v）包含7种代表性缓解技术的防御工具包。我们的代码和数据集可在https://github.com/bboylyg/BackdoorLLM上获取。我们将不断结合新兴的攻击和防御方法，以支持研究，提高LLM的安全性和可靠性。



## **49. Bullying the Machine: How Personas Increase LLM Vulnerability**

欺凌机器：角色扮演如何增加LLM漏洞 cs.AI

**SubmitDate**: 2025-05-19    [abs](http://arxiv.org/abs/2505.12692v1) [paper-pdf](http://arxiv.org/pdf/2505.12692v1)

**Authors**: Ziwei Xu, Udit Sanghi, Mohan Kankanhalli

**Abstract**: Large Language Models (LLMs) are increasingly deployed in interactions where they are prompted to adopt personas. This paper investigates whether such persona conditioning affects model safety under bullying, an adversarial manipulation that applies psychological pressures in order to force the victim to comply to the attacker. We introduce a simulation framework in which an attacker LLM engages a victim LLM using psychologically grounded bullying tactics, while the victim adopts personas aligned with the Big Five personality traits. Experiments using multiple open-source LLMs and a wide range of adversarial goals reveal that certain persona configurations -- such as weakened agreeableness or conscientiousness -- significantly increase victim's susceptibility to unsafe outputs. Bullying tactics involving emotional or sarcastic manipulation, such as gaslighting and ridicule, are particularly effective. These findings suggest that persona-driven interaction introduces a novel vector for safety risks in LLMs and highlight the need for persona-aware safety evaluation and alignment strategies.

摘要: 大型语言模型（LLM）越来越多地部署在交互中，它们会被提示采用角色。本文研究了这种角色条件反射是否会影响欺凌下的模型安全性，欺凌是一种对抗性操纵，施加心理压力以迫使受害者服从攻击者。我们引入了一个模拟框架，其中攻击者LLM使用基于心理的欺凌策略与受害者LLM互动，而受害者则采用与五大人格特征一致的角色。使用多个开源LLM和广泛的对抗目标的实验表明，某些角色配置（例如减弱的宜人性或危险性）会显着增加受害者对不安全输出的易感性。涉及情感或讽刺操纵的欺凌策略，例如煤气灯和嘲笑，尤其有效。这些发现表明，个性驱动的交互为LLM中的安全风险引入了一种新的载体，并强调了对个性意识的安全评估和协调策略的必要性。



## **50. RoVo: Robust Voice Protection Against Unauthorized Speech Synthesis with Embedding-Level Perturbations**

RoVo：针对未经授权的语音合成的强大语音保护，具有嵌入级扰动 cs.LG

**SubmitDate**: 2025-05-19    [abs](http://arxiv.org/abs/2505.12686v1) [paper-pdf](http://arxiv.org/pdf/2505.12686v1)

**Authors**: Seungmin Kim, Sohee Park, Donghyun Kim, Jisu Lee, Daeseon Choi

**Abstract**: With the advancement of AI-based speech synthesis technologies such as Deep Voice, there is an increasing risk of voice spoofing attacks, including voice phishing and fake news, through unauthorized use of others' voices. Existing defenses that inject adversarial perturbations directly into audio signals have limited effectiveness, as these perturbations can easily be neutralized by speech enhancement methods. To overcome this limitation, we propose RoVo (Robust Voice), a novel proactive defense technique that injects adversarial perturbations into high-dimensional embedding vectors of audio signals, reconstructing them into protected speech. This approach effectively defends against speech synthesis attacks and also provides strong resistance to speech enhancement models, which represent a secondary attack threat.   In extensive experiments, RoVo increased the Defense Success Rate (DSR) by over 70% compared to unprotected speech, across four state-of-the-art speech synthesis models. Specifically, RoVo achieved a DSR of 99.5% on a commercial speaker-verification API, effectively neutralizing speech synthesis attack. Moreover, RoVo's perturbations remained robust even under strong speech enhancement conditions, outperforming traditional methods. A user study confirmed that RoVo preserves both naturalness and usability of protected speech, highlighting its effectiveness in complex and evolving threat scenarios.

摘要: 随着Deep Voice等基于人工智能的语音合成技术的进步，通过未经授权使用他人语音进行语音欺骗攻击的风险越来越大，包括语音网络钓鱼和假新闻。将对抗性扰动直接注入音频信号的现有防御措施的有效性有限，因为这些扰动很容易被语音增强方法抵消。为了克服这一限制，我们提出了RoVo（鲁棒语音），这是一种新型的主动防御技术，它将对抗性扰动注入音频信号的多维嵌入载体中，将它们重建为受保护的语音。这种方法有效地防御语音合成攻击，并且还对代表二次攻击威胁的语音增强模型提供了强大的抵抗力。   在广泛的实验中，与无保护语音相比，RoVo在四种最先进的语音合成模型中将防御成功率（SVR）提高了70%以上。具体来说，RoVo在商业说话者验证API上实现了99.5%的SWR，有效地中和了语音合成攻击。此外，即使在强语音增强条件下，RoVo的扰动也保持稳健，优于传统方法。一项用户研究证实，RoVo保留了受保护语音的自然性和可用性，凸显了其在复杂且不断变化的威胁场景中的有效性。



