# Latest Adversarial Attack Papers
**update at 2025-12-04 16:39:21**

翻译来自 https://cloud.tencent.com/document/product/551/15619

## **1. Out-of-the-box: Black-box Causal Attacks on Object Detectors**

开箱即用：对对象检测器的黑匣子因果攻击 cs.CV

**SubmitDate**: 2025-12-03    [abs](http://arxiv.org/abs/2512.03730v1) [paper-pdf](https://arxiv.org/pdf/2512.03730v1)

**Authors**: Melane Navaratnarajah, David A. Kelly, Hana Chockler

**Abstract**: Adversarial perturbations are a useful way to expose vulnerabilities in object detectors. Existing perturbation methods are frequently white-box and architecture specific. More importantly, while they are often successful, it is rarely clear why they work. Insights into the mechanism of this success would allow developers to understand and analyze these attacks, as well as fine-tune the model to prevent them. This paper presents BlackCAtt, a black-box algorithm and a tool, which uses minimal, causally sufficient pixel sets to construct explainable, imperceptible, reproducible, architecture-agnostic attacks on object detectors. BlackCAtt combines causal pixels with bounding boxes produced by object detectors to create adversarial attacks that lead to the loss, modification or addition of a bounding box. BlackCAtt works across different object detectors of different sizes and architectures, treating the detector as a black box. We compare the performance of BlackCAtt with other black-box attack methods and show that identification of causal pixels leads to more precisely targeted and less perceptible attacks. On the COCO test dataset, our approach is 2.7 times better than the baseline in removing a detection, 3.86 times better in changing a detection, and 5.75 times better in triggering new, spurious, detections. The attacks generated by BlackCAtt are very close to the original image, and hence imperceptible, demonstrating the power of causal pixels.

摘要: 对抗性扰动是暴露对象检测器漏洞的有用方法。现有的扰动方法通常是白盒和特定于架构的。更重要的是，虽然它们往往是成功的，但人们很少清楚它们为何有效。深入了解这一成功的机制将使开发人员能够理解和分析这些攻击，并微调模型以防止它们。本文介绍了BlackCAtt，这是一种黑匣子算法和工具，它使用最小的、因果关系充分的像素集来构建对对象检测器的可解释、不可感知、可复制、架构不可知的攻击。BlackCAtt将因果像素与对象检测器产生的边界框相结合，以创建导致边界框丢失、修改或添加的对抗攻击。BlackCAtt适用于不同尺寸和架构的不同对象检测器，将检测器视为黑匣子。我们将BlackCAtt的性能与其他黑匣子攻击方法进行了比较，并表明因果像素的识别会导致更精确的针对性和更难感知的攻击。在COCO测试数据集上，我们的方法在消除检测方面比基线好2.7倍，在改变检测方面比基线好3.86倍，在触发新的虚假检测方面好5.75倍。BlackCAtt生成的攻击非常接近原始图像，因此难以察觉，证明了因果像素的力量。



## **2. Context-Aware Hierarchical Learning: A Two-Step Paradigm towards Safer LLMs**

上下文感知分层学习：实现更安全的LLM的两步范式 cs.CR

**SubmitDate**: 2025-12-03    [abs](http://arxiv.org/abs/2512.03720v1) [paper-pdf](https://arxiv.org/pdf/2512.03720v1)

**Authors**: Tengyun Ma, Jiaqi Yao, Daojing He, Shihao Peng, Yu Li, Shaohui Liu, Zhuotao Tian

**Abstract**: Large Language Models (LLMs) have emerged as powerful tools for diverse applications. However, their uniform token processing paradigm introduces critical vulnerabilities in instruction handling, particularly when exposed to adversarial scenarios. In this work, we identify and propose a novel class of vulnerabilities, termed Tool-Completion Attack (TCA), which exploits function-calling mechanisms to subvert model behavior. To evaluate LLM robustness against such threats, we introduce the Tool-Completion benchmark, a comprehensive security assessment framework, which reveals that even state-of-the-art models remain susceptible to TCA, with surprisingly high attack success rates. To address these vulnerabilities, we introduce Context-Aware Hierarchical Learning (CAHL), a sophisticated mechanism that dynamically balances semantic comprehension with role-specific instruction constraints. CAHL leverages the contextual correlations between different instruction segments to establish a robust, context-aware instruction hierarchy. Extensive experiments demonstrate that CAHL significantly enhances LLM robustness against both conventional attacks and the proposed TCA, exhibiting strong generalization capabilities in zero-shot evaluations while still preserving model performance on generic tasks. Our code is available at https://github.com/S2AILab/CAHL.

摘要: 大型语言模型（LLM）已成为各种应用程序的强大工具。然而，他们的统一令牌处理范式在指令处理中引入了关键漏洞，特别是当暴露于对抗场景时。在这项工作中，我们识别并提出了一类新型漏洞，称为工具完成攻击（MCA），它利用函数调用机制来颠覆模型行为。为了评估LLM针对此类威胁的稳健性，我们引入了工具完成基准，这是一个全面的安全评估框架，它表明即使是最先进的模型仍然容易受到MCA的影响，并且攻击成功率高得惊人。为了解决这些漏洞，我们引入了上下文感知分层学习（CAHL），这是一种复杂的机制，可以动态平衡语义理解与特定角色的指令约束。CAHL利用不同指令段之间的上下文相关性来建立稳健的、上下文感知的指令层次结构。大量实验表明，CAHL显着增强了LLM针对传统攻击和拟议的MCA的鲁棒性，在零激发评估中表现出强大的概括能力，同时仍然保留了通用任务的模型性能。我们的代码可以在https://github.com/S2AILab/CAHL上找到。



## **3. A Descriptive Model for Modelling Attacker Decision-Making in Cyber-Deception**

网络欺骗中攻击者决策建模的描述性模型 cs.CR

24 Pages, 4 Tables

**SubmitDate**: 2025-12-03    [abs](http://arxiv.org/abs/2512.03641v1) [paper-pdf](https://arxiv.org/pdf/2512.03641v1)

**Authors**: B. R. Turner, O. Guidetti, N. M. Karie, R. Ryan, Y. Yan

**Abstract**: Cyber-deception is an increasingly important defensive strategy, shaping adversarial decision making through controlled misinformation, uncertainty, and misdirection. Although game-theoretic, Bayesian, Markov decision process, and reinforcement learning models offer insight into deceptive interactions, they typically assume an attacker has already chosen to engage. Such approaches overlook cognitive and perceptual factors that influence an attacker's initial decision to engage or withdraw. This paper presents a descriptive model that incorporates the psychological and strategic elements shaping this decision. The model defines five components, belief (B), scepticism (S), deception fidelity (D), reconnaissance (R), and experience (E), which interact to capture how adversaries interpret deceptive cues and assess whether continued engagement is worthwhile. The framework provides a structured method for analysing engagement decisions in cyber-deception scenarios. A series of experiments has been designed to evaluate this model through Capture the Flag activities incorporating varying levels of deception, supported by behavioural and biometric observations. These experiments have not yet been conducted, and no experimental findings are presented in this paper. These experiments will combine behavioural observations with biometric indicators to produce a multidimensional view of adversarial responses. Findings will improve understanding of the factors influencing engagement decisions and refine the model's relevance to real-world cyber-deception settings. By addressing the gap in existing models that presume engagement, this work supports more cognitively realistic and strategically effective cyber-deception practices.

摘要: 网络欺骗是一种越来越重要的防御策略，通过受控的错误信息、不确定性和误导来塑造对抗性决策。尽管博弈论、Bayesian、Markov决策过程和强化学习模型提供了对欺骗性交互的见解，但它们通常假设攻击者已经选择参与。此类方法忽视了影响攻击者最初参与或退出决定的认知和感知因素。本文提出了一个描述性模型，其中融合了塑造这一决定的心理和战略因素。该模型定义了五个组成部分：信念（B）、怀疑论（S）、欺骗忠实度（D）、侦察（R）和经验（E），它们相互作用以捕捉对手如何解释欺骗性线索并评估继续参与是否值得。该框架提供了一种结构化方法来分析网络欺骗场景中的参与决策。设计了一系列实验来通过包含不同程度欺骗的夺旗活动来评估该模型，并由行为和生物识别观察支持。这些实验尚未进行，本文中也没有给出实验结果。这些实验将将行为观察与生物识别指标相结合，以产生对抗反应的多维视图。研究结果将提高对影响参与决策的因素的理解，并完善模型与现实世界网络欺骗环境的相关性。通过解决假设参与的现有模型中的差距，这项工作支持更加认知现实和战略有效的网络欺骗实践。



## **4. FeatureLens: A Highly Generalizable and Interpretable Framework for Detecting Adversarial Examples Based on Image Features**

EnterpriseLens：一个高度可概括和可解释的框架，用于基于图像特征检测对抗性示例 cs.CV

**SubmitDate**: 2025-12-03    [abs](http://arxiv.org/abs/2512.03625v1) [paper-pdf](https://arxiv.org/pdf/2512.03625v1)

**Authors**: Zhigang Yang, Yuan Liu, Jiawei Zhang, Puning Zhang, Xinqiang Ma

**Abstract**: Although the remarkable performance of deep neural networks (DNNs) in image classification, their vulnerability to adversarial attacks remains a critical challenge. Most existing detection methods rely on complex and poorly interpretable architectures, which compromise interpretability and generalization. To address this, we propose FeatureLens, a lightweight framework that acts as a lens to scrutinize anomalies in image features. Comprising an Image Feature Extractor (IFE) and shallow classifiers (e.g., SVM, MLP, or XGBoost) with model sizes ranging from 1,000 to 30,000 parameters, FeatureLens achieves high detection accuracy ranging from 97.8% to 99.75% in closed-set evaluation and 86.17% to 99.6% in generalization evaluation across FGSM, PGD, CW, and DAmageNet attacks, using only 51 dimensional features. By combining strong detection performance with excellent generalization, interpretability, and computational efficiency, FeatureLens offers a practical pathway toward transparent and effective adversarial defense.

摘要: 尽管深度神经网络（DNN）在图像分类方面表现出色，但其对对抗攻击的脆弱性仍然是一个严峻的挑战。大多数现有的检测方法依赖于复杂且难以解释的架构，这会损害可解释性和概括性。为了解决这个问题，我们提出了DeliverureLens，这是一个轻量级框架，可以充当镜头来检查图像特征中的异常。包括图像特征提取器（IFE）和浅层分类器（例如，ASM、MLP或XGboost）的模型大小范围为1，000至30，000个参数，EnterpriseLens在FGSM、PVD、CW和DAmageNet攻击中仅使用51维特征，在封闭集评估中实现了97.8%至99.75%的高检测准确率，在概括评估中实现了86.17%至99.6%的高检测准确率。通过将强大的检测性能与出色的概括性、可解释性和计算效率相结合，InspectureLens提供了一条实现透明有效对抗防御的实用途径。



## **5. Tuning for TraceTarnish: Techniques, Trends, and Testing Tangible Traits**

TraceTarnish调整：技术、趋势和测试纹理特征 cs.CR

20 pages, 8 figures, 2 tables

**SubmitDate**: 2025-12-03    [abs](http://arxiv.org/abs/2512.03465v1) [paper-pdf](https://arxiv.org/pdf/2512.03465v1)

**Authors**: Robert Dilworth

**Abstract**: In this study, we more rigorously evaluated our attack script $\textit{TraceTarnish}$, which leverages adversarial stylometry principles to anonymize the authorship of text-based messages. To ensure the efficacy and utility of our attack, we sourced, processed, and analyzed Reddit comments--comments that were later alchemized into $\textit{TraceTarnish}$ data--to gain valuable insights. The transformed $\textit{TraceTarnish}$ data was then further augmented by $\textit{StyloMetrix}$ to manufacture stylometric features--features that were culled using the Information Gain criterion, leaving only the most informative, predictive, and discriminative ones. Our results found that function words and function word types ($L\_FUNC\_A$ $\&$ $L\_FUNC\_T$); content words and content word types ($L\_CONT\_A$ $\&$ $L\_CONT\_T$); and the Type-Token Ratio ($ST\_TYPE\_TOKEN\_RATIO\_LEMMAS$) yielded significant Information-Gain readings. The identified stylometric cues--function-word frequencies, content-word distributions, and the Type-Token Ratio--serve as reliable indicators of compromise (IoCs), revealing when a text has been deliberately altered to mask its true author. Similarly, these features could function as forensic beacons, alerting defenders to the presence of an adversarial stylometry attack; granted, in the absence of the original message, this signal may go largely unnoticed, as it appears to depend on a pre- and post-transformation comparison. "In trying to erase a trace, you often imprint a larger one." Armed with this understanding, we framed $\textit{TraceTarnish}$'s operations and outputs around these five isolated features, using them to conceptualize and implement enhancements that further strengthen the attack.

摘要: 在这项研究中，我们更严格地评估了我们的攻击脚本$\textit{TraceTarnish}$，该脚本利用对抗性样式学原则来匿名基于文本的消息的作者身份。为了确保攻击的有效性和实用性，我们对Reddit评论进行了来源、处理和分析，这些评论后来被子序列化为$\textit{TraceTarnish}$ data-以获得有价值的见解。然后，转换后的$\textit{TraceTarnish}$数据进一步由$\textit{StyloMeetup}$扩展，以制造文体特征--使用信息收益标准剔除的特征，只留下信息量最大、预测性最强和区分性最强的特征。我们的结果发现，功能词和功能词类型（$L\_FSYS\_A$\&$ $L\_FSYS\_T$）;内容词和内容词类型（$L\_CONT\_A$\&$L\_CONT\_T$）;和类型-令牌比（$ST\_GROUP\_TOKEN\_RATIO\_LEMMAS$）产生了显着的信息-收益读数。识别出的文体线索--功能词频率、内容词分布和类型-标记比--可以作为可靠的妥协指标（IoCs），揭示文本何时被故意更改以掩盖其真实作者。同样，这些功能可以充当法医信标，提醒防御者存在对抗性文体攻击;当然，在没有原始消息的情况下，这个信号可能在很大程度上被忽视，因为它似乎取决于转换前后的比较。“在试图擦除痕迹时，你通常会印上更大的痕迹。“有了这一理解，我们围绕这五个独立功能构建了$\textit{TraceTarnish}$的操作和输出，使用它们概念化和实施进一步加强攻击的增强功能。



## **6. Immunity memory-based jailbreak detection: multi-agent adaptive guard for large language models**

基于免疫记忆的越狱检测：大型语言模型的多代理自适应警卫 cs.CR

**SubmitDate**: 2025-12-03    [abs](http://arxiv.org/abs/2512.03356v1) [paper-pdf](https://arxiv.org/pdf/2512.03356v1)

**Authors**: Jun Leng, Litian Zhang, Xi Zhang

**Abstract**: Large language models (LLMs) have become foundational in AI systems, yet they remain vulnerable to adversarial jailbreak attacks. These attacks involve carefully crafted prompts that bypass safety guardrails and induce models to produce harmful content. Detecting such malicious input queries is therefore critical for maintaining LLM safety. Existing methods for jailbreak detection typically involve fine-tuning LLMs as static safety LLMs using fixed training datasets. However, these methods incur substantial computational costs when updating model parameters to improve robustness, especially in the face of novel jailbreak attacks. Inspired by immunological memory mechanisms, we propose the Multi-Agent Adaptive Guard (MAAG) framework for jailbreak detection. The core idea is to equip guard with memory capabilities: upon encountering novel jailbreak attacks, the system memorizes attack patterns, enabling it to rapidly and accurately identify similar threats in future encounters. Specifically, MAAG first extracts activation values from input prompts and compares them to historical activations stored in a memory bank for quick preliminary detection. A defense agent then simulates responses based on these detection results, and an auxiliary agent supervises the simulation process to provide secondary filtering of the detection outcomes. Extensive experiments across five open-source models demonstrate that MAAG significantly outperforms state-of-the-art (SOTA) methods, achieving 98% detection accuracy and a 96% F1-score across a diverse range of attack scenarios.

摘要: 大型语言模型（LLM）已成为人工智能系统的基础，但它们仍然容易受到敌对越狱攻击。这些攻击涉及精心设计的提示，绕过安全护栏并诱导模型产生有害内容。因此，检测此类恶意输入查询对于维护LLM安全至关重要。现有的越狱检测方法通常涉及使用固定训练数据集将LLM微调为静态安全LLM。然而，这些方法在更新模型参数以提高鲁棒性时会产生巨大的计算成本，尤其是在面对新型越狱攻击时。受免疫记忆机制的启发，我们提出了用于越狱检测的多智能体自适应警卫（MAAG）框架。核心想法是为警卫配备记忆能力：在遇到新颖的越狱攻击时，系统会记住攻击模式，使其能够在未来遇到类似威胁时快速准确地识别出类似威胁。具体来说，MAAG首先从输入提示中提取激活值，并将其与存储在存储库中的历史激活进行比较，以进行快速初步检测。然后，防御代理根据这些检测结果模拟响应，辅助代理监督模拟过程，以提供检测结果的二次过滤。针对五个开源模型的广泛实验表明，MAAG的性能明显优于最先进的（SOTA）方法，在各种攻击场景中实现了98%的检测准确率和96%的F1评分。



## **7. Invasive Context Engineering to Control Large Language Models**

控制大型语言模型的侵入性上下文工程 cs.AI

4 pages

**SubmitDate**: 2025-12-02    [abs](http://arxiv.org/abs/2512.03001v1) [paper-pdf](https://arxiv.org/pdf/2512.03001v1)

**Authors**: Thomas Rivasseau

**Abstract**: Current research on operator control of Large Language Models improves model robustness against adversarial attacks and misbehavior by training on preference examples, prompting, and input/output filtering. Despite good results, LLMs remain susceptible to abuse, and jailbreak probability increases with context length. There is a need for robust LLM security guarantees in long-context situations. We propose control sentences inserted into the LLM context as invasive context engineering to partially solve the problem. We suggest this technique can be generalized to the Chain-of-Thought process to prevent scheming. Invasive Context Engineering does not rely on LLM training, avoiding data shortage pitfalls which arise in training models for long context situations.

摘要: 当前对大型语言模型操作员控制的研究通过对偏好示例、提示和输入/输出过滤进行训练，提高了模型针对对抗性攻击和不当行为的鲁棒性。尽管结果良好，但LLM仍然容易受到滥用，越狱可能性随着上下文长度的增加而增加。在长期背景下需要强有力的LLM安全保证。我们建议将控制句插入到LLM上下文中，作为侵入性上下文工程，以部分解决问题。我们建议这种技术可以推广到思想链过程中，以防止阴谋。侵入式上下文工程不依赖于LLM培训，从而避免了长期上下文情况的训练模型中出现的数据短缺陷阱。



## **8. Defense That Attacks: How Robust Models Become Better Attackers**

攻击的防御：稳健模型如何成为更好的攻击者 cs.CV

**SubmitDate**: 2025-12-03    [abs](http://arxiv.org/abs/2512.02830v2) [paper-pdf](https://arxiv.org/pdf/2512.02830v2)

**Authors**: Mohamed Awad, Mahmoud Akrm, Walid Gomaa

**Abstract**: Deep learning has achieved great success in computer vision, but remains vulnerable to adversarial attacks. Adversarial training is the leading defense designed to improve model robustness. However, its effect on the transferability of attacks is underexplored. In this work, we ask whether adversarial training unintentionally increases the transferability of adversarial examples. To answer this, we trained a diverse zoo of 36 models, including CNNs and ViTs, and conducted comprehensive transferability experiments. Our results reveal a clear paradox: adversarially trained (AT) models produce perturbations that transfer more effectively than those from standard models, which introduce a new ecosystem risk. To enable reproducibility and further study, we release all models, code, and experimental scripts. Furthermore, we argue that robustness evaluations should assess not only the resistance of a model to transferred attacks but also its propensity to produce transferable adversarial examples.

摘要: 深度学习在计算机视觉领域取得了巨大成功，但仍然容易受到对抗攻击。对抗训练是旨在提高模型稳健性的主要防御措施。然而，它对攻击可转移性的影响尚未得到充分研究。在这项工作中，我们询问对抗性训练是否无意中增加了对抗性示例的可移植性。为了解决这个问题，我们训练了一个由36个模型组成的多样化动物园，包括CNN和ViT，并进行了全面的可移植性实验。我们的结果揭示了一个明显的悖论：对抗训练（AT）模型会产生比标准模型更有效地传递的扰动，从而引入了新的生态系统风险。为了实现可重复性和进一步研究，我们发布了所有模型、代码和实验脚本。此外，我们认为稳健性评估不仅应该评估模型对转移攻击的抵抗力，还应该评估其产生可转移对抗示例的倾向。



## **9. Decryption Through Polynomial Ambiguity: Noise-Enhanced High-Memory Convolutional Codes for Post-Quantum Cryptography**

通过多项歧义解密：后量子密码学的降噪高内存卷积码 cs.CR

23 pages, 3 figures. arXiv admin note: substantial text overlap with arXiv:2510.15515

**SubmitDate**: 2025-12-03    [abs](http://arxiv.org/abs/2512.02822v2) [paper-pdf](https://arxiv.org/pdf/2512.02822v2)

**Authors**: Meir Ariel

**Abstract**: We present a novel approach to post-quantum cryptography that employs directed-graph decryption of noise-enhanced high-memory convolutional codes. The proposed construction generates random-like generator matrices that effectively conceal algebraic structure and resist known structural attacks. Security is further reinforced by the deliberate injection of strong noise during decryption, arising from polynomial division: while legitimate recipients retain polynomial-time decoding, adversaries face exponential-time complexity. As a result, the scheme achieves cryptanalytic security margins surpassing those of Classic McEliece by factors exceeding 2^(200). Beyond its enhanced security, the method offers greater design flexibility, supporting arbitrary plaintext lengths with linear-time decryption and uniform per-bit computational cost, enabling seamless scalability to long messages. Practical deployment is facilitated by parallel arrays of directed-graph decoders, which identify the correct plaintext through polynomial ambiguity while allowing efficient hardware and software implementations. Altogether, the scheme represents a compelling candidate for robust, scalable, and quantum-resistant public-key cryptography.

摘要: 我们提出了一种新的后量子密码学方法，该方法采用噪音增强的高内存卷积码的有向图解密。提出的结构生成类随机生成矩阵，可以有效地隐藏代数结构并抵抗已知的结构攻击。解密期间故意注入强噪音，这是由多项分解引起的，进一步加强了安全性：虽然合法接收者保留了多项时间解码，但对手面临着指数时间复杂性。结果，该计划实现的密码分析安全利润超过了Classic McEliece的安全利润，其系数超过了2 ^（200）。除了增强的安全性之外，该方法还提供了更大的设计灵活性，支持任意明文长度，具有线性时间解密和统一的每位计算成本，从而实现了对长消息的无缝可扩展性。有向图解码器的并行阵列促进了实际部署，这些解码器通过多元歧义识别正确的明文，同时允许高效的硬件和软件实现。总而言之，该方案代表了稳健、可扩展和抗量子公钥加密的令人信服的候选者。



## **10. Characterizing Cyber Attacks against Space Infrastructures with Missing Data: Framework and Case Study**

描述针对缺失数据的太空基础设施的网络攻击：框架和案例研究 cs.CR

**SubmitDate**: 2025-12-02    [abs](http://arxiv.org/abs/2512.02414v1) [paper-pdf](https://arxiv.org/pdf/2512.02414v1)

**Authors**: Ekzhin Ear, Jose Luis Castanon Remy, Caleb Chang, Qiren Que, Antonia Feffer, Shouhuai Xu

**Abstract**: Cybersecurity of space infrastructures is an emerging topic, despite space-related cybersecurity incidents occurring as early as 1977 (i.e., hijacking of a satellite transmission signal). There is no single dataset that documents cyber attacks against space infrastructures that have occurred in the past; instead, these incidents are often scattered in media reports while missing many details, which we dub the missing-data problem. Nevertheless, even ``low-quality'' datasets containing such reports would be extremely valuable because of the dearth of space cybersecurity data and the sensitivity of space infrastructures which are often restricted from disclosure by governments. This prompts a research question: How can we characterize real-world cyber attacks against space infrastructures? In this paper, we address the problem by proposing a framework, including metrics, while also addressing the missing-data problem by leveraging methodologies such as the Space Attack Research and Tactic Analysis (SPARTA) and the Adversarial Tactics, Techniques, and Common Knowledge (ATT&CK) to ``extrapolate'' the missing data in a principled fashion. We show how the extrapolated data can be used to reconstruct ``hypothetical but plausible'' space cyber kill chains and space cyber attack campaigns that have occurred in practice. To show the usefulness of the framework, we extract data for 108 cyber attacks against space infrastructures and show how to extrapolate this ``low-quality'' dataset containing missing information to derive 6,206 attack technique-level space cyber kill chains. Our findings include: cyber attacks against space infrastructures are getting increasingly sophisticated; successful protection of the link segment between the space and user segments could have thwarted nearly half of the 108 attacks. We will make our dataset available.

摘要: 尽管早在1977年就发生过与太空相关的网络安全事件（即，劫持卫星传输信号）。没有一个数据集可以记录过去发生的针对太空基础设施的网络攻击;相反，这些事件通常分散在媒体报道中，同时缺少许多细节，我们称之为数据丢失问题。然而，即使是包含此类报告的“低质量”数据集也极具价值，因为太空网络安全数据缺乏，而且太空基础设施的敏感性往往受到政府的限制。这引发了一个研究问题：我们如何描述现实世界中针对太空基础设施的网络攻击？在本文中，我们通过提出一个包括指标在内的框架来解决这个问题，同时还通过利用空间攻击研究和战术分析（SPARTA）和对抗战术、技术和常识（ATT & CK）等方法来解决丢失数据的问题，以有原则的方式“推断”丢失的数据。我们展示了如何使用推断的数据来重建“假设但可能”的太空网络杀伤链和实践中发生的太空网络攻击活动。为了展示该框架的有用性，我们提取了针对太空基础设施的108次网络攻击的数据，并展示了如何推断这个包含缺失信息的“低质量”数据集，以推导出6，206个攻击技术级别的太空网络杀伤链。我们的调查结果包括：针对太空基础设施的网络攻击正变得越来越复杂;成功保护太空和用户段之间的链路段可以挫败108次攻击中的近一半。我们将提供我们的数据集。



## **11. LeechHijack: Covert Computational Resource Exploitation in Intelligent Agent Systems**

LeechHijack：智能代理系统中的秘密计算资源开发 cs.CR

**SubmitDate**: 2025-12-02    [abs](http://arxiv.org/abs/2512.02321v1) [paper-pdf](https://arxiv.org/pdf/2512.02321v1)

**Authors**: Yuanhe Zhang, Weiliu Wang, Zhenhong Zhou, Kun Wang, Jie Zhang, Li Sun, Yang Liu, Sen Su

**Abstract**: Large Language Model (LLM)-based agents have demonstrated remarkable capabilities in reasoning, planning, and tool usage. The recently proposed Model Context Protocol (MCP) has emerged as a unifying framework for integrating external tools into agent systems, enabling a thriving open ecosystem of community-built functionalities. However, the openness and composability that make MCP appealing also introduce a critical yet overlooked security assumption -- implicit trust in third-party tool providers. In this work, we identify and formalize a new class of attacks that exploit this trust boundary without violating explicit permissions. We term this new attack vector implicit toxicity, where malicious behaviors occur entirely within the allowed privilege scope. We propose LeechHijack, a Latent Embedded Exploit for Computation Hijacking, in which an adversarial MCP tool covertly expropriates the agent's computational resources for unauthorized workloads. LeechHijack operates through a two-stage mechanism: an implantation stage that embeds a benign-looking backdoor in a tool, and an exploitation stage where the backdoor activates upon predefined triggers to establish a command-and-control channel. Through this channel, the attacker injects additional tasks that the agent executes as if they were part of its normal workflow, effectively parasitizing the user's compute budget. We implement LeechHijack across four major LLM families. Experiments show that LeechHijack achieves an average success rate of 77.25%, with a resource overhead of 18.62% compared to the baseline. This study highlights the urgent need for computational provenance and resource attestation mechanisms to safeguard the emerging MCP ecosystem.

摘要: 基于大型语言模型（LLM）的代理在推理、规划和工具使用方面表现出了非凡的能力。最近提出的模型上下文协议（HCP）已成为将外部工具集成到代理系统中的统一框架，从而实现社区构建功能的蓬勃发展的开放生态系统。然而，使HCP具有吸引力的开放性和可组合性也引入了一个关键但被忽视的安全假设--对第三方工具提供商的隐性信任。在这项工作中，我们识别并正式化一类新的攻击，这些攻击利用此信任边界，而不违反显式许可。我们将这种新的攻击向量称为隐式毒性，其中恶意行为完全发生在允许的权限范围内。我们提出了LeechHijack，一个潜在的嵌入式利用计算劫持，其中一个敌对的MCP工具隐蔽地征用代理的计算资源用于未经授权的工作负载。LeechHijack通过两个阶段的机制运行：植入阶段，在工具中嵌入一个看起来很好的后门，以及利用阶段，后门在预定义的触发器上激活以建立命令和控制通道。通过此通道，攻击者注入代理执行的额外任务，就好像它们是其正常工作流的一部分，有效地寄生了用户的计算预算。我们在四个主要LLM家族中实施LeechHijack。实验表明，LeechHijack的平均成功率为77.25%，与基线相比资源消耗为18.62%。这项研究强调了迫切需要计算出处和资源证明机制来保护新兴的LCP生态系统。



## **12. COGNITION: From Evaluation to Defense against Multimodal LLM CAPTCHA Solvers**

认知：从评估到防御多模式LLM验证码解决器 cs.CR

**SubmitDate**: 2025-12-03    [abs](http://arxiv.org/abs/2512.02318v2) [paper-pdf](https://arxiv.org/pdf/2512.02318v2)

**Authors**: Junyu Wang, Changjia Zhu, Yuanbo Zhou, Lingyao Li, Xu He, Junjie Xiong

**Abstract**: This paper studies how multimodal large language models (MLLMs) undermine the security guarantees of visual CAPTCHA. We identify the attack surface where an adversary can cheaply automate CAPTCHA solving using off-the-shelf models. We evaluate 7 leading commercial and open-source MLLMs across 18 real-world CAPTCHA task types, measuring single-shot accuracy, success under limited retries, end-to-end latency, and per-solve cost. We further analyze the impact of task-specific prompt engineering and few-shot demonstrations on solver effectiveness. We reveal that MLLMs can reliably solve recognition-oriented and low-interaction CAPTCHA tasks at human-like cost and latency, whereas tasks requiring fine-grained localization, multi-step spatial reasoning, or cross-frame consistency remain significantly harder for current models. By examining the reasoning traces of such MLLMs, we investigate the underlying mechanisms of why models succeed/fail on specific CAPTCHA puzzles and use these insights to derive defense-oriented guidelines for selecting and strengthening CAPTCHA tasks. We conclude by discussing implications for platform operators deploying CAPTCHA as part of their abuse-mitigation pipeline.Code Availability (https://anonymous.4open.science/r/Captcha-465E/).

摘要: 本文研究了多模式大型语言模型（MLLM）如何破坏视觉验证码的安全保证。我们确定了对手可以使用现成模型廉价地自动化验证码解决的攻击表面。我们评估了18种现实世界的CAPTCHA任务类型中的7种领先的商业和开源MLLM，衡量单次准确性、有限再试下的成功率、端到端延迟和每次解决的成本。我们进一步分析了特定任务的即时工程和少数镜头演示对求解器有效性的影响。我们发现，MLLM可以以类似于人类的成本和延迟可靠地解决面向认知和低交互性的CAPTCHA任务，而对于当前的模型来说，需要细粒度本地化、多步空间推理或跨框架一致性的任务仍然明显困难。通过检查此类MLLM的推理痕迹，我们研究模型为何在特定验证码难题上成功/失败的潜在机制，并利用这些见解来得出选择和加强验证码任务的防御导向指南。最后，我们讨论了部署CAPTCHA作为其虐待缓解管道的一部分对平台运营商的影响。代码可用性（https：//anonymous.4open.science/r/Captcha-465E/）。



## **13. Adversarial Robustness of Traffic Classification under Resource Constraints: Input Structure Matters**

资源约束下流量分类的对抗鲁棒性：输入结构很重要 cs.NI

Accepted at the 2025 IEEE International Symposium on Networks, Computers and Communications (ISNCC)

**SubmitDate**: 2025-12-01    [abs](http://arxiv.org/abs/2512.02276v1) [paper-pdf](https://arxiv.org/pdf/2512.02276v1)

**Authors**: Adel Chehade, Edoardo Ragusa, Paolo Gastaldo, Rodolfo Zunino

**Abstract**: Traffic classification (TC) plays a critical role in cybersecurity, particularly in IoT and embedded contexts, where inspection must often occur locally under tight hardware constraints. We use hardware-aware neural architecture search (HW-NAS) to derive lightweight TC models that are accurate, efficient, and deployable on edge platforms. Two input formats are considered: a flattened byte sequence and a 2D packet-wise time series; we examine how input structure affects adversarial vulnerability when using resource-constrained models. Robustness is assessed against white-box attacks, specifically Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD). On USTC-TFC2016, both HW-NAS models achieve over 99% clean-data accuracy while remaining within 65k parameters and 2M FLOPs. Yet under perturbations of strength 0.1, their robustness diverges: the flat model retains over 85% accuracy, while the time-series variant drops below 35%. Adversarial fine-tuning delivers robust gains, with flat-input accuracy exceeding 96% and the time-series variant recovering over 60 percentage points in robustness, all without compromising efficiency. The results underscore how input structure influences adversarial vulnerability, and show that even compact, resource-efficient models can attain strong robustness, supporting their practical deployment in secure edge-based TC.

摘要: 流量分类（TC）在网络安全中发挥着关键作用，特别是在物联网和嵌入式环境中，检查通常必须在严格的硬件限制下在本地进行。我们使用硬件感知神经架构搜索（HW-NAS）来推导准确、高效且可在边缘平台上部署的轻量级TC模型。考虑了两种输入格式：扁平字节序列和2D逐包时间序列;我们研究了使用资源受限模型时输入结构如何影响对抗脆弱性。鲁棒性是针对白盒攻击进行评估的，特别是快速梯度符号法（FGSM）和投影梯度下降（PVD）。在USTC-TFC 2016上，这两种HW-NAS型号都实现了超过99%的干净数据准确性，同时保持在65 k个参数和2 M个FLOP范围内。然而，在强度0.1的扰动下，它们的鲁棒性出现了分歧：平坦模型保留了85%以上的准确性，而时间序列变体则降至35%以下。对抗性微调可带来稳健的收益，平坦输入准确性超过96%，时间序列变量的稳健性恢复超过60个百分点，而所有这些都不会影响效率。结果强调了输入结构如何影响对抗脆弱性，并表明即使是紧凑、资源高效的模型也可以获得强大的鲁棒性，支持其在安全的基于边缘的TC中的实际部署。



## **14. TradeTrap: Are LLM-based Trading Agents Truly Reliable and Faithful?**

TradeTrap：基于LLM的贸易代理真的可靠和忠诚吗？ cs.AI

**SubmitDate**: 2025-12-01    [abs](http://arxiv.org/abs/2512.02261v1) [paper-pdf](https://arxiv.org/pdf/2512.02261v1)

**Authors**: Lewen Yan, Jilin Mei, Tianyi Zhou, Lige Huang, Jie Zhang, Dongrui Liu, Jing Shao

**Abstract**: LLM-based trading agents are increasingly deployed in real-world financial markets to perform autonomous analysis and execution. However, their reliability and robustness under adversarial or faulty conditions remain largely unexamined, despite operating in high-risk, irreversible financial environments. We propose TradeTrap, a unified evaluation framework for systematically stress-testing both adaptive and procedural autonomous trading agents. TradeTrap targets four core components of autonomous trading agents: market intelligence, strategy formulation, portfolio and ledger handling, and trade execution, and evaluates their robustness under controlled system-level perturbations. All evaluations are conducted in a closed-loop historical backtesting setting on real US equity market data with identical initial conditions, enabling fair and reproducible comparisons across agents and attacks. Extensive experiments show that small perturbations at a single component can propagate through the agent decision loop and induce extreme concentration, runaway exposure, and large portfolio drawdowns across both agent types, demonstrating that current autonomous trading agents can be systematically misled at the system level. Our code is available at https://github.com/Yanlewen/TradeTrap.

摘要: 基于LLM的交易代理越来越多地部署在现实世界的金融市场中，以执行自主分析和执行。然而，尽管它们在高风险、不可逆转的金融环境中运营，但它们在对抗或故障条件下的可靠性和稳健性在很大程度上仍未得到检验。我们提出TradeTrap，这是一个统一的评估框架，用于系统性地对适应性和程序性自主交易代理进行压力测试。TradeTrap针对自主交易代理的四个核心组件：市场情报、策略制定、投资组合和分类帐处理以及交易执行，并评估其在受控系统级扰动下的稳健性。所有评估都是在闭环历史回溯测试环境中对真实美国股市数据进行的，初始条件相同，从而实现对代理人和攻击进行公平且可重复的比较。大量实验表明，单一成分的微小扰动可以通过代理人决策循环传播，并在两种代理人类型中引发极端集中、失控的风险敞口和大规模投资组合提款，这表明当前的自主交易代理人可能会在系统层面被系统性地误导。我们的代码可以在https://github.com/Yanlewen/TradeTrap上找到。



## **15. Physical ID-Transfer Attacks against Multi-Object Tracking via Adversarial Trajectory**

通过对抗轨迹对多目标跟踪的物理ID传输攻击 cs.CV

Accepted to Annual Computer Security Applications Conference (ACSAC) 2024

**SubmitDate**: 2025-12-01    [abs](http://arxiv.org/abs/2512.01934v1) [paper-pdf](https://arxiv.org/pdf/2512.01934v1)

**Authors**: Chenyi Wang, Yanmao Man, Raymond Muller, Ming Li, Z. Berkay Celik, Ryan Gerdes, Jonathan Petit

**Abstract**: Multi-Object Tracking (MOT) is a critical task in computer vision, with applications ranging from surveillance systems to autonomous driving. However, threats to MOT algorithms have yet been widely studied. In particular, incorrect association between the tracked objects and their assigned IDs can lead to severe consequences, such as wrong trajectory predictions. Previous attacks against MOT either focused on hijacking the trackers of individual objects, or manipulating the tracker IDs in MOT by attacking the integrated object detection (OD) module in the digital domain, which are model-specific, non-robust, and only able to affect specific samples in offline datasets. In this paper, we present AdvTraj, the first online and physical ID-manipulation attack against tracking-by-detection MOT, in which an attacker uses adversarial trajectories to transfer its ID to a targeted object to confuse the tracking system, without attacking OD. Our simulation results in CARLA show that AdvTraj can fool ID assignments with 100% success rate in various scenarios for white-box attacks against SORT, which also have high attack transferability (up to 93% attack success rate) against state-of-the-art (SOTA) MOT algorithms due to their common design principles. We characterize the patterns of trajectories generated by AdvTraj and propose two universal adversarial maneuvers that can be performed by a human walker/driver in daily scenarios. Our work reveals under-explored weaknesses in the object association phase of SOTA MOT systems, and provides insights into enhancing the robustness of such systems.

摘要: 多目标跟踪（MOT）是计算机视觉中的一项关键任务，其应用范围从监控系统到自动驾驶。然而，MOT算法面临的威胁尚未得到广泛研究。特别是，被跟踪对象与其分配的ID之间的不正确关联可能会导致严重的后果，例如错误的轨迹预测。之前针对MOT的攻击要么集中在劫持单个对象的跟踪器上，要么通过攻击数字域中的集成对象检测（OD）模块来操纵MOT中的跟踪器ID，这些模块是特定于模型的、不鲁棒的，并且只能影响离线数据集中的特定样本。在本文中，我们介绍了AdvTraj，这是第一个针对检测跟踪MOT的在线和物理ID操纵攻击，其中攻击者使用对抗轨迹将其ID传输到目标对象以混淆跟踪系统，而不攻击OD。我们在CARLA中的模拟结果表明，AdvTraj可以在针对SORT的白盒攻击的各种场景中以100%的成功率欺骗ID分配，由于其共同的设计原则，SOTA还具有针对最先进（SOTA）MOT算法的高攻击转移性（高达93%的攻击成功率）。我们描述了AdvTraj生成的轨迹模式，并提出了两种可由人类步行者/驾驶员在日常场景中执行的通用对抗动作。我们的工作揭示了SOTA MOT系统对象关联阶段未充分探索的弱点，并提供了增强此类系统鲁棒性的见解。



## **16. Many-to-One Adversarial Consensus: Exposing Multi-Agent Collusion Risks in AI-Based Healthcare**

多对一对抗性共识：暴露基于人工智能的医疗保健中的多代理合谋风险 cs.CR

7 pages Conference level paper

**SubmitDate**: 2025-12-01    [abs](http://arxiv.org/abs/2512.03097v1) [paper-pdf](https://arxiv.org/pdf/2512.03097v1)

**Authors**: Adeela Bashir, The Anh han, Zia Ush Shamszaman

**Abstract**: The integration of large language models (LLMs) into healthcare IoT systems promises faster decisions and improved medical support. LLMs are also deployed as multi-agent teams to assist AI doctors by debating, voting, or advising on decisions. However, when multiple assistant agents interact, coordinated adversaries can collude to create false consensus, pushing an AI doctor toward harmful prescriptions. We develop an experimental framework with scripted and unscripted doctor agents, adversarial assistants, and a verifier agent that checks decisions against clinical guidelines. Using 50 representative clinical questions, we find that collusion drives the Attack Success Rate (ASR) and Harmful Recommendation Rates (HRR) up to 100% in unprotected systems. In contrast, the verifier agent restores 100% accuracy by blocking adversarial consensus. This work provides the first systematic evidence of collusion risk in AI healthcare and demonstrates a practical, lightweight defence that ensures guideline fidelity.

摘要: 将大型语言模型（LLM）集成到医疗保健物联网系统中有望实现更快的决策并改善医疗支持。LLM还被部署为多代理团队，通过辩论、投票或就决策提供建议来协助人工智能医生。然而，当多个助理特工互动时，协调一致的对手可能会勾结以建立错误共识，将人工智能医生推向有害的处方。我们开发了一个实验框架，其中包含有脚本和无脚本的医生代理、对抗助理和根据临床指南检查决策的验证者代理。通过使用50个代表性的临床问题，我们发现共谋导致未受保护的系统中的攻击成功率（ASB）和有害推荐率（HRR）高达100%。相比之下，验证者代理通过阻止对抗共识来恢复100%的准确性。这项工作提供了人工智能医疗保健中共谋风险的第一个系统性证据，并展示了一种实用、轻量级的防御，可以确保指南的忠实性。



## **17. Securing Large Language Models (LLMs) from Prompt Injection Attacks**

保护大型语言模型（LLM）免受提示注入攻击 cs.CR

10 pages, 1 figure, 1 table

**SubmitDate**: 2025-12-01    [abs](http://arxiv.org/abs/2512.01326v1) [paper-pdf](https://arxiv.org/pdf/2512.01326v1)

**Authors**: Omar Farooq Khan Suri, John McCrae

**Abstract**: Large Language Models (LLMs) are increasingly being deployed in real-world applications, but their flexibility exposes them to prompt injection attacks. These attacks leverage the model's instruction-following ability to make it perform malicious tasks. Recent work has proposed JATMO, a task-specific fine-tuning approach that trains non-instruction-tuned base models to perform a single function, thereby reducing susceptibility to adversarial instructions. In this study, we evaluate the robustness of JATMO against HOUYI, a genetic attack framework that systematically mutates and optimizes adversarial prompts. We adapt HOUYI by introducing custom fitness scoring, modified mutation logic, and a new harness for local model testing, enabling a more accurate assessment of defense effectiveness. We fine-tuned LLaMA 2-7B, Qwen1.5-4B, and Qwen1.5-0.5B models under the JATMO methodology and compared them with a fine-tuned GPT-3.5-Turbo baseline. Results show that while JATMO reduces attack success rates relative to instruction-tuned models, it does not fully prevent injections; adversaries exploiting multilingual cues or code-related disruptors still bypass defenses. We also observe a trade-off between generation quality and injection vulnerability, suggesting that better task performance often correlates with increased susceptibility. Our results highlight both the promise and limitations of fine-tuning-based defenses and point toward the need for layered, adversarially informed mitigation strategies.

摘要: 大型语言模型（LLM）越来越多地被部署在现实世界的应用程序中，但它们的灵活性使它们容易受到提示注入攻击。这些攻击利用模型的描述跟踪能力使其执行恶意任务。最近的工作提出了JATMO，这是一种针对任务的微调方法，它训练非指令调优的基本模型来执行单一功能，从而减少对对抗性指令的敏感性。在这项研究中，我们评估了JATMO对HOUYI的稳健性，HOUYI是一种系统性突变和优化对抗提示的基因攻击框架。我们通过引入自定义适应度评分、修改后的突变逻辑和用于本地模型测试的新工具来调整HOUYI，从而能够更准确地评估防御有效性。我们根据JATMO方法对LLaMA 2- 7 B、Qwen 1.5 - 4 B和Qwen 1.5 -0.5B模型进行了微调，并将它们与微调的GPT-3.5-Turbo基线进行了比较。结果表明，虽然JATMO相对于经描述调整的模型降低了攻击成功率，但它并不能完全阻止注入;利用多语言线索或代码相关破坏者的对手仍然绕过防御。我们还观察到生成质量和注入脆弱性之间的权衡，表明更好的任务性能通常与易感性的增加相关。我们的结果强调了基于微调的防御的前景和局限性，并指出需要分层、了解对手情况的缓解策略。



## **18. DPAC: Distribution-Preserving Adversarial Control for Diffusion Sampling**

DPAC：扩散抽样的分布保持对抗控制 cs.CV

**SubmitDate**: 2025-12-01    [abs](http://arxiv.org/abs/2512.01153v1) [paper-pdf](https://arxiv.org/pdf/2512.01153v1)

**Authors**: Han-Jin Lee, Han-Ju Lee, Jin-Seong Kim, Seok-Hwan Choi

**Abstract**: Adversarially guided diffusion sampling often achieves the target class, but sample quality degrades as deviations between the adversarially controlled and nominal trajectories accumulate. We formalize this degradation as a path-space Kullback-Leibler divergence(path-KL) between controlled and nominal (uncontrolled) diffusion processes, thereby showing via Girsanov's theorem that it exactly equals the control energy. Building on this stochastic optimal control (SOC) view, we theoretically establish that minimizing this path-KL simultaneously tightens upper bounds on both the 2-Wasserstein distance and Fréchet Inception Distance (FID), revealing a principled connection between adversarial control energy and perceptual fidelity. From a variational perspective, we derive a first-order optimality condition for the control: among all directions that yield the same classification gain, the component tangent to iso-(log-)density surfaces (i.e., orthogonal to the score) minimizes path-KL, whereas the normal component directly increases distributional drift. This leads to DPAC (Distribution-Preserving Adversarial Control), a diffusion guidance rule that projects adversarial gradients onto the tangent space defined by the generative score geometry. We further show that in discrete solvers, the tangent projection cancels the O(Δt) leading error term in the Wasserstein distance, achieving an O(Δt^2) quality gap; moreover, it remains second-order robust to score or metric approximation. Empirical studies on ImageNet-100 validate the theoretical predictions, confirming that DPAC achieves lower FID and estimated path-KL at matched attack success rates.

摘要: 不利引导的扩散采样通常可以达到目标类别，但随着不利控制轨迹和名义轨迹之间偏差的积累，样本质量会下降。我们将这种退化形式化为受控和名义（非受控）扩散过程之间的路径空间Kullback-Leibler分歧（路径-KL），从而通过吉萨诺夫定理表明它完全等于控制能。基于这种随机最优控制（SOC）观点，我们从理论上确定，最小化这条路径KL同时收紧了2-Wasserstein距离和Fréchet初始距离（DID）的上界，揭示了对抗控制能量和感知保真度之间的原则联系。从变分的角度来看，我们推导出控制的一阶最优性条件：在产生相同分类收益的所有方向中，与等（log）密度表面切向的分量（即，与分数垂直）最小化路径KL，而正向分量直接增加分布漂移。这导致了DPAC（分布保持对抗控制），这是一种扩散指导规则，将对抗梯度投影到生成分数几何定义的切空间上。我们进一步表明，在离散求解器中，切向投影消除了沃瑟斯坦距离中的O（Δt）领先误差项，从而实现了O（Δ t ' 2）质量差距;此外，它对得分或度量逼近保持二阶鲁棒性。ImageNet-100上的实证研究验证了理论预测，证实DPAC在匹配的攻击成功率下实现了较低的DID和估计路径KL。



## **19. Mitigating Indirect Prompt Injection via Instruction-Following Intent Analysis**

通过遵循指示的意图分析减轻间接提示注射 cs.CR

**SubmitDate**: 2025-11-30    [abs](http://arxiv.org/abs/2512.00966v1) [paper-pdf](https://arxiv.org/pdf/2512.00966v1)

**Authors**: Mintong Kang, Chong Xiang, Sanjay Kariyappa, Chaowei Xiao, Bo Li, Edward Suh

**Abstract**: Indirect prompt injection attacks (IPIAs), where large language models (LLMs) follow malicious instructions hidden in input data, pose a critical threat to LLM-powered agents. In this paper, we present IntentGuard, a general defense framework based on instruction-following intent analysis. The key insight of IntentGuard is that the decisive factor in IPIAs is not the presence of malicious text, but whether the LLM intends to follow instructions from untrusted data. Building on this insight, IntentGuard leverages an instruction-following intent analyzer (IIA) to identify which parts of the input prompt the model recognizes as actionable instructions, and then flag or neutralize any overlaps with untrusted data segments. To instantiate the framework, we develop an IIA that uses three "thinking intervention" strategies to elicit a structured list of intended instructions from reasoning-enabled LLMs. These techniques include start-of-thinking prefilling, end-of-thinking refinement, and adversarial in-context demonstration. We evaluate IntentGuard on two agentic benchmarks (AgentDojo and Mind2Web) using two reasoning-enabled LLMs (Qwen-3-32B and gpt-oss-20B). Results demonstrate that IntentGuard achieves (1) no utility degradation in all but one setting and (2) strong robustness against adaptive prompt injection attacks (e.g., reducing attack success rates from 100% to 8.5% in a Mind2Web scenario).

摘要: 间接提示注入攻击（IPIA）（大型语言模型（LLM）遵循隐藏在输入数据中的恶意指令）对LLM驱动的代理构成了严重威胁。在本文中，我们介绍了IntentGuard，这是一个基于遵循策略的意图分析的通用防御框架。IntentGuard的关键见解是，IPIA的决定性因素不是恶意文本的存在，而是LLM是否打算遵循不受信任数据的指示。基于这一见解，IntentGuard利用描述跟踪意图分析器（RIA）来识别模型将输入提示的哪些部分识别为可操作指令，然后标记或抵消与不受信任数据段的任何重叠。为了实例化该框架，我们开发了一个RIA，该RIA使用三种“思维干预”策略来从支持推理的LLM中获取预期指令的结构化列表。这些技术包括思维起点预填充、思维终点细化和对抗性上下文演示。我们使用两个支持推理的LLM（Qwen-3- 32 B和gtt-oss-20 B）在两个代理基准测试（AgentDojo和Mind 2 Web）上评估IntentGuard。结果表明，IntentGuard实现了（1）在除一种设置之外的所有设置中没有效用下降，以及（2）针对自适应提示注入攻击（例如，在Mind 2 Web场景中将攻击成功率从100%降低到8.5%）。



## **20. Bias Injection Attacks on RAG Databases and Sanitization Defenses**

对RAG数据库和清理防御的偏见注入攻击 cs.CR

**SubmitDate**: 2025-11-30    [abs](http://arxiv.org/abs/2512.00804v1) [paper-pdf](https://arxiv.org/pdf/2512.00804v1)

**Authors**: Hao Wu, Prateek Saxena

**Abstract**: This paper explores attacks and defenses on vector databases in retrieval-augmented generation (RAG) systems. Prior work on knowledge poisoning attacks primarily inject false or toxic content, which fact-checking or linguistic analysis easily detects. We reveal a new and subtle threat: bias injection attacks, which insert factually correct yet semantically biased passages into the knowledge base to covertly influence the ideological framing of answers generated by large language models (LLMs). We demonstrate that these adversarial passages, though linguistically coherent and truthful, can systematically crowd out opposing views from the retrieved context and steer LLM answers toward the attacker's intended perspective.   We precisely characterize this class of attacks and then develop a post-retrieval filtering defense, BiasDef. We construct a comprehensive benchmark based on public question answering datasets to evaluate them. Our results show that: (1) the proposed attack induces significant perspective shifts in LLM answers, effectively evading existing retrieval-based sanitization defenses; and (2) BiasDef outperforms existing methods by reducing adversarial passages retrieved by 15\% which mitigates perspective shift by 6.2\times in answers, while enabling the retrieval of 62\% more benign passages.

摘要: 本文探讨了检索增强生成（RAG）系统中对载体数据库的攻击和防御。之前关于知识中毒攻击的工作主要注入虚假或有毒内容，事实检查或语言分析很容易检测到这些内容。我们揭示了一个新的微妙威胁：偏见注入攻击，它将事实正确但语义有偏见的段落插入知识库，以秘密影响大型语言模型（LLM）生成的答案的意识形态框架。我们证明，这些对抗性的段落尽管在语言上连贯且真实，但可以系统地从检索到的上下文中剔除相反的观点，并将LLM的答案引导到攻击者的预期角度。   我们精确地描述这类攻击，然后开发检索后过滤防御BiasDef。我们基于公开问答数据集构建一个全面的基准来评估它们。我们的结果表明：（1）拟议的攻击在LLM答案中引发了显着的视角转变，有效地规避了现有的基于检索的清理防御;和（2）BiasDef通过将检索到的对抗性段落减少15%，从而减少了6.2%答案的视角转变，同时能够检索出62%的良性段落。



## **21. The Outline of Deception: Physical Adversarial Attacks on Traffic Signs Using Edge Patches**

欺骗概述：使用边缘补丁对交通标志进行物理对抗攻击 cs.CV

**SubmitDate**: 2025-12-03    [abs](http://arxiv.org/abs/2512.00765v2) [paper-pdf](https://arxiv.org/pdf/2512.00765v2)

**Authors**: Haojie Ji, Te Hu, Haowen Li, Long Jin, Chongshi Xin, Yuchi Yao, Jiarui Xiao

**Abstract**: Intelligent driving systems are vulnerable to physical adversarial attacks on traffic signs. These attacks can cause misclassification, leading to erroneous driving decisions that compromise road safety. Moreover, within V2X networks, such misinterpretations can propagate, inducing cascading failures that disrupt overall traffic flow and system stability. However, a key limitation of current physical attacks is their lack of stealth. Most methods apply perturbations to central regions of the sign, resulting in visually salient patterns that are easily detectable by human observers, thereby limiting their real-world practicality. This study proposes TESP-Attack, a novel stealth-aware adversarial patch method for traffic sign classification. Based on the observation that human visual attention primarily focuses on the central regions of traffic signs, we employ instance segmentation to generate edge-aligned masks that conform to the shape characteristics of the signs. A U-Net generator is utilized to craft adversarial patches, which are then optimized through color and texture constraints along with frequency domain analysis to achieve seamless integration with the background environment, resulting in highly effective visual concealment. The proposed method demonstrates outstanding attack success rates across traffic sign classification models with varied architectures, achieving over 90% under limited query budgets. It also exhibits strong cross-model transferability and maintains robust real-world performance that remains stable under varying angles and distances.

摘要: 智能驾驶系统很容易受到交通标志的物理对抗攻击。这些攻击可能会导致错误分类，导致错误的驾驶决定，从而危及道路安全。此外，在V2X网络中，此类误解可能会传播，引发连锁故障，从而扰乱整体流量和系统稳定性。然而，当前物理攻击的一个关键局限性是缺乏隐形性。大多数方法都会对标志的中心区域施加扰动，从而产生人类观察者很容易检测到的视觉上明显的模式，从而限制了它们在现实世界中的实用性。这项研究提出了TESP-Attack，这是一种用于交通标志分类的新型隐形感知对抗补丁方法。根据人类视觉注意力主要集中在交通标志的中心区域的观察，我们采用实例分割来生成符合标志形状特征的边缘对齐的面罩。使用U-Net生成器来制作对抗补丁，然后通过颜色和纹理约束以及频域分析对其进行优化，以实现与背景环境的无缝集成，从而实现高效的视觉隐藏。所提出的方法在具有不同架构的交通标志分类模型中表现出出色的攻击成功率，在有限的查询预算下实现了90%以上。它还展现出强大的跨模型可移植性，并保持稳健的现实世界性能，在不同角度和距离下保持稳定。



## **22. Extended Abstract: Synthesizable Low-overhead Circuit-level Countermeasures and Pro-Active Detection Techniques for Power and EM SCA**

扩展摘要：用于电源和EM SCA的可综合低开销电路级对策和主动检测技术 cs.CR

This extended abstract is archived for educational purposes as an example for different PhD forum competitions. Total page is 3

**SubmitDate**: 2025-11-29    [abs](http://arxiv.org/abs/2512.00635v1) [paper-pdf](https://arxiv.org/pdf/2512.00635v1)

**Authors**: Archisman Ghosh

**Abstract**: The gamut of todays internet-connected embedded devices has led to increased concerns regarding the security and confidentiality of data. Most internet-connected embedded devices employ mathematically secure cryptographic algorithms to address security vulnerabilities. Despite such mathematical guarantees, as these algorithms are often implemented in silicon, they leak critical information in terms of power consumption, electromagnetic (EM) radiation, timing, cache hits and misses, photonic emission and so on, leading to side-channel analysis (SCA) attacks. This thesis focuses on low overhead generic circuit-level yet synthesizable countermeasures against power and EM SCA. Existing countermeasures (including proposed) still have relatively high overhead which bars them from being used in energy-constraint IoT devices. We propose a zero-overhead integrated inductive sensor which is able to detect i)EM SCA ii) Clock glitch-based Fault Injection Attack (FIA), and iii) Voltage-glitch based Fault Injection Attack by using a simple ML algorithm. Advent of quantum computer research will open new possibilities for theoretical attacks against existing cryptographic protocols. National Institute of Standard & Technology (NIST) has standardized post-quantum cryptographic algorithms to secure crypto-systems against quantum adversary. I contribute to the standardization procedure by introducing the first silicon-verified Saber (a NIST finalist modulo Learning with Rounding scheme) which consumes lowest energy and area till date amongst all the candidates.

摘要: 当今互联网连接的嵌入式设备的范围导致人们对数据安全性和机密性的担忧日益增加。大多数连接互联网的嵌入式设备都采用数学上安全的加密算法来解决安全漏洞。尽管有这样的数学保证，但由于这些算法通常在硅中实现，它们会泄露功耗、电磁（EM）辐射、定时、缓存命中和未命中、光发射等方面的关键信息，从而导致侧通道分析（SCA）攻击。本论文重点关注低开销的通用电路级但可综合的针对电源和EM SCA的对策。现有的应对措施（包括拟议的）仍然具有相对较高的管理费用，这阻碍了它们用于能源限制的物联网设备。我们提出了一种零开销集成感性传感器，它能够通过使用简单的ML算法检测i）EM SCA ii）基于时钟毛刺的故障注入攻击（FIA），以及iii）基于电压毛刺的故障注入攻击。量子计算机研究的出现将为针对现有加密协议的理论攻击开辟新的可能性。美国国家标准与技术研究所（NIH）已经标准化了后量子加密算法，以保护加密系统免受量子对手的侵害。我通过引入第一款经过硅验证的Saber（一款NIH决赛入围的带四舍五入模学习方案），为标准化程序做出了贡献，该产品迄今为止消耗的能量和面积是所有候选产品中最低的。



## **23. Superpixel Attack: Enhancing Black-box Adversarial Attack with Image-driven Division Areas**

超像素攻击：利用图像驱动的分区增强黑匣子对抗攻击 cs.CR

**SubmitDate**: 2025-11-29    [abs](http://arxiv.org/abs/2512.02062v1) [paper-pdf](https://arxiv.org/pdf/2512.02062v1)

**Authors**: Issa Oe, Keiichiro Yamamura, Hiroki Ishikura, Ryo Hamahira, Katsuki Fujisawa

**Abstract**: Deep learning models are used in safety-critical tasks such as automated driving and face recognition. However, small perturbations in the model input can significantly change the predictions. Adversarial attacks are used to identify small perturbations that can lead to misclassifications. More powerful black-box adversarial attacks are required to develop more effective defenses. A promising approach to black-box adversarial attacks is to repeat the process of extracting a specific image area and changing the perturbations added to it. Existing attacks adopt simple rectangles as the areas where perturbations are changed in a single iteration. We propose applying superpixels instead, which achieve a good balance between color variance and compactness. We also propose a new search method, versatile search, and a novel attack method, Superpixel Attack, which applies superpixels and performs versatile search. Superpixel Attack improves attack success rates by an average of 2.10% compared with existing attacks. Most models used in this study are robust against adversarial attacks, and this improvement is significant for black-box adversarial attacks. The code is avilable at https://github.com/oe1307/SuperpixelAttack.git.

摘要: 深度学习模型用于自动驾驶和面部识别等安全关键任务。然而，模型输入中的微小扰动可能会显着改变预测。对抗攻击用于识别可能导致错误分类的小扰动。需要更强大的黑匣子对抗攻击来开发更有效的防御。黑匣子对抗攻击的一种有希望的方法是重复提取特定图像区域并改变添加到其中的扰动的过程。现有的攻击采用简单的矩形作为在单次迭代中改变扰动的区域。我们建议应用超像素，这在颜色方差和紧凑性之间实现了良好的平衡。我们还提出了一种新的搜索方法--通用搜索和一种新型攻击方法--超像素攻击，它应用超像素并执行通用搜索。与现有攻击相比，Superpixel Attack的攻击成功率平均提高2.10%。本研究中使用的大多数模型都对对抗攻击具有鲁棒性，这种改进对于黑匣子对抗攻击具有重要意义。该代码可在https://github.com/oe1307/SuperpixelAttack.git上获取。



## **24. Teleportation-Based Defenses for Privacy in Approximate Machine Unlearning**

基于远程传输的近乎机器遗忘中的隐私辩护 cs.LG

**SubmitDate**: 2025-11-29    [abs](http://arxiv.org/abs/2512.00272v1) [paper-pdf](https://arxiv.org/pdf/2512.00272v1)

**Authors**: Mohammad M Maheri, Xavier Cadet, Peter Chin, Hamed Haddadi

**Abstract**: Approximate machine unlearning aims to efficiently remove the influence of specific data points from a trained model, offering a practical alternative to full retraining. However, it introduces privacy risks: an adversary with access to pre- and post-unlearning models can exploit their differences for membership inference or data reconstruction. We show these vulnerabilities arise from two factors: large gradient norms of forget-set samples and the close proximity of unlearned parameters to the original model. To demonstrate their severity, we propose unlearning-specific membership inference and reconstruction attacks, showing that several state-of-the-art methods (e.g., NGP, SCRUB) remain vulnerable. To mitigate this leakage, we introduce WARP, a plug-and-play teleportation defense that leverages neural network symmetries to reduce forget-set gradient energy and increase parameter dispersion while preserving predictions. This reparameterization obfuscates the signal of forgotten data, making it harder for attackers to distinguish forgotten samples from non-members or recover them via reconstruction. Across six unlearning algorithms, our approach achieves consistent privacy gains, reducing adversarial advantage (AUC) by up to 64% in black-box and 92% in white-box settings, while maintaining accuracy on retained data. These results highlight teleportation as a general tool for reducing attack success in approximate unlearning.

摘要: 近似机器去学习旨在有效地消除训练模型中特定数据点的影响，为完全再培训提供实用的替代方案。然而，它会带来隐私风险：能够访问取消学习前和取消学习后模型的对手可以利用它们的差异进行成员资格推断或数据重建。我们表明，这些漏洞源于两个因素：遗忘集样本的大梯度规范以及未学习的参数与原始模型的接近性。为了证明它们的严重性，我们提出了特定于未学习的隶属度推断和重建攻击，表明几种最先进的方法（例如，NGP、SCUTE）仍然脆弱。为了减轻这种泄漏，我们引入了WARP，这是一种即插即用的传送防御，它利用神经网络对称性来减少遗忘集的梯度能量并增加参数离散度，同时保留预测。这种重新参数化混淆了被遗忘数据的信号，使攻击者更难区分被遗忘的样本与非成员或通过重建恢复它们。在六种取消学习算法中，我们的方法实现了一致的隐私收益，在黑匣子环境中将对抗优势（AUDA）降低高达64%，在白盒环境中将对抗优势（AUDA）降低92%，同时保持保留数据的准确性。这些结果凸显了隐形传输是一种在大约取消学习中减少攻击成功的通用工具。



## **25. SD-CGAN: Conditional Sinkhorn Divergence GAN for DDoS Anomaly Detection in IoT Networks**

SD-CGAN：用于物联网网络中DDOS异常检测的条件Sinkhorn分歧GAN cs.LG

7 pages, 6 figures, camera-ready version accepted for presentation at IEEE ICNC 2026

**SubmitDate**: 2025-11-28    [abs](http://arxiv.org/abs/2512.00251v1) [paper-pdf](https://arxiv.org/pdf/2512.00251v1)

**Authors**: Henry Onyeka, Emmanuel Samson, Liang Hong, Tariqul Islam, Imtiaz Ahmed, Kamrul Hasan

**Abstract**: The increasing complexity of IoT edge networks presents significant challenges for anomaly detection, particularly in identifying sophisticated Denial-of-Service (DoS) attacks and zero-day exploits under highly dynamic and imbalanced traffic conditions. This paper proposes SD-CGAN, a Conditional Generative Adversarial Network framework enhanced with Sinkhorn Divergence, tailored for robust anomaly detection in IoT edge environments. The framework incorporates CTGAN-based synthetic data augmentation to address class imbalance and leverages Sinkhorn Divergence as a geometry-aware loss function to improve training stability and reduce mode collapse. The model is evaluated on exploitative attack subsets from the CICDDoS2019 dataset and compared against baseline deep learning and GAN-based approaches. Results show that SD-CGAN achieves superior detection accuracy, precision, recall, and F1-score while maintaining computational efficiency suitable for deployment in edge-enabled IoT environments.

摘要: 物联网边缘网络日益复杂，给异常检测带来了重大挑战，特别是在高度动态和不平衡的流量条件下识别复杂的拒绝服务（DPS）攻击和零日漏洞利用方面。本文提出了SD-CGAN，这是一个通过Sinkhorn Divergence增强的条件生成对抗网络框架，专为物联网边缘环境中的鲁棒异常检测而定制。该框架结合了基于CTGAN的合成数据增强来解决类失衡问题，并利用Sinkhorn Divergence作为几何感知损失函数来提高训练稳定性并减少模式崩溃。该模型是在来自CICDDoS 2019数据集的剥削性攻击子集上进行评估的，并与基线深度学习和基于GAN的方法进行比较。结果表明，SD-CGAN实现了卓越的检测准确性、精确性、召回率和F1评分，同时保持适合在支持边缘的物联网环境中部署的计算效率。



## **26. DeFi TrustBoost: Blockchain and AI for Trustworthy Decentralized Financial Decisions**

DeFi TrustBoost：区块链和人工智能实现值得信赖的去中心化财务决策 cs.CR

19 pages

**SubmitDate**: 2025-11-28    [abs](http://arxiv.org/abs/2512.00142v1) [paper-pdf](https://arxiv.org/pdf/2512.00142v1)

**Authors**: Swati Sachan, Dale S. Fickett

**Abstract**: This research introduces the Decentralized Finance (DeFi) TrustBoost Framework, which combines blockchain technology and Explainable AI to address challenges faced by lenders underwriting small business loan applications from low-wealth households. The framework is designed with a strong emphasis on fulfilling four crucial requirements of blockchain and AI systems: confidentiality, compliance with data protection laws, resistance to adversarial attacks, and compliance with regulatory audits. It presents a technique for tamper-proof auditing of automated AI decisions and a strategy for on-chain (inside-blockchain) and off-chain data storage to facilitate collaboration within and across financial organizations.

摘要: 这项研究引入了去中心化金融（DeFi）TrustBoost框架，该框架将区块链技术和可解释人工智能结合起来，以解决承保低财富家庭小企业贷款申请的贷方面临的挑战。该框架的设计非常注重满足区块链和人工智能系统的四个关键要求：保密性、遵守数据保护法、抵抗对抗性攻击以及遵守监管审计。它提供了一种对自动化人工智能决策进行防篡改审计的技术，以及一种链上（区块链内部）和链下数据存储的策略，以促进金融组织内部和金融组织之间的协作。



## **27. A Game-Theoretic Approach for Adversarial Information Fusion in Distributed Sensor Networks**

分布式传感器网络中对抗信息融合的博弈论方法 cs.CR

My PhD Thesis in Information Engineering and Sciences defended at University of Siena in Italy in 2017 under the supervision of Professor Mauro Barni

**SubmitDate**: 2025-11-28    [abs](http://arxiv.org/abs/2511.23026v1) [paper-pdf](https://arxiv.org/pdf/2511.23026v1)

**Authors**: Kassem Kallas

**Abstract**: Every day we share our personal information through digital systems which are constantly exposed to threats. For this reason, security-oriented disciplines of signal processing have received increasing attention in the last decades: multimedia forensics, digital watermarking, biometrics, network monitoring, steganography and steganalysis are just a few examples. Even though each of these fields has its own peculiarities, they all have to deal with a common problem: the presence of one or more adversaries aiming at making the system fail. Adversarial Signal Processing lays the basis of a general theory that takes into account the impact that the presence of an adversary has on the design of effective signal processing tools. By focusing on the application side of Adversarial Signal Processing, namely adversarial information fusion in distributed sensor networks, and adopting a game-theoretic approach, this thesis contributes to the above mission by addressing four issues. First, we address decision fusion in distributed sensor networks by developing a novel soft isolation defense scheme that protect the network from adversaries, specifically, Byzantines. Second, we develop an optimum decision fusion strategy in the presence of Byzantines. In the next step, we propose a technique to reduce the complexity of the optimum fusion by relying on a novel near-optimum message passing algorithm based on factor graphs. Finally, we introduce a defense mechanism to protect decentralized networks running consensus algorithm against data falsification attacks.

摘要: 我们每天都通过不断面临威胁的数字系统共享我们的个人信息。因此，在过去几十年里，以安全为导向的信号处理学科越来越受到关注：多媒体取证、数字水印、生物识别、网络监控、隐写术和隐写分析只是其中几个例子。尽管这些领域都有自己的特点，但它们都必须处理一个共同的问题：一个或多个旨在使系统失败的对手的存在。对抗信号处理奠定了一般理论的基础，该理论考虑了对手的存在对有效信号处理工具的设计的影响。本文通过关注对抗信号处理的应用端，即分布式传感器网络中的对抗信息融合，并采用博弈论方法，通过解决四个问题来完成上述任务。首先，我们通过开发一种新型的软隔离防御方案来解决分布式传感器网络中的决策融合问题，该方案可以保护网络免受对手（特别是拜占庭人）的侵害。其次，我们在拜占庭人存在的情况下开发最佳决策融合策略。下一步，我们提出了一种技术，通过依赖基于因子图的新型近最优消息传递算法来降低最优融合的复杂性。最后，我们引入了一种防御机制来保护运行共识算法的去中心化网络免受数据伪造攻击。



## **28. AgentShield: Make MAS more secure and efficient**

AgentShield：让MAS更加安全和高效 cs.MA

**SubmitDate**: 2025-11-28    [abs](http://arxiv.org/abs/2511.22924v1) [paper-pdf](https://arxiv.org/pdf/2511.22924v1)

**Authors**: Kaixiang Wang, Zhaojiacheng Zhou, Bunyod Suvonov, Jiong Lou, Jie LI

**Abstract**: Large Language Model (LLM)-based Multi-Agent Systems (MAS) offer powerful cooperative reasoning but remain vulnerable to adversarial attacks, where compromised agents can undermine the system's overall performance. Existing defenses either depend on single trusted auditors, creating single points of failure, or sacrifice efficiency for robustness. To resolve this tension, we propose \textbf{AgentShield}, a distributed framework for efficient, decentralized auditing. AgentShield introduces a novel three-layer defense: \textbf{(i) Critical Node Auditing} prioritizes high-influence agents via topological analysis; \textbf{(ii) Light Token Auditing} implements a cascade protocol using lightweight sentry models for rapid discriminative verification; and \textbf{(iii) Two-Round Consensus Auditing} triggers heavyweight arbiters only upon uncertainty to ensure global agreement. This principled design optimizes the robustness-efficiency trade-off. Experiments demonstrate that AgentShield achieves a 92.5\% recovery rate and reduces auditing overhead by over 70\% compared to existing methods, maintaining high collaborative accuracy across diverse MAS topologies and adversarial scenarios.

摘要: 基于大型语言模型（LLM）的多智能体系统（MAS）提供强大的合作推理，但仍然容易受到对抗攻击，其中受损的代理可能会破坏系统的整体性能。现有的防御要么依赖于单个受信任的审计员，从而产生单点故障，要么为了稳健性而牺牲效率。为了解决这种紧张关系，我们提出了\textBF{AgentShield}，这是一个用于高效、去中心化审计的分布式框架。AgentShield引入了一种新颖的三层防御：\textBF{（i）关键节点审计}通过拓扑分析优先考虑高影响力代理; \textBF{（ii）轻令牌审计}使用轻量级哨兵模型实现级联协议，以进行快速区分验证;和\textBF{（iii）两轮共识审计}仅在不确定性时触发重量级仲裁员，以确保全球协议。这种原则性的设计优化了稳健性与效率的权衡。实验表明，与现有方法相比，AgentShield实现了92.5%的恢复率，并将审计费用减少了70%以上，在不同MAS布局和对抗场景中保持了高协作准确性。



## **29. FedAU2: Attribute Unlearning for User-Level Federated Recommender Systems with Adaptive and Robust Adversarial Training**

FedAU 2：具有自适应和稳健对抗训练的用户级联邦推荐系统的属性取消学习 cs.IR

**SubmitDate**: 2025-11-28    [abs](http://arxiv.org/abs/2511.22872v1) [paper-pdf](https://arxiv.org/pdf/2511.22872v1)

**Authors**: Yuyuan Li, Junjie Fang, Fengyuan Yu, Xichun Sheng, Tianyu Du, Xuyang Teng, Shaowei Jiang, Linbo Jiang, Jianan Lin, Chaochao Chen

**Abstract**: Federated Recommender Systems (FedRecs) leverage federated learning to protect user privacy by retaining data locally. However, user embeddings in FedRecs often encode sensitive attribute information, rendering them vulnerable to attribute inference attacks. Attribute unlearning has emerged as a promising approach to mitigate this issue. In this paper, we focus on user-level FedRecs, which is a more practical yet challenging setting compared to group-level FedRecs. Adversarial training emerges as the most feasible approach within this context. We identify two key challenges in implementing adversarial training-based attribute unlearning for user-level FedRecs: i) mitigating training instability caused by user data heterogeneity, and ii) preventing attribute information leakage through gradients. To address these challenges, we propose FedAU2, an attribute unlearning method for user-level FedRecs. For CH1, we propose an adaptive adversarial training strategy, where the training dynamics are adjusted in response to local optimization behavior. For CH2, we propose a dual-stochastic variational autoencoder to perturb the adversarial model, effectively preventing gradient-based information leakage. Extensive experiments on three real-world datasets demonstrate that our proposed FedAU2 achieves superior performance in unlearning effectiveness and recommendation performance compared to existing baselines.

摘要: 联合推荐系统（FedRecs）利用联合学习通过在本地保留数据来保护用户隐私。然而，FedRecs中的用户嵌入通常对敏感属性信息进行编码，使其容易受到属性推断攻击。属性取消学习已成为缓解这个问题的一种有希望的方法。在本文中，我们重点关注用户级FedRecs，与组级FedRecs相比，这是一个更实用但更具挑战性的设置。对抗性培训成为这种背景下最可行的方法。我们确定了为用户级FedRecs实施基于对抗训练的属性去学习的两个关键挑战：i）减轻用户数据异类引起的训练不稳定性，以及ii）防止属性信息通过梯度泄露。为了解决这些挑战，我们提出了FedAU 2，这是一种用于用户级FedRecs的属性取消学习方法。对于CH 1，我们提出了一种自适应对抗训练策略，其中训练动态根据局部优化行为进行调整。对于CH 2，我们提出了一种双随机变分自动编码器来扰动对抗模型，有效地防止基于梯度的信息泄露。对三个现实世界数据集的广泛实验表明，与现有基线相比，我们提出的FedAU 2在取消学习有效性和推荐性能方面实现了更好的性能。



## **30. Ghosting Your LLM: Without The Knowledge of Your Gradient and Data**

幽灵你的法学硕士：不了解你的梯度和数据 cs.CR

**SubmitDate**: 2025-11-27    [abs](http://arxiv.org/abs/2511.22700v1) [paper-pdf](https://arxiv.org/pdf/2511.22700v1)

**Authors**: Abeer Matar A. Almalky, Ziyan Wang, Mohaiminul Al Nahian, Li Yang, Adnan Siraj Rakin

**Abstract**: In recent years, large language models (LLMs) have achieved substantial advancements and are increasingly integrated into critical applications across various domains. This growing adoption underscores the need to ensure their security and robustness. In this work, we focus on the impact of Bit Flip Attacks (BFAs) on LLMs, which exploits hardware faults to corrupt model parameters, posing a significant threat to model integrity and performance. Existing studies on BFA against LLMs adopt a progressive bit-search strategy that predominantly relies on gradient-based techniques to identify sensitive layers or weights. However, computing gradients comes with two specific challenges: First, in the context of LLMs, it increases computational and memory costs exponentially, and Second, it requires access to a sample victim dataset or knowledge of the victim domain to compute the gradient. In this work, we investigate beyond the scope of attack efficacy and aim to develop an efficient, practical Gradient-Data-free Bit-Flip Attack. The challenge lies in the core principle of adversarial attacks, which relies heavily on computing gradients from sample test/train data and manipulating model weights based on gradient information. To overcome this, we propose novel vulnerability index metrics that can identify vulnerable weight bits in LLMs independent of any gradient or data knowledge. By removing the dependency on gradient computation, our approach drastically reduces memory requirements and scales efficiently across multiple tasks with constant complexity. Experimental results demonstrate the efficiency of our method, requiring as few as a single bit flip to achieve adversarial objectives for five open-source LLMs.

摘要: 近年来，大型语言模型（LLM）取得了长足的进步，并越来越多地集成到各个领域的关键应用程序中。这种日益增长的采用凸显了确保其安全性和稳健性的必要性。在这项工作中，我们重点关注位翻转攻击（BFA）对LLM的影响，LLM利用硬件故障破坏模型参数，对模型完整性和性能构成重大威胁。针对LLM的BFA现有研究采用渐进式位搜索策略，该策略主要依赖于基于梯度的技术来识别敏感层或权重。然而，计算梯度面临两个具体挑战：首先，在LLM的环境中，它会呈指数级增加计算和内存成本，其次，它需要访问样本受害者数据集或受害者领域的知识来计算梯度。在这项工作中，我们研究了攻击功效的范围之外的问题，旨在开发一种高效、实用的无干扰数据位翻转攻击。挑战在于对抗攻击的核心原则，它严重依赖于从样本测试/训练数据计算梯度并根据梯度信息操纵模型权重。为了克服这个问题，我们提出了新颖的漏洞指数指标，该指标可以独立于任何梯度或数据知识来识别LLM中的漏洞权重位。通过消除对梯度计算的依赖，我们的方法大幅降低了内存需求，并在具有恒定复杂性的多个任务中高效扩展。实验结果证明了我们方法的效率，只需一位翻转即可实现五个开源LLM的对抗目标。



## **31. When Robots Obey the Patch: Universal Transferable Patch Attacks on Vision-Language-Action Models**

当机器人遵守补丁时：通用可转移补丁对视觉-语言-动作模型的攻击 cs.CV

**SubmitDate**: 2025-11-30    [abs](http://arxiv.org/abs/2511.21192v2) [paper-pdf](https://arxiv.org/pdf/2511.21192v2)

**Authors**: Hui Lu, Yi Yu, Yiming Yang, Chenyu Yi, Qixin Zhang, Bingquan Shen, Alex C. Kot, Xudong Jiang

**Abstract**: Vision-Language-Action (VLA) models are vulnerable to adversarial attacks, yet universal and transferable attacks remain underexplored, as most existing patches overfit to a single model and fail in black-box settings. To address this gap, we present a systematic study of universal, transferable adversarial patches against VLA-driven robots under unknown architectures, finetuned variants, and sim-to-real shifts. We introduce UPA-RFAS (Universal Patch Attack via Robust Feature, Attention, and Semantics), a unified framework that learns a single physical patch in a shared feature space while promoting cross-model transfer. UPA-RFAS combines (i) a feature-space objective with an $\ell_1$ deviation prior and repulsive InfoNCE loss to induce transferable representation shifts, (ii) a robustness-augmented two-phase min-max procedure where an inner loop learns invisible sample-wise perturbations and an outer loop optimizes the universal patch against this hardened neighborhood, and (iii) two VLA-specific losses: Patch Attention Dominance to hijack text$\to$vision attention and Patch Semantic Misalignment to induce image-text mismatch without labels. Experiments across diverse VLA models, manipulation suites, and physical executions show that UPA-RFAS consistently transfers across models, tasks, and viewpoints, exposing a practical patch-based attack surface and establishing a strong baseline for future defenses.

摘要: 视觉-语言-动作（VLA）模型容易受到对抗性攻击，但普遍和可转移的攻击仍然未充分研究，因为大多数现有补丁过于适合单一模型，并且在黑匣子设置中失败。为了解决这一差距，我们对未知架构、微调变体和模拟到真实转变下针对VLA驱动机器人的通用、可转移的对抗补丁进行了系统研究。我们引入了UPA-RFAS（通过稳健特征、注意力和语义进行通用补丁攻击），这是一个统一框架，可以在共享特征空间中学习单个物理补丁，同时促进跨模型传输。UPA-RFAS结合了（i）特征空间目标与$\ell_1 $偏差先验和排斥性InfoNSO损失，以引发可转移的表示转移，（ii）鲁棒增强的两阶段最小-最大程序，其中内循环学习不可见的样本式扰动，外循环针对此硬化邻居优化通用补丁，以及（iii）两个特定于VLA的损失：补丁注意力支配性劫持文本$\到$视觉注意力，补丁语义失准以在没有标签的情况下引发图像与文本不匹配。跨不同VLA模型、操作套件和物理执行的实验表明，UPA-RFAS能够在模型、任务和观点之间一致地传输，暴露了实用的基于补丁的攻击表面，并为未来的防御建立了强大的基线。



## **32. Adversarial Confusion Attack: Disrupting Multimodal Large Language Models**

对抗性混乱攻击：扰乱多模式大型语言模型 cs.CL

**SubmitDate**: 2025-12-01    [abs](http://arxiv.org/abs/2511.20494v3) [paper-pdf](https://arxiv.org/pdf/2511.20494v3)

**Authors**: Jakub Hoscilowicz, Artur Janicki

**Abstract**: We introduce the Adversarial Confusion Attack, a new class of threats against multimodal large language models (MLLMs). Unlike jailbreaks or targeted misclassification, the goal is to induce systematic disruption that makes the model generate incoherent or confidently incorrect outputs. Practical applications include embedding such adversarial images into websites to prevent MLLM-powered AI Agents from operating reliably. The proposed attack maximizes next-token entropy using a small ensemble of open-source MLLMs. In the white-box setting, we show that a single adversarial image can disrupt all models in the ensemble, both in the full-image and Adversarial CAPTCHA settings. Despite relying on a basic adversarial technique (PGD), the attack generates perturbations that transfer to both unseen open-source (e.g., Qwen3-VL) and proprietary (e.g., GPT-5.1) models.

摘要: 我们引入了对抗性混乱攻击，这是针对多模式大型语言模型（MLLM）的一类新型威胁。与越狱或有针对性的错误分类不同，目标是引发系统性破坏，使模型生成不连贯或自信地错误的输出。实际应用包括将这种对抗性图像嵌入网站，以防止MLLM驱动的AI代理可靠地运行。拟议的攻击使用一小部分开源MLLM来最大化下一个令牌的熵。在白盒设置中，我们表明，单个对抗图像可以扰乱集合中的所有模型，无论是在完整图像还是对抗验证码设置中。尽管依赖于基本的对抗技术（PVD），但攻击会产生转移到两个看不见的开源的扰动（例如，Qwen 3-DL）和专有（例如，GPT-5.1）型号。



## **33. NegBLEURT Forest: Leveraging Inconsistencies for Detecting Jailbreak Attacks**

NegBLEURT Forest：利用Inbox检测越狱攻击 cs.CR

This paper has been accepted in IEEE Consumer Communications & Networking Conference 2026

**SubmitDate**: 2025-11-28    [abs](http://arxiv.org/abs/2511.11784v2) [paper-pdf](https://arxiv.org/pdf/2511.11784v2)

**Authors**: Lama Sleem, Jerome Francois, Lujun Li, Nathan Foucher, Niccolo Gentile, Radu State

**Abstract**: Jailbreak attacks designed to bypass safety mechanisms pose a serious threat by prompting LLMs to generate harmful or inappropriate content, despite alignment with ethical guidelines. Crafting universal filtering rules remains difficult due to their inherent dependence on specific contexts. To address these challenges without relying on threshold calibration or model fine-tuning, this work introduces a semantic consistency analysis between successful and unsuccessful responses, demonstrating that a negation-aware scoring approach captures meaningful patterns. Building on this insight, a novel detection framework called NegBLEURT Forest is proposed to evaluate the degree of alignment between outputs elicited by adversarial prompts and expected safe behaviors. It identifies anomalous responses using the Isolation Forest algorithm, enabling reliable jailbreak detection. Experimental results show that the proposed method consistently achieves top-tier performance, ranking first or second in accuracy across diverse models using the crafted dataset, while competing approaches exhibit notable sensitivity to model and data variations.

摘要: 旨在绕过安全机制的越狱攻击尽管符合道德准则，但会促使LLM生成有害或不适当的内容，从而构成严重威胁。由于其对特定上下文的固有依赖性，制定通用过滤规则仍然很困难。为了在不依赖阈值校准或模型微调的情况下解决这些挑战，这项工作在成功和不成功的响应之间引入了语义一致性分析，证明消极感知评分方法可以捕捉到有意义的模式。基于这一见解，提出了一种名为NegBLEURT Forest的新型检测框架，以评估对抗性提示引发的输出与预期安全行为之间的一致程度。它使用隔离森林算法识别异常响应，从而实现可靠的越狱检测。实验结果表明，所提出的方法始终实现顶级性能，在使用精心设计的数据集的不同模型中，准确性排名第一或第二，而竞争方法对模型和数据变化表现出显着的敏感性。



## **34. Reasoning Up the Instruction Ladder for Controllable Language Models**

可控语言模型的指令阶梯推理 cs.CL

**SubmitDate**: 2025-12-01    [abs](http://arxiv.org/abs/2511.04694v3) [paper-pdf](https://arxiv.org/pdf/2511.04694v3)

**Authors**: Zishuo Zheng, Vidhisha Balachandran, Chan Young Park, Faeze Brahman, Sachin Kumar

**Abstract**: As large language model (LLM) based systems take on high-stakes roles in real-world decision-making, they must reconcile competing instructions from multiple sources (e.g., model developers, users, and tools) within a single prompt context. Thus, enforcing an instruction hierarchy (IH) in LLMs, where higher-level directives override lower-priority requests, is critical for the reliability and controllability of LLMs. In this work, we reframe instruction hierarchy resolution as a reasoning task. Specifically, the model must first "think" about the relationship between a given user prompt and higher-priority (system) instructions before generating a response. To enable this capability via training, we construct VerIH, an instruction hierarchy dataset of constraint-following tasks with verifiable answers. This dataset comprises ~7K aligned and conflicting system-user instructions. We show that lightweight reinforcement learning with VerIH effectively transfers general reasoning capabilities of models to instruction prioritization. Our finetuned models achieve consistent improvements on instruction following and instruction hierarchy benchmarks, achieving roughly a 20% improvement on the IHEval conflict setup. This reasoning ability also generalizes to safety-critical settings beyond the training distribution. By treating safety issues as resolving conflicts between adversarial user inputs and predefined higher-priority policies, our trained model enhances robustness against jailbreak and prompt injection attacks, providing up to a 20% reduction in attack success rate (ASR). These results demonstrate that reasoning over instruction hierarchies provides a practical path to reliable LLMs, where updates to system prompts yield controllable and robust changes in model behavior.

摘要: 随着基于大型语言模型（LLM）的系统在现实世界的决策中扮演着高风险的角色，它们必须协调来自多个来源的竞争指令（例如，模型开发人员、用户和工具）在单个提示上下文中。因此，在LLM中强制执行指令层次结构（IHS）（其中更高级的指令优先于较低优先级的请求）对于LLM的可靠性和可控性至关重要。在这项工作中，我们将指令层次结构分解重新构建为一项推理任务。具体来说，模型必须在生成响应之前首先“思考”给定用户提示和更高优先级（系统）指令之间的关系。为了通过训练实现这种能力，我们构建了VerIHS，这是一个具有可验证答案的约束遵循任务的指令层次数据集。此数据集包括约7K个对齐且冲突的系统用户指令。我们表明，使用VerIHS的轻量级强化学习可以有效地将模型的一般推理能力转移到指令优先级。我们的微调模型在指令遵循和指令层次基准方面实现了一致的改进，在IHEval冲突设置方面实现了大约20%的改进。这种推理能力还推广到培训分布以外的安全关键环境。通过将安全问题视为解决敌对用户输入和预定义的高优先级策略之间的冲突，我们训练的模型增强了针对越狱和即时注入攻击的鲁棒性，将攻击成功率（ASB）降低高达20%。这些结果表明，对指令层次结构的推理提供了一条通往可靠LLM的实用途径，其中对系统提示的更新会产生模型行为的可控且稳健的变化。



## **35. SpoofTrackBench: Interpretable AI for Spoof-Aware UAV Tracking and Benchmarking**

SpoofTrackBench：用于欺骗感知无人机跟踪和基准的可解释人工智能 cs.CR

**SubmitDate**: 2025-11-28    [abs](http://arxiv.org/abs/2510.22726v2) [paper-pdf](https://arxiv.org/pdf/2510.22726v2)

**Authors**: Van Le, Tan Le

**Abstract**: SpoofTrackBench is a reproducible, modular benchmark for evaluating adversarial robustness in real-time localization and tracking (RTLS) systems under radar spoofing. Leveraging the Hampton University Skyler Radar Sensor dataset, we simulate drift, ghost, and mirror-type spoofing attacks and evaluate tracker performance using both Joint Probabilistic Data Association (JPDA) and Global Nearest Neighbor (GNN) architectures. Our framework separates clean and spoofed detection streams, visualizes spoof-induced trajectory divergence, and quantifies assignment errors via direct drift-from-truth metrics. Clustering overlays, injection-aware timelines, and scenario-adaptive visualizations enable interpretability across spoof types and configurations. Evaluation figures and logs are auto-exported for reproducible comparison. SpoofTrackBench sets a new standard for open, ethical benchmarking of spoof-aware tracking pipelines, enabling rigorous cross-architecture analysis and community validation.

摘要: SpoofTrackBench是一个可复制的模块化基准测试，用于评估雷达欺骗下实时定位和跟踪（RTLS）系统的对抗鲁棒性。利用汉普顿大学Skyler雷达传感器数据集，我们模拟漂移，幽灵和镜像型欺骗攻击，并使用联合概率数据协会（JPDA）和全球最近邻（GNN）架构评估跟踪器性能。我们的框架分离干净的和欺骗的检测流，可视化欺骗诱导的轨迹发散，并通过直接漂移从真相度量量化分配错误。聚类覆盖、注入感知时间线和自适应可视化支持跨欺骗类型和配置的可解释性。评估数字和日志会自动输出，以便进行可重复的比较。SpoofTrackBench为欺骗感知跟踪管道的开放、道德基准设定了新标准，实现了严格的跨架构分析和社区验证。



## **36. TBRD: TESLA Authenticated UAS Broadcast Remote ID**

TBRD：TESLA认证的UAS广播远程ID cs.CR

**SubmitDate**: 2025-11-28    [abs](http://arxiv.org/abs/2510.11343v2) [paper-pdf](https://arxiv.org/pdf/2510.11343v2)

**Authors**: Jason Veara, Manav Jain, Kyle Moy, Aanjhan Ranganathan

**Abstract**: Mysterious sightings of Unmanned Aircraft Systems (UAS) over U.S. military facilities, suburban neighborhoods, and commercial airports have intensified scrutiny of drone activity. To increase accountability, the Federal Aviation Administration (FAA) introduced a Remote ID mandate, requiring unmanned aircraft to broadcast their location, operator's location, and identity in real-time. However, current standards leave authentication mechanisms underspecified, enabling spoofing, relay, and replay attacks that can undermine surveillance efforts and potentially disrupt UAS-to-UAS coordination in future deployments. In this paper, we propose TBRD, a practical system for authenticating Remote ID messages in a manner that aligns with existing standards and UAS capabilities. TBRD leverages the TESLA protocol and mobile device TEEs, and introduces a verification mechanism to build a lightweight, mission-scoped authentication system that is both computationally efficient and requires a low communication footprint. We evaluate the performance of TBRD using both an FAA-requirements compatible proof-of-concept implementation for performance metrics and a simulated 4-drone swarm mission scenario to demonstrate its security guarantees under adversarial conditions. Our system provides a 50\% reduction in authentication overhead compared to digital signatures and a 100x reduction in computation time. Our results demonstrate that TBRD can be integrated into current Remote ID infrastructures to provide a scalable, standards-compliant message authentication for both regulatory and operational use cases.

摘要: 在美国军事设施、郊区和商业机场上空神秘发现无人机系统（UAS），这加剧了对无人机活动的审查。为了加强问责制，美国联邦航空管理局（FAA）引入了远程ID强制令，要求无人驾驶飞机实时广播其位置、操作员位置和身份。然而，当前的标准对身份验证机制的规定不足，从而导致欺骗、中继和重播攻击，这可能会破坏监视工作，并可能会破坏未来部署中的UAS到UAS协调。在本文中，我们提出了TBRD，这是一个用于以符合现有标准和UAS功能的方式认证远程ID消息的实用系统。TBRD利用TESLA协议和移动终端TEEs，并引入验证机制来构建轻量级、任务范围的身份验证系统，该系统既计算高效，又需要低通信占用空间。我们使用与FAA要求兼容的性能指标概念验证实现和模拟的4无人机群任务场景来评估TBRD的性能，以证明其在对抗条件下的安全保证。与数字签名相比，我们的系统提供了50%的验证负担，并减少了100倍的计算时间。我们的结果表明，TBRD可以集成到当前的远程ID基础设施中，为监管和运营用例提供可扩展的、符合标准的消息身份验证。



## **37. Get RICH or Die Scaling: Profitably Trading Inference Compute for Robustness**

获得丰富或模具缩放：有利可图的交易推理计算的鲁棒性 cs.LG

21 pages

**SubmitDate**: 2025-12-01    [abs](http://arxiv.org/abs/2510.06790v2) [paper-pdf](https://arxiv.org/pdf/2510.06790v2)

**Authors**: Tavish McDonald, Bo Lei, Stanislav Fort, Bhavya Kailkhura, Brian Bartoldson

**Abstract**: Models are susceptible to adversarially out-of-distribution (OOD) data despite large training-compute investments into their robustification. Zaremba et al. (2025) make progress on this problem at test time, showing LLM reasoning improves satisfaction of model specifications designed to thwart attacks, resulting in a correlation between reasoning effort and robustness to jailbreaks. However, this benefit of test compute fades when attackers are given access to gradients or multimodal inputs. We address this gap, clarifying that inference-compute offers benefits even in such cases. Our approach argues that compositional generalization, through which OOD data is understandable via its in-distribution (ID) components, enables adherence to defensive specifications on adversarially OOD inputs. Namely, we posit the Robustness from Inference Compute Hypothesis (RICH): inference-compute defenses profit as the model's training data better reflects the attacked data's components. We empirically support this hypothesis across vision language model and attack types, finding robustness gains from test-time compute if specification following on OOD data is unlocked by compositional generalization. For example, InternVL 3.5 gpt-oss 20B gains little robustness when its test compute is scaled, but such scaling adds significant robustness if we first robustify its vision encoder. This correlation of inference-compute's robustness benefit with base model robustness is the rich-get-richer dynamic of the RICH: attacked data components are more ID for robustified models, aiding compositional generalization to OOD data. Thus, we advise layering train-time and test-time defenses to obtain their synergistic benefit.

摘要: 尽管模型的鲁棒性投入了大量的训练计算投资，但它们仍然容易受到不利的分布外（OOD）数据的影响。Zaremba等人（2025）在测试时在这个问题上取得了进展，表明LLM推理提高了旨在阻止攻击的模型规范的满意度，从而导致推理工作量和越狱稳健性之间的相关性。然而，当攻击者能够访问梯度或多模式输入时，测试计算的这种好处就会消失。我们解决了这一差距，澄清了即使在这种情况下，推理计算也能带来好处。我们的方法认为，组合概括（OOD数据可以通过其内分布（ID）组件来理解）使得能够遵守针对敌对OOD输入的防御规范。也就是说，我们从推理计算假设（RICH）中验证了鲁棒性：由于模型的训练数据更好地反映了受攻击数据的成分，推理计算防御会获利。我们在视觉语言模型和攻击类型中从经验上支持了这一假设，如果OOD数据上的规范通过组合概括解锁，则可以从测试时计算中找到鲁棒性的收益。例如，InternVL3.5 gtt-oss 20 B在扩展其测试计算时几乎没有获得鲁棒性，但如果我们首先对其视觉编码器进行鲁棒性验证，这种扩展会增加显着的鲁棒性。推理计算的稳健性优势与基础模型稳健性的这种相关性是RICH的富而富的动态：受攻击的数据组件对于稳健模型来说更具ID，有助于组合概括OOD数据。因此，我们建议将训练时和测试时防御分层，以获得协同效益。



## **38. SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations**

SECA：引发LLM幻觉的语义等效和一致攻击 cs.CL

Accepted at NeurIPS 2025. Code is available at https://github.com/Buyun-Liang/SECA

**SubmitDate**: 2025-11-30    [abs](http://arxiv.org/abs/2510.04398v2) [paper-pdf](https://arxiv.org/pdf/2510.04398v2)

**Authors**: Buyun Liang, Liangzu Peng, Jinqi Luo, Darshan Thaker, Kwan Ho Ryan Chan, René Vidal

**Abstract**: Large Language Models (LLMs) are increasingly deployed in high-risk domains. However, state-of-the-art LLMs often produce hallucinations, raising serious concerns about their reliability. Prior work has explored adversarial attacks for hallucination elicitation in LLMs, but it often produces unrealistic prompts, either by inserting gibberish tokens or by altering the original meaning. As a result, these approaches offer limited insight into how hallucinations may occur in practice. While adversarial attacks in computer vision often involve realistic modifications to input images, the problem of finding realistic adversarial prompts for eliciting LLM hallucinations has remained largely underexplored. To address this gap, we propose Semantically Equivalent and Coherent Attacks (SECA) to elicit hallucinations via realistic modifications to the prompt that preserve its meaning while maintaining semantic coherence. Our contributions are threefold: (i) we formulate finding realistic attacks for hallucination elicitation as a constrained optimization problem over the input prompt space under semantic equivalence and coherence constraints; (ii) we introduce a constraint-preserving zeroth-order method to effectively search for adversarial yet feasible prompts; and (iii) we demonstrate through experiments on open-ended multiple-choice question answering tasks that SECA achieves higher attack success rates while incurring almost no semantic equivalence or semantic coherence errors compared to existing methods. SECA highlights the sensitivity of both open-source and commercial gradient-inaccessible LLMs to realistic and plausible prompt variations. Code is available at https://github.com/Buyun-Liang/SECA.

摘要: 大型语言模型（LLM）越来越多地部署在高风险领域。然而，最先进的LLM经常会产生幻觉，从而引发人们对其可靠性的严重担忧。之前的工作探索了LLM中幻觉引发的对抗攻击，但它经常会产生不切实际的提示，要么通过插入胡言乱语的标记，要么通过改变原来的含义。因此，这些方法对幻觉在实践中如何发生的了解有限。虽然计算机视觉中的对抗性攻击通常涉及对输入图像的现实修改，但寻找引发LLM幻觉的现实对抗性提示的问题在很大程度上仍未得到充分探索。为了解决这一差距，我们提出了语义等效和连贯的攻击（SECA）引起幻觉通过现实的修改提示，保留其意义，同时保持语义连贯性。我们的贡献有三重：（i）我们将为幻觉诱导寻找现实攻击制定为语义等价和一致性约束下的输入提示空间上的约束优化问题;（ii）我们引入约束保持零阶方法来有效地搜索对抗性但可行的提示;和（iii）我们通过开放式多-的实验来证明与现有方法相比，SECA的选择问题回答任务实现了更高的攻击成功率，同时几乎不会导致语义等效或语义一致性错误。SECA强调了开源和商业梯度不可访问的LLM对现实且合理的提示变化的敏感性。代码可在https://github.com/Buyun-Liang/SECA上获取。



## **39. Less is More: Towards Simple Graph Contrastive Learning**

少即是多：走向简单图形对比学习 cs.LG

Submitted to ICLR 2026

**SubmitDate**: 2025-12-01    [abs](http://arxiv.org/abs/2509.25742v2) [paper-pdf](https://arxiv.org/pdf/2509.25742v2)

**Authors**: Yanan Zhao, Feng Ji, Jingyang Dai, Jiaze Ma, Wee Peng Tay

**Abstract**: Graph Contrastive Learning (GCL) has shown strong promise for unsupervised graph representation learning, yet its effectiveness on heterophilic graphs, where connected nodes often belong to different classes, remains limited. Most existing methods rely on complex augmentation schemes, intricate encoders, or negative sampling, which raises the question of whether such complexity is truly necessary in this challenging setting. In this work, we revisit the foundations of supervised and unsupervised learning on graphs and uncover a simple yet effective principle for GCL: mitigating node feature noise by aggregating it with structural features derived from the graph topology. This observation suggests that the original node features and the graph structure naturally provide two complementary views for contrastive learning. Building on this insight, we propose an embarrassingly simple GCL model that uses a GCN encoder to capture structural features and an MLP encoder to isolate node feature noise. Our design requires neither data augmentation nor negative sampling, yet achieves state-of-the-art results on heterophilic benchmarks with minimal computational and memory overhead, while also offering advantages in homophilic graphs in terms of complexity, scalability, and robustness. We provide theoretical justification for our approach and validate its effectiveness through extensive experiments, including robustness evaluations against both black-box and white-box adversarial attacks.

摘要: 图对比学习（GCL）在无监督图表示学习方面表现出了强大的前景，但它对异性图（其中连接的节点通常属于不同类别）的有效性仍然有限。大多数现有的方法依赖于复杂的增强方案、复杂的编码器或负采样，这引发了这样的复杂性在这种具有挑战性的环境中是否确实必要的问题。在这项工作中，我们重新审视了图上有监督和无监督学习的基础，并揭示了GCL的一个简单而有效的原则：通过将节点特征噪音与从图布局中衍生的结构特征聚合来减轻节点特征噪音。这一观察表明，原始的节点特征和图结构自然地为对比学习提供了两个补充的视图。基于这一见解，我们提出了一个极其简单的GCL模型，该模型使用GCN编码器来捕获结构特征，并使用MLP编码器来隔离节点特征噪音。我们的设计既不需要数据增强，也不需要负采样，但以最少的计算和内存负担在异嗜基准上实现了最先进的结果，同时还在同嗜图中提供了复杂性、可扩展性和鲁棒性方面的优势。我们为我们的方法提供了理论依据，并通过大量实验验证了其有效性，包括针对黑匣子和白盒对抗攻击的鲁棒性评估。



## **40. Accuracy-Robustness Trade Off via Spiking Neural Network Gradient Sparsity Trail**

通过峰值神经网络梯度稀疏度追踪实现准确性与稳健性权衡 cs.NE

Work under peer-review

**SubmitDate**: 2025-12-03    [abs](http://arxiv.org/abs/2509.23762v3) [paper-pdf](https://arxiv.org/pdf/2509.23762v3)

**Authors**: Luu Trong Nhan, Luu Trung Duong, Pham Ngoc Nam, Truong Cong Thang

**Abstract**: Spiking Neural Networks (SNNs) have attracted growing interest in both computational neuroscience and artificial intelligence, primarily due to their inherent energy efficiency and compact memory footprint. However, achieving adversarial robustness in SNNs, (particularly for vision-related tasks) remains a nascent and underexplored challenge. Recent studies have proposed leveraging sparse gradients as a form of regularization to enhance robustness against adversarial perturbations. In this work, we present a surprising finding: under specific architectural configurations, SNNs exhibit natural gradient sparsity and can achieve state-of-the-art adversarial defense performance without the need for any explicit regularization. Further analysis reveals a trade-off between robustness and generalization: while sparse gradients contribute to improved adversarial resilience, they can impair the model's ability to generalize; conversely, denser gradients support better generalization but increase vulnerability to attacks. Our findings offer new insights into the dual role of gradient sparsity in SNN training.

摘要: 尖峰神经网络（SNN）引起了计算神经科学和人工智能日益增长的兴趣，主要是由于其固有的能源效率和紧凑的内存占用。然而，在SNN中实现对抗稳健性（特别是对于视觉相关任务）仍然是一个新生且未充分探索的挑战。最近的研究提出利用稀疏梯度作为一种正规化形式，以增强针对对抗性扰动的鲁棒性。在这项工作中，我们提出了一个令人惊讶的发现：在特定的架构配置下，SNN表现出自然的梯度稀疏性，并且可以在不需要任何显式正规化的情况下实现最先进的对抗防御性能。进一步的分析揭示了鲁棒性和概括性之间的权衡：虽然稀疏梯度有助于提高对抗弹性，但它们可能会损害模型的概括能力;相反，更密集的梯度支持更好的概括性，但会增加对攻击的脆弱性。我们的研究结果为梯度稀疏性在SNN训练中的双重作用提供了新的见解。



## **41. Observation-Free Attacks on Online Learning to Rank**

对在线学习排名的无观察攻击 cs.LG

**SubmitDate**: 2025-12-02    [abs](http://arxiv.org/abs/2509.22855v4) [paper-pdf](https://arxiv.org/pdf/2509.22855v4)

**Authors**: Sameep Chattopadhyay, Nikhil Karamchandani, Sharayu Moharir

**Abstract**: Online learning to rank (OLTR) plays a critical role in information retrieval and machine learning systems, with a wide range of applications in search engines and content recommenders. However, despite their extensive adoption, the susceptibility of OLTR algorithms to coordinated adversarial attacks remains poorly understood. In this work, we present a novel framework for attacking some of the widely used OLTR algorithms. Our framework is designed to promote a set of target items so that they appear in the list of top-K recommendations for T - o(T) rounds, while simultaneously inducing linear regret in the learning algorithm. We propose two novel attack strategies: CascadeOFA for CascadeUCB1 and PBMOFA for PBM-UCB . We provide theoretical guarantees showing that both strategies require only O(log T) manipulations to succeed. Additionally, we supplement our theoretical analysis with empirical results on real-world data.

摘要: 在线排名学习（OLTR）在信息检索和机器学习系统中发挥着至关重要的作用，在搜索引擎和内容排序器中有广泛的应用。然而，尽管OLTR算法被广泛采用，但人们对OLTR算法对协同对抗攻击的敏感性仍然知之甚少。在这项工作中，我们提出了一个新颖的框架来攻击一些广泛使用的OLTR算法。我们的框架旨在推广一组目标项，使它们出现在T-o（T）轮的前K推荐列表中，同时在学习算法中引发线性遗憾。我们提出了两种新颖的攻击策略：针对CascadeUCB 1的CascadeOFA和针对PBM-UCB的PBMOFA。我们提供了理论保证，表明这两种策略只需要O（log T）操作即可成功。此外，我们还通过现实世界数据的实证结果来补充理论分析。



## **42. SAEmnesia: Erasing Concepts in Diffusion Models with Supervised Sparse Autoencoders**

SAEmnesia：使用监督稀疏自动编码器消除扩散模型中的概念 cs.CV

**SubmitDate**: 2025-11-28    [abs](http://arxiv.org/abs/2509.21379v2) [paper-pdf](https://arxiv.org/pdf/2509.21379v2)

**Authors**: Enrico Cassano, Riccardo Renzulli, Marco Nurisso, Mirko Zaffaroni, Alan Perotti, Marco Grangetto

**Abstract**: Concept unlearning in diffusion models is hampered by feature splitting, where concepts are distributed across many latent features, making their removal challenging and computationally expensive. We introduce SAEmnesia, a supervised sparse autoencoder framework that overcomes this by enforcing one-to-one concept-neuron mappings. By systematically labeling concepts during training, our method achieves feature centralization, binding each concept to a single, interpretable neuron. This enables highly targeted and efficient concept erasure. SAEmnesia reduces hyperparameter search by 96.7% and achieves a 9.2% improvement over the state-of-the-art on the UnlearnCanvas benchmark. Our method also demonstrates superior scalability in sequential unlearning, improving accuracy by 28.4% when removing nine objects, establishing a new standard for precise and controllable concept erasure. Moreover, SAEmnesia mitigates the possibility of generating unwanted content under adversarial attack and effectively removes nudity when evaluated with I2P.

摘要: 扩散模型中的概念去学习受到特征分裂的阻碍，其中概念分布在许多潜在特征中，使得它们的去除具有挑战性且计算成本高昂。我们引入了SAEmnesia，这是一个有监督的稀疏自动编码器框架，它通过实施一对一的概念-神经元映射来克服这个问题。通过在训练期间系统地标记概念，我们的方法实现了特征集中化，将每个概念绑定到单个可解释的神经元。这使得具有高度针对性且高效的概念擦除成为可能。SAEmnesia将超参数搜索减少了96.7%，并比UnlearnCanvas基准测试的最新水平提高了9.2%。我们的方法还在顺序取消学习方面表现出了卓越的可扩展性，在删除9个对象时将准确性提高了28.4%，为精确且可控的概念擦除建立了新标准。此外，SAEmnesia还降低了在对抗攻击下生成不想要内容的可能性，并在使用I2 P评估时有效地删除裸体。



## **43. On the Robustness of Verbal Confidence of LLMs in Adversarial Attacks**

论LLM在对抗性攻击中言语信心的稳健性 cs.CL

Published in NeurIPS 2025

**SubmitDate**: 2025-11-28    [abs](http://arxiv.org/abs/2507.06489v2) [paper-pdf](https://arxiv.org/pdf/2507.06489v2)

**Authors**: Stephen Obadinma, Xiaodan Zhu

**Abstract**: Robust verbal confidence generated by large language models (LLMs) is crucial for the deployment of LLMs to help ensure transparency, trust, and safety in many applications, including those involving human-AI interactions. In this paper, we present the first comprehensive study on the robustness of verbal confidence under adversarial attacks. We introduce attack frameworks targeting verbal confidence scores through both perturbation and jailbreak-based methods, and demonstrate that these attacks can significantly impair verbal confidence estimates and lead to frequent answer changes. We examine a variety of prompting strategies, model sizes, and application domains, revealing that current verbal confidence is vulnerable and that commonly used defence techniques are largely ineffective or counterproductive. Our findings underscore the need to design robust mechanisms for confidence expression in LLMs, as even subtle semantic-preserving modifications can lead to misleading confidence in responses.

摘要: 大型语言模型（LLM）产生的强大言语信心对于LLM的部署至关重要，以帮助确保许多应用程序（包括涉及人机交互的应用程序）的透明度、信任和安全性。在本文中，我们首次对对抗攻击下言语信心的稳健性进行了全面研究。我们通过干扰和基于越狱的方法引入了针对言语信心分数的攻击框架，并证明这些攻击会显着损害言语信心估计并导致答案频繁变化。我们检查了各种提示策略、模型大小和应用领域，揭示了当前的言语自信很脆弱，并且常用的防御技术在很大程度上无效或适得其反。我们的研究结果强调了为LLM中的信心表达设计稳健的机制的必要性，因为即使是微妙的语义保留修改也可能导致反应中的误导性信心。



## **44. SafeGenes: Evaluating the Adversarial Robustness of Genomic Foundation Models**

SafeGenes：评估基因组基础模型的对抗稳健性 cs.CR

**SubmitDate**: 2025-12-02    [abs](http://arxiv.org/abs/2506.00821v2) [paper-pdf](https://arxiv.org/pdf/2506.00821v2)

**Authors**: Huixin Zhan, Clovis Barbour, Jason H. Moore

**Abstract**: Genomic Foundation Models (GFMs), such as Evolutionary Scale Modeling (ESM), have demonstrated significant success in variant effect prediction. However, their adversarial robustness remains largely unexplored. To address this gap, we propose SafeGenes: a framework for Secure analysis of genomic foundation models, leveraging adversarial attacks to evaluate robustness against both engineered near-identical adversarial Genes and embedding-space manipulations. In this study, we assess the adversarial vulnerabilities of GFMs using two approaches: the Fast Gradient Sign Method (FGSM) and a soft prompt attack. FGSM introduces minimal perturbations to input sequences, while the soft prompt attack optimizes continuous embeddings to manipulate model predictions without modifying the input tokens. By combining these techniques, SafeGenes provides a comprehensive assessment of GFM susceptibility to adversarial manipulation. Targeted soft prompt attacks induced severe degradation in MLM-based shallow architectures such as ProteinBERT, while still producing substantial failure modes even in high-capacity foundation models such as ESM1b and ESM1v. These findings expose critical vulnerabilities in current foundation models, opening new research directions toward improving their security and robustness in high-stakes genomic applications such as variant effect prediction.

摘要: 基因组基础模型（GFM），如进化尺度模型（ESM），已经证明在变异效应预测方面取得了显着的成功。然而，它们的对抗性鲁棒性在很大程度上仍未被探索。为了解决这一差距，我们提出了SafeGenes：一个用于基因组基础模型安全分析的框架，利用对抗性攻击来评估对工程化的几乎相同的对抗性基因和嵌入空间操作的鲁棒性。在这项研究中，我们使用两种方法评估GFM的对抗性漏洞：快速梯度符号方法（FGSM）和软提示攻击。FGSM对输入序列引入了最小的扰动，而软提示攻击优化了连续嵌入，以在不修改输入令牌的情况下操纵模型预测。通过结合这些技术，SafeGenes提供了一个全面的评估GFM对对抗性操纵的敏感性。有针对性的软提示攻击导致基于MLM的浅层架构（如ProteinBERT）严重退化，同时即使在高容量的基础模型（如ESM 1b和ESM 1v）中仍会产生大量故障模式。这些发现暴露了当前基础模型中的关键漏洞，为提高其在高风险基因组应用（如变异效应预测）中的安全性和鲁棒性开辟了新的研究方向。



## **45. Curvature Dynamic Black-box Attack: revisiting adversarial robustness via dynamic curvature estimation**

弯曲动态黑匣子攻击：通过动态弯曲估计重新审视对抗鲁棒性 cs.LG

**SubmitDate**: 2025-11-28    [abs](http://arxiv.org/abs/2505.19194v3) [paper-pdf](https://arxiv.org/pdf/2505.19194v3)

**Authors**: Peiran Sun

**Abstract**: Adversarial attack reveals the vulnerability of deep learning models. It is assumed that high curvature may give rise to rough decision boundary and thus result in less robust models. However, the most commonly used \textit{curvature} is the curvature of loss function, scores or other parameters from within the model as opposed to decision boundary curvature, since the former can be relatively easily formed using second order derivative. In this paper, we propose a new query-efficient method, dynamic curvature estimation (DCE), to estimate the decision boundary curvature in a black-box setting. Our approach is based on CGBA, a black-box adversarial attack. By performing DCE on a wide range of classifiers, we discovered, statistically, a connection between decision boundary curvature and adversarial robustness. We also propose a new attack method, curvature dynamic black-box attack (CDBA) with improved performance using the estimated curvature.

摘要: 对抗性攻击揭示了深度学习模型的脆弱性。假设高弯曲可能会导致粗糙的决策边界，从而导致模型鲁棒性较差。然而，最常用的\textit{currency}是模型内的损失函数、分数或其他参数的弯曲，而不是决策边界弯曲，因为前者可以相对容易地使用二阶求导形成。在本文中，我们提出了一种新的查询高效方法--动态弯曲估计（VCE），来估计黑匣子环境下的决策边界弯曲。我们的方法基于CGBA，这是一种黑匣子对抗攻击。通过对广泛的分类器执行VCE，我们从统计上发现了决策边界弯曲和对抗鲁棒性之间的联系。我们还提出了一种新的攻击方法：弯曲动态黑匣子攻击（CDBA），使用估计的弯曲来提高性能。



## **46. AED: Automatic Discovery of Effective and Diverse Vulnerabilities for Autonomous Driving Policy with Large Language Models**

AED：利用大型语言模型自动发现自动驾驶政策的有效且多样化的漏洞 cs.CR

**SubmitDate**: 2025-11-30    [abs](http://arxiv.org/abs/2503.20804v2) [paper-pdf](https://arxiv.org/pdf/2503.20804v2)

**Authors**: Le Qiu, Zelai Xu, Qixin Tan, Wenhao Tang, Chao Yu, Yu Wang

**Abstract**: Assessing the safety of autonomous driving policy is of great importance, and reinforcement learning (RL) has emerged as a powerful method for discovering critical vulnerabilities in driving policies. However, existing RL-based approaches often struggle to identify vulnerabilities that are both effective-meaning the autonomous vehicle is genuinely responsible for the accidents-and diverse-meaning they span various failure types. To address these challenges, we propose AED, a framework that uses large language models (LLMs) to automatically discover effective and diverse vulnerabilities in autonomous driving policies. We first utilize an LLM to automatically design reward functions for RL training. Then we let the LLM consider a diverse set of accident types and train adversarial policies for different accident types in parallel. Finally, we use preference-based learning to filter ineffective accidents and enhance the effectiveness of each vulnerability. Experiments across multiple simulated traffic scenarios and tested policies show that AED uncovers a broader range of vulnerabilities and achieves higher attack success rates compared with expert-designed rewards, thereby reducing the need for manual reward engineering and improving the diversity and effectiveness of vulnerability discovery. The implementation can be found on: https://github.com/thu-nics/AED .

摘要: 评估自动驾驶策略的安全性非常重要，强化学习（RL）已成为发现驾驶策略中关键漏洞的强大方法。然而，现有的基于RL的方法通常很难识别既有效的漏洞（这意味着自动驾驶汽车真正对事故负责）又多样化的漏洞（这意味着它们跨越各种故障类型）。为了应对这些挑战，我们提出AED，这是一个使用大型语言模型（LLM）自动发现自动驾驶政策中有效且多样化的漏洞的框架。我们首先利用LLM来自动设计RL培训的奖励函数。然后，我们让LLM考虑一系列不同的事故类型，并并行训练不同事故类型的对抗政策。最后，我们使用基于偏好的学习来过滤无效事故并增强每个漏洞的有效性。跨多个模拟流量场景和测试策略的实验表明，与专家设计的奖励相比，AED发现了更广泛的漏洞，并实现了更高的攻击成功率，从而减少了手动奖励工程的需求，提高了漏洞发现的多样性和有效性。该实现可在https://github.com/thu-nics/AED上找到。



## **47. Bones of Contention: Exploring Query-Efficient Attacks against Skeleton Recognition Systems**

争夺之骨：探索针对骨架识别系统的查询高效攻击 cs.CR

Accepted to IEEE Transactions on Information Forensics and Security (TIFS)

**SubmitDate**: 2025-12-02    [abs](http://arxiv.org/abs/2501.16843v2) [paper-pdf](https://arxiv.org/pdf/2501.16843v2)

**Authors**: Yuxin Cao, Kai Ye, Derui Wang, Minhui Xue, Hao Ge, Chenxiong Qian, Jin Song Dong

**Abstract**: Skeleton action recognition models have secured more attention than video-based ones in various applications due to privacy preservation and lower storage requirements. Skeleton data are typically transmitted to cloud servers for action recognition, with results returned to clients via Apps/APIs. However, the vulnerability of skeletal models against adversarial perturbations gradually reveals the unreliability of these systems. Existing black-box attacks all operate in a decision-based manner, resulting in numerous queries that hinder efficiency and feasibility in real-world applications. Moreover, all attacks off the shelf focus on only restricted perturbations, while ignoring model weaknesses when encountered with non-semantic perturbations. In this paper, we propose two query-effIcient Skeletal Adversarial AttaCks, ISAAC-K and ISAAC-N. As a black-box attack, ISAAC-K utilizes Grad-CAM in a surrogate model to extract key joints where minor sparse perturbations are then added to fool the classifier. To guarantee natural adversarial motions, we introduce constraints of both bone length and temporal consistency. ISAAC-K finds stronger adversarial examples on the $\ell_\infty$ norm, which can encompass those on other norms. Exhaustive experiments substantiate that ISAAC-K can uplift the attack efficiency of the perturbations under 10 skeletal models. Additionally, as a byproduct, ISAAC-N fools the classifier by replacing skeletons unrelated to the action. We surprisingly find that skeletal models are vulnerable to large perturbations where the part-wise non-semantic joints are just replaced, leading to a query-free no-box attack without any prior knowledge. Based on that, four adaptive defenses are eventually proposed to improve the robustness of skeleton recognition models.

摘要: 由于隐私保护和较低的存储要求，骨架动作识别模型在各种应用中比基于视频的动作识别模型受到了更多关注。骨架数据通常传输到云服务器进行动作识别，结果通过应用程序/API返回给客户端。然而，骨架模型对对抗性扰动的脆弱性逐渐揭示了这些系统的不可靠性。现有的黑匣子攻击都以基于决策的方式运行，从而导致大量查询，阻碍了现实世界应用程序的效率和可行性。此外，所有现成的攻击都只关注有限的扰动，而在遇到非语义扰动时忽略了模型的弱点。在本文中，我们提出了两种查询效率高的Skopper对抗AttaCks，ISAAC-K和ISAAC-N。作为黑匣子攻击，ISAAC-K在代理模型中利用Grad-CAM来提取关键关节，然后添加微小的稀疏扰动来欺骗分类器。为了保证自然的对抗运动，我们引入了骨骼长度和时间一致性的限制。ISAAC-K在$\ell_\infty$规范上找到了更强的对抗性例子，其中可以涵盖其他规范上的例子。详尽的实验证实ISAAC-K可以在10个骨架模型下提高扰动的攻击效率。此外，作为副产品，ISAAC-N通过替换与动作无关的骨架来愚弄分类器。我们惊讶地发现，骨架模型容易受到大的扰动，其中部分非语义关节刚刚被替换，从而导致在没有任何先验知识的情况下进行无查询无框攻击。在此基础上，最终提出了四种自适应防御方法来提高骨架识别模型的鲁棒性。



## **48. DiffProtect: Generate Adversarial Examples with Diffusion Models for Facial Privacy Protection**

迪夫保护：使用扩散模型生成对抗示例以保护面部隐私 cs.CV

Code is at https://github.com/joellliu/DiffProtect/

**SubmitDate**: 2025-11-30    [abs](http://arxiv.org/abs/2305.13625v4) [paper-pdf](https://arxiv.org/pdf/2305.13625v4)

**Authors**: Jiang Liu, Chun Pong Lau, Zhongliang Guo, Yuxiang Guo, Zhaoyang Wang, Rama Chellappa

**Abstract**: The increasingly pervasive facial recognition (FR) systems raise serious concerns about personal privacy, especially for billions of users who have publicly shared their photos on social media. Several attempts have been made to protect individuals from being identified by unauthorized FR systems utilizing adversarial attacks to generate encrypted face images. However, existing methods suffer from poor visual quality or low attack success rates, which limit their utility. Recently, diffusion models have achieved tremendous success in image generation. In this work, we ask: can diffusion models be used to generate adversarial examples to improve both visual quality and attack performance? We propose DiffProtect, which utilizes a diffusion autoencoder to generate semantically meaningful perturbations on FR systems. Extensive experiments demonstrate that DiffProtect produces more natural-looking encrypted images than state-of-the-art methods while achieving significantly higher attack success rates, e.g., 24.5% and 25.1% absolute improvements on the CelebA-HQ and FFHQ datasets.

摘要: 日益普及的面部识别（FR）系统引发了人们对个人隐私的严重担忧，尤其是对于在社交媒体上公开分享照片的数十亿用户来说。人们已经做出了多次尝试来保护个人免受未经授权的FR系统利用对抗攻击来生成加密的面部图像的识别。然而，现有方法的视觉质量较差或攻击成功率较低，这限制了它们的实用性。最近，扩散模型在图像生成方面取得了巨大成功。在这项工作中，我们问：扩散模型能否用于生成对抗性示例，以提高视觉质量和攻击性能？我们提出了迪夫保护，它利用扩散自动编码器来在FR系统上生成具有语义意义的扰动。大量实验表明，与最先进的方法相比，迪夫Protect可以生成看起来更自然的加密图像，同时实现显着更高的攻击成功率，例如CelebA-HQ和FFHQ数据集的绝对改进了24.5%和25.1%。



## **49. GPS-Spoofing Attack Detection Mechanism for UAV Swarms**

无人机群的GPS欺骗攻击检测机制 cs.CR

8 pages, 3 figures

**SubmitDate**: 2025-12-02    [abs](http://arxiv.org/abs/2301.12766v3) [paper-pdf](https://arxiv.org/pdf/2301.12766v3)

**Authors**: Pavlo Mykytyn, Marcin Brzozowski, Zoya Dyka, Peter Langendoerfer

**Abstract**: Recently autonomous and semi-autonomous Unmanned Aerial Vehicle (UAV) swarms started to receive a lot of research interest and demand from various civil application fields. However, for successful mission execution, UAV swarms require Global navigation satellite system signals and in particular, Global Positioning System (GPS) signals for navigation. Unfortunately, civil GPS signals are unencrypted and unauthenticated, which facilitates the execution of GPS spoofing attacks. During these attacks, adversaries mimic the authentic GPS signal and broadcast it to the targeted UAV in order to change its course, and force it to land or crash. In this study, we propose a GPS spoofing detection mechanism capable of detecting single-transmitter and multi-transmitter GPS spoofing attacks to prevent the outcomes mentioned above. Our detection mechanism is based on comparing the distance between each two swarm members calculated from their GPS coordinates to the distance acquired from Impulse Radio Ultra-Wideband ranging between the same swarm members. If the difference in distances is larger than a chosen threshold the GPS spoofing attack is declared detected.

摘要: 近年来，自主和半自主无人机（UF）群体开始受到各个民用应用领域的大量研究兴趣和需求。然而，为了成功执行任务，无人机群需要全球导航卫星系统信号，特别是全球定位系统（GPS）信号进行导航。不幸的是，民用GPS信号未经加密且未经验证，这有利于执行GPS欺骗攻击。在这些攻击过程中，对手模仿真实的GPS信号并将其广播给目标无人机，以改变其航线，并迫使其着陆或坠毁。在这项研究中，我们提出了一种能够检测单发射机和多发射机GPS欺骗攻击的GPS欺骗检测机制，以防止上述结果。我们的检测机制基于将根据GPS坐标计算的每两个群体成员之间的距离与从脉冲无线电超宽带测量相同群体成员之间的距离进行比较。如果距离差大于选定的阈值，则宣布检测到GPS欺骗攻击。



## **50. A review of mechanistic and data-driven models of terrorism and radicalization**

恐怖主义和激进化的机械和数据驱动模型回顾 physics.soc-ph

80 pages, 17 figures

**SubmitDate**: 2025-12-02    [abs](http://arxiv.org/abs/1903.08485v3) [paper-pdf](https://arxiv.org/pdf/1903.08485v3)

**Authors**: Yao-li Chuang, Maria R. D'Orsogna

**Abstract**: The rapid spread of radical ideologies in recent years has led to a worldwide string of terrorist attacks. Understanding how extremist tendencies germinate, develop, and drive individuals to action is important from a cultural standpoint, but also to help formulate response and prevention strategies. Demographic studies, interviews with radicalized subjects, analysis of terrorist databases, reveal that the path to radicalization occurs along progressive steps, where age, social context and peer-to-peer exchange of extremist ideas play major roles. Furthermore, the advent of social media has offered new channels of communication, facilitated recruitment, and hastened the leap from mild discontent to unbridled fanaticism. While a complete sociological understanding of the processes and circumstances that lead to full-fledged extremism is still lacking, quantitative approaches, using modeling and data analyses, can offer useful insight. We review some approaches from statistical mechanics, applied mathematics, data science, that can help describe and understand radicalization and terrorist activity. Specifically, we focus on compartment models of populations harboring extremist views, continuous time models for age-structured radical populations, radicalization as social contagion processes on lattices and social networks, adversarial evolutionary games coupling terrorists and counter-terrorism agents, and point processes to study the spatiotemporal clustering of terrorist events. We also present recent applications of machine learning methods on open-source terrorism databases. Finally, we discuss the role of institutional intervention and the stages at which de-radicalization strategies might be most effective.

摘要: 近年来，激进意识形态的迅速传播导致了一系列全球恐怖袭击。从文化的角度来看，了解极端主义倾向如何萌芽、发展和驱使个人采取行动非常重要，而且有助于制定应对和预防策略。人口研究、对激进对象的采访、对恐怖分子数据库的分析表明，激进化的道路是沿着渐进的步骤发生的，其中年龄、社会背景和极端主义思想的点对点交流发挥着重要作用。此外，社交媒体的出现提供了新的沟通渠道，促进了招聘，并加速了从轻微不满到肆无忌惮的狂热的飞跃。虽然仍然缺乏对导致全面极端主义的过程和环境的完整社会学理解，但使用建模和数据分析的定量方法可以提供有用的见解。我们回顾了统计力学、应用数学、数据科学的一些方法，这些方法可以帮助描述和理解激进化和恐怖活动。具体来说，我们重点关注持有极端主义观点的人群的隔间模型、年龄结构激进人群的连续时间模型、作为格子和社交网络上社会传染过程的激进化、将恐怖分子和反恐特工相结合的对抗进化游戏，以及研究恐怖事件时空聚集的点过程。我们还介绍了机器学习方法在开源恐怖主义数据库上的最新应用。最后，我们讨论了制度干预的作用以及去激进化策略可能最有效的阶段。



