# Latest Adversarial Attack Papers
**update at 2025-09-26 20:48:18**

翻译来自 https://cloud.tencent.com/document/product/551/15619

## **1. IDEATOR: Jailbreaking and Benchmarking Large Vision-Language Models Using Themselves**

IDEATOR：使用自己越狱和基准大型视觉语言模型 cs.CV

**SubmitDate**: 2025-09-25    [abs](http://arxiv.org/abs/2411.00827v6) [paper-pdf](http://arxiv.org/pdf/2411.00827v6)

**Authors**: Ruofan Wang, Juncheng Li, Yixu Wang, Bo Wang, Xiaosen Wang, Yan Teng, Yingchun Wang, Xingjun Ma, Yu-Gang Jiang

**Abstract**: As large Vision-Language Models (VLMs) gain prominence, ensuring their safe deployment has become critical. Recent studies have explored VLM robustness against jailbreak attacks-techniques that exploit model vulnerabilities to elicit harmful outputs. However, the limited availability of diverse multimodal data has constrained current approaches to rely heavily on adversarial or manually crafted images derived from harmful text datasets, which often lack effectiveness and diversity across different contexts. In this paper, we propose IDEATOR, a novel jailbreak method that autonomously generates malicious image-text pairs for black-box jailbreak attacks. IDEATOR is grounded in the insight that VLMs themselves could serve as powerful red team models for generating multimodal jailbreak prompts. Specifically, IDEATOR leverages a VLM to create targeted jailbreak texts and pairs them with jailbreak images generated by a state-of-the-art diffusion model. Extensive experiments demonstrate IDEATOR's high effectiveness and transferability, achieving a 94% attack success rate (ASR) in jailbreaking MiniGPT-4 with an average of only 5.34 queries, and high ASRs of 82%, 88%, and 75% when transferred to LLaVA, InstructBLIP, and Chameleon, respectively. Building on IDEATOR's strong transferability and automated process, we introduce the VLJailbreakBench, a safety benchmark comprising 3,654 multimodal jailbreak samples. Our benchmark results on 11 recently released VLMs reveal significant gaps in safety alignment. For instance, our challenge set achieves ASRs of 46.31% on GPT-4o and 19.65% on Claude-3.5-Sonnet, underscoring the urgent need for stronger defenses. VLJailbreakBench is publicly available at https://roywang021.github.io/VLJailbreakBench.

摘要: 随着大型视觉语言模型（VLM）的日益突出，确保其安全部署变得至关重要。最近的研究探索了VLM针对越狱攻击的鲁棒性--利用模型漏洞来引发有害输出的技术。然而，多样化多模式数据的可用性有限，限制了当前的方法严重依赖于从有害文本数据集派生的对抗性或手动制作的图像，而这些图像通常缺乏跨不同背景的有效性和多样性。本文中，我们提出了IDEATOR，这是一种新型越狱方法，可以自主生成用于黑匣子越狱攻击的恶意图像-文本对。IDEATOR基于这样的见解：VLM本身可以充当强大的红队模型，用于生成多模式越狱提示。具体来说，IDEATOR利用VLM创建有针对性的越狱文本，并将其与由最先进的扩散模型生成的越狱图像配对。大量实验证明了IDEATOR的高效率和可移植性，在越狱MiniGPT-4中平均只需5.34次查询即可实现94%的攻击成功率（ASB），转移到LLaVA、INSTBLIP和Chameleon时，攻击成功率分别为82%、88%和75%。基于IDEATOR强大的可移植性和自动化流程，我们推出了VLJailbreakBench，这是一个由3，654个多模式越狱样本组成的安全基准。我们对最近发布的11个VLM的基准结果揭示了安全一致方面的显着差距。例如，我们的挑战集在GPT-4 o上实现了46.31%的ASB，在Claude-3.5-十四行诗上实现了19.65%的ASB，这凸显了迫切需要更强的防御。VLJailbreakBench可在https://roywang021.github.io/VLJailbreakBench上公开获取。



## **2. Improving LLM Unlearning Robustness via Random Perturbations**

通过随机扰动提高LLM无学习鲁棒性 cs.CL

29 pages, 13 figures, 8 tables

**SubmitDate**: 2025-09-25    [abs](http://arxiv.org/abs/2501.19202v4) [paper-pdf](http://arxiv.org/pdf/2501.19202v4)

**Authors**: Dang Huu-Tien, Hoang Thanh-Tung, Anh Bui, Minh-Phuong Nguyen, Le-Minh Nguyen, Naoya Inoue

**Abstract**: Here, we show that current state-of-the-art LLM unlearning methods inherently reduce models' robustness, causing them to misbehave even when a single non-adversarial forget-token is present in the retain-query. Toward understanding underlying causes, we propose a novel theoretical framework that reframes the unlearning process as backdoor attacks and defenses: forget-tokens act as backdoor triggers that, when activated in retain-queries, cause disruptions in unlearned models' behaviors, similar to successful backdoor attacks. The sense that, LLM unlearning methods themselves poison the model, make it more vulnerable to forget-tokens, and hide rather than erase target knowledge, describes their true mechanism. To mitigate the vulnerability caused by the forgetting process, we reinterpret the retaining process as a backdoor defense and propose Random Noise Augmentation (RNA), a lightweight, model and method-agnostic approach with theoretical guarantees for improving the robustness of models. Extensive experiments demonstrate that RNA significantly improves the robustness of unlearned models while preserving forget and retain performances. This backdoor attack-defense framework offers insights into the mechanism of unlearning that can shed light on future research directions for improving unlearning robustness.

摘要: 在这里，我们展示了当前最先进的LLM非学习方法本质上降低了模型的鲁棒性，导致它们即使在保留查询中存在单个非对抗性遗忘令牌时也会表现不佳。为了理解根本原因，我们提出了一个新的理论框架，将遗忘过程重新定义为后门攻击和防御：遗忘令牌充当后门触发器，当在保留查询中激活时，会导致未学习模型的行为中断，类似于成功的后门攻击。LLM遗忘方法本身会毒害模型，使其更容易受到遗忘令牌的影响，并且隐藏而不是删除目标知识，这描述了它们的真正机制。为了减轻遗忘过程所造成的脆弱性，我们重新解释保留过程作为后门防御，并提出随机噪声增强（RNA），一个轻量级的，模型和方法不可知的方法与理论保证，以提高模型的鲁棒性。大量实验表明，RNA显着提高了未学习模型的鲁棒性，同时保留了遗忘和保留性能。这个后门攻击-防御框架提供了对取消学习机制的见解，可以为未来提高取消学习鲁棒性的研究方向提供线索。



## **3. Adversarial generalization of unfolding (model-based) networks**

展开（基于模型的）网络的对抗概括 cs.LG

Accepted in NeurIPS2025

**SubmitDate**: 2025-09-25    [abs](http://arxiv.org/abs/2509.15370v2) [paper-pdf](http://arxiv.org/pdf/2509.15370v2)

**Authors**: Vicky Kouni

**Abstract**: Unfolding networks are interpretable networks emerging from iterative algorithms, incorporate prior knowledge of data structure, and are designed to solve inverse problems like compressed sensing, which deals with recovering data from noisy, missing observations. Compressed sensing finds applications in critical domains, from medical imaging to cryptography, where adversarial robustness is crucial to prevent catastrophic failures. However, a solid theoretical understanding of the performance of unfolding networks in the presence of adversarial attacks is still in its infancy. In this paper, we study the adversarial generalization of unfolding networks when perturbed with $l_2$-norm constrained attacks, generated by the fast gradient sign method. Particularly, we choose a family of state-of-the-art overaparameterized unfolding networks and deploy a new framework to estimate their adversarial Rademacher complexity. Given this estimate, we provide adversarial generalization error bounds for the networks under study, which are tight with respect to the attack level. To our knowledge, this is the first theoretical analysis on the adversarial generalization of unfolding networks. We further present a series of experiments on real-world data, with results corroborating our derived theory, consistently for all data. Finally, we observe that the family's overparameterization can be exploited to promote adversarial robustness, shedding light on how to efficiently robustify neural networks.

摘要: 展开网络是从迭代算法中产生的可解释网络，融合了数据结构的先验知识，旨在解决压缩感知等逆问题，该感知处理从有噪、缺失的观察中恢复数据。压缩感知在从医学成像到密码学的关键领域得到了应用，这些领域的对抗鲁棒性对于防止灾难性故障至关重要。然而，对存在对抗性攻击的情况下展开的网络的性能的坚实理论理解仍处于起步阶段。本文研究了当受快速梯度符号法产生的$l_2$-模约束攻击扰动时，展开网络的对抗推广。特别是，我们选择了一系列最先进的过度参数化展开网络，并部署一个新的框架来估计其对抗性Rademacher复杂性。给定这一估计，我们为所研究的网络提供了对抗概括误差界限，该界限相对于攻击级别来说是严格的。据我们所知，这是对展开网络的对抗性概括的第一次理论分析。我们进一步对现实世界数据进行了一系列实验，其结果证实了我们的衍生理论，并且对所有数据都一致。最后，我们观察到家庭的过度参数化可以用来促进对抗鲁棒性，从而揭示如何有效地增强神经网络的鲁棒性。



## **4. SwarmRaft: Leveraging Consensus for Robust Drone Swarm Coordination in GNSS-Degraded Environments**

SwarmRaft：利用共识在GNSS降级环境中实现稳健的无人机群协调 cs.DC

**SubmitDate**: 2025-09-25    [abs](http://arxiv.org/abs/2508.00622v2) [paper-pdf](http://arxiv.org/pdf/2508.00622v2)

**Authors**: Kapel Dev, Yash Madhwal, Sofia Shevelo, Pavel Osinenko, Yury Yanovich

**Abstract**: Unmanned aerial vehicle (UAV) swarms are increasingly used in critical applications such as aerial mapping, environmental monitoring, and autonomous delivery. However, the reliability of these systems is highly dependent on uninterrupted access to the Global Navigation Satellite Systems (GNSS) signals, which can be disrupted in real-world scenarios due to interference, environmental conditions, or adversarial attacks, causing disorientation, collision risks, and mission failure. This paper proposes SwarmRaft, a blockchain-inspired positioning and consensus framework for maintaining coordination and data integrity in UAV swarms operating under GNSS-denied conditions. SwarmRaft leverages the Raft consensus algorithm to enable distributed drones (nodes) to agree on state updates such as location and heading, even in the absence of GNSS signals for one or more nodes. In our prototype, each node uses GNSS and local sensing, and communicates over WiFi in a simulated swarm. Upon signal loss, consensus is used to reconstruct or verify the position of the failed node based on its last known state and trajectory. Our system demonstrates robustness in maintaining swarm coherence and fault tolerance through a lightweight, scalable communication model. This work offers a practical and secure foundation for decentralized drone operation in unpredictable environments.

摘要: 无人机（UF）群越来越多地用于航空测绘、环境监测和自主交付等关键应用。然而，这些系统的可靠性高度依赖于不间断地访问全球导航卫星系统（GNSS）信号，而在现实世界场景中，这些信号可能会因干扰、环境条件或对抗性攻击而中断，从而导致方向迷失、碰撞风险和任务失败。本文提出了SwarmRaft，这是一个受区块链启发的定位和共识框架，用于维护在GNSS拒绝的条件下运行的无人机群的协调和数据完整性。SwarmRaft利用Raft共识算法，使分布式无人机（节点）能够就位置和航向等状态更新达成一致，即使在一个或多个节点没有GNSS信号的情况下也是如此。在我们的原型中，每个节点使用全球导航卫星系统和本地传感，并在模拟群中通过WiFi进行通信。信号丢失后，使用共识根据故障节点的最后已知状态和轨迹重建或验证故障节点的位置。我们的系统通过一个轻量级的，可扩展的通信模型，在保持群体的一致性和容错性表现出鲁棒性。这项工作为在不可预测的环境中进行分散式无人机操作提供了实用和安全的基础。



## **5. Sparse Representations Improve Adversarial Robustness of Neural Network Classifiers**

稀疏表示提高神经网络分类器的对抗鲁棒性 cs.LG

Killian Steunou is the main contributor and corresponding author of  this work

**SubmitDate**: 2025-09-25    [abs](http://arxiv.org/abs/2509.21130v1) [paper-pdf](http://arxiv.org/pdf/2509.21130v1)

**Authors**: Killian Steunou, Sigurd Saue, Théo Druilhe

**Abstract**: Deep neural networks perform remarkably well on image classification tasks but remain vulnerable to carefully crafted adversarial perturbations. This work revisits linear dimensionality reduction as a simple, data-adapted defense. We empirically compare standard Principal Component Analysis (PCA) with its sparse variant (SPCA) as front-end feature extractors for downstream classifiers, and we complement these experiments with a theoretical analysis. On the theory side, we derive exact robustness certificates for linear heads applied to SPCA features: for both $\ell_\infty$ and $\ell_2$ threat models (binary and multiclass), the certified radius grows as the dual norms of $W^\top u$ shrink, where $W$ is the projection and $u$ the head weights. We further show that for general (non-linear) heads, sparsity reduces operator-norm bounds through a Lipschitz composition argument, predicting lower input sensitivity. Empirically, with a small non-linear network after the projection, SPCA consistently degrades more gracefully than PCA under strong white-box and black-box attacks while maintaining competitive clean accuracy. Taken together, the theory identifies the mechanism (sparser projections reduce adversarial leverage) and the experiments verify that this benefit persists beyond the linear setting. Our code is available at https://github.com/killian31/SPCARobustness.

摘要: 深度神经网络在图像分类任务中表现出色，但仍然容易受到精心设计的对抗性扰动的影响。这项工作重新审视了线性降维作为一种简单的、适应数据的防御。我们根据经验比较了标准主成分分析（PCA）与其稀疏变体（SPCA）作为下游分类器的前端特征提取器，并通过理论分析补充这些实验。在理论方面，我们为应用于SPCA特征的线性头部推导出精确的鲁棒性证书：对于$\ell_\infty$和$\ell_2 $威胁模型（二元和多类），认证半径随着$W &\top u$的双重规范缩小而增加，其中$W$是投影，$u$是头部重量。我们进一步表明，对于一般（非线性）头部，稀疏性通过Lipschitz合成论点减少了操作符规范界限，从而预测了较低的输入敏感性。从经验上看，投影后存在一个小型非线性网络，在强白盒和黑匣子攻击下，SPCA始终比PCA降级得更优雅，同时保持有竞争力的干净准确性。总而言之，该理论确定了机制（稀疏的预测减少了对抗杠杆），实验验证了这种好处在线性环境之外仍然存在。我们的代码可在https://github.com/killian31/SPCARobustness上获取。



## **6. EvoMail: Self-Evolving Cognitive Agents for Adaptive Spam and Phishing Email Defense**

EvoMail：用于自适应垃圾邮件和网络钓鱼电子邮件防御的自我进化认知代理 cs.LG

**SubmitDate**: 2025-09-25    [abs](http://arxiv.org/abs/2509.21129v1) [paper-pdf](http://arxiv.org/pdf/2509.21129v1)

**Authors**: Wei Huang, De-Tian Chu, Lin-Yuan Bai, Wei Kang, Hai-Tao Zhang, Bo Li, Zhi-Mo Han, Jing Ge, Hai-Feng Lin

**Abstract**: Modern email spam and phishing attacks have evolved far beyond keyword blacklists or simple heuristics. Adversaries now craft multi-modal campaigns that combine natural-language text with obfuscated URLs, forged headers, and malicious attachments, adapting their strategies within days to bypass filters. Traditional spam detection systems, which rely on static rules or single-modality models, struggle to integrate heterogeneous signals or to continuously adapt, leading to rapid performance degradation.   We propose EvoMail, a self-evolving cognitive agent framework for robust detection of spam and phishing. EvoMail first constructs a unified heterogeneous email graph that fuses textual content, metadata (headers, senders, domains), and embedded resources (URLs, attachments). A Cognitive Graph Neural Network enhanced by a Large Language Model (LLM) performs context-aware reasoning across these sources to identify coordinated spam campaigns. Most critically, EvoMail engages in an adversarial self-evolution loop: a ''red-team'' agent generates novel evasion tactics -- such as character obfuscation or AI-generated phishing text -- while the ''blue-team'' detector learns from failures, compresses experiences into a memory module, and reuses them for future reasoning.   Extensive experiments on real-world datasets (Enron-Spam, Ling-Spam, SpamAssassin, and TREC) and synthetic adversarial variants demonstrate that EvoMail consistently outperforms state-of-the-art baselines in detection accuracy, adaptability to evolving spam tactics, and interpretability of reasoning traces. These results highlight EvoMail's potential as a resilient and explainable defense framework against next-generation spam and phishing threats.

摘要: 现代电子邮件垃圾邮件和网络钓鱼攻击的发展远远超出了关键词黑名单或简单的启发式方法。对手现在精心设计多模式营销活动，将自然语言文本与模糊的URL、伪造的标题和恶意附件相结合，并在几天内调整策略以绕过过滤器。传统的垃圾邮件检测系统依赖于静态规则或单模式模型，难以集成异类信号或持续适应，从而导致性能迅速下降。   我们提出EvoMail，这是一个自进化的认知代理框架，用于稳健检测垃圾邮件和网络钓鱼。EvoMail首先构建了一个统一的异类电子邮件图，融合了文本内容、元数据（标头、收件箱、域）和嵌入式资源（URL、附件）。由大型语言模型（LLM）增强的认知图神经网络跨这些源执行上下文感知推理，以识别协调的垃圾邮件活动。最关键的是，EvoMail参与了一个对抗性的自我进化循环：“红队”代理生成新颖的逃避策略--例如字符混淆或人工智能生成的网络钓鱼文本--而“蓝队”检测器则从失败中学习，将经验压缩到存储模块中，并重新使用它们用于未来的推理。   对现实世界数据集（Enron-Spam、Ling-Spam、SpamAssassin和TREC）和合成对抗变体的广泛实验表明，EvoMail在检测准确性、对不断发展的垃圾邮件策略的适应性以及推理痕迹的可解释性方面始终优于最先进的基线。这些结果凸显了EvoMail作为针对下一代垃圾邮件和网络钓鱼威胁的弹性且可解释的防御框架的潜力。



## **7. Are Modern Speech Enhancement Systems Vulnerable to Adversarial Attacks?**

现代语音增强系统容易受到对抗攻击吗？ eess.AS

Copyright 2026 IEEE. Personal use of this material is permitted.  Permission from IEEE must be obtained for all other uses, in any current or  future media, including reprinting/republishing this material for advertising  or promotional purposes, creating new collective works, for resale or  redistribution to servers or lists, or reuse of any copyrighted component of  this work in other works

**SubmitDate**: 2025-09-25    [abs](http://arxiv.org/abs/2509.21087v1) [paper-pdf](http://arxiv.org/pdf/2509.21087v1)

**Authors**: Rostislav Makarov, Lea Schönherr, Timo Gerkmann

**Abstract**: Machine learning approaches for speech enhancement are becoming increasingly expressive, enabling ever more powerful modifications of input signals. In this paper, we demonstrate that this expressiveness introduces a vulnerability: advanced speech enhancement models can be susceptible to adversarial attacks. Specifically, we show that adversarial noise, carefully crafted and psychoacoustically masked by the original input, can be injected such that the enhanced speech output conveys an entirely different semantic meaning. We experimentally verify that contemporary predictive speech enhancement models can indeed be manipulated in this way. Furthermore, we highlight that diffusion models with stochastic samplers exhibit inherent robustness to such adversarial attacks by design.

摘要: 用于语音增强的机器学习方法正变得越来越具有表达力，从而能够对输入信号进行更强大的修改。在本文中，我们证明了这种表现力引入了一个漏洞：高级语音增强模型可能容易受到对抗性攻击。具体来说，我们表明，可以注入经过精心设计并由原始输入在心理声学上掩盖的对抗性噪音，以便增强的语音输出传达完全不同的语义含义。我们通过实验验证了当代预测语音增强模型确实可以以这种方式操纵。此外，我们强调，具有随机采样器的扩散模型通过设计表现出对此类对抗性攻击的固有鲁棒性。



## **8. Vision Transformers: the threat of realistic adversarial patches**

视觉变形金刚：现实对抗补丁的威胁 cs.CV

Submitted to Sensors + Imaging; presented on 17th of September  (Artificial Intelligence for Security and Defence Applications III)

**SubmitDate**: 2025-09-25    [abs](http://arxiv.org/abs/2509.21084v1) [paper-pdf](http://arxiv.org/pdf/2509.21084v1)

**Authors**: Kasper Cools, Clara Maathuis, Alexander M. van Oers, Claudia S. Hübner, Nikos Deligiannis, Marijke Vandewal, Geert De Cubber

**Abstract**: The increasing reliance on machine learning systems has made their security a critical concern. Evasion attacks enable adversaries to manipulate the decision-making processes of AI systems, potentially causing security breaches or misclassification of targets. Vision Transformers (ViTs) have gained significant traction in modern machine learning due to increased 1) performance compared to Convolutional Neural Networks (CNNs) and 2) robustness against adversarial perturbations. However, ViTs remain vulnerable to evasion attacks, particularly to adversarial patches, unique patterns designed to manipulate AI classification systems. These vulnerabilities are investigated by designing realistic adversarial patches to cause misclassification in person vs. non-person classification tasks using the Creases Transformation (CT) technique, which adds subtle geometric distortions similar to those occurring naturally when wearing clothing. This study investigates the transferability of adversarial attack techniques used in CNNs when applied to ViT classification models. Experimental evaluation across four fine-tuned ViT models on a binary person classification task reveals significant vulnerability variations: attack success rates ranged from 40.04% (google/vit-base-patch16-224-in21k) to 99.97% (facebook/dino-vitb16), with google/vit-base-patch16-224 achieving 66.40% and facebook/dinov3-vitb16 reaching 65.17%. These results confirm the cross-architectural transferability of adversarial patches from CNNs to ViTs, with pre-training dataset scale and methodology strongly influencing model resilience to adversarial attacks.

摘要: 对机器学习系统的依赖日益增加，使其安全性成为一个关键问题。规避攻击使对手能够操纵人工智能系统的决策过程，可能导致安全漏洞或目标分类错误。视觉变形器（ViT）在现代机器学习中获得了显着的吸引力，因为1）与卷积神经网络（CNN）相比性能有所提高; 2）对对抗性扰动的鲁棒性。然而，ViT仍然容易受到规避攻击，特别是对抗补丁，即旨在操纵人工智能分类系统的独特模式。这些漏洞是通过设计现实的对抗补丁来研究的，以使用折痕变换（CT）技术在个人与非人分类任务中造成错误分类，该技术添加了类似于穿着衣服时自然发生的微妙几何失真。本研究调查了CNN中使用的对抗攻击技术应用于ViT分类模型时的可移植性。针对二进制人分类任务的四个微调ViT模型的实验评估显示了显着的漏洞差异：攻击成功率从40.04%（google/vit-base-patch 16 -224-in 21 k）到99.97%（Facebook/dino-vitb 16），其中google/vit-base-patch 16 -224达到6.40%，Facebook/dinov 3-vitb 16达到65.17%。这些结果证实了对抗性补丁从CNN到ViT的跨架构可移植性，预训练数据集规模和方法论强烈影响模型对对抗性攻击的弹性。



## **9. The Use of the Simplex Architecture to Enhance Safety in Deep-Learning-Powered Autonomous Systems**

使用Simpson架构增强深度学习驱动的自治系统的安全性 eess.SY

**SubmitDate**: 2025-09-25    [abs](http://arxiv.org/abs/2509.21014v1) [paper-pdf](http://arxiv.org/pdf/2509.21014v1)

**Authors**: Federico Nesti, Niko Salamini, Mauro Marinoni, Giorgio Maria Cicero, Gabriele Serra, Alessandro Biondi, Giorgio Buttazzo

**Abstract**: Recently, the outstanding performance reached by neural networks in many tasks has led to their deployment in autonomous systems, such as robots and vehicles. However, neural networks are not yet trustworthy, being prone to different types of misbehavior, such as anomalous samples, distribution shifts, adversarial attacks, and other threats. Furthermore, frameworks for accelerating the inference of neural networks typically run on rich operating systems that are less predictable in terms of timing behavior and present larger surfaces for cyber-attacks.   To address these issues, this paper presents a software architecture for enhancing safety, security, and predictability levels of learning-based autonomous systems. It leverages two isolated execution domains, one dedicated to the execution of neural networks under a rich operating system, which is deemed not trustworthy, and one responsible for running safety-critical functions, possibly under a different operating system capable of handling real-time constraints.   Both domains are hosted on the same computing platform and isolated through a type-1 real-time hypervisor enabling fast and predictable inter-domain communication to exchange real-time data. The two domains cooperate to provide a fail-safe mechanism based on a safety monitor, which oversees the state of the system and switches to a simpler but safer backup module, hosted in the safety-critical domain, whenever its behavior is considered untrustworthy.   The effectiveness of the proposed architecture is illustrated by a set of experiments performed on two control systems: a Furuta pendulum and a rover. The results confirm the utility of the fall-back mechanism in preventing faults due to the learning component.

摘要: 最近，神经网络在许多任务中取得的出色性能导致它们被部署在机器人和车辆等自主系统中。然而，神经网络尚未值得信赖，容易出现不同类型的不当行为，例如异常样本、分布漂移、对抗性攻击和其他威胁。此外，用于加速神经网络推理的框架通常运行在丰富的操作系统上，这些操作系统在计时行为方面难以预测，并且为网络攻击提供了更大的表面。   为了解决这些问题，本文提出了一种软件架构，用于增强基于学习的自治系统的安全性、保障性和可预测性水平。它利用两个独立的执行域，一个专用于在富操作系统下执行神经网络，这被认为是不可信的，另一个负责运行安全关键功能，可能在能够处理实时约束的不同操作系统下。   这两个域都托管在同一个计算平台上，并通过类型1实时虚拟机管理程序进行隔离，从而实现快速和可预测的域间通信，以交换实时数据。这两个域合作以提供基于安全监视器的故障安全机制，该安全监视器监视系统的状态，并在其行为被认为不可信时切换到托管在安全关键域中的更简单但更安全的备份模块。   所提出的架构的有效性说明了一组实验上进行的两个控制系统：古田摆和漫游车。结果证实了回退机制在防止学习组件引起的故障方面的实用性。



## **10. A Single Neuron Works: Precise Concept Erasure in Text-to-Image Diffusion Models**

单个神经元起作用：文本到图像扩散模型中的精确概念擦除 cs.CV

**SubmitDate**: 2025-09-25    [abs](http://arxiv.org/abs/2509.21008v1) [paper-pdf](http://arxiv.org/pdf/2509.21008v1)

**Authors**: Qinqin He, Jiaqi Weng, Jialing Tao, Hui Xue

**Abstract**: Text-to-image models exhibit remarkable capabilities in image generation. However, they also pose safety risks of generating harmful content. A key challenge of existing concept erasure methods is the precise removal of target concepts while minimizing degradation of image quality. In this paper, we propose Single Neuron-based Concept Erasure (SNCE), a novel approach that can precisely prevent harmful content generation by manipulating only a single neuron. Specifically, we train a Sparse Autoencoder (SAE) to map text embeddings into a sparse, disentangled latent space, where individual neurons align tightly with atomic semantic concepts. To accurately locate neurons responsible for harmful concepts, we design a novel neuron identification method based on the modulated frequency scoring of activation patterns. By suppressing activations of the harmful concept-specific neuron, SNCE achieves surgical precision in concept erasure with minimal disruption to image quality. Experiments on various benchmarks demonstrate that SNCE achieves state-of-the-art results in target concept erasure, while preserving the model's generation capabilities for non-target concepts. Additionally, our method exhibits strong robustness against adversarial attacks, significantly outperforming existing methods.

摘要: 文本到图像模型在图像生成方面表现出非凡的能力。然而，它们也带来了产生有害内容的安全风险。现有概念擦除方法的一个关键挑战是精确去除目标概念，同时最大限度地减少图像质量的退化。在本文中，我们提出了基于单神经元的概念擦除（SNCE），这是一种新颖的方法，可以通过仅操纵单个神经元来精确防止有害内容的生成。具体来说，我们训练稀疏自动编码器（AE）将文本嵌入映射到稀疏、解开的潜在空间中，其中各个神经元与原子语义概念紧密对齐。为了准确定位有害概念的神经元，我们设计了一种基于激活模式的调制频率评分的新型神经元识别方法。通过抑制有害的概念特定神经元的激活，SNCE在概念擦除方面实现了外科手术精确度，同时对图像质量的干扰最小。各种基准测试的实验表明，SNCE在目标概念擦除方面实现了最先进的结果，同时保留了模型针对非目标概念的生成能力。此外，我们的方法对对抗攻击表现出很强的鲁棒性，显着优于现有方法。



## **11. CTI Dataset Construction from Telegram**

从Telegram构建RTI数据集 cs.CR

**SubmitDate**: 2025-09-25    [abs](http://arxiv.org/abs/2509.20943v1) [paper-pdf](http://arxiv.org/pdf/2509.20943v1)

**Authors**: Dincy R. Arikkat, Sneha B. T., Serena Nicolazzo, Antonino Nocera, Vinod P., Rafidha Rehiman K. A., Karthika R

**Abstract**: Cyber Threat Intelligence (CTI) enables organizations to anticipate, detect, and mitigate evolving cyber threats. Its effectiveness depends on high-quality datasets, which support model development, training, evaluation, and benchmarking. Building such datasets is crucial, as attack vectors and adversary tactics continually evolve. Recently, Telegram has gained prominence as a valuable CTI source, offering timely and diverse threat-related information that can help address these challenges. In this work, we address these challenges by presenting an end-to-end automated pipeline that systematically collects and filters threat-related content from Telegram. The pipeline identifies relevant Telegram channels and scrapes 145,349 messages from 12 curated channels out of 150 identified sources. To accurately filter threat intelligence messages from generic content, we employ a BERT-based classifier, achieving an accuracy of 96.64%. From the filtered messages, we compile a dataset of 86,509 malicious Indicators of Compromise, including domains, IPs, URLs, hashes, and CVEs. This approach not only produces a large-scale, high-fidelity CTI dataset but also establishes a foundation for future research and operational applications in cyber threat detection.

摘要: 网络威胁情报（RTI）使组织能够预测、检测和缓解不断变化的网络威胁。其有效性取决于支持模型开发、培训、评估和基准的高质量数据集。随着攻击载体和对手战术的不断发展，构建此类数据集至关重要。最近，Telegram作为宝贵的RTI来源而声名鹊起，提供及时且多样化的威胁相关信息，帮助应对这些挑战。在这项工作中，我们通过提出一个端到端的自动化管道来解决这些挑战，该管道系统地收集和过滤来自Telegram的威胁相关内容。该管道识别相关的Telegram渠道，并从150个已识别来源中的12个策划渠道中抓取145，349条消息。为了从一般内容中准确过滤威胁情报消息，我们采用了基于BERT的分类器，准确率达到96.64%。从过滤后的消息中，我们编译了一个包含86，509个恶意妥协指标的数据集，包括域，IP，URL，哈希和CVE。这种方法不仅产生了大规模、高保真度的RTI数据集，而且还为网络威胁检测的未来研究和运营应用奠定了基础。



## **12. RLCracker: Exposing the Vulnerability of LLM Watermarks with Adaptive RL Attacks**

RL Cracker：通过自适应RL攻击暴露LLM水印的漏洞 cs.CR

**SubmitDate**: 2025-09-25    [abs](http://arxiv.org/abs/2509.20924v1) [paper-pdf](http://arxiv.org/pdf/2509.20924v1)

**Authors**: Hanbo Huang, Yiran Zhang, Hao Zheng, Xuan Gong, Yihan Li, Lin Liu, Shiyu Liang

**Abstract**: Large Language Models (LLMs) watermarking has shown promise in detecting AI-generated content and mitigating misuse, with prior work claiming robustness against paraphrasing and text editing. In this paper, we argue that existing evaluations are not sufficiently adversarial, obscuring critical vulnerabilities and overstating the security. To address this, we introduce adaptive robustness radius, a formal metric that quantifies watermark resilience against adaptive adversaries. We theoretically prove that optimizing the attack context and model parameters can substantially reduce this radius, making watermarks highly susceptible to paraphrase attacks. Leveraging this insight, we propose RLCracker, a reinforcement learning (RL)-based adaptive attack that erases watermarks while preserving semantic fidelity. RLCracker requires only limited watermarked examples and zero access to the detector. Despite weak supervision, it empowers a 3B model to achieve 98.5% removal success and an average 0.92 P-SP score on 1,500-token Unigram-marked texts after training on only 100 short samples. This performance dramatically exceeds 6.75% by GPT-4o and generalizes across five model sizes over ten watermarking schemes. Our results confirm that adaptive attacks are broadly effective and pose a fundamental threat to current watermarking defenses.

摘要: 大型语言模型（LLM）水印在检测人工智能生成的内容和减少滥用方面表现出了希望，之前的工作声称对重述和文本编辑具有鲁棒性。在本文中，我们认为现有的评估不够对抗，掩盖了关键漏洞并夸大了安全性。为了解决这个问题，我们引入了自适应鲁棒性半径，这是一种形式指标，可以量化水印针对自适应对手的弹性。我们从理论上证明，优化攻击上下文和模型参数可以大幅缩小这个半径，使水印极易受到转述攻击。利用这一见解，我们提出了RL Cracker，这是一种基于强化学习（RL）的自适应攻击，可以擦除水印，同时保留语义保真度。RL Cracker仅需要有限的带有水印的示例，并且对检测器的访问为零。尽管监管薄弱，但它使3B模型能够在仅对100个短样本进行训练后，在1，500个标记Unigram的文本上实现98.5%的删除成功率和平均0.92的P-SP评分。GPT-4 o的这一性能显着超过了6.75%，并在十种水印方案中推广了五种模型大小。我们的结果证实，自适应攻击广泛有效，并对当前的水印防御构成根本威胁。



## **13. Security-aware Semantic-driven ISAC via Paired Adversarial Residual Networks**

通过配对对抗剩余网络的安全感知语义驱动的ISAC cs.CR

**SubmitDate**: 2025-09-25    [abs](http://arxiv.org/abs/2509.20835v1) [paper-pdf](http://arxiv.org/pdf/2509.20835v1)

**Authors**: Yu Liu, Boxiang He, Fanggang Wang

**Abstract**: This paper proposes a novel and flexible security-aware semantic-driven integrated sensing and communication (ISAC) framework, namely security semantic ISAC (SS-ISAC). Inspired by the positive impact of the adversarial attack, a pair of pluggable encryption and decryption modules is designed in the proposed SS-ISAC framework. The encryption module is installed after the semantic transmitter, adopting a trainable adversarial residual network (ARN) to create the adversarial attack. Correspondingly, the decryption module before the semantic receiver utilizes another trainable ARN to mitigate the adversarial attack and noise. These two modules can be flexibly assembled considering the system security demands, without drastically modifying the hardware infrastructure. To ensure the sensing and communication (SAC) performance while preventing the eavesdropping threat, the above ARNs are jointly optimized by minimizing a carefully designed loss function that relates to the adversarial attack power, SAC performance, as well as the privacy leakage risk. Simulation results validate the effectiveness of the proposed SS-ISAC framework in terms of both SAC and eavesdropping prevention performance.

摘要: 提出了一种新颖灵活的安全感知语义驱动的集成感知与通信（ISAC）框架，即安全语义ISAC（SS-ISAC）。受对抗性攻击的积极影响的启发，在所提出的SS-ISAC框架中设计了一对可插入的加密和解密模块。加密模块安装在语义发送器之后，采用可训练的对抗性剩余网络（ARN）来创建对抗性攻击。相应地，语义接收器之前的解密模块利用另一个可训练的ARN来减轻对抗性攻击和噪声。考虑到系统安全需求，这两个模块可以灵活组装，而无需大幅修改硬件基础设施。为了确保传感和通信（SAC）性能，同时防止窃听威胁，通过最小化精心设计的与对抗攻击功率、SAC性能以及隐私泄露风险相关的损失函数，对上述ARN进行了联合优化。仿真结果验证了所提出的SS-ISAC框架在SAC和窃听防止性能方面的有效性。



## **14. Trustworthy Semantic Communication for Vehicular Networks: Challenges and Solutions**

车载网络值得信赖的语义通信：挑战和解决方案 cs.NI

8 pages, 8 figures, accepted by IEEE Vehicular Technology Magazine

**SubmitDate**: 2025-09-25    [abs](http://arxiv.org/abs/2509.20830v1) [paper-pdf](http://arxiv.org/pdf/2509.20830v1)

**Authors**: Yanghe Pan, Yuntao Wang, Shaolong Guo, Chengyu Yin, Ruidong Li, Zhou Su, Yuan Wu

**Abstract**: Semantic communication (SemCom) has the potential to significantly reduce communication delay in vehicle-to-everything (V2X) communications within vehicular networks (VNs). However, the deployment of vehicular SemCom networks (VN-SemComNets) faces critical trust challenges in information transmission, semantic encoding, and communication entity reliability. This paper proposes an innovative three-layer trustworthy VN-SemComNet architecture. Specifically, we introduce a semantic camouflage transmission mechanism leveraging defensive adversarial noise for active eavesdropping defense, a robust federated encoder-decoder training framework to mitigate encoder-decoder poisoning attacks, and an audit game-based distributed vehicle trust management mechanism to deter untrustworthy vehicles. A case study validates the effectiveness of the proposed solutions. Lastly, essential future research directions are pointed out to advance this emerging field.

摘要: 语义通信（SemCom）有潜力显着减少车载网络（VN）内车对一切（V2X）通信的通信延迟。然而，车载SemCom网络（VN-SemComNets）的部署在信息传输、语义编码和通信实体可靠性方面面临着关键的信任挑战。本文提出了一种创新的三层可信VN-SemComNet架构。具体来说，我们引入了一种利用防御性对抗噪音进行主动窃听防御的语义伪装传输机制、一种用于减轻编码器-解码器中毒攻击的强大联邦编码器-解码器训练框架，以及一种基于审计游戏的分布式车辆信任管理机制，以阻止不值得信赖的车辆。案例研究验证了所提出解决方案的有效性。最后，指出了未来重要的研究方向，以推进这一新兴领域。



## **15. Exploring the Secondary Risks of Large Language Models**

探索大型语言模型的次要风险 cs.LG

18 pages, 5 figures

**SubmitDate**: 2025-09-25    [abs](http://arxiv.org/abs/2506.12382v3) [paper-pdf](http://arxiv.org/pdf/2506.12382v3)

**Authors**: Jiawei Chen, Zhengwei Fang, Xiao Yang, Chao Yu, Zhaoxia Yin, Hang Su

**Abstract**: Ensuring the safety and alignment of Large Language Models is a significant challenge with their growing integration into critical applications and societal functions. While prior research has primarily focused on jailbreak attacks, less attention has been given to non-adversarial failures that subtly emerge during benign interactions. We introduce secondary risks a novel class of failure modes marked by harmful or misleading behaviors during benign prompts. Unlike adversarial attacks, these risks stem from imperfect generalization and often evade standard safety mechanisms. To enable systematic evaluation, we introduce two risk primitives verbose response and speculative advice that capture the core failure patterns. Building on these definitions, we propose SecLens, a black-box, multi-objective search framework that efficiently elicits secondary risk behaviors by optimizing task relevance, risk activation, and linguistic plausibility. To support reproducible evaluation, we release SecRiskBench, a benchmark dataset of 650 prompts covering eight diverse real-world risk categories. Experimental results from extensive evaluations on 16 popular models demonstrate that secondary risks are widespread, transferable across models, and modality independent, emphasizing the urgent need for enhanced safety mechanisms to address benign yet harmful LLM behaviors in real-world deployments.

摘要: 随着大型语言模型越来越多地集成到关键应用程序和社会功能中，确保大型语言模型的安全性和一致性是一项重大挑战。虽然之前的研究主要集中在越狱攻击上，但对良性互动中微妙出现的非对抗性失败的关注较少。我们引入了二级风险，这是一种新型的失败模式，其特征是良性提示期间的有害或误导行为。与对抗性攻击不同，这些风险源于不完美的概括，并且常常逃避标准安全机制。为了实现系统性评估，我们引入了两个风险基元：详细响应和推测性建议，以捕捉核心故障模式。在这些定义的基础上，我们提出了SecLens，这是一个黑匣子、多目标搜索框架，通过优化任务相关性、风险激活和语言合理性来有效地引发次要风险行为。为了支持可重复的评估，我们发布了SecRiskBench，这是一个由650个提示组成的基准数据集，涵盖八个不同的现实世界风险类别。对16种流行模型进行广泛评估的实验结果表明，次级风险是普遍存在的，可以跨模型转移，并且独立于模式，这强调了迫切需要增强的安全机制来解决现实世界部署中良性但有害的LLM行为。



## **16. FERD: Fairness-Enhanced Data-Free Robustness Distillation**

FERD：公平增强的无数据稳健蒸馏 cs.LG

**SubmitDate**: 2025-09-25    [abs](http://arxiv.org/abs/2509.20793v1) [paper-pdf](http://arxiv.org/pdf/2509.20793v1)

**Authors**: Zhengxiao Li, Liming Lu, Xu Zheng, Siyuan Liang, Zhenghan Chen, Yongbin Zhou, Shuchao Pang

**Abstract**: Data-Free Robustness Distillation (DFRD) aims to transfer the robustness from the teacher to the student without accessing the training data. While existing methods focus on overall robustness, they overlook the robust fairness issues, leading to severe disparity of robustness across different categories. In this paper, we find two key problems: (1) student model distilled with equal class proportion data behaves significantly different across distinct categories; and (2) the robustness of student model is not stable across different attacks target. To bridge these gaps, we present the first Fairness-Enhanced data-free Robustness Distillation (FERD) framework to adjust the proportion and distribution of adversarial examples. For the proportion, FERD adopts a robustness-guided class reweighting strategy to synthesize more samples for the less robust categories, thereby improving robustness of them. For the distribution, FERD generates complementary data samples for advanced robustness distillation. It generates Fairness-Aware Examples (FAEs) by enforcing a uniformity constraint on feature-level predictions, which suppress the dominance of class-specific non-robust features, providing a more balanced representation across all categories. Then, FERD constructs Uniform-Target Adversarial Examples (UTAEs) from FAEs by applying a uniform target class constraint to avoid biased attack directions, which distribute the attack targets across all categories and prevents overfitting to specific vulnerable categories. Extensive experiments on three public datasets show that FERD achieves state-of-the-art worst-class robustness under all adversarial attack (e.g., the worst-class robustness under FGSM and AutoAttack are improved by 15.1\% and 6.4\% using MobileNet-V2 on CIFAR-10), demonstrating superior performance in both robustness and fairness aspects.

摘要: 无数据稳健蒸馏（DFRD）旨在将稳健性从教师转移到学生，而无需访问训练数据。虽然现有方法关注整体稳健性，但它们忽视了稳健的公平性问题，导致不同类别之间稳健性的严重差异。本文发现了两个关键问题：（1）用等班级比例数据提取的学生模型在不同类别中的表现显着不同;（2）学生模型的稳健性在不同的攻击目标中不稳定。为了弥合这些差距，我们提出了第一个公平增强的无数据稳健蒸馏（FERD）框架，以调整对抗性示例的比例和分布。对于比例，FERD采用鲁棒性引导的类别重新加权策略，为鲁棒性较差的类别合成更多样本，从而提高它们的鲁棒性。对于分发，FERD生成补充数据样本以进行高级稳健性提取。它通过对特征级预测实施一致性约束来生成公平性示例（FAE），这抑制了特定类别的非稳健特征的主导地位，从而在所有类别中提供更平衡的表示。然后，FERD通过应用统一的目标类约束，从FAE中构建统一目标对抗示例（UTAE），以避免有偏见的攻击方向，从而将攻击目标分布在所有类别中，并防止过度适合特定的脆弱类别。在三个公开数据集上的大量实验表明，FERD在所有对抗性攻击（例如，在CIFAR-10上使用MobileNet-V2，FGSM和AutoAttack下的最差类鲁棒性分别提高了15.1%和6.4%，在鲁棒性和公平性方面都表现出了优异的性能。



## **17. DAC-LoRA: Dynamic Adversarial Curriculum for Efficient and Robust Few-Shot Adaptation**

DAC-LoRA：用于高效且稳健的少镜头适应的动态对抗课程 cs.CV

Accepted at ICCV2025 Workshop on Safe and Trustworthy Multimodal AI  Systems

**SubmitDate**: 2025-09-25    [abs](http://arxiv.org/abs/2509.20792v1) [paper-pdf](http://arxiv.org/pdf/2509.20792v1)

**Authors**: Ved Umrajkar

**Abstract**: Vision-Language Models (VLMs) are foundational to critical applications like autonomous driving, medical diagnosis, and content moderation. While Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA enable their efficient adaptation to specialized tasks, these models remain vulnerable to adversarial attacks that can compromise safety-critical decisions. CLIP, the backbone for numerous downstream VLMs, is a high-value target whose vulnerabilities can cascade across the multimodal AI ecosystem. We propose Dynamic Adversarial Curriculum DAC-LoRA, a novel framework that integrates adversarial training into PEFT. The core principle of our method i.e. an intelligent curriculum of progressively challenging attack, is general and can potentially be applied to any iterative attack method. Guided by the First-Order Stationary Condition (FOSC) and a TRADES-inspired loss, DAC-LoRA achieves substantial improvements in adversarial robustness without significantly compromising clean accuracy. Our work presents an effective, lightweight, and broadly applicable method to demonstrate that the DAC-LoRA framework can be easily integrated into a standard PEFT pipeline to significantly enhance robustness.

摘要: 视觉语言模型（VLM）是自动驾驶、医疗诊断和内容审核等关键应用的基础。虽然像LoRA这样的参数高效微调（PEFT）方法能够有效地适应专门任务，但这些模型仍然容易受到对抗攻击的影响，这些攻击可能会损害对安全至关重要的决策。CLIP是众多下游VLM的支柱，是一个高价值目标，其漏洞可能会在多模式人工智能生态系统中蔓延。我们提出了动态对抗课程DAC-LoRA，这是一个将对抗训练集成到PEFT中的新型框架。我们方法的核心原则，即逐步挑战性攻击的智能课程，是通用的，可以应用于任何迭代攻击方法。在一阶平稳条件（FOSC）和TRADES启发的损失的指导下，DAC-LoRA在对抗鲁棒性方面实现了实质性改进，而不会显着损害清晰准确性。我们的工作提出了一种有效、轻量级且广泛适用的方法，以证明DAC-LoRA框架可以轻松集成到标准PEFT管道中，以显着增强稳健性。



## **18. Reformulation is All You Need: Addressing Malicious Text Features in DNNs**

重新制定即可：解决DNN中的恶意文本特征 cs.LG

Accepted by journal "Machine Intelligence Research"

**SubmitDate**: 2025-09-25    [abs](http://arxiv.org/abs/2502.00652v2) [paper-pdf](http://arxiv.org/pdf/2502.00652v2)

**Authors**: Yi Jiang, Oubo Ma, Yong Yang, Tong Zhang, Shouling Ji

**Abstract**: Human language encompasses a wide range of intricate and diverse implicit features, which attackers can exploit to launch adversarial or backdoor attacks, compromising DNN models for NLP tasks. Existing model-oriented defenses often require substantial computational resources as model size increases, whereas sample-oriented defenses typically focus on specific attack vectors or schemes, rendering them vulnerable to adaptive attacks. We observe that the root cause of both adversarial and backdoor attacks lies in the encoding process of DNN models, where subtle textual features, negligible for human comprehension, are erroneously assigned significant weight by less robust or trojaned models. Based on it we propose a unified and adaptive defense framework that is effective against both adversarial and backdoor attacks. Our approach leverages reformulation modules to address potential malicious features in textual inputs while preserving the original semantic integrity. Extensive experiments demonstrate that our framework outperforms existing sample-oriented defense baselines across a diverse range of malicious textual features.

摘要: 人类语言包含广泛复杂且多样化的隐性特征，攻击者可以利用这些特征发起对抗性或后门攻击，从而损害NLP任务的DNN模型。随着模型大小的增加，现有的面向模型的防御通常需要大量的计算资源，而面向样本的防御通常专注于特定的攻击载体或方案，使其容易受到自适应攻击。我们观察到，对抗性攻击和后门攻击的根本原因都在于DNN模型的编码过程，其中对于人类理解来说可以忽略不计的微妙文本特征被不太稳健或木马模型错误地赋予了重要的权重。在此基础上，我们提出了一个统一的、自适应的防御框架，可以有效对抗对抗攻击和后门攻击。我们的方法利用重构模块来解决文本输入中的潜在恶意特征，同时保持原始语义的完整性。大量的实验表明，我们的框架优于现有的面向样本的防御基线在各种各样的恶意文本功能。



## **19. Cryptographic Backdoor for Neural Networks: Boon and Bane**

神经网络的密码后门：布恩和贝恩 cs.CR

Preprint

**SubmitDate**: 2025-09-25    [abs](http://arxiv.org/abs/2509.20714v1) [paper-pdf](http://arxiv.org/pdf/2509.20714v1)

**Authors**: Anh Tu Ngo, Anupam Chattopadhyay, Subhamoy Maitra

**Abstract**: In this paper we show that cryptographic backdoors in a neural network (NN) can be highly effective in two directions, namely mounting the attacks as well as in presenting the defenses as well. On the attack side, a carefully planted cryptographic backdoor enables powerful and invisible attack on the NN. Considering the defense, we present applications: first, a provably robust NN watermarking scheme; second, a protocol for guaranteeing user authentication; and third, a protocol for tracking unauthorized sharing of the NN intellectual property (IP). From a broader theoretical perspective, borrowing the ideas from Goldwasser et. al. [FOCS 2022], our main contribution is to show that all these instantiated practical protocol implementations are provably robust. The protocols for watermarking, authentication and IP tracking resist an adversary with black-box access to the NN, whereas the backdoor-enabled adversarial attack is impossible to prevent under the standard assumptions. While the theoretical tools used for our attack is mostly in line with the Goldwasser et. al. ideas, the proofs related to the defense need further studies. Finally, all these protocols are implemented on state-of-the-art NN architectures with empirical results corroborating the theoretical claims. Further, one can utilize post-quantum primitives for implementing the cryptographic backdoors, laying out foundations for quantum-era applications in machine learning (ML).

摘要: 在本文中，我们表明神经网络（NN）中的加密后门在两个方向上非常有效，即发起攻击以及提供防御。在攻击方面，精心植入的加密后门能够对NN进行强大且不可见的攻击。考虑到防御，我们提出了应用：首先，一种可证明稳健的NN水印方案;第二，一种用于保证用户认证的协议;第三，一种用于跟踪NN知识产权（IP）的未经授权共享的协议。从更广泛的理论角度借鉴戈德瓦瑟等人的思想。al. [FOCS 2022]，我们的主要贡献是表明所有这些实例化的实际协议实现都是可证明的稳健性。水印、认证和IP跟踪协议可以抵御通过黑匣子访问NN的对手，而在标准假设下，后门启用的对抗攻击是不可能防止的。而我们攻击使用的理论工具大多与Goldwasser等人一致。其他想法、与辩护相关的证据需要进一步研究。最后，所有这些协议都在最先进的NN架构上实现，经验结果证实了理论主张。此外，人们可以利用后量子基元来实现加密后门，为机器学习（ML）中的量子时代应用奠定基础。



## **20. Overcoming Black-box Attack Inefficiency with Hybrid and Dynamic Select Algorithms**

用混合和动态选择算法克服黑匣子攻击效率低下 cs.CL

34 pages, 3 figures, Accepted to Findings of EMNLP 2025

**SubmitDate**: 2025-09-25    [abs](http://arxiv.org/abs/2509.20699v1) [paper-pdf](http://arxiv.org/pdf/2509.20699v1)

**Authors**: Abhinay Shankar Belde, Rohit Ramkumar, Jonathan Rusert

**Abstract**: Adversarial text attack research plays a crucial role in evaluating the robustness of NLP models. However, the increasing complexity of transformer-based architectures has dramatically raised the computational cost of attack testing, especially for researchers with limited resources (e.g., GPUs). Existing popular black-box attack methods often require a large number of queries, which can make them inefficient and impractical for researchers. To address these challenges, we propose two new attack selection strategies called Hybrid and Dynamic Select, which better combine the strengths of previous selection algorithms. Hybrid Select merges generalized BinarySelect techniques with GreedySelect by introducing a size threshold to decide which selection algorithm to use. Dynamic Select provides an alternative approach of combining the generalized Binary and GreedySelect by learning which lengths of texts each selection method should be applied to. This greatly reduces the number of queries needed while maintaining attack effectiveness (a limitation of BinarySelect). Across 4 datasets and 6 target models, our best method(sentence-level Hybrid Select) is able to reduce the number of required queries per attack up 25.82\% on average against both encoder models and LLMs, without losing the effectiveness of the attack.

摘要: 对抗性文本攻击研究在评估NLP模型的稳健性方面发挥着至关重要的作用。然而，基于变压器的架构日益复杂，极大地提高了攻击测试的计算成本，特别是对于资源有限的研究人员（例如，图形处理器）。现有流行的黑匣子攻击方法通常需要大量查询，这对研究人员来说可能会导致它们效率低下且不切实际。为了应对这些挑战，我们提出了两种新的攻击选择策略，称为混合和动态选择，它们更好地结合了之前选择算法的优势。Hybrid Select通过引入大小阈值来决定使用哪种选择算法，将广义Binaryselect技术与Greedyselect合并。动态选择提供了一种替代方法，通过学习每种选择方法应应用于哪些文本长度，将广义Binary和Greedyselect结合起来。这大大减少了所需的查询数量，同时保持攻击有效性（Binaryselect的限制）。在4个数据集和6个目标模型中，我们的最佳方法（业务级混合选择）能够将每次攻击所需的查询数量平均减少25.82%，而不会失去攻击的有效性。



## **21. RedHerring Attack: Testing the Reliability of Attack Detection**

RedHerring攻击：测试攻击检测的可靠性 cs.CL

16 pages, 3 figures, Accepted to EMNLP 2025

**SubmitDate**: 2025-09-25    [abs](http://arxiv.org/abs/2509.20691v1) [paper-pdf](http://arxiv.org/pdf/2509.20691v1)

**Authors**: Jonathan Rusert

**Abstract**: In response to adversarial text attacks, attack detection models have been proposed and shown to successfully identify text modified by adversaries. Attack detection models can be leveraged to provide an additional check for NLP models and give signals for human input. However, the reliability of these models has not yet been thoroughly explored. Thus, we propose and test a novel attack setting and attack, RedHerring. RedHerring aims to make attack detection models unreliable by modifying a text to cause the detection model to predict an attack, while keeping the classifier correct. This creates a tension between the classifier and detector. If a human sees that the detector is giving an ``incorrect'' prediction, but the classifier a correct one, then the human will see the detector as unreliable. We test this novel threat model on 4 datasets against 3 detectors defending 4 classifiers. We find that RedHerring is able to drop detection accuracy between 20 - 71 points, while maintaining (or improving) classifier accuracy. As an initial defense, we propose a simple confidence check which requires no retraining of the classifier or detector and increases detection accuracy greatly. This novel threat model offers new insights into how adversaries may target detection models.

摘要: 为了应对对抗性文本攻击，攻击检测模型被提出并证明可以成功识别对手修改的文本。可以利用攻击检测模型来为NLP模型提供额外检查并为人类输入提供信号。然而，这些模型的可靠性尚未得到彻底探索。因此，我们提出并测试了一种新颖的攻击设置和攻击RedHerring。RedHerring的目标是通过修改文本以使检测模型预测攻击，同时保持分类器正确，从而使攻击检测模型变得不可靠。这会在分类器和检测器之间产生紧张关系。如果人类看到检测器给出了“不正确”的预测，但分类器是正确的，那么人类就会认为检测器不可靠。我们在4个数据集上测试了这个新颖的威胁模型，针对保护4个分类器的3个检测器。我们发现RedHerring能够将检测准确性降低20 - 71点之间，同时保持（或提高）分类器准确性。作为初始防御，我们提出了一种简单的置信度检查，它不需要重新训练分类器或检测器，并大大提高了检测准确性。这种新颖的威胁模型为对手如何瞄准检测模型提供了新的见解。



## **22. Model Agnostic Defense against Adversarial Patch Attacks on Object Detection in Unmanned Aerial Vehicles**

无人机目标检测对抗补丁攻击的模型不可知防御 cs.CV

published in IROS 2024

**SubmitDate**: 2025-09-24    [abs](http://arxiv.org/abs/2405.19179v2) [paper-pdf](http://arxiv.org/pdf/2405.19179v2)

**Authors**: Saurabh Pathak, Samridha Shrestha, Abdelrahman AlMahmoud

**Abstract**: Object detection forms a key component in Unmanned Aerial Vehicles (UAVs) for completing high-level tasks that depend on the awareness of objects on the ground from an aerial perspective. In that scenario, adversarial patch attacks on an onboard object detector can severely impair the performance of upstream tasks. This paper proposes a novel model-agnostic defense mechanism against the threat of adversarial patch attacks in the context of UAV-based object detection. We formulate adversarial patch defense as an occlusion removal task. The proposed defense method can neutralize adversarial patches located on objects of interest, without exposure to adversarial patches during training. Our lightweight single-stage defense approach allows us to maintain a model-agnostic nature, that once deployed does not require to be updated in response to changes in the object detection pipeline. The evaluations in digital and physical domains show the feasibility of our method for deployment in UAV object detection pipelines, by significantly decreasing the Attack Success Ratio without incurring significant processing costs. As a result, the proposed defense solution can improve the reliability of object detection for UAVs.

摘要: 物体检测是无人机（UFO）的关键组成部分，用于完成依赖于从空中角度对地面物体的感知的高级任务。在这种情况下，对机载物体检测器的对抗补丁攻击可能会严重损害上游任务的性能。本文提出了一种新型的模型不可知防御机制，以对抗基于无人机的对象检测背景下的对抗补丁攻击的威胁。我们将对抗性补丁防御制定为遮挡去除任务。提出的防御方法可以中和位于感兴趣对象上的对抗补丁，而不会在训练期间接触对抗补丁。我们的轻量级单阶段防御方法使我们能够保持模型不可知的性质，一旦部署，就不需要根据对象检测管道的变化进行更新。数字和物理领域的评估表明，我们的方法在无人机物体检测管道中部署的可行性，可以显着降低攻击成功率，而不会产生显着的处理成本。因此，提出的防御解决方案可以提高无人机目标检测的可靠性。



## **23. Every Character Counts: From Vulnerability to Defense in Phishing Detection**

每个字符都很重要：网络钓鱼检测中的脆弱性到防御 cs.CR

Accepted at ICTAI 2025

**SubmitDate**: 2025-09-24    [abs](http://arxiv.org/abs/2509.20589v1) [paper-pdf](http://arxiv.org/pdf/2509.20589v1)

**Authors**: Maria Chiper, Radu Tudor Ionescu

**Abstract**: Phishing attacks targeting both organizations and individuals are becoming an increasingly significant threat as technology advances. Current automatic detection methods often lack explainability and robustness in detecting new phishing attacks. In this work, we investigate the effectiveness of character-level deep learning models for phishing detection, which can provide both robustness and interpretability. We evaluate three neural architectures adapted to operate at the character level, namely CharCNN, CharGRU, and CharBiLSTM, on a custom-built email dataset, which combines data from multiple sources. Their performance is analyzed under three scenarios: (i) standard training and testing, (ii) standard training and testing under adversarial attacks, and (iii) training and testing with adversarial examples. Aiming to develop a tool that operates as a browser extension, we test all models under limited computational resources. In this constrained setup, CharGRU proves to be the best-performing model across all scenarios. All models show vulnerability to adversarial attacks, but adversarial training substantially improves their robustness. In addition, by adapting the Gradient-weighted Class Activation Mapping (Grad-CAM) technique to character-level inputs, we are able to visualize which parts of each email influence the decision of each model. Our open-source code and data is released at https://github.com/chipermaria/every-character-counts.

摘要: 随着技术的进步，针对组织和个人的网络钓鱼攻击正在成为日益严重的威胁。当前的自动检测方法在检测新的网络钓鱼攻击时通常缺乏可解释性和鲁棒性。在这项工作中，我们研究了字符级深度学习模型用于网络钓鱼检测的有效性，该模型可以提供鲁棒性和可解释性。我们在定制的电子邮件数据集上评估了三种适合在字符级别操作的神经架构，即CharCNN、CharGRU和CharBiLSTM，该数据集结合了来自多个来源的数据。他们的表现在三种场景下进行分析：（i）标准训练和测试，（ii）对抗性攻击下的标准训练和测试，以及（iii）使用对抗性示例进行训练和测试。为了开发一种作为浏览器扩展运行的工具，我们在有限的计算资源下测试所有模型。在这种受限制的设置中，CharGRU被证明是所有场景中性能最好的模型。所有模型都表现出对对抗攻击的脆弱性，但对抗训练大大提高了它们的稳健性。此外，通过将学生加权类激活映射（Grad-CAM）技术适应字符级输入，我们能够可视化每封电子邮件的哪些部分影响每个模型的决策。我们的开源代码和数据在https://github.com/chipermaria/every-character-counts上发布。



## **24. Understanding and Improving Adversarial Robustness of Neural Probabilistic Circuits**

理解和提高神经概率电路的对抗鲁棒性 cs.LG

NeurIPS 2025 Camera Ready

**SubmitDate**: 2025-09-24    [abs](http://arxiv.org/abs/2509.20549v1) [paper-pdf](http://arxiv.org/pdf/2509.20549v1)

**Authors**: Weixin Chen, Han Zhao

**Abstract**: Neural Probabilistic Circuits (NPCs), a new class of concept bottleneck models, comprise an attribute recognition model and a probabilistic circuit for reasoning. By integrating the outputs from these two modules, NPCs produce compositional and interpretable predictions. While offering enhanced interpretability and high performance on downstream tasks, the neural-network-based attribute recognition model remains a black box. This vulnerability allows adversarial attacks to manipulate attribute predictions by introducing carefully crafted subtle perturbations to input images, potentially compromising the final predictions. In this paper, we theoretically analyze the adversarial robustness of NPC and demonstrate that it only depends on the robustness of the attribute recognition model and is independent of the robustness of the probabilistic circuit. Moreover, we propose RNPC, the first robust neural probabilistic circuit against adversarial attacks on the recognition module. RNPC introduces a novel class-wise integration for inference, ensuring a robust combination of outputs from the two modules. Our theoretical analysis demonstrates that RNPC exhibits provably improved adversarial robustness compared to NPC. Empirical results on image classification tasks show that RNPC achieves superior adversarial robustness compared to existing concept bottleneck models while maintaining high accuracy on benign inputs.

摘要: 神经概率电路（NPC）是一类新型概念瓶颈模型，由属性识别模型和推理概率电路组成。通过集成这两个模块的输出，NPC生成合成和可解释的预测。虽然在下游任务中提供了增强的可解释性和高性能，但基于神经网络的属性识别模型仍然是一个黑匣子。该漏洞允许对抗攻击通过向输入图像引入精心设计的微妙扰动来操纵属性预测，从而可能会损害最终的预测。本文从理论上分析了NPC的对抗鲁棒性，证明它仅取决于属性识别模型的鲁棒性，而不依赖于概率电路的鲁棒性。此外，我们提出了RNPC，这是第一个针对识别模块对抗攻击的鲁棒神经概率电路。RNPC引入了一种新颖的逐类集成来进行推理，确保两个模块的输出的稳健组合。我们的理论分析表明，与NPC相比，RNPC表现出明显改善的对抗鲁棒性。图像分类任务的经验结果表明，与现有的概念瓶颈模型相比，RNPC实现了卓越的对抗鲁棒性，同时对良性输入保持高准确性。



## **25. Efficiently Attacking Memorization Scores**

有效地攻击认证分数 cs.LG

**SubmitDate**: 2025-09-24    [abs](http://arxiv.org/abs/2509.20463v1) [paper-pdf](http://arxiv.org/pdf/2509.20463v1)

**Authors**: Tue Do, Varun Chandrasekaran, Daniel Alabi

**Abstract**: Influence estimation tools -- such as memorization scores -- are widely used to understand model behavior, attribute training data, and inform dataset curation. However, recent applications in data valuation and responsible machine learning raise the question: can these scores themselves be adversarially manipulated? In this work, we present a systematic study of the feasibility of attacking memorization-based influence estimators. We characterize attacks for producing highly memorized samples as highly sensitive queries in the regime where a trained algorithm is accurate. Our attack (calculating the pseudoinverse of the input) is practical, requiring only black-box access to model outputs and incur modest computational overhead. We empirically validate our attack across a wide suite of image classification tasks, showing that even state-of-the-art proxies are vulnerable to targeted score manipulations. In addition, we provide a theoretical analysis of the stability of memorization scores under adversarial perturbations, revealing conditions under which influence estimates are inherently fragile. Our findings highlight critical vulnerabilities in influence-based attribution and suggest the need for robust defenses. All code can be found at https://anonymous.4open.science/r/MemAttack-5413/

摘要: 影响力估计工具（例如记忆分数）被广泛用于理解模型行为、属性训练数据并为数据集策展提供信息。然而，最近在数据评估和负责任的机器学习方面的应用提出了一个问题：这些分数本身是否会被敌对操纵？在这项工作中，我们对攻击基于记忆的影响估计器的可行性进行了系统研究。我们将用于产生高度记忆样本的攻击描述为在训练算法准确的情况下的高度敏感查询。我们的攻击（计算输入的伪逆）是实用的，只需要黑匣子访问模型输出，并且会产生适度的计算负担。我们通过经验验证了一系列图像分类任务中的攻击，表明即使是最先进的代理也容易受到有针对性的分数操纵的影响。此外，我们还对对抗性扰动下记忆分数的稳定性进行了理论分析，揭示了影响估计本质上脆弱的条件。我们的研究结果强调了基于影响力的归因中的关键漏洞，并表明需要强有力的防御。所有代码均可在https://anonymous.4open.science/r/MemAttack-5413/上找到



## **26. Differential Privacy of Network Parameters from a System Identification Perspective**

从系统识别的角度研究网络参数的差异隐私 cs.CR

**SubmitDate**: 2025-09-24    [abs](http://arxiv.org/abs/2509.20460v1) [paper-pdf](http://arxiv.org/pdf/2509.20460v1)

**Authors**: Andrew Campbell, Anna Scaglione, Hang Liu, Victor Elvira, Sean Peisert, Daniel Arnold

**Abstract**: This paper addresses the problem of protecting network information from privacy system identification (SI) attacks when sharing cyber-physical system simulations. We model analyst observations of networked states as time-series outputs of a graph filter driven by differentially private (DP) nodal excitations, with the analyst aiming to infer the underlying graph shift operator (GSO). Unlike traditional SI, which estimates system parameters, we study the inverse problem: what assumptions prevent adversaries from identifying the GSO while preserving utility for legitimate analysis. We show that applying DP mechanisms to inputs provides formal privacy guarantees for the GSO, linking the $(\epsilon,\delta)$-DP bound to the spectral properties of the graph filter and noise covariance. More precisely, for DP Gaussian signals, the spectral characteristics of both the filter and noise covariance determine the privacy bound, with smooth filters and low-condition-number covariance yielding greater privacy.

摘要: 本文解决了共享网络物理系统模拟时保护网络信息免受隐私系统识别（SI）攻击的问题。我们将分析师对网络状态的观察建模为由差异专用（DP）节点激励驱动的图过滤器的时间序列输出，分析师的目标是推断底层的图移动操作符（GSO）。与估计系统参数的传统SI不同，我们研究逆问题：哪些假设阻止对手识别GSO，同时保留合法分析的效用。我们表明，将DP机制应用于输入为GSO提供了正式的隐私保证，将$（\，\delta）$-DP界限与图过滤器和噪音协方差的谱属性联系起来。更准确地说，对于DP高斯信号，过滤器和噪音协方差的频谱特征决定隐私界限，平滑过滤器和低条件数协方差产生更大的隐私。



## **27. FlyTrap: Physical Distance-Pulling Attack Towards Camera-based Autonomous Target Tracking Systems**

FlyTrap：对基于摄像机的自主目标跟踪系统的物理拉距攻击 cs.CR

An extended version of the paper accepted by NDSS 2026

**SubmitDate**: 2025-09-24    [abs](http://arxiv.org/abs/2509.20362v1) [paper-pdf](http://arxiv.org/pdf/2509.20362v1)

**Authors**: Shaoyuan Xie, Mohamad Habib Fakih, Junchi Lu, Fayzah Alshammari, Ningfei Wang, Takami Sato, Halima Bouzidi, Mohammad Abdullah Al Faruque, Qi Alfred Chen

**Abstract**: Autonomous Target Tracking (ATT) systems, especially ATT drones, are widely used in applications such as surveillance, border control, and law enforcement, while also being misused in stalking and destructive actions. Thus, the security of ATT is highly critical for real-world applications. Under the scope, we present a new type of attack: distance-pulling attacks (DPA) and a systematic study of it, which exploits vulnerabilities in ATT systems to dangerously reduce tracking distances, leading to drone capturing, increased susceptibility to sensor attacks, or even physical collisions. To achieve these goals, we present FlyTrap, a novel physical-world attack framework that employs an adversarial umbrella as a deployable and domain-specific attack vector. FlyTrap is specifically designed to meet key desired objectives in attacking ATT drones: physical deployability, closed-loop effectiveness, and spatial-temporal consistency. Through novel progressive distance-pulling strategy and controllable spatial-temporal consistency designs, FlyTrap manipulates ATT drones in real-world setups to achieve significant system-level impacts. Our evaluations include new datasets, metrics, and closed-loop experiments on real-world white-box and even commercial ATT drones, including DJI and HoverAir. Results demonstrate FlyTrap's ability to reduce tracking distances within the range to be captured, sensor attacked, or even directly crashed, highlighting urgent security risks and practical implications for the safe deployment of ATT systems.

摘要: 自主目标跟踪（ATF）系统，尤其是ATF无人机，广泛用于监视、边境管制和执法等应用，同时也被滥用于跟踪和破坏行动。因此，ATT的安全性对于现实世界的应用程序非常关键。在该范围下，我们提出了一种新型攻击：拉距离攻击（DPA）及其系统性研究，它利用ATT系统中的漏洞危险地缩短跟踪距离，导致无人机捕获、增加对传感器攻击的敏感性，甚至物理碰撞。为了实现这些目标，我们提出了FlyTrap，这是一种新型的物理世界攻击框架，它采用对抗伞作为可部署的特定领域攻击载体。FlyTrap专门设计用于满足攻击ATT无人机的关键预期目标：物理可部署性、闭环有效性和时空一致性。通过新颖的渐进式距离拉动策略和可控的时空一致性设计，FlyTrap在现实世界的设置中操纵ATT无人机，以实现显着的系统级影响。我们的评估包括新的数据集、指标和对现实世界白盒甚至商用ATT无人机（包括DJI和HoverAir）的闭环实验。结果表明，FlyTrap能够在被捕获、传感器攻击甚至直接坠毁的范围内缩短跟踪距离，凸显了紧迫的安全风险和对安全部署ATT系统的实际影响。



## **28. RAG Security and Privacy: Formalizing the Threat Model and Attack Surface**

RAG安全和隐私：正式化威胁模型和攻击面 cs.CR

Accepted at the 5th ICDM Workshop on September 20, 2025

**SubmitDate**: 2025-09-24    [abs](http://arxiv.org/abs/2509.20324v1) [paper-pdf](http://arxiv.org/pdf/2509.20324v1)

**Authors**: Atousa Arzanipour, Rouzbeh Behnia, Reza Ebrahimi, Kaushik Dutta

**Abstract**: Retrieval-Augmented Generation (RAG) is an emerging approach in natural language processing that combines large language models (LLMs) with external document retrieval to produce more accurate and grounded responses. While RAG has shown strong potential in reducing hallucinations and improving factual consistency, it also introduces new privacy and security challenges that differ from those faced by traditional LLMs. Existing research has demonstrated that LLMs can leak sensitive information through training data memorization or adversarial prompts, and RAG systems inherit many of these vulnerabilities. At the same time, reliance of RAG on an external knowledge base opens new attack surfaces, including the potential for leaking information about the presence or content of retrieved documents, or for injecting malicious content to manipulate model behavior. Despite these risks, there is currently no formal framework that defines the threat landscape for RAG systems. In this paper, we address a critical gap in the literature by proposing, to the best of our knowledge, the first formal threat model for retrieval-RAG systems. We introduce a structured taxonomy of adversary types based on their access to model components and data, and we formally define key threat vectors such as document-level membership inference and data poisoning, which pose serious privacy and integrity risks in real-world deployments. By establishing formal definitions and attack models, our work lays the foundation for a more rigorous and principled understanding of privacy and security in RAG systems.

摘要: 检索增强生成（RAG）是自然语言处理中的一种新兴方法，它将大型语言模型（LLM）与外部文档检索相结合，以产生更准确和更接地的响应。虽然RAG在减少幻觉和提高事实一致性方面表现出强大的潜力，但它也带来了与传统LLM所面临的不同的新的隐私和安全挑战。现有的研究表明，LLM可以通过训练数据记忆或对抗性提示来泄露敏感信息，而RAG系统继承了许多这些漏洞。与此同时，RAG对外部知识库的依赖打开了新的攻击面，包括泄露有关检索文档的存在或内容的信息的可能性，或者注入恶意内容来操纵模型行为的可能性。尽管存在这些风险，但目前还没有正式框架来定义RAG系统的威胁格局。在本文中，据我们所知，我们通过提出第一个用于检索RAG系统的正式威胁模型来解决文献中的一个关键空白。我们根据对手对模型组件和数据的访问权限引入了对手类型的结构化分类，并正式定义了关键威胁载体，例如文档级成员资格推断和数据中毒，这在现实世界的部署中构成了严重的隐私和完整性风险。通过建立正式的定义和攻击模型，我们的工作为更严格、更有原则地理解RAG系统中的隐私和安全性奠定了基础。



## **29. Enhancing Targeted Adversarial Attacks on Large Vision-Language Models via Intermediate Projector**

通过中间投影仪增强对大型视觉语言模型的有针对性的对抗攻击 cs.CV

**SubmitDate**: 2025-09-24    [abs](http://arxiv.org/abs/2508.13739v2) [paper-pdf](http://arxiv.org/pdf/2508.13739v2)

**Authors**: Yiming Cao, Yanjie Li, Kaisheng Liang, Bin Xiao

**Abstract**: The growing deployment of Large Vision-Language Models (VLMs) raises safety concerns, as adversaries may exploit model vulnerabilities to induce harmful outputs, with targeted black-box adversarial attacks posing a particularly severe threat. However, existing methods primarily maximize encoder-level global similarity, which lacks the granularity for stealthy and practical fine-grained attacks, where only specific target should be altered (e.g., modifying a car while preserving its background). Moreover, they largely neglect the projector, a key semantic bridge in VLMs for multimodal alignment. To address these limitations, we propose a novel black-box targeted attack framework that leverages the projector. Specifically, we utilize the widely adopted Querying Transformer (Q-Former) which transforms global image embeddings into fine-grained query outputs, to enhance attack effectiveness and granularity. For standard global targeted attack scenarios, we propose the Intermediate Projector Guided Attack (IPGA), which aligns Q-Former fine-grained query outputs with the target to enhance attack strength and exploits the intermediate pretrained Q-Former that is not fine-tuned for any specific Large Language Model (LLM) to improve attack transferability. For fine-grained attack scenarios, we augment IPGA with the Residual Query Alignment (RQA) module, which preserves unrelated content by constraining non-target query outputs to enhance attack granularity. Extensive experiments demonstrate that IPGA significantly outperforms baselines in global targeted attacks, and IPGA with RQA (IPGA-R) attains superior success rates and unrelated content preservation over baselines in fine-grained attacks. Our method also transfers effectively to commercial VLMs such as Google Gemini and OpenAI GPT.

摘要: 大型视觉语言模型（VLM）的不断增加的部署引发了安全问题，因为对手可能会利用模型漏洞来引发有害输出，其中有针对性的黑匣子对抗攻击构成了特别严重的威胁。然而，现有方法主要最大化编码器级的全局相似性，这缺乏隐蔽和实际的细粒度攻击的粒度，其中只应该改变特定的目标（例如，修改汽车，同时保留其背景）。此外，他们在很大程度上忽视了投影仪，这是VLM中用于多模式对齐的关键语义桥梁。为了解决这些限制，我们提出了一种利用投影仪的新型黑匣子定向攻击框架。具体来说，我们利用广泛采用的查询Transformer（Q-Former）将全局图像嵌入转换为细粒度查询输出，以增强攻击有效性和粒度。对于标准的全球目标攻击场景，我们提出了中间投影仪引导攻击（IPGA），它将Q-Former细粒度查询输出与目标对齐以增强攻击强度，并利用未针对任何特定大型语言模型（LLM）进行微调的中间预训练Q-Former来提高攻击的可转移性。对于细粒度的攻击场景，我们使用剩余查询对齐（RQA）模块来增强IPGA，该模块通过约束非目标查询输出来保留不相关的内容以增强攻击粒度。大量实验表明，IPGA在全球定向攻击中的表现显着优于基线，并且具有RQA的IPGA（IPGA-R）在细粒度攻击中获得了优于基线的更高成功率和不相关内容保留。我们的方法还可以有效地转移到Google Gemini和OpenAI GPT等商业VLM。



## **30. Beyond Sharp Minima: Robust LLM Unlearning via Feedback-Guided Multi-Point Optimization**

超越Sharp Minima：通过反馈引导多点优化实现稳健的LLM反学习 cs.LG

**SubmitDate**: 2025-09-24    [abs](http://arxiv.org/abs/2509.20230v1) [paper-pdf](http://arxiv.org/pdf/2509.20230v1)

**Authors**: Wenhan Wu, Zheyuan Liu, Chongyang Gao, Ren Wang, Kaize Ding

**Abstract**: Current LLM unlearning methods face a critical security vulnerability that undermines their fundamental purpose: while they appear to successfully remove sensitive or harmful knowledge, this ``forgotten" information remains precariously recoverable through relearning attacks. We identify that the root cause is that conventional methods optimizing the forgetting loss at individual data points will drive model parameters toward sharp minima in the loss landscape. In these unstable regions, even minimal parameter perturbations can drastically alter the model's behaviors. Consequently, relearning attacks exploit this vulnerability by using just a few fine-tuning samples to navigate the steep gradients surrounding these unstable regions, thereby rapidly recovering knowledge that was supposedly erased. This exposes a critical robustness gap between apparent unlearning and actual knowledge removal. To address this issue, we propose StableUN, a bi-level feedback-guided optimization framework that explicitly seeks more stable parameter regions via neighborhood-aware optimization. It integrates forgetting feedback, which uses adversarial perturbations to probe parameter neighborhoods, with remembering feedback to preserve model utility, aligning the two objectives through gradient projection. Experiments on WMDP and MUSE benchmarks demonstrate that our method is significantly more robust against both relearning and jailbreaking attacks while maintaining competitive utility performance.

摘要: 当前的LLM取消学习方法面临着一个严重的安全漏洞，该漏洞破坏了其基本目的：虽然它们似乎成功删除了敏感或有害知识，但这些“被遗忘”的信息仍然无法通过重新学习攻击恢复。我们发现，根本原因是优化单个数据点的遗忘损失的传统方法将推动模型参数在损失环境中趋于尖锐的最小值。在这些不稳定区域中，即使是最小的参数扰动也会极大地改变模型的行为。因此，重新学习攻击通过仅使用一些微调样本来导航这些不稳定区域周围的陡峭梯度来利用这一漏洞，从而快速恢复据称被删除的知识。这暴露了明显的取消学习和实际知识删除之间的关键鲁棒性差距。为了解决这个问题，我们提出了StableUN，这是一个两级反馈引导的优化框架，通过邻居感知优化明确地寻求更稳定的参数区域。它集成了遗忘反馈（使用对抗性扰动来探测参数邻居）与记忆反馈以保留模型效用，通过梯度投影对齐两个目标。在WMDP和MUSE基准测试上的实验表明，我们的方法在保持有竞争力的效用性能的同时，对重新学习和越狱攻击具有更强的鲁棒性。



## **31. An Empirical Analysis of Secure Federated Learning for Autonomous Vehicle Applications**

自动驾驶汽车应用的安全联邦学习的实证分析 cs.DC

i3CE 2024, 2024 ASCE International Conference on Computing in Civil  Engineering

**SubmitDate**: 2025-09-24    [abs](http://arxiv.org/abs/2509.20223v1) [paper-pdf](http://arxiv.org/pdf/2509.20223v1)

**Authors**: Md Jueal Mia, M. Hadi Amini

**Abstract**: Federated Learning lends itself as a promising paradigm in enabling distributed learning for autonomous vehicles applications and ensuring data privacy while enhancing and refining predictive model performance through collaborative training on edge client vehicles. However, it remains vulnerable to various categories of cyber-attacks, necessitating more robust security measures to effectively mitigate potential threats. Poisoning attacks and inference attacks are commonly initiated within the federated learning environment to compromise secure system performance. Secure aggregation can limit the disclosure of sensitive information from outsider and insider attackers of the federated learning environment. In this study, our aim is to conduct an empirical analysis on the transportation image dataset (e.g., LISA traffic light) using various secure aggregation techniques and multiparty computation in the presence of diverse categories of cyber-attacks. Multiparty computation serves as a state-of-the-art security mechanism, offering standard privacy for secure aggregation of edge autonomous vehicles local model updates through various security protocols. The presence of adversaries can mislead the autonomous vehicle learning model, leading to the misclassification of traffic lights, and resulting in detrimental impacts. This empirical study explores the resilience of various secure federated learning aggregation techniques and multiparty computation in safeguarding autonomous vehicle applications against various cyber threats during both training and inference times.

摘要: 联邦学习是一种有前途的范式，可以为自动驾驶汽车应用程序提供分布式学习，并确保数据隐私，同时通过边缘客户端车辆上的协作培训来增强和完善预测模型性能。然而，它仍然容易受到各种类型的网络攻击，需要采取更强有力的安全措施来有效减轻潜在威胁。中毒攻击和推理攻击通常在联邦学习环境中发起，以损害安全系统性能。安全聚合可以限制联邦学习环境的外部和内部攻击者泄露敏感信息。在本研究中，我们的目标是对交通图像数据集进行实证分析（例如，LISA交通灯）在存在各种类型的网络攻击的情况下使用各种安全聚合技术和多方计算。多方计算充当最先进的安全机制，为通过各种安全协议安全聚合边缘自动驾驶车辆本地模型更新提供标准隐私。对手的存在可能会误导自动驾驶车辆学习模型，导致交通灯的错误分类，并造成不利影响。这项实证研究探讨了各种安全联邦学习聚合技术和多方计算在训练和推理期间保护自动驾驶汽车应用免受各种网络威胁方面的弹性。



## **32. Universal Camouflage Attack on Vision-Language Models for Autonomous Driving**

自动驾驶视觉语言模型的通用伪装攻击 cs.CV

**SubmitDate**: 2025-09-24    [abs](http://arxiv.org/abs/2509.20196v1) [paper-pdf](http://arxiv.org/pdf/2509.20196v1)

**Authors**: Dehong Kong, Sifan Yu, Siyuan Liang, Jiawei Liang, Jianhou Gan, Aishan Liu, Wenqi Ren

**Abstract**: Visual language modeling for automated driving is emerging as a promising research direction with substantial improvements in multimodal reasoning capabilities. Despite its advanced reasoning abilities, VLM-AD remains vulnerable to serious security threats from adversarial attacks, which involve misleading model decisions through carefully crafted perturbations. Existing attacks have obvious challenges: 1) Physical adversarial attacks primarily target vision modules. They are difficult to directly transfer to VLM-AD systems because they typically attack low-level perceptual components. 2) Adversarial attacks against VLM-AD have largely concentrated on the digital level. To address these challenges, we propose the first Universal Camouflage Attack (UCA) framework for VLM-AD. Unlike previous methods that focus on optimizing the logit layer, UCA operates in the feature space to generate physically realizable camouflage textures that exhibit strong generalization across different user commands and model architectures. Motivated by the observed vulnerability of encoder and projection layers in VLM-AD, UCA introduces a feature divergence loss (FDL) that maximizes the representational discrepancy between clean and adversarial images. In addition, UCA incorporates a multi-scale learning strategy and adjusts the sampling ratio to enhance its adaptability to changes in scale and viewpoint diversity in real-world scenarios, thereby improving training stability. Extensive experiments demonstrate that UCA can induce incorrect driving commands across various VLM-AD models and driving scenarios, significantly surpassing existing state-of-the-art attack methods (improving 30\% in 3-P metrics). Furthermore, UCA exhibits strong attack robustness under diverse viewpoints and dynamic conditions, indicating high potential for practical deployment.

摘要: 自动驾驶视觉语言建模正在成为一个有前途的研究方向，多模式推理能力得到了大幅提高。尽管VLM-AD具有先进的推理能力，但仍然容易受到对抗攻击的严重安全威胁，这些攻击涉及通过精心设计的扰动误导模型决策。现有的攻击存在明显的挑战：1）物理对抗攻击主要针对视觉模块。它们很难直接传输到VLM-AD系统，因为它们通常攻击低级感知分量。2)针对VLM-AD的对抗攻击主要集中在数字层面。为了应对这些挑战，我们提出了第一个针对VLM-AD的通用伪装攻击（UCA）框架。与之前专注于优化logit层的方法不同，UCA在特征空间中操作以生成物理上可实现的伪装纹理，这些纹理在不同的用户命令和模型架构中表现出很强的概括性。受所观察到的VLM-AD中编码器和投影层脆弱性的激励，UCA引入了特征分歧损失（FDL），可以最大化干净图像和对抗图像之间的代表差异。此外，UCA融合了多尺度学习策略，并调整抽样率，增强其对现实场景中规模和视角多样性变化的适应性，从而提高训练稳定性。大量实验表明，UCA可以在各种VLM-AD模型和驾驶场景中引发错误的驾驶命令，显着超越现有的最先进的攻击方法（3-P指标提高了30%）。此外，UCA在不同观点和动态条件下表现出强大的攻击鲁棒性，表明实际部署的巨大潜力。



## **33. Generative Model Inversion Through the Lens of the Manifold Hypothesis**

流形假设下的生成模型反演 cs.LG

NeurIPS 2025

**SubmitDate**: 2025-09-24    [abs](http://arxiv.org/abs/2509.20177v1) [paper-pdf](http://arxiv.org/pdf/2509.20177v1)

**Authors**: Xiong Peng, Bo Han, Fengfei Yu, Tongliang Liu, Feng Liu, Mingyuan Zhou

**Abstract**: Model inversion attacks (MIAs) aim to reconstruct class-representative samples from trained models. Recent generative MIAs utilize generative adversarial networks to learn image priors that guide the inversion process, yielding reconstructions with high visual quality and strong fidelity to the private training data. To explore the reason behind their effectiveness, we begin by examining the gradients of inversion loss with respect to synthetic inputs, and find that these gradients are surprisingly noisy. Further analysis reveals that generative inversion implicitly denoises these gradients by projecting them onto the tangent space of the generator manifold, filtering out off-manifold components while preserving informative directions aligned with the manifold. Our empirical measurements show that, in models trained with standard supervision, loss gradients often exhibit large angular deviations from the data manifold, indicating poor alignment with class-relevant directions. This observation motivates our central hypothesis: models become more vulnerable to MIAs when their loss gradients align more closely with the generator manifold. We validate this hypothesis by designing a novel training objective that explicitly promotes such alignment. Building on this insight, we further introduce a training-free approach to enhance gradient-manifold alignment during inversion, leading to consistent improvements over state-of-the-art generative MIAs.

摘要: 模型反演攻击（MIA）旨在从训练的模型中重建具有类别代表性的样本。最近的生成MIA利用生成对抗网络来学习引导反演过程的图像先验，从而产生具有高视觉质量和对私有训练数据的强保真度的重建。为了探索其有效性背后的原因，我们首先检查反转损失相对于合成输入的梯度，并发现这些梯度是令人惊讶的噪声。进一步的分析表明，生成反演隐式地通过将这些梯度投影到生成流形的切空间上来消除这些梯度的噪声，过滤掉流形外的分量，同时保留与流形对齐的信息方向。我们的经验测量表明，在经过标准监督训练的模型中，损失梯度通常与数据集表现出很大的角度偏差，这表明与类相关方向的一致性较差。这一观察结果激发了我们的中心假设：当模型的损失梯度与发生器管汇更紧密地对齐时，模型变得更容易受到MIA的影响。我们通过设计一个明确促进这种对齐的新颖训练目标来验证这一假设。在这一见解的基础上，我们进一步引入了一种免训练的方法来增强倒置期间的梯度-管汇对齐，从而比最先进的生成式MIA实现一致的改进。



## **34. Adversarial Robustness of Discriminative Self-Supervised Learning in Vision**

视觉中辨别性自我监督学习的对抗鲁棒性 cs.CV

ICCV 2025

**SubmitDate**: 2025-09-24    [abs](http://arxiv.org/abs/2503.06361v2) [paper-pdf](http://arxiv.org/pdf/2503.06361v2)

**Authors**: Ömer Veysel Çağatan, Ömer Faruk Tal, M. Emre Gürsoy

**Abstract**: Self-supervised learning (SSL) has advanced significantly in visual representation learning, yet comprehensive evaluations of its adversarial robustness remain limited. In this study, we evaluate the adversarial robustness of seven discriminative self-supervised models and one supervised model across diverse tasks, including ImageNet classification, transfer learning, segmentation, and detection. Our findings suggest that discriminative SSL models generally exhibit better robustness to adversarial attacks compared to their supervised counterpart on ImageNet, with this advantage extending to transfer learning when using linear evaluation. However, when fine-tuning is applied, the robustness gap between SSL and supervised models narrows considerably. Similarly, this robustness advantage diminishes in segmentation and detection tasks. We also investigate how various factors might influence adversarial robustness, including architectural choices, training duration, data augmentations, and batch sizes. Our analysis contributes to the ongoing exploration of adversarial robustness in visual self-supervised representation systems.

摘要: 自我监督学习（SSL）在视觉表示学习方面取得了显着进步，但对其对抗稳健性的全面评估仍然有限。在这项研究中，我们评估了七种区分性自我监督模型和一种监督模型在不同任务（包括ImageNet分类、迁移学习、分割和检测）中的对抗稳健性。我们的研究结果表明，与ImageNet上的监督模型相比，区分性SSL模型通常对对抗攻击表现出更好的鲁棒性，这一优势在使用线性评估时扩展到了迁移学习。然而，当应用微调时，SSL和监督模型之间的鲁棒性差距会大大缩小。同样，这种稳健性优势在分割和检测任务中会减弱。我们还研究各种因素如何影响对抗稳健性，包括架构选择、训练持续时间、数据增强和批量大小。我们的分析有助于对视觉自我监督表示系统中对抗鲁棒性的持续探索。



## **35. Generative Adversarial Networks Applied for Privacy Preservation in Biometric-Based Authentication and Identification**

生成对抗网络在基于生物识别的认证和识别中应用于隐私保护 cs.CV

**SubmitDate**: 2025-09-24    [abs](http://arxiv.org/abs/2509.20024v1) [paper-pdf](http://arxiv.org/pdf/2509.20024v1)

**Authors**: Lubos Mjachky, Ivan Homoliak

**Abstract**: Biometric-based authentication systems are getting broadly adopted in many areas. However, these systems do not allow participating users to influence the way their data is used. Furthermore, the data may leak and can be misused without the users' knowledge. In this paper, we propose a new authentication method that preserves the privacy of individuals and is based on a generative adversarial network (GAN). Concretely, we suggest using the GAN for translating images of faces to a visually private domain (e.g., flowers or shoes). Classifiers, which are used for authentication purposes, are then trained on the images from the visually private domain. Based on our experiments, the method is robust against attacks and still provides meaningful utility.

摘要: 基于生物识别的认证系统正在许多领域得到广泛采用。然而，这些系统不允许参与用户影响其数据的使用方式。此外，数据可能会泄露并在用户不知情的情况下被滥用。在本文中，我们提出了一种基于生成对抗网络（GAN）的新的认证方法，该方法保留了个人的隐私。具体来说，我们建议使用GAN将面部图像翻译到视觉上的私人域（例如，花或鞋子）。然后，用于验证目的的分类器会在视觉上私有域的图像上进行训练。根据我们的实验，该方法对攻击具有鲁棒性，并且仍然提供有意义的实用性。



## **36. Improving Generalizability and Undetectability for Targeted Adversarial Attacks on Multimodal Pre-trained Models**

提高多模式预训练模型上有针对性对抗攻击的概括性和不可检测性 cs.CV

**SubmitDate**: 2025-09-24    [abs](http://arxiv.org/abs/2509.19994v1) [paper-pdf](http://arxiv.org/pdf/2509.19994v1)

**Authors**: Zhifang Zhang, Jiahan Zhang, Shengjie Zhou, Qi Wei, Shuo He, Feng Liu, Lei Feng

**Abstract**: Multimodal pre-trained models (e.g., ImageBind), which align distinct data modalities into a shared embedding space, have shown remarkable success across downstream tasks. However, their increasing adoption raises serious security concerns, especially regarding targeted adversarial attacks. In this paper, we show that existing targeted adversarial attacks on multimodal pre-trained models still have limitations in two aspects: generalizability and undetectability. Specifically, the crafted targeted adversarial examples (AEs) exhibit limited generalization to partially known or semantically similar targets in cross-modal alignment tasks (i.e., limited generalizability) and can be easily detected by simple anomaly detection methods (i.e., limited undetectability). To address these limitations, we propose a novel method called Proxy Targeted Attack (PTA), which leverages multiple source-modal and target-modal proxies to optimize targeted AEs, ensuring they remain evasive to defenses while aligning with multiple potential targets. We also provide theoretical analyses to highlight the relationship between generalizability and undetectability and to ensure optimal generalizability while meeting the specified requirements for undetectability. Furthermore, experimental results demonstrate that our PTA can achieve a high success rate across various related targets and remain undetectable against multiple anomaly detection methods.

摘要: 多模式预训练模型（例如，Image Bind）将不同的数据模式整合到共享嵌入空间中，在下游任务中取得了显着的成功。然而，它们的越来越多的采用引发了严重的安全问题，特别是在有针对性的对抗攻击方面。在本文中，我们表明现有的针对多模式预训练模型的有针对性的对抗攻击在两个方面仍然存在局限性：可概括性和不可检测性。具体来说，精心设计的有针对性的对抗示例（AE）对跨模式对齐任务中部分已知或语义相似的目标表现出有限的概括（即，有限的概括性）并且可以通过简单的异常检测方法容易地检测到（即，有限的不可检测性）。为了解决这些限制，我们提出了一种名为代理目标攻击（PTA）的新型方法，该方法利用多个源模式和目标模式代理来优化目标AE，确保它们在与多个潜在目标保持一致的同时保持规避防御。我们还提供了理论分析，以突出概化和不可检测性之间的关系，并确保最佳的概化，同时满足规定的不可检测性的要求。此外，实验结果表明，我们的PTA可以在各种相关的目标，并保持对多种异常检测方法检测不到的高成功率。



## **37. Evading Toxicity Detection with ASCII-art: A Benchmark of Spatial Attacks on Moderation Systems**

用ASCII-art逃避毒性检测：对调节系统的空间攻击基准 cs.CL

**SubmitDate**: 2025-09-24    [abs](http://arxiv.org/abs/2409.18708v5) [paper-pdf](http://arxiv.org/pdf/2409.18708v5)

**Authors**: Sergey Berezin, Reza Farahbakhsh, Noel Crespi

**Abstract**: We introduce a novel class of adversarial attacks on toxicity detection models that exploit language models' failure to interpret spatially structured text in the form of ASCII art. To evaluate the effectiveness of these attacks, we propose ToxASCII, a benchmark designed to assess the robustness of toxicity detection systems against visually obfuscated inputs. Our attacks achieve a perfect Attack Success Rate (ASR) across a diverse set of state-of-the-art large language models and dedicated moderation tools, revealing a significant vulnerability in current text-only moderation systems.

摘要: 我们引入了一类新型的针对毒性检测模型的对抗性攻击，这些攻击利用语言模型无法以ASC艺术的形式解释空间结构化文本的能力。为了评估这些攻击的有效性，我们提出了ToxASC，这是一个旨在评估毒性检测系统对视觉模糊输入的鲁棒性的基准。我们的攻击在一系列最先进的大型语言模型和专用审核工具上实现了完美的攻击成功率（ASB），揭示了当前纯文本审核系统中的一个重大漏洞。



## **38. FreezeVLA: Action-Freezing Attacks against Vision-Language-Action Models**

FreezeVLA：针对视觉-语言-动作模型的预设冻结攻击 cs.CV

**SubmitDate**: 2025-09-24    [abs](http://arxiv.org/abs/2509.19870v1) [paper-pdf](http://arxiv.org/pdf/2509.19870v1)

**Authors**: Xin Wang, Jie Li, Zejia Weng, Yixu Wang, Yifeng Gao, Tianyu Pang, Chao Du, Yan Teng, Yingchun Wang, Zuxuan Wu, Xingjun Ma, Yu-Gang Jiang

**Abstract**: Vision-Language-Action (VLA) models are driving rapid progress in robotics by enabling agents to interpret multimodal inputs and execute complex, long-horizon tasks. However, their safety and robustness against adversarial attacks remain largely underexplored. In this work, we identify and formalize a critical adversarial vulnerability in which adversarial images can "freeze" VLA models and cause them to ignore subsequent instructions. This threat effectively disconnects the robot's digital mind from its physical actions, potentially inducing inaction during critical interventions. To systematically study this vulnerability, we propose FreezeVLA, a novel attack framework that generates and evaluates action-freezing attacks via min-max bi-level optimization. Experiments on three state-of-the-art VLA models and four robotic benchmarks show that FreezeVLA attains an average attack success rate of 76.2%, significantly outperforming existing methods. Moreover, adversarial images generated by FreezeVLA exhibit strong transferability, with a single image reliably inducing paralysis across diverse language prompts. Our findings expose a critical safety risk in VLA models and highlight the urgent need for robust defense mechanisms.

摘要: 视觉-语言-动作（VLA）模型使代理能够解释多模式输入并执行复杂的长期任务，正在推动机器人技术的快速进步。然而，它们的安全性和对抗性攻击的稳健性在很大程度上仍然没有得到充分的探索。在这项工作中，我们识别并正式化了一个关键的对抗漏洞，其中对抗图像可以“冻结”VLA模型并导致它们忽略后续指令。这种威胁有效地将机器人的数字思维与其身体行为断开，可能会导致在关键干预期间不采取行动。为了系统性地研究该漏洞，我们提出了FreezeVLA，这是一种新型攻击框架，通过最小-最大双层优化生成和评估动作冻结攻击。对三种最先进的VLA模型和四种机器人基准测试的实验表明，FreezeVLA的平均攻击成功率为76.2%，显着优于现有方法。此外，FreezeVLA生成的对抗图像具有很强的可移植性，单个图像可靠地在不同语言提示中引起瘫痪。我们的研究结果揭示了VLA模型中的关键安全风险，并强调了对强大防御机制的迫切需要。



## **39. Benchmarking Gaslighting Attacks Against Speech Large Language Models**

针对语音大型语言模型的Gaslighting攻击进行基准测试 cs.CL

5 pages, 2 figures, 3 tables

**SubmitDate**: 2025-09-24    [abs](http://arxiv.org/abs/2509.19858v1) [paper-pdf](http://arxiv.org/pdf/2509.19858v1)

**Authors**: Jinyang Wu, Bin Zhu, Xiandong Zou, Qiquan Zhang, Xu Fang, Pan Zhou

**Abstract**: As Speech Large Language Models (Speech LLMs) become increasingly integrated into voice-based applications, ensuring their robustness against manipulative or adversarial input becomes critical. Although prior work has studied adversarial attacks in text-based LLMs and vision-language models, the unique cognitive and perceptual challenges of speech-based interaction remain underexplored. In contrast, speech presents inherent ambiguity, continuity, and perceptual diversity, which make adversarial attacks more difficult to detect. In this paper, we introduce gaslighting attacks, strategically crafted prompts designed to mislead, override, or distort model reasoning as a means to evaluate the vulnerability of Speech LLMs. Specifically, we construct five manipulation strategies: Anger, Cognitive Disruption, Sarcasm, Implicit, and Professional Negation, designed to test model robustness across varied tasks. It is worth noting that our framework captures both performance degradation and behavioral responses, including unsolicited apologies and refusals, to diagnose different dimensions of susceptibility. Moreover, acoustic perturbation experiments are conducted to assess multi-modal robustness. To quantify model vulnerability, comprehensive evaluation across 5 Speech and multi-modal LLMs on over 10,000 test samples from 5 diverse datasets reveals an average accuracy drop of 24.3% under the five gaslighting attacks, indicating significant behavioral vulnerability. These findings highlight the need for more resilient and trustworthy speech-based AI systems.

摘要: 随着语音大型语言模型（Speech LLM）越来越多地集成到基于语音的应用程序中，确保其对操纵或对抗输入的鲁棒性变得至关重要。尽管之前的工作已经研究了基于文本的LLM和视觉语言模型中的对抗攻击，但基于语音的交互的独特认知和感知挑战仍然没有得到充分的研究。相比之下，语音呈现出固有的模糊性、连续性和感知多样性，这使得对抗性攻击更难以检测。在本文中，我们引入了煤气灯攻击，这是一种策略性精心设计的提示，旨在误导、覆盖或扭曲模型推理，作为评估语音LLM脆弱性的一种手段。具体来说，我们构建了五种操纵策略：愤怒，认知中断，讽刺，隐含和专业否定，旨在测试模型在不同任务中的鲁棒性。值得注意的是，我们的框架捕捉性能下降和行为反应，包括主动道歉和拒绝，诊断不同方面的易感性。此外，声扰动实验进行评估的多模态鲁棒性。为了量化模型的脆弱性，对来自5个不同数据集的10，000多个测试样本进行的5个语音和多模态LLM的综合评估显示，在5次煤气灯攻击下，平均准确率下降了24.3%，这表明存在显著的行为脆弱性。这些发现凸显了对更有弹性、更值得信赖的基于语音的人工智能系统的需求。



## **40. LatentGuard: Controllable Latent Steering for Robust Refusal of Attacks and Reliable Response Generation**

LatentGuard：可控的潜在引导，用于稳健拒绝攻击和生成可靠的响应 cs.AI

9-page NeurIPS 2025 preprint including 3 figures and 1 table, with  additional appendix material. Prepared using the NeurIPS 2025 preprint  template and compiled with pdfLaTeX. All references are included via the  provided .bbl file. Figures are in PDF format. No external supplementary  files. All necessary style files and images are included

**SubmitDate**: 2025-09-24    [abs](http://arxiv.org/abs/2509.19839v1) [paper-pdf](http://arxiv.org/pdf/2509.19839v1)

**Authors**: Huizhen Shu, Xuying Li, Zhuo Li

**Abstract**: Achieving robust safety alignment in large language models (LLMs) while preserving their utility remains a fundamental challenge. Existing approaches often struggle to balance comprehensive safety with fine-grained controllability at the representation level. We introduce LATENTGUARD, a novel three-stage framework that combines behavioral alignment with supervised latent space control for interpretable and precise safety steering. Our approach begins by fine-tuning an LLM on rationalized datasets containing both reasoning-enhanced refusal responses to adversarial prompts and reasoning-enhanced normal responses to benign queries, establishing robust behavioral priors across both safety-critical and utility-preserving scenarios. We then train a structured variational autoencoder (VAE) on intermediate MLP activations, supervised by multi-label annotations including attack types, attack methods, and benign indicators. This supervision enables the VAE to learn disentangled latent representations that capture distinct adversarial characteristics while maintaining semantic interpretability. Through targeted manipulation of learned latent dimensions, LATENTGUARD achieves selective refusal behavior, effectively blocking harmful requests while preserving helpfulness for legitimate use cases. Experiments on Qwen3-8B demonstrate significant improvements in both safety controllability and response interpretability without compromising utility. Cross-architecture validation on Mistral-7B confirms the generalizability of our latent steering approach, showing consistent effectiveness across different model families. Our results suggest that structured representation-level intervention offers a promising pathway toward building safer yet practical LLM systems.

摘要: 在大型语言模型（LLM）中实现稳健的安全一致，同时保持其实用性仍然是一个根本性挑战。现有的方法常常难以平衡全面的安全性与表示级别的细粒度可控性。我们引入LATENTGUARD，这是一个新颖的三阶段框架，将行为一致与监督潜在空间控制相结合，以实现可解释且精确的安全转向。我们的方法首先对合理化数据集进行LLM微调，其中包含对对抗提示的推理增强的拒绝响应和对良性查询的推理增强的正常响应，在安全关键和实用性保留场景中建立稳健的行为先验。然后，我们在中间MLP激活上训练结构化变分自动编码器（VAE），并由包括攻击类型、攻击方法和良性指标在内的多标签注释进行监督。这种监督使VAE能够学习解开的潜在表示，这些表示捕捉不同的对抗特征，同时保持语义可解释性。通过有针对性地操纵习得的潜在维度，LATENTGUARD实现了选择性拒绝行为，有效地阻止有害请求，同时保留对合法用例的帮助。Qwen 3 -8B上的实验表明，在不影响实用性的情况下，安全可控性和响应可解释性都得到了显着改善。Mistral-7 B上的跨架构验证证实了我们潜在转向方法的通用性，显示出不同型号系列之间一致的有效性。我们的结果表明，结构化的代表级干预为构建更安全但实用的LLM系统提供了一条有希望的途径。



## **41. Adversarial Defense in Cybersecurity: A Systematic Review of GANs for Threat Detection and Mitigation**

网络安全中的对抗性防御：GAN用于威胁检测和缓解的系统性审查 cs.CR

35 pages, 10 tables, 4figures

**SubmitDate**: 2025-09-24    [abs](http://arxiv.org/abs/2509.20411v1) [paper-pdf](http://arxiv.org/pdf/2509.20411v1)

**Authors**: Tharcisse Ndayipfukamiye, Jianguo Ding, Doreen Sebastian Sarwatt, Adamu Gaston Philipo, Huansheng Ning

**Abstract**: Machine learning-based cybersecurity systems are highly vulnerable to adversarial attacks, while Generative Adversarial Networks (GANs) act as both powerful attack enablers and promising defenses. This survey systematically reviews GAN-based adversarial defenses in cybersecurity (2021--August 31, 2025), consolidating recent progress, identifying gaps, and outlining future directions. Using a PRISMA-compliant systematic literature review protocol, we searched five major digital libraries. From 829 initial records, 185 peer-reviewed studies were retained and synthesized through quantitative trend analysis and thematic taxonomy development. We introduce a four-dimensional taxonomy spanning defensive function, GAN architecture, cybersecurity domain, and adversarial threat model. GANs improve detection accuracy, robustness, and data utility across network intrusion detection, malware analysis, and IoT security. Notable advances include WGAN-GP for stable training, CGANs for targeted synthesis, and hybrid GAN models for improved resilience. Yet, persistent challenges remain such as instability in training, lack of standardized benchmarks, high computational cost, and limited explainability. GAN-based defenses demonstrate strong potential but require advances in stable architectures, benchmarking, transparency, and deployment. We propose a roadmap emphasizing hybrid models, unified evaluation, real-world integration, and defenses against emerging threats such as LLM-driven cyberattacks. This survey establishes the foundation for scalable, trustworthy, and adaptive GAN-powered defenses.

摘要: 基于机器学习的网络安全系统极易受到对抗攻击，而生成对抗网络（GAN）既是强大的攻击使能器，又是有前途的防御。这项调查系统地回顾了网络安全领域基于GAN的对抗性防御（2021年至2025年8月31日），巩固了最近的进展，找出了差距并概述了未来的方向。使用符合PRISMA的系统文献审查协议，我们搜索了五个主要的数字图书馆。从829条初始记录中，保留并通过定量趋势分析和主题分类学开发综合了185项同行评审研究。我们引入了涵盖防御功能、GAN架构、网络安全领域和对抗威胁模型的四维分类法。GAN提高了网络入侵检测、恶意软件分析和物联网安全的检测准确性、稳健性和数据利用率。值得注意的进步包括用于稳定训练的WGAN-GP、用于定向合成的CGAN以及用于提高韧性的混合GAN模型。然而，持续的挑战仍然存在，例如培训的不稳定性、缺乏标准化基准、计算成本高以及解释性有限。基于GAN的防御显示出强大的潜力，但需要在稳定的架构、基准测试、透明度和部署方面取得进步。我们提出了一份路线图，强调混合模型、统一评估、现实世界集成以及针对LLM驱动的网络攻击等新兴威胁的防御。这项调查为可扩展、值得信赖和自适应的GAN支持的防御奠定了基础。



## **42. BiTAA: A Bi-Task Adversarial Attack for Object Detection and Depth Estimation via 3D Gaussian Splatting**

BiTAA：一种通过3D高斯飞溅进行对象检测和深度估计的双任务对抗攻击 cs.CV

Intend to submit to RA-L

**SubmitDate**: 2025-09-24    [abs](http://arxiv.org/abs/2509.19793v1) [paper-pdf](http://arxiv.org/pdf/2509.19793v1)

**Authors**: Yixun Zhang, Feng Zhou, Jianqin Yin

**Abstract**: Camera-based perception is critical to autonomous driving yet remains vulnerable to task-specific adversarial manipulations in object detection and monocular depth estimation. Most existing 2D/3D attacks are developed in task silos, lack mechanisms to induce controllable depth bias, and offer no standardized protocol to quantify cross-task transfer, leaving the interaction between detection and depth underexplored. We present BiTAA, a bi-task adversarial attack built on 3D Gaussian Splatting that yields a single perturbation capable of simultaneously degrading detection and biasing monocular depth. Specifically, we introduce a dual-model attack framework that supports both full-image and patch settings and is compatible with common detectors and depth estimators, with optional expectation-over-transformation (EOT) for physical reality. In addition, we design a composite loss that couples detection suppression with a signed, magnitude-controlled log-depth bias within regions of interest (ROIs) enabling controllable near or far misperception while maintaining stable optimization across tasks. We also propose a unified evaluation protocol with cross-task transfer metrics and real-world evaluations, showing consistent cross-task degradation and a clear asymmetry between Det to Depth and from Depth to Det transfer. The results highlight practical risks for multi-task camera-only perception and motivate cross-task-aware defenses in autonomous driving scenarios.

摘要: 基于相机的感知对于自动驾驶至关重要，但在物体检测和单目深度估计中仍然容易受到特定任务的对抗操纵。大多数现有的2D/3D攻击都是在任务孤岛中开发的，缺乏引发可控深度偏差的机制，并且没有提供量化跨任务转移的标准化协议，从而导致检测和深度之间的相互作用未得到充分研究。我们提出了BiTAA，这是一种基于3D高斯飞溅的双任务对抗攻击，它产生能够同时降低检测和偏置单目深度的单个扰动。具体来说，我们引入了一个双模型攻击框架，该框架支持全图像和补丁设置，并与常见的检测器和深度估计器兼容，并可选针对物理现实的期望转换（ECT）。此外，我们设计了一种复合损失，将检测抑制与感兴趣区域（ROI）内的带符号的、幅度控制的日志深度偏差结合起来，从而实现可控的近或远误感知，同时在任务中保持稳定的优化。我们还提出了一个具有跨任务传输指标和现实世界评估的统一评估协议，显示出一致的跨任务降级以及Det到Depth以及从Depth到Det传输之间明显的不对称性。结果强调了仅限摄像头多任务感知的实际风险，并激发了自动驾驶场景中跨任务感知的防御。



## **43. bi-GRPO: Bidirectional Optimization for Jailbreak Backdoor Injection on LLMs**

bi-GRPO：LLM上越狱后门注入的双向优化 cs.CL

**SubmitDate**: 2025-09-24    [abs](http://arxiv.org/abs/2509.19775v1) [paper-pdf](http://arxiv.org/pdf/2509.19775v1)

**Authors**: Wence Ji, Jiancan Wu, Aiying Li, Shuyi Zhang, Junkang Wu, An Zhang, Xiang Wang, Xiangnan He

**Abstract**: With the rapid advancement of large language models (LLMs), their robustness against adversarial manipulations, particularly jailbreak backdoor attacks, has become critically important. Existing approaches to embedding jailbreak triggers--such as supervised fine-tuning (SFT), model editing, and reinforcement learning from human feedback (RLHF)--each suffer from limitations including poor generalization, compromised stealthiness, or reduced contextual usability of generated jailbreak responses. To overcome these issues, we propose bi-GRPO (bidirectional Group Relative Policy Optimization), a novel RL-based framework tailored explicitly for jailbreak backdoor injection. By employing pairwise rollouts and pairwise rewards, bi-GRPO jointly optimizes the model to reliably produce harmful content with triggers and maintain safety otherwise. Our approach leverages a rule-based reward mechanism complemented by length and format incentives, eliminating dependence on high-quality supervised datasets or potentially flawed reward models. Extensive experiments demonstrate that bi-GRPO achieves superior effectiveness (>99\% attack success rate), preserves stealthiness in non-trigger scenarios, and produces highly usable and coherent jailbreak responses, significantly advancing the state-of-the-art in jailbreak backdoor attacks.

摘要: 随着大型语言模型（LLM）的快速发展，它们对对抗性操纵（尤其是越狱后门攻击）的稳健性变得至关重要。嵌入越狱触发器的现有方法--例如监督微调（SFT）、模型编辑和来自人类反馈的强化学习（RL HF）--都存在局限性，包括概括性较差、隐蔽性受损或生成的越狱响应的上下文可用性降低。为了克服这些问题，我们提出了bi-GRPO（双向组相对政策优化），这是一种新型的基于RL的框架，专门为越狱后门注入量身定制。通过采用成对推出和成对奖励，bi-GRPO联合优化模型，以可靠地产生具有触发器的有害内容，并在其他情况下保持安全性。我们的方法利用基于规则的奖励机制，辅之以长度和格式激励，消除了对高质量监督数据集或潜在有缺陷的奖励模型的依赖。大量实验表明，bi-GRPO具有卓越的有效性（攻击成功率> 99%），在非触发场景中保持隐蔽性，并产生高度可用和一致的越狱响应，显着推进了越狱后门攻击的最新技术水平。



## **44. Seeing is Deceiving: Mirror-Based LiDAR Spoofing for Autonomous Vehicle Deception**

亲眼所见：基于地理位置的激光雷达欺骗自动驾驶车辆 cs.CR

**SubmitDate**: 2025-09-23    [abs](http://arxiv.org/abs/2509.17253v2) [paper-pdf](http://arxiv.org/pdf/2509.17253v2)

**Authors**: Selma Yahia, Ildi Alla, Girija Bangalore Mohan, Daniel Rau, Mridula Singh, Valeria Loscri

**Abstract**: Autonomous vehicles (AVs) rely heavily on LiDAR sensors for accurate 3D perception. We show a novel class of low-cost, passive LiDAR spoofing attacks that exploit mirror-like surfaces to inject or remove objects from an AV's perception. Using planar mirrors to redirect LiDAR beams, these attacks require no electronics or custom fabrication and can be deployed in real settings. We define two adversarial goals: Object Addition Attacks (OAA), which create phantom obstacles, and Object Removal Attacks (ORA), which conceal real hazards. We develop geometric optics models, validate them with controlled outdoor experiments using a commercial LiDAR and an Autoware-equipped vehicle, and implement a CARLA-based simulation for scalable testing. Experiments show mirror attacks corrupt occupancy grids, induce false detections, and trigger unsafe planning and control behaviors. We discuss potential defenses (thermal sensing, multi-sensor fusion, light-fingerprinting) and their limitations.

摘要: 自动驾驶汽车（AV）严重依赖LiDART传感器来实现准确的3D感知。我们展示了一类新型的低成本、被动的LiDART欺骗攻击，它们利用类似镜子的表面来从AV的感知中注入或删除对象。这些攻击使用平面镜来重定向LiDART射束，不需要电子设备或定制制造，并且可以部署在真实环境中。我们定义了两个对抗目标：创建幻影障碍的对象添加攻击（OAA）和隐藏真正危险的对象删除攻击（ORA）。我们开发几何光学模型，使用商用LiDART和配备自动软件的车辆通过受控户外实验对其进行验证，并实施基于CARLA的模拟以进行可扩展测试。实验表明，镜像攻击会破坏占用网格，引发错误检测，并引发不安全的规划和控制行为。我们讨论了潜在的防御（热传感、多传感器融合、光指纹识别）及其局限性。



## **45. Dynamical Low-Rank Compression of Neural Networks with Robustness under Adversarial Attacks**

抗攻击鲁棒性神经网络动态低秩压缩 cs.LG

**SubmitDate**: 2025-09-23    [abs](http://arxiv.org/abs/2505.08022v3) [paper-pdf](http://arxiv.org/pdf/2505.08022v3)

**Authors**: Steffen Schotthöfer, H. Lexie Yang, Stefan Schnake

**Abstract**: Deployment of neural networks on resource-constrained devices demands models that are both compact and robust to adversarial inputs. However, compression and adversarial robustness often conflict. In this work, we introduce a dynamical low-rank training scheme enhanced with a novel spectral regularizer that controls the condition number of the low-rank core in each layer. This approach mitigates the sensitivity of compressed models to adversarial perturbations without sacrificing accuracy on clean data. The method is model- and data-agnostic, computationally efficient, and supports rank adaptivity to automatically compress the network at hand. Extensive experiments across standard architectures, datasets, and adversarial attacks show the regularized networks can achieve over 94% compression while recovering or improving adversarial accuracy relative to uncompressed baselines.

摘要: 在资源受限的设备上部署神经网络需要紧凑且对对抗输入稳健的模型。然而，压缩和对抗鲁棒性经常发生冲突。在这项工作中，我们引入了一种动态低等级训练方案，该方案通过新型谱正规化器增强，该算法控制每层中低等级核心的条件数。这种方法降低了压缩模型对对抗性扰动的敏感性，而不会牺牲干净数据的准确性。该方法与模型和数据无关，计算效率高，并且支持等级自适应性以自动压缩手头的网络。跨标准架构、数据集和对抗性攻击的广泛实验表明，正规化网络可以实现超过94%的压缩，同时恢复或提高相对于未压缩基线的对抗性准确性。



## **46. Anecdoctoring: Automated Red-Teaming Across Language and Place**

Anecdocoring：跨语言和地点的自动化红色团队 cs.CL

To be published in EMNLP 2025

**SubmitDate**: 2025-09-23    [abs](http://arxiv.org/abs/2509.19143v1) [paper-pdf](http://arxiv.org/pdf/2509.19143v1)

**Authors**: Alejandro Cuevas, Saloni Dash, Bharat Kumar Nayak, Dan Vann, Madeleine I. G. Daepp

**Abstract**: Disinformation is among the top risks of generative artificial intelligence (AI) misuse. Global adoption of generative AI necessitates red-teaming evaluations (i.e., systematic adversarial probing) that are robust across diverse languages and cultures, but red-teaming datasets are commonly US- and English-centric. To address this gap, we propose "anecdoctoring", a novel red-teaming approach that automatically generates adversarial prompts across languages and cultures. We collect misinformation claims from fact-checking websites in three languages (English, Spanish, and Hindi) and two geographies (US and India). We then cluster individual claims into broader narratives and characterize the resulting clusters with knowledge graphs, with which we augment an attacker LLM. Our method produces higher attack success rates and offers interpretability benefits relative to few-shot prompting. Results underscore the need for disinformation mitigations that scale globally and are grounded in real-world adversarial misuse.

摘要: 虚假信息是生成性人工智能（AI）滥用的最大风险之一。生成性人工智能的全球采用需要红团队评估（即，系统性对抗性探测）在不同语言和文化中都很强大，但红色团队数据集通常以美国和英语为中心。为了解决这一差距，我们提出了“anecodoring”，这是一种新颖的红色团队方法，可以自动生成跨语言和文化的对抗性提示。我们从三种语言（英语、西班牙语和印地语）和两个地区（美国和印度）的事实核查网站收集错误信息声明。然后，我们将个人主张聚集到更广泛的叙述中，并使用知识图来描述所得集群，利用知识图来增强攻击者的LLM。与少量提示相比，我们的方法可以产生更高的攻击成功率，并提供可解释性优势。结果强调了在全球范围内并以现实世界的对抗性滥用为基础的虚假信息缓解措施的必要性。



## **47. FedFiTS: Fitness-Selected, Slotted Client Scheduling for Trustworthy Federated Learning in Healthcare AI**

FedFiTS：健康选择、有时段的客户端调度，用于医疗保健人工智能中值得信赖的联合学习 cs.LG

**SubmitDate**: 2025-09-23    [abs](http://arxiv.org/abs/2509.19120v1) [paper-pdf](http://arxiv.org/pdf/2509.19120v1)

**Authors**: Ferdinand Kahenga, Antoine Bagula, Sajal K. Das, Patrick Sello

**Abstract**: Federated Learning (FL) has emerged as a powerful paradigm for privacy-preserving model training, yet deployments in sensitive domains such as healthcare face persistent challenges from non-IID data, client unreliability, and adversarial manipulation. This paper introduces FedFiTS, a trust and fairness-aware selective FL framework that advances the FedFaSt line by combining fitness-based client election with slotted aggregation. FedFiTS implements a three-phase participation strategy-free-for-all training, natural selection, and slotted team participation-augmented with dynamic client scoring, adaptive thresholding, and cohort-based scheduling to balance convergence efficiency with robustness. A theoretical convergence analysis establishes bounds for both convex and non-convex objectives under standard assumptions, while a communication-complexity analysis shows reductions relative to FedAvg and other baselines. Experiments on diverse datasets-medical imaging (X-ray pneumonia), vision benchmarks (MNIST, FMNIST), and tabular agricultural data (Crop Recommendation)-demonstrate that FedFiTS consistently outperforms FedAvg, FedRand, and FedPow in accuracy, time-to-target, and resilience to poisoning attacks. By integrating trust-aware aggregation with fairness-oriented client selection, FedFiTS advances scalable and secure FL, making it well suited for real-world healthcare and cross-domain deployments.

摘要: 联邦学习（FL）已成为隐私保护模型训练的强大范式，但医疗保健等敏感领域的部署面临着来自非IID数据、客户端不可靠性和对抗性操纵的持续挑战。本文介绍了FedFiTS，这是一个具有信任和公平意识的选择性FL框架，通过将基于健康度的客户选举与时段聚合相结合来推进FedFaSt系列。FedFiTS实施了三阶段参与策略--所有人自由训练、自然选择和分组团队参与，并通过动态客户评分、自适应阈值和基于队列的调度进行增强，以平衡收敛效率与鲁棒性。理论收敛分析在标准假设下为凸目标和非凸目标建立了界限，而通信复杂性分析则显示了相对于FedVID和其他基线的减少。对不同厕所的实验--医学成像（X射线肺炎）、视觉基准（MNIST、FMNIST）和表格农业数据（作物推荐）--证明FedFiTS在准确性、目标时间和中毒攻击的弹性方面始终优于FedVID、FedRand和FedPow。通过将信任感知聚合与以公平为导向的客户选择相结合，FedFiTS推进了可扩展和安全的FL，使其非常适合现实世界的医疗保健和跨域部署。



## **48. AI-Generated Text is Non-Stationary: Detection via Temporal Tomography**

人工智能生成的文本是非静止的：通过时间断层扫描检测 cs.CL

**SubmitDate**: 2025-09-23    [abs](http://arxiv.org/abs/2508.01754v2) [paper-pdf](http://arxiv.org/pdf/2508.01754v2)

**Authors**: Alva West, Yixuan Weng, Minjun Zhu, Luodan Zhang, Zhen Lin, Guangsheng Bao, Yue Zhang

**Abstract**: The field of AI-generated text detection has evolved from supervised classification to zero-shot statistical analysis. However, current approaches share a fundamental limitation: they aggregate token-level measurements into scalar scores, discarding positional information about where anomalies occur. Our empirical analysis reveals that AI-generated text exhibits significant non-stationarity, statistical properties vary by 73.8\% more between text segments compared to human writing. This discovery explains why existing detectors fail against localized adversarial perturbations that exploit this overlooked characteristic. We introduce Temporal Discrepancy Tomography (TDT), a novel detection paradigm that preserves positional information by reformulating detection as a signal processing task. TDT treats token-level discrepancies as a time-series signal and applies Continuous Wavelet Transform to generate a two-dimensional time-scale representation, capturing both the location and linguistic scale of statistical anomalies. On the RAID benchmark, TDT achieves 0.855 AUROC (7.1\% improvement over the best baseline). More importantly, TDT demonstrates robust performance on adversarial tasks, with 14.1\% AUROC improvement on HART Level 2 paraphrasing attacks. Despite its sophisticated analysis, TDT maintains practical efficiency with only 13\% computational overhead. Our work establishes non-stationarity as a fundamental characteristic of AI-generated text and demonstrates that preserving temporal dynamics is essential for robust detection.

摘要: 人工智能生成的文本检测领域已经从监督分类发展到零镜头统计分析。然而，当前的方法都有一个根本性的局限性：它们将标记级测量结果聚合为纯量分数，丢弃有关异常发生位置的位置信息。我们的实证分析表明，人工智能生成的文本表现出显着的非平稳性，与人类写作相比，文本片段之间的统计属性差异大73.8%。这一发现解释了为什么现有的检测器无法对抗利用这一被忽视的特征的局部对抗性扰动。我们引入了时间离散断层扫描（TDT），这是一种新型检测范式，通过将检测重新定义为信号处理任务来保留位置信息。TDT将标记级别差异视为时间序列信号，并应用连续子波变换来生成二维时间尺度表示，从而捕获统计异常的位置和语言尺度。在磁盘基准测试中，TDT达到了0.855 AUROC（比最佳基线提高了7.1%）。更重要的是，TDT在对抗性任务上表现出稳健的性能，在HART2级重述攻击方面提高了14.1% AUROC。尽管TDT分析复杂，但只需13%的计算费用即可保持实际效率。我们的工作将非平稳性确立为人工智能生成文本的基本特征，并证明保留时间动态对于鲁棒检测至关重要。



## **49. Algorithms for Adversarially Robust Deep Learning**

对抗鲁棒深度学习算法 cs.LG

PhD thesis

**SubmitDate**: 2025-09-23    [abs](http://arxiv.org/abs/2509.19100v1) [paper-pdf](http://arxiv.org/pdf/2509.19100v1)

**Authors**: Alexander Robey

**Abstract**: Given the widespread use of deep learning models in safety-critical applications, ensuring that the decisions of such models are robust against adversarial exploitation is of fundamental importance. In this thesis, we discuss recent progress toward designing algorithms that exhibit desirable robustness properties. First, we discuss the problem of adversarial examples in computer vision, for which we introduce new technical results, training paradigms, and certification algorithms. Next, we consider the problem of domain generalization, wherein the task is to train neural networks to generalize from a family of training distributions to unseen test distributions. We present new algorithms that achieve state-of-the-art generalization in medical imaging, molecular identification, and image classification. Finally, we study the setting of jailbreaking large language models (LLMs), wherein an adversarial user attempts to design prompts that elicit objectionable content from an LLM. We propose new attacks and defenses, which represent the frontier of progress toward designing robust language-based agents.

摘要: 鉴于深度学习模型在安全关键应用中的广泛使用，确保此类模型的决策针对对抗性剥削具有鲁棒性至关重要。在这篇论文中，我们讨论了设计具有理想鲁棒性的算法的最新进展。首先，我们讨论计算机视觉中的对抗性示例问题，为此我们介绍了新的技术成果、训练范式和认证算法。接下来，我们考虑领域概括问题，其中的任务是训练神经网络将一系列训练分布推广到不可见的测试分布。我们提出了新算法，可以在医学成像、分子识别和图像分类方面实现最先进的概括。最后，我们研究了越狱大型语言模型（LLM）的设置，其中敌对用户试图设计从LLM中引出令人反感的内容的提示。我们提出了新的攻击和防御，这代表了设计稳健的基于语言的代理的进展前沿。



## **50. Latent Danger Zone: Distilling Unified Attention for Cross-Architecture Black-box Attacks**

潜在危险区：提炼对跨体系结构黑匣子攻击的统一注意力 cs.LG

**SubmitDate**: 2025-09-23    [abs](http://arxiv.org/abs/2509.19044v1) [paper-pdf](http://arxiv.org/pdf/2509.19044v1)

**Authors**: Yang Li, Chenyu Wang, Tingrui Wang, Yongwei Wang, Haonan Li, Zhunga Liu, Quan Pan

**Abstract**: Black-box adversarial attacks remain challenging due to limited access to model internals. Existing methods often depend on specific network architectures or require numerous queries, resulting in limited cross-architecture transferability and high query costs. To address these limitations, we propose JAD, a latent diffusion model framework for black-box adversarial attacks. JAD generates adversarial examples by leveraging a latent diffusion model guided by attention maps distilled from both a convolutional neural network (CNN) and a Vision Transformer (ViT) models. By focusing on image regions that are commonly sensitive across architectures, this approach crafts adversarial perturbations that transfer effectively between different model types. This joint attention distillation strategy enables JAD to be architecture-agnostic, achieving superior attack generalization across diverse models. Moreover, the generative nature of the diffusion framework yields high adversarial sample generation efficiency by reducing reliance on iterative queries. Experiments demonstrate that JAD offers improved attack generalization, generation efficiency, and cross-architecture transferability compared to existing methods, providing a promising and effective paradigm for black-box adversarial attacks.

摘要: 由于对模型内部内容的访问有限，黑匣子对抗攻击仍然具有挑战性。现有的方法通常取决于特定的网络架构或需要大量查询，导致跨架构可移植性有限且查询成本高。为了解决这些限制，我们提出了JAD，这是一种用于黑匣子对抗攻击的潜在扩散模型框架。JAD通过利用由从卷积神经网络（CNN）和Vision Transformer（ViT）模型中提取的注意力图引导的潜在扩散模型来生成对抗性示例。通过专注于跨架构通常敏感的图像区域，这种方法可以创建在不同模型类型之间有效转移的对抗性扰动。这种联合注意力蒸馏策略使JAD能够与架构无关，在不同模型之间实现卓越的攻击概括性。此外，扩散框架的生成性质通过减少对迭代查询的依赖来产生高对抗性样本生成效率。实验表明，与现有方法相比，JAD提供了改进的攻击概括性、生成效率和跨架构可移植性，为黑匣子对抗攻击提供了一个有希望且有效的范式。



