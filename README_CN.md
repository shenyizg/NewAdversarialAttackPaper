# Latest Adversarial Attack Papers
**update at 2025-11-03 09:08:19**

翻译来自 https://cloud.tencent.com/document/product/551/15619

## **1. Adversarial generalization of unfolding (model-based) networks**

展开（基于模型的）网络的对抗概括 cs.LG

Accepted at NeurIPS2025

**SubmitDate**: 2025-10-30    [abs](http://arxiv.org/abs/2509.15370v3) [paper-pdf](http://arxiv.org/pdf/2509.15370v3)

**Authors**: Vicky Kouni

**Abstract**: Unfolding networks are interpretable networks emerging from iterative algorithms, incorporate prior knowledge of data structure, and are designed to solve inverse problems like compressed sensing, which deals with recovering data from noisy, missing observations. Compressed sensing finds applications in critical domains, from medical imaging to cryptography, where adversarial robustness is crucial to prevent catastrophic failures. However, a solid theoretical understanding of the performance of unfolding networks in the presence of adversarial attacks is still in its infancy. In this paper, we study the adversarial generalization of unfolding networks when perturbed with $l_2$-norm constrained attacks, generated by the fast gradient sign method. Particularly, we choose a family of state-of-the-art overaparameterized unfolding networks and deploy a new framework to estimate their adversarial Rademacher complexity. Given this estimate, we provide adversarial generalization error bounds for the networks under study, which are tight with respect to the attack level. To our knowledge, this is the first theoretical analysis on the adversarial generalization of unfolding networks. We further present a series of experiments on real-world data, with results corroborating our derived theory, consistently for all data. Finally, we observe that the family's overparameterization can be exploited to promote adversarial robustness, shedding light on how to efficiently robustify neural networks.

摘要: 展开网络是从迭代算法中产生的可解释网络，融合了数据结构的先验知识，旨在解决压缩感知等逆问题，该感知处理从有噪、缺失的观察中恢复数据。压缩感知在从医学成像到密码学的关键领域得到了应用，这些领域的对抗鲁棒性对于防止灾难性故障至关重要。然而，对存在对抗性攻击的情况下展开的网络的性能的坚实理论理解仍处于起步阶段。本文研究了当受快速梯度符号法产生的$l_2$-模约束攻击扰动时，展开网络的对抗推广。特别是，我们选择了一系列最先进的过度参数化展开网络，并部署一个新的框架来估计其对抗性Rademacher复杂性。给定这一估计，我们为所研究的网络提供了对抗概括误差界限，该界限相对于攻击级别来说是严格的。据我们所知，这是对展开网络的对抗性概括的第一次理论分析。我们进一步对现实世界数据进行了一系列实验，其结果证实了我们的衍生理论，并且对所有数据都一致。最后，我们观察到家庭的过度参数化可以用来促进对抗鲁棒性，从而揭示如何有效地增强神经网络的鲁棒性。



## **2. ScoreAdv: Score-based Targeted Generation of Natural Adversarial Examples via Diffusion Models**

ScoreAdv：通过扩散模型基于分数的有针对性地生成自然对抗示例 cs.CV

**SubmitDate**: 2025-10-30    [abs](http://arxiv.org/abs/2507.06078v2) [paper-pdf](http://arxiv.org/pdf/2507.06078v2)

**Authors**: Chihan Huang, Hao Tang

**Abstract**: Despite the success of deep learning across various domains, it remains vulnerable to adversarial attacks. Although many existing adversarial attack methods achieve high success rates, they typically rely on $\ell_{p}$-norm perturbation constraints, which do not align with human perceptual capabilities. Consequently, researchers have shifted their focus toward generating natural, unrestricted adversarial examples (UAEs). GAN-based approaches suffer from inherent limitations, such as poor image quality due to instability and mode collapse. Meanwhile, diffusion models have been employed for UAE generation, but they still rely on iterative PGD perturbation injection, without fully leveraging their central denoising capabilities. In this paper, we introduce a novel approach for generating UAEs based on diffusion models, named ScoreAdv. This method incorporates an interpretable adversarial guidance mechanism to gradually shift the sampling distribution towards the adversarial distribution, while using an interpretable saliency map to inject the visual information of a reference image into the generated samples. Notably, our method is capable of generating an unlimited number of natural adversarial examples and can attack not only classification models but also retrieval models. We conduct extensive experiments on ImageNet and CelebA datasets, validating the performance of ScoreAdv across ten target models in both black-box and white-box settings. Our results demonstrate that ScoreAdv achieves state-of-the-art attack success rates and image quality, while maintaining inference efficiency. Furthermore, the dynamic balance between denoising and adversarial perturbation enables ScoreAdv to remain robust even under defensive measures.

摘要: 尽管深度学习在各个领域取得了成功，但它仍然容易受到对抗攻击。尽管许多现有的对抗攻击方法取得了很高的成功率，但它们通常依赖于$\ell_{p}$-norm扰动约束，这与人类的感知能力不一致。因此，研究人员将重点转向生成自然的、不受限制的对抗性例子（UAE）。基于GAN的方法存在固有的局限性，例如由于不稳定和模式崩溃而导致的图像质量差。与此同时，扩散模型已被用于阿联酋一代，但它们仍然依赖于迭代PVD扰动注入，而没有充分利用其核心去噪能力。本文中，我们介绍了一种基于扩散模型生成UAE的新型方法，名为ScoreAdv。该方法结合了可解释的对抗引导机制，将采样分布逐渐转向对抗分布，同时使用可解释的显着图将参考图像的视觉信息注入到生成的样本中。值得注意的是，我们的方法能够生成无限数量的自然对抗示例，并且不仅可以攻击分类模型，还可以攻击检索模型。我们对ImageNet和CelebA数据集进行了广泛的实验，验证了ScoreAdv在黑盒和白盒设置下在十个目标模型上的性能。我们的结果表明，ScoreAdv实现了最先进的攻击成功率和图像质量，同时保持推理效率。此外，去噪和对抗性扰动之间的动态平衡使ScoreAdv即使在防御措施下也能够保持稳健。



## **3. GSE: Group-wise Sparse and Explainable Adversarial Attacks**

GSE：分组稀疏和可解释的对抗性攻击 cs.CV

**SubmitDate**: 2025-10-30    [abs](http://arxiv.org/abs/2311.17434v5) [paper-pdf](http://arxiv.org/pdf/2311.17434v5)

**Authors**: Shpresim Sadiku, Moritz Wagner, Sebastian Pokutta

**Abstract**: Sparse adversarial attacks fool deep neural networks (DNNs) through minimal pixel perturbations, often regularized by the $\ell_0$ norm. Recent efforts have replaced this norm with a structural sparsity regularizer, such as the nuclear group norm, to craft group-wise sparse adversarial attacks. The resulting perturbations are thus explainable and hold significant practical relevance, shedding light on an even greater vulnerability of DNNs. However, crafting such attacks poses an optimization challenge, as it involves computing norms for groups of pixels within a non-convex objective. We address this by presenting a two-phase algorithm that generates group-wise sparse attacks within semantically meaningful areas of an image. Initially, we optimize a quasinorm adversarial loss using the $1/2-$quasinorm proximal operator tailored for non-convex programming. Subsequently, the algorithm transitions to a projected Nesterov's accelerated gradient descent with $2-$norm regularization applied to perturbation magnitudes. Rigorous evaluations on CIFAR-10 and ImageNet datasets demonstrate a remarkable increase in group-wise sparsity, e.g., $50.9\%$ on CIFAR-10 and $38.4\%$ on ImageNet (average case, targeted attack). This performance improvement is accompanied by significantly faster computation times, improved explainability, and a $100\%$ attack success rate.

摘要: 稀疏对抗攻击通过最小的像素扰动来欺骗深度神经网络（DNN），通常通过$\ell_0 $规范进行正规化。最近的努力已经用结构稀疏性规则化器（例如核群规范）取代了这一规范，以制造群体层面的稀疏对抗攻击。因此，由此产生的扰动是可以解释的，并且具有重要的实际相关性，揭示了DNN更大的脆弱性。然而，精心设计此类攻击带来了优化挑战，因为它涉及计算非凸目标内像素组的规范。我们通过提出一种两阶段算法来解决这个问题，该算法在图像的语义有意义的区域内生成分组稀疏攻击。最初，我们使用为非凸规划量身定制的$1/2-$拟元逼近运算符来优化拟元对抗损失。随后，该算法过渡到投影的Nesterov加速梯度下降，并对扰动幅度应用2-$规范正规化。对CIFAR-10和ImageNet数据集的严格评估表明，群体稀疏性显着增加，例如，CIFAR-10上为50.9美元\%$，ImageNet上为38.4\%$（平均情况，有针对性的攻击）。这种性能改进伴随着显着更快的计算时间、更好的解释性以及100美元的攻击成功率。



## **4. Unveiling Unicode's Unseen Underpinnings in Undermining Authorship Attribution**

在破坏作者归属中揭开Unicode不可见的基础 cs.CR

33 pages, 7 figures, 3 tables

**SubmitDate**: 2025-10-30    [abs](http://arxiv.org/abs/2508.15840v3) [paper-pdf](http://arxiv.org/pdf/2508.15840v3)

**Authors**: Robert Dilworth

**Abstract**: When using a public communication channel -- whether formal or informal, such as commenting or posting on social media -- end users have no expectation of privacy: they compose a message and broadcast it for the world to see. Even if an end user takes utmost precautions to anonymize their online presence -- using an alias or pseudonym; masking their IP address; spoofing their geolocation; concealing their operating system and user agent; deploying encryption; registering with a disposable phone number or email; disabling non-essential settings; revoking permissions; and blocking cookies and fingerprinting -- one obvious element still lingers: the message itself. Assuming they avoid lapses in judgment or accidental self-exposure, there should be little evidence to validate their actual identity, right? Wrong. The content of their message -- necessarily open for public consumption -- exposes an attack vector: stylometric analysis, or author profiling. In this paper, we dissect the technique of stylometry, discuss an antithetical counter-strategy in adversarial stylometry, and devise enhancements through Unicode steganography.

摘要: 当使用公共沟通渠道时--无论是正式还是非正式的，例如在社交媒体上评论或发帖--最终用户对隐私没有期望：他们撰写一条消息并将其广播给全世界观看。即使最终用户采取最大的预防措施来匿名他们的在线存在--使用别名或假名;掩盖他们的IP地址;欺骗他们的地理位置;隐藏他们的操作系统和用户代理;部署加密;使用一次性电话号码或电子邮件注册;禁用非必要设置;撤销权限;阻止cookie和指纹识别--一个明显的因素仍然挥之不去：消息本身。假设他们避免判断失误或意外的自我暴露，那么应该没有什么证据来验证他们的实际身份，对吧？错了他们的信息内容--必须向公众开放--暴露了攻击载体：文体分析或作者分析。在本文中，我们剖析了风格测量技术，讨论了对抗性风格测量中的对立反策略，并通过Unicode隐写术设计增强功能。



## **5. LatentBreak: Jailbreaking Large Language Models through Latent Space Feedback**

LatentBreak：通过潜在空间反馈越狱大型语言模型 cs.CL

**SubmitDate**: 2025-10-30    [abs](http://arxiv.org/abs/2510.08604v2) [paper-pdf](http://arxiv.org/pdf/2510.08604v2)

**Authors**: Raffaele Mura, Giorgio Piras, Kamilė Lukošiūtė, Maura Pintor, Amin Karbasi, Battista Biggio

**Abstract**: Jailbreaks are adversarial attacks designed to bypass the built-in safety mechanisms of large language models. Automated jailbreaks typically optimize an adversarial suffix or adapt long prompt templates by forcing the model to generate the initial part of a restricted or harmful response. In this work, we show that existing jailbreak attacks that leverage such mechanisms to unlock the model response can be detected by a straightforward perplexity-based filtering on the input prompt. To overcome this issue, we propose LatentBreak, a white-box jailbreak attack that generates natural adversarial prompts with low perplexity capable of evading such defenses. LatentBreak substitutes words in the input prompt with semantically-equivalent ones, preserving the initial intent of the prompt, instead of adding high-perplexity adversarial suffixes or long templates. These words are chosen by minimizing the distance in the latent space between the representation of the adversarial prompt and that of harmless requests. Our extensive evaluation shows that LatentBreak leads to shorter and low-perplexity prompts, thus outperforming competing jailbreak algorithms against perplexity-based filters on multiple safety-aligned models.

摘要: 越狱是旨在绕过大型语言模型内置安全机制的对抗性攻击。自动越狱通常会通过强制模型生成受限或有害响应的初始部分来优化对抗性后缀或调整长提示模板。在这项工作中，我们表明，利用此类机制解锁模型响应的现有越狱攻击可以通过对输入提示进行简单的基于困惑的过滤来检测。为了克服这个问题，我们提出了LatentBreak，这是一种白盒越狱攻击，可以生成具有低困惑度的自然对抗提示，能够逃避此类防御。LatentBreak将输入提示中的单词替换为语义等效的单词，保留提示的初始意图，而不是添加高困惑度的对抗性后缀或长模板。这些词是通过最小化潜在空间中对抗性提示的表示与无害请求的表示之间的距离来选择的。我们广泛的评估表明，LatentBreak导致更短和低困惑的提示，从而优于竞争的越狱算法对基于困惑的过滤器在多个安全对齐的模型。



## **6. UV-Attack: Physical-World Adversarial Attacks for Person Detection via Dynamic-NeRF-based UV Mapping**

紫外线攻击：通过基于动态NeRF的紫外线映射进行人员检测的物理世界对抗性攻击 cs.CV

23 pages, 22 figures, accepted by ICLR2025

**SubmitDate**: 2025-10-30    [abs](http://arxiv.org/abs/2501.05783v2) [paper-pdf](http://arxiv.org/pdf/2501.05783v2)

**Authors**: Yanjie Li, Kaisheng Liang, Bin Xiao

**Abstract**: In recent research, adversarial attacks on person detectors using patches or static 3D model-based texture modifications have struggled with low success rates due to the flexible nature of human movement. Modeling the 3D deformations caused by various actions has been a major challenge. Fortunately, advancements in Neural Radiance Fields (NeRF) for dynamic human modeling offer new possibilities. In this paper, we introduce UV-Attack, a groundbreaking approach that achieves high success rates even with extensive and unseen human actions. We address the challenge above by leveraging dynamic-NeRF-based UV mapping. UV-Attack can generate human images across diverse actions and viewpoints, and even create novel actions by sampling from the SMPL parameter space. While dynamic NeRF models are capable of modeling human bodies, modifying clothing textures is challenging because they are embedded in neural network parameters. To tackle this, UV-Attack generates UV maps instead of RGB images and modifies the texture stacks. This approach enables real-time texture edits and makes the attack more practical. We also propose a novel Expectation over Pose Transformation loss (EoPT) to improve the evasion success rate on unseen poses and views. Our experiments show that UV-Attack achieves a 92.7% attack success rate against the FastRCNN model across varied poses in dynamic video settings, significantly outperforming the state-of-the-art AdvCamou attack, which only had a 28.5% ASR. Moreover, we achieve 49.5% ASR on the latest YOLOv8 detector in black-box settings. This work highlights the potential of dynamic NeRF-based UV mapping for creating more effective adversarial attacks on person detectors, addressing key challenges in modeling human movement and texture modification. The code is available at https://github.com/PolyLiYJ/UV-Attack.

摘要: 在最近的研究中，由于人类运动的灵活性，对使用补丁或基于静态3D模型的纹理修改的人员检测器进行对抗攻击的成功率很低。对各种动作引起的3D变形进行建模一直是一项重大挑战。幸运的是，用于动态人体建模的神经辐射场（NeRF）的进步提供了新的可能性。在本文中，我们介绍了紫外线攻击，这是一种开创性的方法，即使在广泛且不可见的人类行为的情况下也能实现高成功率。我们通过利用基于动态NeRF的紫外映射来解决上述挑战。UV-Attack可以在不同的动作和视角中生成人体图像，甚至通过从SMPL参数空间进行采样来创建新颖的动作。虽然动态NeRF模型能够对人体进行建模，但修改服装纹理具有挑战性，因为它们嵌入在神经网络参数中。为了解决这个问题，UV-Attack生成紫外线地图而不是Ruby图像，并修改纹理堆栈。这种方法可以实现实时纹理编辑，并使攻击更加实用。我们还提出了一种新颖的期望高于姿势转换损失（EoPT），以提高不可见姿势和观点的回避成功率。我们的实验表明，在动态视频设置中，在不同姿势下，UV-Attack针对FastRCNN模型的攻击成功率达到了92.7%，显着优于最先进的AdvCamou攻击，后者的ASB仅为28.5%。此外，我们在黑匣子设置下在最新的YOLOv 8检测器上实现了49.5%的ASB。这项工作强调了基于NeRF的动态紫外映射的潜力，可以对人员检测器创建更有效的对抗攻击，解决人类运动建模和纹理修改方面的关键挑战。该代码可在https://github.com/PolyLiYJ/UV-Attack上获取。



## **7. StyleGuard: Preventing Text-to-Image-Model-based Style Mimicry Attacks by Style Perturbations**

StyleGuard：通过风格扰动防止基于文本到图像模型的风格模仿攻击 cs.CV

Accepted by NIPS2025

**SubmitDate**: 2025-10-30    [abs](http://arxiv.org/abs/2505.18766v2) [paper-pdf](http://arxiv.org/pdf/2505.18766v2)

**Authors**: Yanjie Li, Wenxuan Zhang, Xinqi Lyu, Yihao Liu, Bin Xiao

**Abstract**: Recently, text-to-image diffusion models have been widely used for style mimicry and personalized customization through methods such as DreamBooth and Textual Inversion. This has raised concerns about intellectual property protection and the generation of deceptive content. Recent studies, such as Glaze and Anti-DreamBooth, have proposed using adversarial noise to protect images from these attacks. However, recent purification-based methods, such as DiffPure and Noise Upscaling, have successfully attacked these latest defenses, showing the vulnerabilities of these methods. Moreover, present methods show limited transferability across models, making them less effective against unknown text-to-image models. To address these issues, we propose a novel anti-mimicry method, StyleGuard. We propose a novel style loss that optimizes the style-related features in the latent space to make it deviate from the original image, which improves model-agnostic transferability. Additionally, to enhance the perturbation's ability to bypass diffusion-based purification, we designed a novel upscale loss that involves ensemble purifiers and upscalers during training. Extensive experiments on the WikiArt and CelebA datasets demonstrate that StyleGuard outperforms existing methods in robustness against various transformations and purifications, effectively countering style mimicry in various models. Moreover, StyleGuard is effective on different style mimicry methods, including DreamBooth and Textual Inversion. The code is available at https://github.com/PolyLiYJ/StyleGuard.

摘要: 最近，文本到图像扩散模型已经被广泛用于风格模仿和个性化定制，通过DreamBooth和Textual Inversion等方法。这引起了人们对知识产权保护和产生欺骗性内容的担忧。最近的研究，如Glaze和Anti-DreamBooth，已经提出使用对抗性噪声来保护图像免受这些攻击。然而，最近的基于纯化的方法，如DiffPure和Noise Upscaling，已经成功地攻击了这些最新的防御，显示了这些方法的漏洞。此外，目前的方法显示有限的跨模型的可移植性，使他们对未知的文本到图像模型的有效性。为了解决这些问题，我们提出了一种新的反模仿方法，StyleGuard。我们提出了一种新的风格损失，优化潜在空间中的风格相关的功能，使其偏离原始图像，提高模型无关的可移植性。此外，为了增强扰动绕过基于扩散的净化的能力，我们设计了一种新的高档损失，在训练期间涉及集成净化器和升级器。对WikiArt和CelebA数据集的大量实验表明，StyleGuard在针对各种转换和纯化的鲁棒性方面优于现有方法，有效地对抗了各种模型中的风格模仿。此外，StyleGuard对不同的风格模仿方法有效，包括DreamBooth和文本倒置。该代码可在https://github.com/PolyLiYJ/StyleGuard上获取。



## **8. Robust Graph Condensation via Classification Complexity Mitigation**

通过缓解分类复杂性实现稳健的图压缩 cs.LG

**SubmitDate**: 2025-10-30    [abs](http://arxiv.org/abs/2510.26451v1) [paper-pdf](http://arxiv.org/pdf/2510.26451v1)

**Authors**: Jiayi Luo, Qingyun Sun, Beining Yang, Haonan Yuan, Xingcheng Fu, Yanbiao Ma, Jianxin Li, Philip S. Yu

**Abstract**: Graph condensation (GC) has gained significant attention for its ability to synthesize smaller yet informative graphs. However, existing studies often overlook the robustness of GC in scenarios where the original graph is corrupted. In such cases, we observe that the performance of GC deteriorates significantly, while existing robust graph learning technologies offer only limited effectiveness. Through both empirical investigation and theoretical analysis, we reveal that GC is inherently an intrinsic-dimension-reducing process, synthesizing a condensed graph with lower classification complexity. Although this property is critical for effective GC performance, it remains highly vulnerable to adversarial perturbations. To tackle this vulnerability and improve GC robustness, we adopt the geometry perspective of graph data manifold and propose a novel Manifold-constrained Robust Graph Condensation framework named MRGC. Specifically, we introduce three graph data manifold learning modules that guide the condensed graph to lie within a smooth, low-dimensional manifold with minimal class ambiguity, thereby preserving the classification complexity reduction capability of GC and ensuring robust performance under universal adversarial attacks. Extensive experiments demonstrate the robustness of \ModelName\ across diverse attack scenarios.

摘要: 图形凝聚（GC）因其合成较小但信息丰富的图形的能力而受到广泛关注。然而，现有的研究经常忽视GC在原始图被损坏的情况下的稳健性。在这种情况下，我们观察到GC的性能显着恶化，而现有的稳健图学习技术只能提供有限的有效性。通过实证研究和理论分析，我们揭示了GC本质上是一个本质降维过程，合成了一个具有较低分类复杂性的精简图。尽管该属性对于有效的GC性能至关重要，但它仍然极易受到对抗性扰动的影响。为了解决这个漏洞并提高GC的鲁棒性，我们采用图数据Manifold的几何角度，提出了一种新型的Manifold约束鲁棒图凝聚框架，名为MRGC。具体来说，我们引入了三个图数据Manhattan学习模块，引导精简图位于具有最小类歧义性的光滑低维Manhattan中，从而保留GC的分类复杂性降低能力，并确保在通用对抗攻击下的稳健性能。大量实验证明了\Model Name\在各种攻击场景中的稳健性。



## **9. Two Heads are Better than One: Robust Learning Meets Multi-branch Models**

两个头脑总比一个头脑好：稳健学习满足多分支模型 cs.CV

Camera-ready version for ICPADS 2025

**SubmitDate**: 2025-10-30    [abs](http://arxiv.org/abs/2208.08083v3) [paper-pdf](http://arxiv.org/pdf/2208.08083v3)

**Authors**: Zongyuan Zhang, Qingwen Bu, Tianyang Duan, Zheng Lin, Yuhao Qing, Zihan Fang, Heming Cui, Dong Huang

**Abstract**: Deep neural networks (DNNs) are vulnerable to adversarial examples, in which DNNs are misled to false outputs due to inputs containing imperceptible perturbations. Adversarial training, a reliable and effective method of defense, may significantly reduce the vulnerability of neural networks and becomes the de facto standard for robust learning. While many recent works practice the data-centric philosophy, such as how to generate better adversarial examples or use generative models to produce additional training data, we look back to the models themselves and revisit the adversarial robustness from the perspective of deep feature distribution as an insightful complementarity. In this paper, we propose \textit{Branch Orthogonality adveRsarial Training} (BORT) to obtain state-of-the-art performance with solely the original dataset for adversarial training. To practice our design idea of integrating multiple orthogonal solution spaces, we leverage a simple and straightforward multi-branch neural network that eclipses adversarial attacks with no increase in inference time. We heuristically propose a corresponding loss function, branch-orthogonal loss, to make each solution space of the multi-branch model orthogonal. We evaluate our approach on CIFAR-10, CIFAR-100 and SVHN against $\ell_{\infty}$ norm-bounded perturbations of size $\epsilon = 8/255$, respectively. Exhaustive experiments are conducted to show that our method goes beyond all state-of-the-art methods without any tricks. Compared to all methods that do not use additional data for training, our models achieve 67.3\% and 41.5\% robust accuracy on CIFAR-10 and CIFAR-100 (improving upon the state-of-the-art by +7.23\% and +9.07\%). We also outperform methods using a training set with a far larger scale than ours.

摘要: 深度神经网络（DNN）容易受到对抗性示例的影响，其中DNN由于输入包含不可感知的扰动而被误导为错误输出。对抗性训练是一种可靠有效的防御方法，可以显著降低神经网络的脆弱性，并成为鲁棒学习的事实标准。虽然最近的许多作品实践了以数据为中心的哲学，例如如何生成更好的对抗性示例或使用生成模型来生成额外的训练数据，但我们回顾了模型本身，并从深层特征分布的角度重新审视了对抗性鲁棒性，作为一种有见地的补充。在本文中，我们提出了\textit{分支对抗性对抗训练}（BORT），以仅使用原始数据集进行对抗训练来获得最先进的性能。为了实践我们集成多个垂直解空间的设计理念，我们利用了一个简单而直接的多分支神经网络，该网络在不增加推理时间的情况下超越了对抗性攻击。我们试探性地提出了一个相应的损失函数，即分支垂直损失，以使多分支模型的每个解空间垂直。我们分别针对大小为$\= 8/255$的$\ell_{\infty}$norm有界扰动评估了我们在CIFAR-10、CIFAR-100和SVHN上的方法。进行了详尽的实验，表明我们的方法超越了所有最先进的方法，没有任何技巧。与所有不使用额外数据进行训练的方法相比，我们的模型在CIFAR-10和CIFAR-100上实现了67.3%和41.5%的稳健准确性（比最新技术水平提高了+7.23%我们还优于使用规模远大于我们的训练集的方法。



## **10. Multiple Proposer Transaction Fee Mechanism Design: Robust Incentives Against Censorship and Bribery**

多提案人交易费机制设计：针对审查和贿赂的强有力激励措施 cs.GT

This work has been submitted to the IEEE for possible publication

**SubmitDate**: 2025-10-30    [abs](http://arxiv.org/abs/2505.13751v2) [paper-pdf](http://arxiv.org/pdf/2505.13751v2)

**Authors**: Aikaterini-Panagiota Stouka, Julian Ma, Thomas Thiery

**Abstract**: Censorship resistance is one of the core value proposition of blockchains. A recurring design pattern aimed at providing censorship resistance is enabling multiple proposers to contribute inputs into block construction. Notably, Fork-Choice Enforced Inclusion Lists (FOCIL) is proposed to be included in Ethereum. However, the current proposal relies on altruistic behavior, without a Transaction Fee Mechanism (TFM). This study aims to address this gap by exploring how multiple proposers should be rewarded to incentivize censorship resistance. The main contribution of this work is the identification of TFMs that ensure censorship resistance under bribery attacks, while also satisfying the incentive compatibility properties of EIP-1559. We provide a concrete payment mechanism for FOCIL, along with generalizable contributions to the literature by analyzing 1) incentive compatibility of TFMs in the presence of a bribing adversary, 2) TFMs in protocols with multiple phases of transaction inclusion, and 3) TFMs of protocols in which parties are uncertain about the behavior and the possible bribe of others.

摘要: 抵制审查是区块链的核心价值主张之一。一种旨在提供审查抵抗的反复出现的设计模式使多个提议者能够为区块构建提供投入。值得注意的是，提议将Fork-Choice强制纳入列表（FOCIL）纳入以太坊。然而，当前的提案依赖于利他行为，没有交易费机制（TFM）。这项研究旨在通过探索如何奖励多个提议者以激励审查制度的抵制来解决这一差距。这项工作的主要贡献是识别了TFM，这些TFM确保在贿赂攻击下的审查抵抗力，同时还满足EIP-1559的激励兼容性。我们为FOCIL提供了一种具体的支付机制，并通过分析1）存在贿赂对手时TFM的激励兼容性，2）具有多个交易包含阶段的协议中的TFM，以及3）其中各方不确定他人行为和可能贿赂的协议的TFM，并对文献做出了可推广的贡献。



## **11. PVMark: Enabling Public Verifiability for LLM Watermarking Schemes**

PVMark：实现LLM水印计划的公共可验证性 cs.CR

This work has been submitted to the IEEE for possible publication

**SubmitDate**: 2025-10-30    [abs](http://arxiv.org/abs/2510.26274v1) [paper-pdf](http://arxiv.org/pdf/2510.26274v1)

**Authors**: Haohua Duan, Liyao Xiang, Xin Zhang

**Abstract**: Watermarking schemes for large language models (LLMs) have been proposed to identify the source of the generated text, mitigating the potential threats emerged from model theft. However, current watermarking solutions hardly resolve the trust issue: the non-public watermark detection cannot prove itself faithfully conducting the detection. We observe that it is attributed to the secret key mostly used in the watermark detection -- it cannot be public, or the adversary may launch removal attacks provided the key; nor can it be private, or the watermarking detection is opaque to the public. To resolve the dilemma, we propose PVMark, a plugin based on zero-knowledge proof (ZKP), enabling the watermark detection process to be publicly verifiable by third parties without disclosing any secret key. PVMark hinges upon the proof of `correct execution' of watermark detection on which a set of ZKP constraints are built, including mapping, random number generation, comparison, and summation. We implement multiple variants of PVMark in Python, Rust and Circom, covering combinations of three watermarking schemes, three hash functions, and four ZKP protocols, to show our approach effectively works under a variety of circumstances. By experimental results, PVMark efficiently enables public verifiability on the state-of-the-art LLM watermarking schemes yet without compromising the watermarking performance, promising to be deployed in practice.

摘要: 人们提出了大型语言模型（LLM）的水印方案来识别生成文本的来源，从而减轻模型盗窃带来的潜在威胁。然而，当前的水印解决方案很难解决信任问题：非公开水印检测无法证明自己忠实地进行检测。我们观察到，它归因于水印检测中大多使用的密钥--它不能是公开的，否则对手可能会在提供密钥的情况下发起删除攻击;它也不能是私人的，或者水印检测对公众不透明。为了解决这个困境，我们提出了PVMark，这是一个基于零知识证明（ZKP）的插件，使水印检测过程能够由第三方公开验证，而无需披露任何密钥。PVMark取决于水印检测“正确执行”的证明，在此基础上构建了一组ZKP约束，包括映射、随机数生成、比较和总和。我们在Python、Rust和Circom中实现了PVMark的多个变体，涵盖了三种水印方案、三种哈希函数和四种ZKP协议的组合，以表明我们的方法在各种情况下有效工作。根据实验结果，PVMark有效地实现了最先进的LLM水印方案的公开验证性，同时又不损害水印性能，有望在实践中部署。



## **12. Defending Multimodal Backdoored Models by Repulsive Visual Prompt Tuning**

通过排斥性视觉提示调整捍卫多模式后门模型 cs.CV

**SubmitDate**: 2025-10-30    [abs](http://arxiv.org/abs/2412.20392v4) [paper-pdf](http://arxiv.org/pdf/2412.20392v4)

**Authors**: Zhifang Zhang, Shuo He, Haobo Wang, Bingquan Shen, Lei Feng

**Abstract**: Multimodal contrastive learning models (e.g., CLIP) can learn high-quality representations from large-scale image-text datasets, while they exhibit significant vulnerabilities to backdoor attacks, raising serious safety concerns. In this paper, we reveal that CLIP's vulnerabilities primarily stem from its tendency to encode features beyond in-dataset predictive patterns, compromising its visual feature resistivity to input perturbations. This makes its encoded features highly susceptible to being reshaped by backdoor triggers. To address this challenge, we propose Repulsive Visual Prompt Tuning (RVPT), a novel defense approach that employs deep visual prompt tuning with a specially designed feature-repelling loss. Specifically, RVPT adversarially repels the encoded features from deeper layers while optimizing the standard cross-entropy loss, ensuring that only predictive features in downstream tasks are encoded, thereby enhancing CLIP's visual feature resistivity against input perturbations and mitigating its susceptibility to backdoor attacks. Unlike existing multimodal backdoor defense methods that typically require the availability of poisoned data or involve fine-tuning the entire model, RVPT leverages few-shot downstream clean samples and only tunes a small number of parameters. Empirical results demonstrate that RVPT tunes only 0.27\% of the parameters in CLIP, yet it significantly outperforms state-of-the-art defense methods, reducing the attack success rate from 89.70\% to 2.76\% against the most advanced multimodal attacks on ImageNet and effectively generalizes its defensive capabilities across multiple datasets.

摘要: 多模式对比学习模型（例如，CLIP）可以从大规模图像文本数据集中学习高质量的表示，同时它们对后门攻击表现出显着的漏洞，从而引发了严重的安全问题。在本文中，我们揭示了CLIP的漏洞主要源于它倾向于编码数据集内预测模式之外的特征，从而损害了其视觉特征对输入扰动的抵抗力。这使得其编码功能极易被后门触发器重塑。为了应对这一挑战，我们提出了排斥性视觉提示调整（RVPT），这是一种新型防御方法，采用深度视觉提示调整，并具有专门设计的特征排斥损失。具体来说，RVPT会从更深的层中反向排斥编码特征，同时优化标准交叉熵损失，确保仅对下游任务中的预测特征进行编码，从而增强CLIP的视觉特征对输入扰动的抵抗力，并减轻其对后门攻击的敏感性。与现有的多模式后门防御方法（通常需要有毒数据的可用性或涉及微调整个模型）不同，RVPT利用了少量下游干净样本，并且只调整了少量参数。经验结果表明，RVPT仅调整了CLIP中0.27%的参数，但它的性能显着优于最先进的防御方法，将针对ImageNet上最先进的多模式攻击的攻击成功率从89.70%降低到2.76%，并有效地将其防御能力推广到多个数据集。



## **13. Security Risk of Misalignment between Text and Image in Multi-modal Model**

多模式模型中文本与图像不对齐的安全风险 cs.CV

**SubmitDate**: 2025-10-30    [abs](http://arxiv.org/abs/2510.26105v1) [paper-pdf](http://arxiv.org/pdf/2510.26105v1)

**Authors**: Xiaosen Wang, Zhijin Ge, Shaokang Wang

**Abstract**: Despite the notable advancements and versatility of multi-modal diffusion models, such as text-to-image models, their susceptibility to adversarial inputs remains underexplored. Contrary to expectations, our investigations reveal that the alignment between textual and Image modalities in existing diffusion models is inadequate. This misalignment presents significant risks, especially in the generation of inappropriate or Not-Safe-For-Work (NSFW) content. To this end, we propose a novel attack called Prompt-Restricted Multi-modal Attack (PReMA) to manipulate the generated content by modifying the input image in conjunction with any specified prompt, without altering the prompt itself. PReMA is the first attack that manipulates model outputs by solely creating adversarial images, distinguishing itself from prior methods that primarily generate adversarial prompts to produce NSFW content. Consequently, PReMA poses a novel threat to the integrity of multi-modal diffusion models, particularly in image-editing applications that operate with fixed prompts. Comprehensive evaluations conducted on image inpainting and style transfer tasks across various models confirm the potent efficacy of PReMA.

摘要: 尽管文本到图像模型等多模式扩散模型取得了显着的进步和多功能性，但它们对对抗性输入的敏感性仍然没有得到充分的研究。与预期相反，我们的调查显示，现有扩散模型中文本和图像模式之间的一致性不足。这种错位带来了巨大的风险，特别是在生成不适当或不安全的工作（NSFW）内容时。为此，我们提出了一种名为预算限制多模式攻击（PReMA）的新型攻击，通过结合任何指定提示修改输入图像来操纵生成的内容，而不改变提示本身。PReMA是第一个仅通过创建对抗性图像来操纵模型输出的攻击，与主要生成对抗性提示以产生NSFW内容的现有方法区分开来。因此，PReMA对多模式扩散模型的完整性构成了新的威胁，特别是在使用固定提示操作的图像编辑应用程序中。对各种模型的图像修复和风格转移任务进行的全面评估证实了PReMA的强大功效。



## **14. ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio-Language Models**

ALMGGuard：安全捷径以及在哪里找到它们作为音频语言模型的护栏 cs.SD

Accepted to NeurIPS 2025

**SubmitDate**: 2025-10-30    [abs](http://arxiv.org/abs/2510.26096v1) [paper-pdf](http://arxiv.org/pdf/2510.26096v1)

**Authors**: Weifei Jin, Yuxin Cao, Junjie Su, Minhui Xue, Jie Hao, Ke Xu, Jin Song Dong, Derui Wang

**Abstract**: Recent advances in Audio-Language Models (ALMs) have significantly improved multimodal understanding capabilities. However, the introduction of the audio modality also brings new and unique vulnerability vectors. Previous studies have proposed jailbreak attacks that specifically target ALMs, revealing that defenses directly transferred from traditional audio adversarial attacks or text-based Large Language Model (LLM) jailbreaks are largely ineffective against these ALM-specific threats. To address this issue, we propose ALMGuard, the first defense framework tailored to ALMs. Based on the assumption that safety-aligned shortcuts naturally exist in ALMs, we design a method to identify universal Shortcut Activation Perturbations (SAPs) that serve as triggers that activate the safety shortcuts to safeguard ALMs at inference time. To better sift out effective triggers while preserving the model's utility on benign tasks, we further propose Mel-Gradient Sparse Mask (M-GSM), which restricts perturbations to Mel-frequency bins that are sensitive to jailbreaks but insensitive to speech understanding. Both theoretical analyses and empirical results demonstrate the robustness of our method against both seen and unseen attacks. Overall, \MethodName reduces the average success rate of advanced ALM-specific jailbreak attacks to 4.6% across four models, while maintaining comparable utility on benign benchmarks, establishing it as the new state of the art. Our code and data are available at https://github.com/WeifeiJin/ALMGuard.

摘要: 音频语言模型（ILM）的最新进展显着提高了多模式理解能力。然而，音频模式的引入也带来了新的独特漏洞载体。之前的研究提出了专门针对ILM的越狱攻击，揭示了直接从传统音频对抗攻击或基于文本的大型语言模型（LLM）越狱转移的防御对于这些ILM特定的威胁基本上无效。为了解决这个问题，我们提出了ALMGGuard，这是第一个针对ILM量身定制的防御框架。基于APM中自然存在安全对齐快捷方式这一假设，我们设计了一种方法来识别通用收件箱激活扰动（SAP），该扰动充当触发器，在推理时激活安全快捷方式以保护APM。为了更好地筛选出有效的触发器，同时保留模型对良性任务的实用性，我们进一步提出了Mel梯度稀疏屏蔽（M-GSM），它将扰动限制在对越狱敏感但对语音理解不敏感的Mel频率箱。理论分析和经验结果都证明了我们的方法对可见和不可见的攻击的鲁棒性。总体而言，\MethodName将四种模型中高级ILM特定越狱攻击的平均成功率降低至4.6%，同时在良性基准上保持了相当的实用性，使其成为最新的最新技术水平。我们的代码和数据可在https://github.com/WeifeiJin/ALMGuard上获取。



## **15. Chain-of-Scrutiny: Detecting Backdoor Attacks for Large Language Models**

审查链：检测大型语言模型的后门攻击 cs.CR

This paper has been accepted to ACL Findings 2025

**SubmitDate**: 2025-10-30    [abs](http://arxiv.org/abs/2406.05948v4) [paper-pdf](http://arxiv.org/pdf/2406.05948v4)

**Authors**: Xi Li, Ruofan Mao, Yusen Zhang, Renze Lou, Chen Wu, Jiaqi Wang

**Abstract**: Large Language Models (LLMs), especially those accessed via APIs, have demonstrated impressive capabilities across various domains. However, users without technical expertise often turn to (untrustworthy) third-party services, such as prompt engineering, to enhance their LLM experience, creating vulnerabilities to adversarial threats like backdoor attacks. Backdoor-compromised LLMs generate malicious outputs to users when inputs contain specific "triggers" set by attackers. Traditional defense strategies, originally designed for small-scale models, are impractical for API-accessible LLMs due to limited model access, high computational costs, and data requirements. To address these limitations, we propose Chain-of-Scrutiny (CoS) which leverages LLMs' unique reasoning abilities to mitigate backdoor attacks. It guides the LLM to generate reasoning steps for a given input and scrutinizes for consistency with the final output -- any inconsistencies indicating a potential attack. It is well-suited for the popular API-only LLM deployments, enabling detection at minimal cost and with little data. User-friendly and driven by natural language, it allows non-experts to perform the defense independently while maintaining transparency. We validate the effectiveness of CoS through extensive experiments on various tasks and LLMs, with results showing greater benefits for more powerful LLMs.

摘要: 大型语言模型（LLM），尤其是通过API访问的模型，在各个领域都表现出了令人印象深刻的能力。然而，没有技术专业知识的用户经常转向（不值得信赖的）第三方服务（例如即时工程）来增强他们的LLM体验，从而造成后门攻击等对抗威胁的漏洞。当输入包含攻击者设置的特定“触发器”时，受后门破坏的LLM会向用户生成恶意输出。传统的防御策略最初是为小规模模型设计的，但由于模型访问有限、计算成本高和数据要求，对于API可访问的LLM来说是不切实际的。为了解决这些限制，我们提出了审查链（CoS），它利用LLM独特的推理能力来减轻后门攻击。它指导LLM为给定输入生成推理步骤，并检查与最终输出的一致性--任何不一致都表明潜在的攻击。它非常适合流行的纯API LLM部署，能够以最低的成本和较少的数据进行检测。它用户友好并由自然语言驱动，允许非专家独立进行辩护，同时保持透明度。我们通过对各种任务和LLM的广泛实验来验证CoS的有效性，结果显示更强大的LLM可以带来更大的好处。



## **16. Boosting Generative Adversarial Transferability with Self-supervised Vision Transformer Features**

利用自监督视觉Transformer特征提升生成对抗可转移性 cs.CV

14 pages, 9 figures, accepted at ICCV 2025

**SubmitDate**: 2025-10-30    [abs](http://arxiv.org/abs/2506.21046v2) [paper-pdf](http://arxiv.org/pdf/2506.21046v2)

**Authors**: Shangbo Wu, Yu-an Tan, Ruinan Ma, Wencong Ma, Dehua Zhu, Yuanzhang Li

**Abstract**: The ability of deep neural networks (DNNs) come from extracting and interpreting features from the data provided. By exploiting intermediate features in DNNs instead of relying on hard labels, we craft adversarial perturbation that generalize more effectively, boosting black-box transferability. These features ubiquitously come from supervised learning in previous work. Inspired by the exceptional synergy between self-supervised learning and the Transformer architecture, this paper explores whether exploiting self-supervised Vision Transformer (ViT) representations can improve adversarial transferability. We present dSVA -- a generative dual self-supervised ViT features attack, that exploits both global structural features from contrastive learning (CL) and local textural features from masked image modeling (MIM), the self-supervised learning paradigm duo for ViTs. We design a novel generative training framework that incorporates a generator to create black-box adversarial examples, and strategies to train the generator by exploiting joint features and the attention mechanism of self-supervised ViTs. Our findings show that CL and MIM enable ViTs to attend to distinct feature tendencies, which, when exploited in tandem, boast great adversarial generalizability. By disrupting dual deep features distilled by self-supervised ViTs, we are rewarded with remarkable black-box transferability to models of various architectures that outperform state-of-the-arts. Code available at https://github.com/spencerwooo/dSVA.

摘要: 深度神经网络（DNN）的能力来自于从所提供的数据中提取和解释特征。通过利用DNN中的中间特征而不是依赖硬标签，我们设计了更有效地概括的对抗性扰动，从而提高了黑匣子的可移植性。这些功能普遍来自之前工作中的监督学习。受自我监督学习和Transformer架构之间卓越协同作用的启发，本文探讨了利用自我监督Vision Transformer（ViT）表示是否可以提高对抗性可移植性。我们提出了dSVA --一种生成式双重自我监督ViT特征攻击，它利用来自对比学习（CL）的全局结构特征和来自掩蔽图像建模（TIM）的局部纹理特征，这是ViT的自我监督学习范式二人组。我们设计了一个新颖的生成式训练框架，其中包含一个生成器来创建黑匣子对抗示例，以及通过利用联合特征和自我监督ViT的注意力机制来训练生成器的策略。我们的研究结果表明，CL和TIM使ViT能够关注不同的特征趋势，当协同利用时，这些特征趋势具有很强的对抗概括性。通过破坏由自我监督ViT提炼的双重深度特征，我们获得了出色的黑匣子可移植性，以转移到性能优于最新技术水平的各种架构的模型。代码可访问https://github.com/spencerwooo/dSVA。



## **17. FGGM: Formal Grey-box Gradient Method for Attacking DRL-based MU-MIMO Scheduler**

FGGM：用于攻击基于DRL的MU-MMO的正式灰箱梯度方法 cs.NI

**SubmitDate**: 2025-10-30    [abs](http://arxiv.org/abs/2510.26075v1) [paper-pdf](http://arxiv.org/pdf/2510.26075v1)

**Authors**: Thanh Le, Hai Duong, Yusheng Ji, ThanhVu Nguyen, John C. S. Lui

**Abstract**: In 5G mobile communication systems, MU-MIMO has been applied to enhance spectral efficiency and support high data rates. To maximize spectral efficiency while providing fairness among users, the base station (BS) needs to selects a subset of users for data transmission. Given that this problem is NP-hard, DRL-based methods have been proposed to infer the near-optimal solutions in real-time, yet this approach has an intrinsic security problem. This paper investigates how a group of adversarial users can exploit unsanitized raw CSIs to launch a throughput degradation attack. Most existing studies only focused on systems in which adversarial users can obtain the exact values of victims' CSIs, but this is impractical in the case of uplink transmission in LTE/5G mobile systems. We note that the DRL policy contains an observation normalizer which has the mean and variance of the observation to improve training convergence. Adversarial users can then estimate the upper and lower bounds of the local observations including the CSIs of victims based solely on that observation normalizer. We develop an attacking scheme FGGM by leveraging polytope abstract domains, a technique used to bound the outputs of a neural network given the input ranges. Our goal is to find one set of intentionally manipulated CSIs which can achieve the attacking goals for the whole range of local observations of victims. Experimental results demonstrate that FGGM can determine a set of adversarial CSI vector controlled by adversarial users, then reuse those CSIs throughout the simulation to reduce the network throughput of a victim up to 70\% without knowing the exact value of victims' local observations. This study serves as a case study and can be applied to many other DRL-based problems, such as a knapsack-oriented resource allocation problems.

摘要: 在5G移动通信系统中，MU-MMO已被应用于提高频谱效率并支持高数据速率。为了最大限度地提高频谱效率，同时提供用户之间的公平性，基站（BS）需要选择用户子集进行数据传输。鉴于这个问题是NP难的，人们提出了基于DRL的方法来实时推断接近最优的解决方案，但这种方法存在固有的安全问题。本文研究了一组敌对用户如何利用未经消毒的原始CSC发起吞吐量下降攻击。大多数现有的研究仅关注对抗用户可以获得受害者CSI的精确值的系统，但这在LTE/5G移动系统中的上行链路传输的情况下是不切实际的。我们注意到，DRL策略包含一个观测归一化器，它具有观测的均值和方差，以提高训练收敛性。然后，敌对用户可以仅基于该观察归一化器来估计包括受害者的CSI在内的局部观察的上界和下界。我们开发了一个攻击计划FGGM利用多面体抽象域，一种用于绑定给定输入范围的神经网络的输出的技术。我们的目标是找到一组故意操纵的CSC，可以实现对受害者进行整个局部观察的攻击目标。实验结果表明，FGGM可以确定一组由对抗用户控制的对抗性SI载体，然后在整个模拟过程中重复使用这些CI，以在不知道受害者本地观察的确切值的情况下将受害者的网络吞吐量降低高达70%。本研究作为案例研究，可以应用于许多其他基于DRL的问题，例如面向背包的资源分配问题。



## **18. TextCrafter: Optimization-Calibrated Noise for Defending Against Text Embedding Inversion**

TextCrafter：用于防御文本嵌入倒置的优化校准噪音 cs.CR

More sufficient and convincing experiments are needed

**SubmitDate**: 2025-10-30    [abs](http://arxiv.org/abs/2509.17302v3) [paper-pdf](http://arxiv.org/pdf/2509.17302v3)

**Authors**: Duoxun Tang, Xinhang Jiang, Jiajun Niu

**Abstract**: Text embedding inversion attacks reconstruct original sentences from latent representations, posing severe privacy threats in collaborative inference and edge computing. We propose TextCrafter, an optimization-based adversarial perturbation mechanism that combines RL learned, geometry aware noise injection orthogonal to user embeddings with cluster priors and PII signal guidance to suppress inversion while preserving task utility. Unlike prior defenses either non learnable or agnostic to perturbation direction, TextCrafter provides a directional protective policy that balances privacy and utility. Under strong privacy setting, TextCrafter maintains 70 percentage classification accuracy on four datasets and consistently outperforms Gaussian/LDP baselines across lower privacy budgets, demonstrating a superior privacy utility trade off.

摘要: 文本嵌入倒置攻击从潜在表示中重建原始句子，在协作推理和边缘计算中构成严重的隐私威胁。我们提出了TextCrafter，这是一种基于优化的对抗性扰动机制，它将RL学习的、与用户嵌入垂直的几何感知噪音注入与集群先验和PRI信号引导相结合，以抑制倒置，同时保留任务效用。与之前的防御（无论是不可学习的还是对扰动方向不可知的）不同，TextCrafter提供了一种平衡隐私和实用性的定向保护政策。在强大的隐私设置下，TextCrafter在四个数据集上保持了70%的分类准确性，并且在较低的隐私预算下始终优于高斯/LDP基线，展示了卓越的隐私实用工具权衡。



## **19. Improving LLM Safety Alignment with Dual-Objective Optimization**

通过双目标优化改善LLM安全一致性 cs.CL

ICML 2025

**SubmitDate**: 2025-10-30    [abs](http://arxiv.org/abs/2503.03710v3) [paper-pdf](http://arxiv.org/pdf/2503.03710v3)

**Authors**: Xuandong Zhao, Will Cai, Tianneng Shi, David Huang, Licong Lin, Song Mei, Dawn Song

**Abstract**: Existing training-time safety alignment techniques for large language models (LLMs) remain vulnerable to jailbreak attacks. Direct preference optimization (DPO), a widely deployed alignment method, exhibits limitations in both experimental and theoretical contexts as its loss function proves suboptimal for refusal learning. Through gradient-based analysis, we identify these shortcomings and propose an improved safety alignment that disentangles DPO objectives into two components: (1) robust refusal training, which encourages refusal even when partial unsafe generations are produced, and (2) targeted unlearning of harmful knowledge. This approach significantly increases LLM robustness against a wide range of jailbreak attacks, including prefilling, suffix, and multi-turn attacks across both in-distribution and out-of-distribution scenarios. Furthermore, we introduce a method to emphasize critical refusal tokens by incorporating a reward-based token-level weighting mechanism for refusal learning, which further improves the robustness against adversarial exploits. Our research also suggests that robustness to jailbreak attacks is correlated with token distribution shifts in the training process and internal representations of refusal and harmful tokens, offering valuable directions for future research in LLM safety alignment. The code is available at https://github.com/wicai24/DOOR-Alignment

摘要: 现有的大型语言模型（LLM）训练时安全对齐技术仍然容易受到越狱攻击。直接偏好优化（DPO）是一种广泛应用的对齐方法，在实验和理论背景下都表现出局限性，因为其损失函数被证明对于拒绝学习来说次优。通过基于梯度的分析，我们发现了这些缺点，并提出了一种改进的安全调整，将DPO目标分解为两个部分：（1）稳健的拒绝训练，即使在产生部分不安全的世代时也鼓励拒绝，和（2）有针对性地忘记有害知识。这种方法显着提高了LLM针对各种越狱攻击的稳健性，包括跨分发和跨分发场景的预填充、后缀和多回合攻击。此外，我们引入了一种强调关键拒绝令牌的方法，通过结合基于奖励的令牌级加权机制进行拒绝学习，这进一步提高了针对对抗性利用的鲁棒性。我们的研究还表明，对越狱攻击的稳健性与训练过程中的代币分布变化以及拒绝和有害代币的内部表示相关，为LLM安全性调整的未来研究提供了有价值的方向。该代码可在https://github.com/wicai24/DOOR-Alignment上获取



## **20. SIRAJ: Diverse and Efficient Red-Teaming for LLM Agents via Distilled Structured Reasoning**

SIRAJ：通过蒸馏结构化推理为LLM代理提供多元化、高效的红色团队 cs.CR

**SubmitDate**: 2025-10-30    [abs](http://arxiv.org/abs/2510.26037v1) [paper-pdf](http://arxiv.org/pdf/2510.26037v1)

**Authors**: Kaiwen Zhou, Ahmed Elgohary, A S M Iftekhar, Amin Saied

**Abstract**: The ability of LLM agents to plan and invoke tools exposes them to new safety risks, making a comprehensive red-teaming system crucial for discovering vulnerabilities and ensuring their safe deployment. We present SIRAJ: a generic red-teaming framework for arbitrary black-box LLM agents. We employ a dynamic two-step process that starts with an agent definition and generates diverse seed test cases that cover various risk outcomes, tool-use trajectories, and risk sources. Then, it iteratively constructs and refines model-based adversarial attacks based on the execution trajectories of former attempts. To optimize the red-teaming cost, we present a model distillation approach that leverages structured forms of a teacher model's reasoning to train smaller models that are equally effective. Across diverse evaluation agent settings, our seed test case generation approach yields 2 -- 2.5x boost to the coverage of risk outcomes and tool-calling trajectories. Our distilled 8B red-teamer model improves attack success rate by 100%, surpassing the 671B Deepseek-R1 model. Our ablations and analyses validate the effectiveness of the iterative framework, structured reasoning, and the generalization of our red-teamer models.

摘要: LLM代理规划和调用工具的能力使他们面临新的安全风险，这使得全面的红色团队系统对于发现漏洞并确保其安全部署至关重要。我们介绍SIRAJ：用于任意黑匣子LLM代理的通用红团队框架。我们采用一个动态的两步流程，从代理定义开始，并生成涵盖各种风险结果、工具使用轨迹和风险源的多样化种子测试案例。然后，它根据之前尝试的执行轨迹迭代地构建和改进基于模型的对抗攻击。为了优化红色团队成本，我们提出了一种模型提炼方法，该方法利用教师模型推理的结构化形式来训练同样有效的较小模型。在不同的评估代理环境中，我们的种子测试案例生成方法将风险结果和工具调用轨迹的覆盖范围提高了2 - 2.5倍。我们提炼出的8B红团队模型将攻击成功率提高了100%，超过了671 B Deepseek-R1模型。我们的消融和分析验证了迭代框架、结构化推理和红团队模型概括的有效性。



## **21. Adversarial Paraphrasing: A Universal Attack for Humanizing AI-Generated Text**

对抗性重述：人工智能生成文本人性化的普遍攻击 cs.CL

NeurIPS 2025

**SubmitDate**: 2025-10-29    [abs](http://arxiv.org/abs/2506.07001v2) [paper-pdf](http://arxiv.org/pdf/2506.07001v2)

**Authors**: Yize Cheng, Vinu Sankar Sadasivan, Mehrdad Saberi, Shoumik Saha, Soheil Feizi

**Abstract**: The increasing capabilities of Large Language Models (LLMs) have raised concerns about their misuse in AI-generated plagiarism and social engineering. While various AI-generated text detectors have been proposed to mitigate these risks, many remain vulnerable to simple evasion techniques such as paraphrasing. However, recent detectors have shown greater robustness against such basic attacks. In this work, we introduce Adversarial Paraphrasing, a training-free attack framework that universally humanizes any AI-generated text to evade detection more effectively. Our approach leverages an off-the-shelf instruction-following LLM to paraphrase AI-generated content under the guidance of an AI text detector, producing adversarial examples that are specifically optimized to bypass detection. Extensive experiments show that our attack is both broadly effective and highly transferable across several detection systems. For instance, compared to simple paraphrasing attack--which, ironically, increases the true positive at 1% false positive (T@1%F) by 8.57% on RADAR and 15.03% on Fast-DetectGPT--adversarial paraphrasing, guided by OpenAI-RoBERTa-Large, reduces T@1%F by 64.49% on RADAR and a striking 98.96% on Fast-DetectGPT. Across a diverse set of detectors--including neural network-based, watermark-based, and zero-shot approaches--our attack achieves an average T@1%F reduction of 87.88% under the guidance of OpenAI-RoBERTa-Large. We also analyze the tradeoff between text quality and attack success to find that our method can significantly reduce detection rates, with mostly a slight degradation in text quality. Our adversarial setup highlights the need for more robust and resilient detection strategies in the light of increasingly sophisticated evasion techniques.

摘要: 大型语言模型（LLM）的能力不断增强，引发了人们对其在人工智能生成的抄袭和社会工程中滥用的担忧。虽然人们提出了各种人工智能生成的文本检测器来减轻这些风险，但许多文本检测器仍然容易受到简单规避技术（例如重述）的影响。然而，最近的检测器对此类基本攻击表现出更强的鲁棒性。在这项工作中，我们引入了对抗性重述，这是一种免训练的攻击框架，它普遍人性化任何人工智能生成的文本，以更有效地逃避检测。我们的方法利用现成的描述跟踪LLM在AI文本检测器的指导下解释AI生成的内容，生成经过专门优化以绕过检测的对抗性示例。大量实验表明，我们的攻击不仅广泛有效，而且在多个检测系统中高度可转移。例如，与简单的解释攻击相比--讽刺的是，它在RADART上将1%假阳性（T@1%F）的真阳性增加了8.57%，在Fast-DetectGPT上将15.03%--由OpenAI-RoBERTa-Large指导的对抗性解释在RADART上将T@1%F降低了64.49%，在Fast-DetectGPT上降低了98.96%。在一组不同的检测器中--包括基于神经网络、基于水印和零射击方法--在OpenAI-RoBERTa-Large的指导下，我们的攻击实现了T@1%F平均降低87.88%。我们还分析了文本质量和攻击成功之间的权衡，发现我们的方法可以显着降低检测率，但文本质量大多略有下降。鉴于日益复杂的规避技术，我们的对抗设置凸显了对更强大和更有弹性的检测策略的需求。



## **22. Distribution System Reconfiguration to Mitigate Load Altering Attacks via Stackelberg Games**

通过Stackelberg Games重新配置配电系统以减轻负载改变攻击 eess.SY

**SubmitDate**: 2025-10-29    [abs](http://arxiv.org/abs/2407.07065v6) [paper-pdf](http://arxiv.org/pdf/2407.07065v6)

**Authors**: Sajjad Maleki, E. Veronica Belmaga, Charalambos Konstantinou, Subhash Lakshminarayana

**Abstract**: The widespread integration of IoT-controllable devices (e.g., smart EV charging stations and heat pumps) into modern power systems enhances capabilities but introduces critical cybersecurity risks. Specifically, these devices are susceptible to load-altering attacks (LAAs) that can compromise power system safety. This paper quantifies the impact of LAAs on nodal voltage constraint violations in distribution networks (DNs). We first present closed-form expressions to analytically characterize LAA effects and quantify the minimum number of compromised devices for a successful LAA. Based on these insights, we propose a reactive defense mechanism that mitigates LAAs through DN reconfiguration. To address strategic adversaries, we then formulate defense strategies using a non-cooperative sequential game, which models the knowledgeable and strategic attacker, accounting for the worst-case scenario and enabling the reactive defender to devise an efficient and robust defense. Further, our formulation also accounts for uncertainties in attack localization. A novel Bayesian optimization approach is introduced to compute the Stackelberg equilibrium, significantly reducing computational burden efficiently. The game-theoretic strategy effectively mitigates the attack's impact while ensuring minimal system reconfiguration.

摘要: 物联网可控设备的广泛集成（例如，智能电动汽车充电站和热泵）融入现代电力系统可以增强功能，但也会带来严重的网络安全风险。具体来说，这些设备很容易受到负载改变攻击（LAA），从而危及电力系统安全。本文量化了LAA对配电网（DN）中节点电压约束违规的影响。我们首先提出封闭形式的表达来分析描述LAA效应，并量化成功LAA的受损设备的最少数量。基于这些见解，我们提出了一种反应式防御机制，通过DN重新配置减轻LAA。为了应对战略对手，我们然后使用非合作顺序博弈来制定防御策略，该博弈对知识渊博且具有战略意义的攻击者进行建模，考虑到最坏的情况，并使反应型防御者能够设计出高效且稳健的防御。此外，我们的公式还考虑了攻击定位的不确定性。引入了一种新颖的Bayesian优化方法来计算Stackelberg均衡，有效地显着减少了计算负担。博弈论策略有效地减轻了攻击的影响，同时确保最少的系统重新配置。



## **23. Bob's Confetti: Phonetic Memorization Attacks in Music and Video Generation**

鲍勃的五彩纸屑：音乐和视频生成中的语音同步攻击 cs.SD

**SubmitDate**: 2025-10-29    [abs](http://arxiv.org/abs/2507.17937v3) [paper-pdf](http://arxiv.org/pdf/2507.17937v3)

**Authors**: Jaechul Roh, Zachary Novack, Yuefeng Peng, Niloofar Mireshghallah, Taylor Berg-Kirkpatrick, Amir Houmansadr

**Abstract**: Generative AI systems for music and video commonly use text-based filters to prevent the regurgitation of copyrighted material. We expose a fundamental flaw in this approach by introducing Adversarial PhoneTic Prompting (APT), a novel attack that bypasses these safeguards by exploiting phonetic memorization. The APT attack replaces iconic lyrics with homophonic but semantically unrelated alternatives (e.g., "mom's spaghetti" becomes "Bob's confetti"), preserving acoustic structure while altering meaning; we identify high-fidelity phonetic matches using CMU pronouncing dictionary. We demonstrate that leading Lyrics-to-Song (L2S) models like SUNO and YuE regenerate songs with striking melodic and rhythmic similarity to their copyrighted originals when prompted with these altered lyrics. More surprisingly, this vulnerability extends across modalities. When prompted with phonetically modified lyrics from a song, a Text-to-Video (T2V) model like Veo 3 reconstructs visual scenes from the original music video-including specific settings and character archetypes-despite the absence of any visual cues in the prompt. Our findings reveal that models memorize deep, structural patterns tied to acoustics, not just verbatim text. This phonetic-to-visual leakage represents a critical vulnerability in transcript-conditioned generative models, rendering simple copyright filters ineffective and raising urgent concerns about the secure deployment of multimodal AI systems. Demo examples are available at our project page (https://jrohsc.github.io/music_attack/).

摘要: 音乐和视频的生成人工智能系统通常使用基于文本的过滤器来防止受版权保护的材料的回流。我们通过引入对抗音素提取（APT）来暴露这种方法的一个根本缺陷，APT是一种通过利用语音记忆绕过这些保障措施的新型攻击。APT攻击用谐音但语义无关的替代方案取代标志性歌词（例如，“妈妈的意大利面”变成了“鲍勃的五彩纸屑”），在改变含义的同时保留了声学结构;我们使用CMU发音词典识别高保真语音匹配。我们证明，当提示这些修改后的歌词时，SUNO和YuE等领先的歌词转歌曲（L2 S）模型会以与受版权保护的原创歌曲具有惊人的旋律和节奏相似性。更令人惊讶的是，这种脆弱性延伸到各个模式中。当提示歌曲中经过语音修改的歌词时，Veo 3等文本转视频（T2 V）模型会从原始音乐视频中重建视觉场景-包括特定的设置和角色原型-尽管提示中没有任何视觉提示。我们的研究结果表明，模型会记住与声学相关的深层结构模式，而不仅仅是逐字逐句的文本。这种语音到视觉的泄露代表了转录条件生成模型中的一个关键漏洞，使简单的版权过滤器无效，并引发了对多模式人工智能系统安全部署的紧迫担忧。演示示例可在我们的项目页面（https：//jrohsc.github.io/music_attack/）上找到。



## **24. NetEcho: From Real-World Streaming Side-Channels to Full LLM Conversation Recovery**

NetEcho：从现实世界的流媒体副频道到完整的LLM对话恢复 cs.CR

**SubmitDate**: 2025-10-29    [abs](http://arxiv.org/abs/2510.25472v1) [paper-pdf](http://arxiv.org/pdf/2510.25472v1)

**Authors**: Zheng Zhang, Guanlong Wu, Sen Deng, Shuai Wang, Yinqian Zhang

**Abstract**: In the rapidly expanding landscape of Large Language Model (LLM) applications, real-time output streaming has become the dominant interaction paradigm. While this enhances user experience, recent research reveals that it exposes a non-trivial attack surface through network side-channels. Adversaries can exploit patterns in encrypted traffic to infer sensitive information and reconstruct private conversations. In response, LLM providers and third-party services are deploying defenses such as traffic padding and obfuscation to mitigate these vulnerabilities.   This paper starts by presenting a systematic analysis of contemporary side-channel defenses in mainstream LLM applications, with a focus on services from vendors like OpenAI and DeepSeek. We identify and examine seven representative deployment scenarios, each incorporating active/passive mitigation techniques. Despite these enhanced security measures, our investigation uncovers significant residual information that remains vulnerable to leakage within the network traffic.   Building on this discovery, we introduce NetEcho, a novel, LLM-based framework that comprehensively unleashes the network side-channel risks of today's LLM applications. NetEcho is designed to recover entire conversations -- including both user prompts and LLM responses -- directly from encrypted network traffic. It features a deliberate design that ensures high-fidelity text recovery, transferability across different deployment scenarios, and moderate operational cost. In our evaluations on medical and legal applications built upon leading models like DeepSeek-v3 and GPT-4o, NetEcho can recover avg $\sim$70\% information of each conversation, demonstrating a critical limitation in current defense mechanisms. We conclude by discussing the implications of our findings and proposing future directions for augmenting network traffic security.

摘要: 在大型语言模型（LLM）应用程序的迅速扩大中，实时输出流已成为主导的交互范式。虽然这增强了用户体验，但最近的研究表明，它通过网络侧渠道暴露了一个不平凡的攻击表面。对手可以利用加密流量中的模式来推断敏感信息并重建私人对话。作为回应，LLM提供商和第三方服务正在部署流量填充和混淆等防御措施来缓解这些漏洞。   本文首先对主流LLM应用程序中的当代侧通道防御进行了系统分析，重点关注OpenAI和DeepSeek等供应商的服务。我们确定并检查了七种代表性的部署场景，每种场景都结合了主动/被动缓解技术。尽管采取了这些增强的安全措施，我们的调查仍发现了大量残留信息，这些信息仍然容易在网络流量中泄露。   在这一发现的基础上，我们引入了NetEcho，这是一个基于LLM的新颖框架，可以全面释放当今LLM应用程序的网络侧通道风险。NetEcho旨在直接从加密的网络流量中恢复整个对话（包括用户提示和LLM响应）。它采用精心设计的设计，确保高保真文本恢复、不同部署场景之间的可移植性以及适度的运营成本。在我们对基于DeepSeek-v3和GPT-4 o等领先模型的医疗和法律应用程序的评估中，NetEcho可以恢复每次对话的平均$70%信息，这表明了当前防御机制的严重局限性。最后，我们讨论了我们研究结果的影响，并提出了增强网络流量安全的未来方向。



## **25. Timestamp Manipulation: Timestamp-based Nakamoto-style Blockchains are Vulnerable**

时间戳操纵：基于时间戳的中本风格区块链很脆弱 cs.CR

25 pages, 6 figures

**SubmitDate**: 2025-10-29    [abs](http://arxiv.org/abs/2505.05328v5) [paper-pdf](http://arxiv.org/pdf/2505.05328v5)

**Authors**: Junjie Hu, Sisi Duan

**Abstract**: Nakamoto consensus are the most widely adopted decentralized consensus mechanism in cryptocurrency systems. Since it was proposed in 2008, many studies have focused on analyzing its security. Most of them focus on maximizing the profit of the adversary. Examples include the selfish mining attack [FC '14] and the recent riskless uncle maker (RUM) attack [CCS '23]. In this work, we introduce the Staircase-Unrestricted Uncle Maker (SUUM), the first block withholding attack targeting the timestamp-based Nakamoto-style blockchain. Through block withholding, timestamp manipulation, and difficulty risk control, SUUM adversaries are capable of launching persistent attacks with zero cost and minimal difficulty risk characteristics, indefinitely exploiting rewards from honest participants. This creates a self-reinforcing cycle that threatens the security of blockchains. We conduct a comprehensive and systematic evaluation of SUUM, including the attack conditions, its impact on blockchains, and the difficulty risks. Finally, we further discuss four feasible mitigation measures against SUUM.

摘要: Nakamoto共识是加密货币系统中最广泛采用的去中心化共识机制。自2008年提出以来，许多研究都集中在分析其安全性上。他们中的大多数都专注于使对手的利润最大化。例子包括自私的采矿攻击[FC ' 14]和最近的无风险叔叔制造商（RUM）攻击[CS ' 23]。在这项工作中，我们介绍了Staircase-Unrestricted Uncle Maker（SUUM），这是针对基于时间戳的Nakamoto风格区块链的第一个阻止攻击。通过区块扣留、时间戳操纵和难度风险控制，SUUM对手能够以零成本和最小难度风险特征发起持续攻击，无限期地利用诚实参与者的回报。这造成了一个自我强化的循环，威胁区块链的安全。我们对SUUM进行了全面系统的评估，包括攻击条件，对区块链的影响以及难度风险。最后，我们进一步讨论了四个可行的缓解措施，对SUUM。



## **26. A Unified Bilevel Model for Adversarial Learning and A Case Study**

对抗学习的统一二层模型及案例研究 cs.LG

**SubmitDate**: 2025-10-29    [abs](http://arxiv.org/abs/2510.25121v1) [paper-pdf](http://arxiv.org/pdf/2510.25121v1)

**Authors**: Yutong Zheng, Qingna Li

**Abstract**: Adversarial learning has been attracting more and more attention thanks to the fast development of machine learning and artificial intelligence. However, due to the complicated structure of most machine learning models, the mechanism of adversarial attacks is not well interpreted. How to measure the effect of attack is still not quite clear. In this paper, we propose a unified bilevel model for adversarial learning. We further investigate the adversarial attack in clustering models and interpret it from data perturbation point of view. We reveal that when the data perturbation is relatively small, the clustering model is robust, whereas if it is relatively large, the clustering result changes, which leads to an attack. To measure the effect of attacks for clustering models, we analyse the well-definedness of the so-called $\delta$-measure, which can be used in the proposed bilevel model for adversarial learning of clustering models.

摘要: 随着机器学习和人工智能的快速发展，对抗性学习越来越受到关注。然而，由于大多数机器学习模型的结构复杂，对抗性攻击的机制没有得到很好的解释。如何衡量攻击的效果还不太清楚。在本文中，我们提出了一个统一的对抗学习二层模型。我们进一步研究了集群模型中的对抗攻击，并从数据扰动的角度对其进行解释。我们发现，当数据扰动相对较小时，集群模型是稳健的，而如果数据扰动相对较大，集群结果就会发生变化，从而导致攻击。为了衡量攻击对集群模型的影响，我们分析了所谓的$\delta$-测量的明确性，该测量可用于提出的二层模型，用于集群模型的对抗学习。



## **27. An Adversarial-Driven Experimental Study on Deep Learning for RF Fingerprinting**

对抗驱动的RF指纹深度学习实验研究 cs.CR

**SubmitDate**: 2025-10-28    [abs](http://arxiv.org/abs/2507.14109v2) [paper-pdf](http://arxiv.org/pdf/2507.14109v2)

**Authors**: Xinyu Cao, Bimal Adhikari, Shangqing Zhao, Jingxian Wu, Yanjun Pan

**Abstract**: Radio frequency (RF) fingerprinting, which extracts unique hardware imperfections of radio devices, has emerged as a promising physical-layer device identification mechanism in zero trust architectures and beyond 5G networks. In particular, deep learning (DL) methods have demonstrated state-of-the-art performance in this domain. However, existing approaches have primarily focused on enhancing system robustness against temporal and spatial variations in wireless environments, while the security vulnerabilities of these DL-based approaches have often been overlooked. In this work, we systematically investigate the security risks of DL-based RF fingerprinting systems through an adversarial-driven experimental analysis. We observe a consistent misclassification behavior for DL models under domain shifts, where a device is frequently misclassified as another specific one. Our analysis based on extensive real-world experiments demonstrates that this behavior can be exploited as an effective backdoor to enable external attackers to intrude into the system. Furthermore, we show that training DL models on raw received signals causes the models to entangle RF fingerprints with environmental and signal-pattern features, creating additional attack vectors that cannot be mitigated solely through post-processing security methods such as confidence thresholds.

摘要: 射频（RF）指纹识别提取无线电设备独特的硬件缺陷，已成为零信任架构和5G网络以外的一种有前途的物理层设备识别机制。特别是，深度学习（DL）方法在该领域展示了最先进的性能。然而，现有方法主要集中在增强系统针对无线环境中的时间和空间变化的鲁棒性，而这些基于DL的方法的安全漏洞常常被忽视。在这项工作中，我们通过对抗驱动的实验分析系统地调查了基于DL的RF指纹识别系统的安全风险。我们观察到DL模型在域转移下存在一致的错误分类行为，其中设备经常被错误分类为另一个特定设备。我们基于广泛的现实实验的分析表明，这种行为可以被利用为有效的后门，使外部攻击者能够入侵系统。此外，我们表明，在原始接收信号上训练DL模型会导致模型将RF指纹与环境和信号模式特征纠缠在一起，从而创建额外的攻击载体，这些攻击载体无法仅通过置信阈值等后处理安全方法来缓解。



## **28. Jailbreak Transferability Emerges from Shared Representations**

越狱可转让性源于共享表示 cs.LG

**SubmitDate**: 2025-10-28    [abs](http://arxiv.org/abs/2506.12913v2) [paper-pdf](http://arxiv.org/pdf/2506.12913v2)

**Authors**: Rico Angell, Jannik Brinkmann, He He

**Abstract**: Jailbreak transferability is the surprising phenomenon when an adversarial attack compromising one model also elicits harmful responses from other models. Despite widespread demonstrations, there is little consensus on why transfer is possible: is it a quirk of safety training, an artifact of model families, or a more fundamental property of representation learning? We present evidence that transferability emerges from shared representations rather than incidental flaws. Across 20 open-weight models and 33 jailbreak attacks, we find two factors that systematically shape transfer: (1) representational similarity under benign prompts, and (2) the strength of the jailbreak on the source model. To move beyond correlation, we show that deliberately increasing similarity through benign only distillation causally increases transfer. Our qualitative analyses reveal systematic transferability patterns across different types of jailbreaks. For example, persona-style jailbreaks transfer far more often than cipher-based prompts, consistent with the idea that natural-language attacks exploit models' shared representation space, whereas cipher-based attacks rely on idiosyncratic quirks that do not generalize. Together, these results reframe jailbreak transfer as a consequence of representation alignment rather than a fragile byproduct of safety training.

摘要: 当损害一个模型的对抗攻击同时也会引发其他模型的有害反应时，越狱可移植性是一种令人惊讶的现象。尽管进行了广泛的演示，但对于为什么可以转移几乎没有达成共识：这是安全培训的怪癖、模范家庭的产物，还是代表学习的更基本属性？我们提供的证据表明，可转让性源于共同的表示，而不是偶然的缺陷。在20个开放权重模型和33次越狱攻击中，我们发现了两个系统性影响转移的因素：（1）良性提示下的代表相似性，（2）越狱对源模型的强度。为了超越相关性，我们表明通过良性蒸馏故意增加相似性会导致转移。我们的定性分析揭示了不同类型越狱的系统性转移模式。例如，人物风格的越狱传输频率远高于基于密码的提示，这与自然语言攻击利用模型的共享表示空间的观点一致，而基于密码的攻击依赖于不概括的独特怪癖。总而言之，这些结果将越狱转移重新定义为代表一致的结果，而不是安全培训的脆弱副产品。



## **29. Cybersecurity AI Benchmark (CAIBench): A Meta-Benchmark for Evaluating Cybersecurity AI Agents**

网络安全人工智能基准（CAIBench）：评估网络安全人工智能代理的元基准 cs.CR

**SubmitDate**: 2025-10-28    [abs](http://arxiv.org/abs/2510.24317v1) [paper-pdf](http://arxiv.org/pdf/2510.24317v1)

**Authors**: María Sanz-Gómez, Víctor Mayoral-Vilches, Francesco Balassone, Luis Javier Navarrete-Lozano, Cristóbal R. J. Veas Chavez, Maite del Mundo de Torres

**Abstract**: Cybersecurity spans multiple interconnected domains, complicating the development of meaningful, labor-relevant benchmarks. Existing benchmarks assess isolated skills rather than integrated performance. We find that pre-trained knowledge of cybersecurity in LLMs does not imply attack and defense abilities, revealing a gap between knowledge and capability. To address this limitation, we present the Cybersecurity AI Benchmark (CAIBench), a modular meta-benchmark framework that allows evaluating LLM models and agents across offensive and defensive cybersecurity domains, taking a step towards meaningfully measuring their labor-relevance. CAIBench integrates five evaluation categories, covering over 10,000 instances: Jeopardy-style CTFs, Attack and Defense CTFs, Cyber Range exercises, knowledge benchmarks, and privacy assessments. Key novel contributions include systematic simultaneous offensive-defensive evaluation, robotics-focused cybersecurity challenges (RCTF2), and privacy-preserving performance assessment (CyberPII-Bench). Evaluation of state-of-the-art AI models reveals saturation on security knowledge metrics (~70\% success) but substantial degradation in multi-step adversarial (A\&D) scenarios (20-40\% success), or worse in robotic targets (22\% success). The combination of framework scaffolding and LLM model choice significantly impacts performance; we find that proper matches improve up to 2.6$\times$ variance in Attack and Defense CTFs. These results demonstrate a pronounced gap between conceptual knowledge and adaptive capability, emphasizing the need for a meta-benchmark.

摘要: 网络安全跨越多个相互关联的领域，使有意义的、与劳动力相关的基准的开发变得复杂。现有的基准评估孤立的技能而不是综合的绩效。我们发现，LLM中预先训练的网络安全知识并不意味着攻击和防御能力，这揭示了知识和能力之间的差距。为了解决这一局限性，我们提出了网络安全人工智能基准（CAIBench），这是一个模块化元基准框架，允许评估跨进攻性和防御性网络安全领域的LLM模型和代理，朝着有意义地衡量其劳动相关性迈出了一步。CAIBench集成了五个评估类别，涵盖10，000多个实例：《危险边缘》风格的CTF、攻击和防御CTF、网络范围练习、知识基准和隐私评估。主要的创新贡献包括系统性的同时进攻-防御评估、以机器人为中心的网络安全挑战（RCTF 2）和隐私保护性能评估（CyberPII-Bench）。对最先进人工智能模型的评估显示，安全知识指标已饱和（成功率~ 70%），但在多步对抗（A & D）场景中大幅下降（成功率为20- 40%），或者在机器人目标中更糟（成功率为22%）。框架脚手架和LLM模型选择的结合显着影响性能;我们发现适当的匹配可以提高攻击和防御CTF中高达2.6 $\times $方差。这些结果表明概念知识和适应能力之间存在明显差距，强调了元基准的必要性。



## **30. MixAT: Combining Continuous and Discrete Adversarial Training for LLMs**

MixAT：结合LLM的连续和离散对抗训练 cs.LG

Published at 39th Conference on Neural Information Processing Systems  (NeurIPS 2025)

**SubmitDate**: 2025-10-28    [abs](http://arxiv.org/abs/2505.16947v2) [paper-pdf](http://arxiv.org/pdf/2505.16947v2)

**Authors**: Csaba Dékány, Stefan Balauca, Robin Staab, Dimitar I. Dimitrov, Martin Vechev

**Abstract**: Despite recent efforts in Large Language Model (LLM) safety and alignment, current adversarial attacks on frontier LLMs can still consistently force harmful generations. Although adversarial training has been widely studied and shown to significantly improve the robustness of traditional machine learning models, its strengths and weaknesses in the context of LLMs are less understood. Specifically, while existing discrete adversarial attacks are effective at producing harmful content, training LLMs with concrete adversarial prompts is often computationally expensive, leading to reliance on continuous relaxations. At the same time, despite their effectiveness and generalization capabilities, training with continuous perturbations does not always capture the full spectrum of vulnerabilities exploited by discrete attacks. In this work, we aim to bridge this gap by introducing MixAT, a novel method that combines stronger discrete and faster continuous attacks during training. We rigorously evaluate MixAT across a wide spectrum of state-of-the-art attacks, proposing the At Least One Attack Success Rate (ALO-ASR) metric to capture the worst-case vulnerability of models. We show MixAT achieves substantially better robustness (ALO-ASR < 20%) compared to prior defenses (ALO-ASR > 50%), while maintaining a runtime comparable to methods based on continuous relaxations. We further analyze MixAT in realistic deployment settings, exploring how chat templates, quantization, low-rank adapters, and temperature affect both adversarial training and evaluation, revealing additional blind spots in current methodologies. Our results demonstrate that MixAT's discrete-continuous defense offers a principled and superior robustness-accuracy tradeoff with minimal computational overhead, highlighting its promise for building safer LLMs. We provide our code and models at https://github.com/insait-institute/MixAT.

摘要: 尽管最近在大型语言模型（LLM）的安全性和对齐方面做出了努力，但目前对前沿LLM的对抗性攻击仍然可以持续地迫使有害的世代。尽管对抗性训练已经被广泛研究，并被证明可以显着提高传统机器学习模型的鲁棒性，但它在LLM背景下的优势和劣势却鲜为人知。具体来说，虽然现有的离散对抗性攻击在产生有害内容方面是有效的，但用具体的对抗性提示训练LLM通常在计算上是昂贵的，导致对连续松弛的依赖。与此同时，尽管具有有效性和泛化能力，但连续扰动训练并不总是能够捕获离散攻击所利用的全部漏洞。在这项工作中，我们的目标是通过引入MixAT来弥合这一差距，MixAT是一种新颖的方法，在训练期间结合了更强的离散攻击和更快的连续攻击。我们对MixAT进行了广泛的最先进攻击，提出了至少一次攻击成功率（ALO-ASB）指标来捕捉模型的最坏情况漏洞。我们表明，与之前的防御（ALO-ASB> 50%）相比，MixAT实现了更好的鲁棒性（ALO-ASB < 20%），同时保持与基于连续松弛的方法相当的运行时间。我们进一步分析了现实部署环境中的MixAT，探索聊天模板、量化、低等级适配器和温度如何影响对抗训练和评估，从而揭示了当前方法中的其他盲点。我们的结果表明，MixAT的离散-连续防御以最小的计算负担提供了原则性且卓越的鲁棒性-准确性权衡，凸显了其构建更安全的LLM的承诺。我们在https://github.com/insait-institute/MixAT上提供我们的代码和模型。



## **31. Is It Certainly a Deepfake? Reliability Analysis in Detection & Generation Ecosystem**

难道真的是假的吗？检测与发电生态系统中的可靠性分析 cs.AI

Accepted for publication at the ICCV 2025 workshop - STREAM

**SubmitDate**: 2025-10-28    [abs](http://arxiv.org/abs/2509.17550v3) [paper-pdf](http://arxiv.org/pdf/2509.17550v3)

**Authors**: Neslihan Kose, Anthony Rhodes, Umur Aybars Ciftci, Ilke Demir

**Abstract**: As generative models are advancing in quality and quantity for creating synthetic content, deepfakes begin to cause online mistrust. Deepfake detectors are proposed to counter this effect, however, misuse of detectors claiming fake content as real or vice versa further fuels this misinformation problem. We present the first comprehensive uncertainty analysis of deepfake detectors, systematically investigating how generative artifacts influence prediction confidence. As reflected in detectors' responses, deepfake generators also contribute to this uncertainty as their generative residues vary, so we cross the uncertainty analysis of deepfake detectors and generators. Based on our observations, the uncertainty manifold holds enough consistent information to leverage uncertainty for deepfake source detection. Our approach leverages Bayesian Neural Networks and Monte Carlo dropout to quantify both aleatoric and epistemic uncertainties across diverse detector architectures. We evaluate uncertainty on two datasets with nine generators, with four blind and two biological detectors, compare different uncertainty methods, explore region- and pixel-based uncertainty, and conduct ablation studies. We conduct and analyze binary real/fake, multi-class real/fake, source detection, and leave-one-out experiments between the generator/detector combinations to share their generalization capability, model calibration, uncertainty, and robustness against adversarial attacks. We further introduce uncertainty maps that localize prediction confidence at the pixel level, revealing distinct patterns correlated with generator-specific artifacts. Our analysis provides critical insights for deploying reliable deepfake detection systems and establishes uncertainty quantification as a fundamental requirement for trustworthy synthetic media detection.

摘要: 随着生成模型在创建合成内容的质量和数量上不断进步，深度造假开始引起在线不信任。Deepfake检测器被提出来对抗这种影响，然而，滥用检测器将虚假内容称为真实内容或反之亦然，进一步加剧了这种错误信息问题。我们首次对Deepfake检测器进行了全面的不确定性分析，系统地研究生成伪影如何影响预测置信度。正如检测器的响应所反映的那样，Deepfake生成器也会导致这种不确定性，因为它们的生成残余有所不同，因此我们交叉了Deepfake检测器和生成器的不确定性分析。根据我们的观察，不确定性流形拥有足够的一致信息，可以利用不确定性进行深度伪造源检测。我们的方法利用Bayesian神经网络和Monte Carlo dropout来量化不同检测器架构中的任意性和认识性不确定性。我们评估了两个数据集的不确定性，该数据集具有九个发生器、四个盲探测器和两个生物探测器，比较不同的不确定性方法，探索基于区域和像素的不确定性，并进行消融研究。我们在生成器/检测器组合之间进行并分析二进制实/假、多类实/假、源检测和留一实验，以分享它们的概括能力、模型校准、不确定性和对抗性攻击的鲁棒性。我们进一步引入不确定性地图，在像素级定位预测的信心，揭示了不同的模式与发电机特定的文物。我们的分析为部署可靠的deepfake检测系统提供了重要见解，并将不确定性量化确定为可信合成媒体检测的基本要求。



## **32. Vanish into Thin Air: Cross-prompt Universal Adversarial Attacks for SAM2**

消失在稀薄的空气中：针对SAM 2的跨提示通用对抗攻击 cs.CV

Accepted by NeurIPS 2025

**SubmitDate**: 2025-10-28    [abs](http://arxiv.org/abs/2510.24195v1) [paper-pdf](http://arxiv.org/pdf/2510.24195v1)

**Authors**: Ziqi Zhou, Yifan Hu, Yufei Song, Zijing Li, Shengshan Hu, Leo Yu Zhang, Dezhong Yao, Long Zheng, Hai Jin

**Abstract**: Recent studies reveal the vulnerability of the image segmentation foundation model SAM to adversarial examples. Its successor, SAM2, has attracted significant attention due to its strong generalization capability in video segmentation. However, its robustness remains unexplored, and it is unclear whether existing attacks on SAM can be directly transferred to SAM2. In this paper, we first analyze the performance gap of existing attacks between SAM and SAM2 and highlight two key challenges arising from their architectural differences: directional guidance from the prompt and semantic entanglement across consecutive frames. To address these issues, we propose UAP-SAM2, the first cross-prompt universal adversarial attack against SAM2 driven by dual semantic deviation. For cross-prompt transferability, we begin by designing a target-scanning strategy that divides each frame into k regions, each randomly assigned a prompt, to reduce prompt dependency during optimization. For effectiveness, we design a dual semantic deviation framework that optimizes a UAP by distorting the semantics within the current frame and disrupting the semantic consistency across consecutive frames. Extensive experiments on six datasets across two segmentation tasks demonstrate the effectiveness of the proposed method for SAM2. The comparative results show that UAP-SAM2 significantly outperforms state-of-the-art (SOTA) attacks by a large margin.

摘要: 最近的研究揭示了图像分割基础模型Sam对对抗性示例的脆弱性。其继任者SAM 2因其在视频分割中强大的概括能力而受到广泛关注。然而，其稳健性仍有待探索，目前还不清楚对Sam的现有攻击是否可以直接转移到Sam 2。在本文中，我们首先分析了萨姆和萨姆2之间现有攻击的性能差距，并强调了它们的架构差异带来的两个关键挑战：来自提示的方向引导和连续帧之间的语义纠缠。为了解决这些问题，我们提出了UAP-SAM 2，这是第一个由双重语义偏差驱动的针对SAM 2的交叉提示通用对抗攻击。对于跨提示可移植性，我们首先设计一种目标扫描策略，将每个帧分为k个区域，每个区域随机分配一个提示，以减少优化期间的提示依赖性。为了有效性，我们设计了一个双重语义偏差框架，该框架通过扭曲当前帧内的语义并破坏连续帧之间的语义一致性来优化UAP。在两个分割任务中对六个数据集进行的广泛实验证明了所提出的SAM 2方法的有效性。比较结果表明，UAP-SAM 2明显优于最先进的（SOTA）攻击的大幅度。



## **33. Untargeted Jailbreak Attack**

无目标越狱攻击 cs.CR

**SubmitDate**: 2025-10-28    [abs](http://arxiv.org/abs/2510.02999v2) [paper-pdf](http://arxiv.org/pdf/2510.02999v2)

**Authors**: Xinzhe Huang, Wenjing Hu, Tianhang Zheng, Kedong Xiu, Xiaojun Jia, Di Wang, Zhan Qin, Kui Ren

**Abstract**: Existing gradient-based jailbreak attacks on Large Language Models (LLMs), such as Greedy Coordinate Gradient (GCG) and COLD-Attack, typically optimize adversarial suffixes to align the LLM output with a predefined target response. However, by restricting the optimization objective as inducing a predefined target, these methods inherently constrain the adversarial search space, which limit their overall attack efficacy. Furthermore, existing methods typically require a large number of optimization iterations to fulfill the large gap between the fixed target and the original model response, resulting in low attack efficiency.   To overcome the limitations of targeted jailbreak attacks, we propose the first gradient-based untargeted jailbreak attack (UJA), aiming to elicit an unsafe response without enforcing any predefined patterns. Specifically, we formulate an untargeted attack objective to maximize the unsafety probability of the LLM response, which can be quantified using a judge model. Since the objective is non-differentiable, we further decompose it into two differentiable sub-objectives for optimizing an optimal harmful response and the corresponding adversarial prompt, with a theoretical analysis to validate the decomposition. In contrast to targeted jailbreak attacks, UJA's unrestricted objective significantly expands the search space, enabling a more flexible and efficient exploration of LLM vulnerabilities.Extensive evaluations demonstrate that UJA can achieve over 80% attack success rates against recent safety-aligned LLMs with only 100 optimization iterations, outperforming the state-of-the-art gradient-based attacks such as I-GCG and COLD-Attack by over 20%.

摘要: 现有的对大型语言模型（LLM）的基于梯度的越狱攻击，例如贪婪协调梯度（GCG）和COLD-Attack，通常会优化对抗性后缀，以将LLM输出与预定义的目标响应保持一致。然而，通过将优化目标限制为诱导预定义的目标，这些方法本质上限制了对抗搜索空间，从而限制了其总体攻击功效。此外，现有方法通常需要大量的优化迭代来满足固定目标和原始模型响应之间的大差距，导致攻击效率低。   为了克服定向越狱攻击的局限性，我们提出了第一个基于梯度的非定向越狱攻击（UJA），旨在在不强制执行任何预定义模式的情况下引发不安全的响应。具体来说，我们制定了一个无针对性的攻击目标，以最大化LLM响应的不安全概率，该概率可以使用判断模型进行量化。由于目标是不可微的，因此我们进一步将其分解为两个可微的子目标，用于优化最佳有害反应和相应的对抗提示，并通过理论分析来验证分解。与有针对性的越狱攻击相比，UJA的无限制目标显着扩大了搜索空间，从而能够更灵活、更高效地探索LLM漏洞。广泛的评估表明，UJA只需100次优化迭代即可针对最近的安全一致LLM实现超过80%的攻击成功率，比I-GCG和COLD-Attack等最先进的基于梯度的攻击性能高出20%以上。



## **34. Learning to Attack: Uncovering Privacy Risks in Sequential Data Releases**

学会攻击：揭露连续数据发布中的隐私风险 cs.CR

**SubmitDate**: 2025-10-28    [abs](http://arxiv.org/abs/2510.24807v1) [paper-pdf](http://arxiv.org/pdf/2510.24807v1)

**Authors**: Ziyao Cui, Minxing Zhang, Jian Pei

**Abstract**: Privacy concerns have become increasingly critical in modern AI and data science applications, where sensitive information is collected, analyzed, and shared across diverse domains such as healthcare, finance, and mobility. While prior research has focused on protecting privacy in a single data release, many real-world systems operate under sequential or continuous data publishing, where the same or related data are released over time. Such sequential disclosures introduce new vulnerabilities, as temporal correlations across releases may enable adversaries to infer sensitive information that remains hidden in any individual release. In this paper, we investigate whether an attacker can compromise privacy in sequential data releases by exploiting dependencies between consecutive publications, even when each individual release satisfies standard privacy guarantees. To this end, we propose a novel attack model that captures these sequential dependencies by integrating a Hidden Markov Model with a reinforcement learning-based bi-directional inference mechanism. This enables the attacker to leverage both earlier and later observations in the sequence to infer private information. We instantiate our framework in the context of trajectory data, demonstrating how an adversary can recover sensitive locations from sequential mobility datasets. Extensive experiments on Geolife, Porto Taxi, and SynMob datasets show that our model consistently outperforms baseline approaches that treat each release independently. The results reveal a fundamental privacy risk inherent to sequential data publishing, where individually protected releases can collectively leak sensitive information when analyzed temporally. These findings underscore the need for new privacy-preserving frameworks that explicitly model temporal dependencies, such as time-aware differential privacy or sequential data obfuscation strategies.

摘要: 隐私问题在现代人工智能和数据科学应用中变得越来越重要，敏感信息是在医疗保健、金融和移动等不同领域收集、分析和共享的。虽然之前的研究重点是在单个数据发布中保护隐私，但许多现实世界的系统在顺序或连续数据发布下运行，其中相同或相关的数据会随着时间的推移而发布。此类连续披露引入了新的漏洞，因为版本之间的时间相关性可能使对手能够推断出隐藏在任何单个版本中的敏感信息。在本文中，我们研究攻击者是否可以通过利用连续发布之间的依赖性来损害连续数据发布中的隐私，即使每个单独的发布都满足标准隐私保证。为此，我们提出了一种新颖的攻击模型，该模型通过将隐马尔科夫模型与基于强化学习的双向推理机制集成来捕获这些顺序依赖关系。这使攻击者能够利用序列中早期和后期的观察来推断私人信息。我们在轨迹数据的上下文中实例化了我们的框架，展示了对手如何从顺序移动数据集中恢复敏感位置。对Geolife、Porto Taxi和SynMob数据集的广泛实验表明，我们的模型始终优于独立处理每个版本的基线方法。结果揭示了顺序数据发布固有的基本隐私风险，其中单独受保护的发布在进行临时分析时可能会集体泄露敏感信息。这些发现强调了对新的隐私保护框架的需求，这些框架可以显式地建模时间依赖性，例如时间感知的差异隐私或顺序数据混淆策略。



## **35. Enhancing CLIP Robustness via Cross-Modality Alignment**

通过跨模式对齐增强CLIP稳健性 cs.CV

NeurIPS 2025 Spotlight

**SubmitDate**: 2025-10-28    [abs](http://arxiv.org/abs/2510.24038v1) [paper-pdf](http://arxiv.org/pdf/2510.24038v1)

**Authors**: Xingyu Zhu, Beier Zhu, Shuo Wang, Kesen Zhao, Hanwang Zhang

**Abstract**: Vision-language models (VLMs) such as CLIP demonstrate strong generalization in zero-shot classification but remain highly vulnerable to adversarial perturbations. Existing methods primarily focus on adversarial fine-tuning or prompt optimization; they often overlook the gaps in CLIP's encoded features, which is shown as the text and image features lie far apart from each other. This misalignment is significantly amplified under adversarial perturbations, leading to severe degradation in classification performance. To address this problem, we propose Cross-modality Alignment, dubbed COLA, an optimal transport-based framework that explicitly addresses adversarial misalignment by restoring both global image-text alignment and local structural consistency in the feature space. (1) COLA first projects adversarial image embeddings onto a subspace spanned by class text features, effectively filtering out non-semantic distortions while preserving discriminative information. (2) It then models images and texts as discrete distributions over multiple augmented views and refines their alignment via OT, with the subspace projection seamlessly integrated into the cost computation. This design ensures stable cross-modal alignment even under adversarial conditions. COLA is training-free and compatible with existing fine-tuned models. Extensive evaluations across 14 zero-shot classification benchmarks demonstrate the effectiveness of COLA, especially with an average improvement of 6.7% on ImageNet and its variants under PGD adversarial attacks, while maintaining high accuracy on clean samples.

摘要: CLIP等视觉语言模型（VLM）在零镜头分类中表现出很强的概括性，但仍然极易受到对抗性扰动的影响。现有的方法主要关注于对抗性微调或即时优化;它们经常忽视CLIP编码特征中的差距，该差距表现为文本和图像特征彼此相距很远。这种不对准在对抗性扰动下被显着放大，导致分类性能严重下降。为了解决这个问题，我们提出了跨模式对齐（称为COLA），这是一个基于传输的最佳框架，通过恢复特征空间中的全局图像-文本对齐和局部结构一致性来明确解决对抗性失调。(1)COLA首先将对抗性图像嵌入投影到由类文本特征跨越的子空间上，有效地过滤掉非语义失真，同时保留区分性信息。(2)然后，它将图像和文本建模为多个增强视图上的离散分布，并通过OT细化它们的对齐，并将子空间投影无缝集成到成本计算中。这种设计即使在对抗条件下也能确保稳定的跨模式对齐。COLA无需培训，并与现有的微调型号兼容。对14个零镜头分类基准的广泛评估证明了COLA的有效性，特别是在PVD对抗攻击下ImageNet及其变体的平均改进了6.7%，同时在干净样本上保持了高准确性。



## **36. A Volumetric Privacy Measure for Dynamical Systems With Bounded Disturbance**

具有有限干扰的动态系统的体积隐私测量 eess.SY

**SubmitDate**: 2025-10-28    [abs](http://arxiv.org/abs/2501.02893v5) [paper-pdf](http://arxiv.org/pdf/2501.02893v5)

**Authors**: Chuanghong Weng, Ehsan Nekouei

**Abstract**: In this paper, we first present a volumetric privacy measure for dynamical systems with bounded disturbances, wherein the states of the system contain private information and an adversary with access to sensor measurements attempts to infer the set of potential values of the private information. Under the proposed privacy measure, the volume of the uncertainty set of the adversary given the sensor measurements is considered as the privacy level of the system. We next characteristic the time evolution of the proposed privacy measure and study its properties for a particular system with both public and private states, where a set containing the public state is shared as the observation. Approximate set-membership estimation techniques are developed to compute the private-state uncertainty set, and the properties of the privacy measure are analyzed, demonstrating that the uncertainty reduction of the adversary is bounded by the information gain from the observation set. Furthermore, an optimization-based privacy filter design problem is formulated, employing randomization and linear programming to enhance the privacy level. The effectiveness of the proposed approach is validated through a production-inventory case study. Results show that the optimal privacy filter significantly improves robustness against inference attacks and outperforms two baseline mechanisms based on additive noise and quantization.

摘要: 在本文中，我们首先针对具有有界干扰的动态系统提出了一种体积隐私测量，其中系统的状态包含私人信息，并且可以访问传感器测量结果的对手试图推断私人信息的潜在值集。根据提出的隐私测量，给定传感器测量结果的对手不确定性集的量被视为系统的隐私级别。接下来，我们描述了所提出的隐私测量的时间演变，并研究其在具有公共和私人状态的特定系统中的属性，其中包含公共状态的集合作为观察被共享。开发了近似集隶属度估计技术来计算私人状态不确定性集，并分析了隐私度量的性质，证明了对手的不确定性降低受到观察集的信息收益的限制。此外，还提出了一个基于优化的隐私过滤器设计问题，采用随机化和线性规划来提高隐私级别。通过生产库存案例研究验证了所提出方法的有效性。结果表明，最佳隐私过滤器显着提高了对推理攻击的鲁棒性，并且优于基于添加性噪音和量化的两种基线机制。



## **37. Modeling Object Attention in Mobile AR for Intrinsic Cognitive Security**

基于内在认知安全的移动AR中目标注意力建模 cs.HC

Conference Paper, 5 pages. Published at the 2025 ACM the  International Symposium on Theory, Algorithmic Foundations, and Protocol  Design for Mobile Networks and Mobile Computing (MobiHoc)

**SubmitDate**: 2025-10-28    [abs](http://arxiv.org/abs/2510.24004v1) [paper-pdf](http://arxiv.org/pdf/2510.24004v1)

**Authors**: Shane Dirksen, Radha Kumaran, You-Jin Kim, Yilin Wang, Tobias Höllerer

**Abstract**: We study attention in mobile Augmented Reality (AR) using object recall as a proxy outcome. We observe that the ability to recall an object (physical or virtual) that was encountered in a mobile AR experience depends on many possible impact factors and attributes, with some objects being readily recalled while others are not, and some people recalling objects overall much better or worse than others. This opens up a potential cognitive attack in which adversaries might create conditions that make an AR user not recall certain potentially mission-critical objects. We explore whether a calibrated predictor of object recall can help shield against such cognitive attacks. We pool data from four mobile AR studies (with a total of 1,152 object recall probes) and fit a Partial Least Squares Structural Equation Model (PLS-SEM) with formative Object, Scene, and User State composites predicting recall, also benchmarking against Random Forest and multilayer perceptron classifiers. PLS-SEM attains the best F1 score in three of four studies. Additionally, path estimates identify lighting, augmentation density, AR registration stability, cognitive load, and AR familiarity as primary drivers. The model outputs per-object recall probabilities that can drive interface adjustments when predicted recall falls. Overall, PLS-SEM provides competitive accuracy with interpretable levers for design and evaluation in mobile AR.

摘要: 我们使用对象回忆作为代理结果来研究移动增强现实（AR）中的注意力。我们观察到，回忆起移动AR体验中遇到的对象（物理或虚拟）的能力取决于许多可能的影响因素和属性，有些对象很容易被回忆起，而另一些则不然，有些人回忆起对象总体上比其他人好得多或差得多。这引发了潜在的认知攻击，对手可能会创造条件，使AR用户无法回忆起某些潜在的关键任务对象。我们探讨了对象回忆的校准预测器是否可以帮助抵御此类认知攻击。我们汇集了来自四项移动AR研究的数据（共有1，152个对象召回探针），并拟合了偏最小二乘结构方程模型（PLS-SEM），其中包含预测召回的形成性对象，场景和用户状态复合材料，还对随机森林和多层感知器分类器进行了基准测试。PLS-SEM在四项研究中的三项中获得最佳F1评分。此外，路径估计将照明、增强密度、AR配准稳定性、认知负荷和AR熟悉度确定为主要驱动因素。该模型输出每个对象的召回概率，当预测召回下降时，可以驱动界面调整。总体而言，PLS-TEM为移动AR的设计和评估提供了有竞争力的准确性和可解释的杠杆。



## **38. Fortytwo: Swarm Inference with Peer-Ranked Consensus**

42：具有同行排名共识的群体推理 cs.LG

**SubmitDate**: 2025-10-27    [abs](http://arxiv.org/abs/2510.24801v1) [paper-pdf](http://arxiv.org/pdf/2510.24801v1)

**Authors**: Vladyslav Larin, Ihor Naumenko, Aleksei Ivashov, Ivan Nikitin, Alexander Firsov

**Abstract**: As centralized AI hits compute ceilings and diminishing returns from ever-larger training runs, meeting demand requires an inference layer that scales horizontally in both capacity and capability. We present Fortytwo, a novel protocol that leverages swarm intelligence principles and distributed pairwise ranking consensus to achieve superior performance in AI inference. Our approach reimagines collaboration among AI nodes using swarm inference: a peer-ranked, reputation-weighted consensus across heterogeneous models that surfaces the highest-quality responses. Using pairwise ranking with a custom Bradley-Terry-style aggregation model, we demonstrate that swarm inference substantially outperforms majority voting, achieving 85.90% on GPQA Diamond versus 68.69% for majority voting with the same model set - an improvement of +17.21 percentage points (approximately +25.1% relative). The protocol incorporates on-chain reputation so node influence adapts to demonstrated accuracy over time, yielding a meritocratic consensus that filters low-quality or malicious participants. To resist Sybil attacks, Fortytwo employs proof-of-capability in its consensus: nodes must successfully complete calibration/test requests and stake reputation to enter ranking rounds, making multi-identity attacks economically unattractive while preserving openness. Across six challenging benchmarks, including GPQA Diamond, LiveCodeBench, and AIME, our evaluation indicates higher accuracy and strong resilience to adversarial and noisy free-form prompting (e.g., prompt-injection degradation of only 0.12% versus 6.20% for a monolithic single-model baseline), while retaining practical deployability. Together, these results establish a foundation for decentralized AI systems - democratizing access to high-quality inference through collective intelligence without sacrificing reliability or security.

摘要: 随着集中式人工智能达到计算上限，并且越来越大规模的培训运行带来的回报不断减少，满足需求需要一个在容量和能力方面水平扩展的推理层。我们介绍了Fortytwo，这是一种新型协议，利用群体智能原则和分布式成对排名共识来实现人工智能推理的卓越性能。我们的方法使用群体推理重新构想人工智能节点之间的协作：跨异类模型的同行排名、声誉加权共识，以呈现最高质量的响应。使用自定义Bradley-Terry式聚合模型的成对排名，我们证明群体推理的表现大大优于多数投票，GPQA Diamond上的比例为85.90%，而使用相同模型集的多数投票为68.69%-提高了+17.21个百分点（相对约+25.1%）。该协议结合了链上声誉，以便随着时间的推移，节点影响力适应所证明的准确性，从而产生精英共识，过滤低质量或恶意参与者。为了抵抗Sybil攻击，Fortytwo在其共识中采用了能力证明：节点必须成功完成校准/测试请求并赢得声誉才能进入排名轮，这使得多身份攻击在经济上没有吸引力，同时保持开放性。在六个具有挑战性的基准测试中，包括GPQA Diamond、LiveCodeBench和AIME，我们的评估表明对对抗性和有噪音的自由形式提示具有更高的准确性和较强的弹性（例如，预算注入降级仅为0.12%，而整体单一型号基线为6.20%），同时保留了实际的可部署性。这些结果共同为去中心化人工智能系统奠定了基础--在不牺牲可靠性或安全性的情况下，通过集体智能实现高质量推理的民主化。



## **39. Secure Control of Connected and Autonomous Electrified Vehicles Under Adversarial Cyber-Attacks**

对抗性网络攻击下互联和自主电动汽车的安全控制 eess.SY

**SubmitDate**: 2025-10-27    [abs](http://arxiv.org/abs/2510.23922v1) [paper-pdf](http://arxiv.org/pdf/2510.23922v1)

**Authors**: Shashank Dhananjay Vyas, Satadru Dey

**Abstract**: Connected and Autonomous Electrified Vehicles (CAEV) is the solution to the future smart mobility having benefits of efficient traffic flow and cleaner environmental impact. Although CAEV has advantages they are still susceptible to adversarial cyber attacks due to their autonomous electric operation and the involved connectivity. To alleviate this issue, we propose a secure control architecture of CAEV. Particularly, we design an additional control input using Reinforcement Learning (RL) to be applied to the vehicle powertrain along with the input commanded by the battery. We present simulation case studies to demonstrate the potential of the proposed approach in keeping the CAEV platoon operating safely without collisions by curbing the effect of adversarial attacks.

摘要: 互联和自主电动汽车（CAEV）是未来智能出行的解决方案，具有高效的交通流量和更清洁的环境影响。尽管CAEV具有优势，但由于其自主电动操作和相关的连接性，它们仍然容易受到对抗性网络攻击。为了缓解这个问题，我们提出了CAEV的安全控制架构。特别是，我们使用强化学习（RL）设计了额外的控制输入，与电池命令的输入一起应用于车辆动力总成。我们提出了模拟案例研究，以证明所提出的方法通过遏制对抗攻击的影响来保持CAEV排安全运行而无碰撞的潜力。



## **40. Securing Transfer-Learned Networks with Reverse Homomorphic Encryption**

使用反向同形加密保护传输学习网络 cs.CR

added protection via RHE and black box attacks

**SubmitDate**: 2025-10-27    [abs](http://arxiv.org/abs/2505.14323v2) [paper-pdf](http://arxiv.org/pdf/2505.14323v2)

**Authors**: Robert Allison, Tomasz Maciążek, Henry Bourne

**Abstract**: The growing body of literature on training-data reconstruction attacks raises significant concerns about deploying neural network classifiers trained on sensitive data. However, differentially private (DP) training (e.g. using DP-SGD) can defend against such attacks with large training datasets causing only minimal loss of network utility. Folklore, heuristics, and (albeit pessimistic) DP bounds suggest this fails for networks trained with small per-class datasets, yet to the best of our knowledge the literature offers no compelling evidence. We directly demonstrate this vulnerability by significantly extending reconstruction attack capabilities under a realistic adversary threat model for few-shot transfer learned image classifiers. We design new white-box and black-box attacks and find that DP-SGD is unable to defend against these without significant classifier utility loss. To address this, we propose a novel homomorphic encryption (HE) method that protects training data without degrading model's accuracy. Conventional HE secures model's input data and requires costly homomorphic implementation of the entire classifier. In contrast, our new scheme is computationally efficient and protects training data rather than input data. This is achieved by means of a simple role-reversal where classifier input data is unencrypted but transfer-learned weights are encrypted. Classifier outputs remain encrypted, thus preventing both white-box and black-box (and any other) training-data reconstruction attacks. Under this new scheme only a trusted party with a private decryption key can obtain the classifier class decisions.

摘要: 越来越多的关于训练数据重建攻击的文献引发了人们对部署在敏感数据上训练的神经网络分类器的严重担忧。然而，差异私密（DP）训练（例如使用DP-BCD）可以通过大型训练数据集抵御此类攻击，只会导致最小的网络效用损失。民间传说、启发式和（尽管悲观）DP界限表明，对于使用每类较小数据集训练的网络来说，这是失败的，但据我们所知，文献没有提供令人信服的证据。我们通过在现实的对手威胁模型下显着扩展重建攻击能力来直接证明这一漏洞。我们设计了新的白盒和黑匣子攻击，发现DP-BCD无法在不造成重大分类器效用损失的情况下抵御这些攻击。为了解决这个问题，我们提出了一种新型的同质加密（HE）方法，该方法可以保护训练数据，而不会降低模型的准确性。传统的HE保护模型的输入数据，并且需要整个分类器的昂贵的homomorphic实现。相比之下，我们的新方案计算效率高，并且保护训练数据而不是输入数据。这是通过简单的角色倒置来实现的，其中分类器输入数据未加密，但转移学习权重被加密。分类器输出保持加密，从而防止白盒和黑匣子（以及任何其他）训练数据重建攻击。在这个新方案下，只有具有私有解密密钥的可信方才能获得分类器类别决策。



## **41. Apollo: A Posteriori Label-Only Membership Inference Attack Towards Machine Unlearning**

Apollo：针对机器取消学习的后验纯标签成员推理攻击 cs.LG

**SubmitDate**: 2025-10-27    [abs](http://arxiv.org/abs/2506.09923v2) [paper-pdf](http://arxiv.org/pdf/2506.09923v2)

**Authors**: Liou Tang, James Joshi, Ashish Kundu

**Abstract**: Machine Unlearning (MU) aims to update Machine Learning (ML) models following requests to remove training samples and their influences on a trained model efficiently without retraining the original ML model from scratch. While MU itself has been employed to provide privacy protection and regulatory compliance, it can also increase the attack surface of the model. Existing privacy inference attacks towards MU that aim to infer properties of the unlearned set rely on the weaker threat model that assumes the attacker has access to both the unlearned model and the original model, limiting their feasibility toward real-life scenarios. We propose a novel privacy attack, A Posteriori Label-Only Membership Inference Attack towards MU, Apollo, that infers whether a data sample has been unlearned, following a strict threat model where an adversary has access to the label-output of the unlearned model only. We demonstrate that our proposed attack, while requiring less access to the target model compared to previous attacks, can achieve relatively high precision on the membership status of the unlearned samples.

摘要: 机器非学习（MU）旨在根据请求更新机器学习（ML）模型，以有效地删除训练样本及其对训练模型的影响，而无需从头开始重新训练原始ML模型。虽然MU本身被用来提供隐私保护和法规遵从性，但它也可以增加模型的攻击面。针对MU的现有隐私推断攻击旨在推断未学习集的属性，依赖于较弱的威胁模型，该模型假设攻击者可以访问未学习模型和原始模型，从而限制了其对现实生活场景的可行性。我们提出了一种新的隐私攻击，一个后验标签只有成员资格推理攻击对MU，阿波罗，推断数据样本是否已被unlearned，以下严格的威胁模型，对手只能访问未学习模型的标签输出。我们证明，与之前的攻击相比，我们提出的攻击虽然需要更少的访问目标模型，但可以对未学习样本的成员身份状态实现相对高的精确度。



## **42. UNDREAM: Bridging Differentiable Rendering and Photorealistic Simulation for End-to-end Adversarial Attacks**

UNDREAM：为端到端对抗性攻击搭建差异渲染和真实感模拟的桥梁 cs.CR

**SubmitDate**: 2025-10-27    [abs](http://arxiv.org/abs/2510.16923v2) [paper-pdf](http://arxiv.org/pdf/2510.16923v2)

**Authors**: Mansi Phute, Matthew Hull, Haoran Wang, Alec Helbling, ShengYun Peng, Willian Lunardi, Martin Andreoni, Wenke Lee, Duen Horng Chau

**Abstract**: Deep learning models deployed in safety critical applications like autonomous driving use simulations to test their robustness against adversarial attacks in realistic conditions. However, these simulations are non-differentiable, forcing researchers to create attacks that do not integrate simulation environmental factors, reducing attack success. To address this limitation, we introduce UNDREAM, the first software framework that bridges the gap between photorealistic simulators and differentiable renderers to enable end-to-end optimization of adversarial perturbations on any 3D objects. UNDREAM enables manipulation of the environment by offering complete control over weather, lighting, backgrounds, camera angles, trajectories, and realistic human and object movements, thereby allowing the creation of diverse scenes. We showcase a wide array of distinct physically plausible adversarial objects that UNDREAM enables researchers to swiftly explore in different configurable environments. This combination of photorealistic simulation and differentiable optimization opens new avenues for advancing research of physical adversarial attacks.

摘要: 部署在自动驾驶等安全关键应用中的深度学习模型使用模拟来测试其在现实条件下对抗攻击的稳健性。然而，这些模拟是不可区分的，迫使研究人员创建不集成模拟环境因素的攻击，从而降低了攻击成功率。为了解决这一局限性，我们引入了UNDREAM，这是第一个弥合真实感模拟器和可区分渲染器之间差距的软件框架，能够对任何3D对象上的对抗性扰动进行端到端优化。UNDREAM通过对天气、灯光、背景、摄像机角度、轨迹以及真实的人和物体运动进行完全控制来操纵环境，从而允许创建多样化的场景。我们展示了各种不同的物理上看似合理的对抗对象，UNDREAM使研究人员能够在不同的可配置环境中快速探索。真实感模拟和可区分优化的结合为推进物理对抗攻击的研究开辟了新的途径。



## **43. On the Stability of Graph Convolutional Neural Networks: A Probabilistic Perspective**

图卷积神经网络的稳定性：概率的角度 cs.LG

**SubmitDate**: 2025-10-27    [abs](http://arxiv.org/abs/2506.01213v4) [paper-pdf](http://arxiv.org/pdf/2506.01213v4)

**Authors**: Ning Zhang, Henry Kenlay, Li Zhang, Mihai Cucuringu, Xiaowen Dong

**Abstract**: Graph convolutional neural networks (GCNNs) have emerged as powerful tools for analyzing graph-structured data, achieving remarkable success across diverse applications. However, the theoretical understanding of the stability of these models, i.e., their sensitivity to small changes in the graph structure, remains in rather limited settings, hampering the development and deployment of robust and trustworthy models in practice. To fill this gap, we study how perturbations in the graph topology affect GCNN outputs and propose a novel formulation for analyzing model stability. Unlike prior studies that focus only on worst-case perturbations, our distribution-aware formulation characterizes output perturbations across a broad range of input data. This way, our framework enables, for the first time, a probabilistic perspective on the interplay between the statistical properties of the node data and perturbations in the graph topology. We conduct extensive experiments to validate our theoretical findings and demonstrate their benefits over existing baselines, in terms of both representation stability and adversarial attacks on downstream tasks. Our results demonstrate the practical significance of the proposed formulation and highlight the importance of incorporating data distribution into stability analysis.

摘要: 图卷积神经网络（GCNN）已成为分析图结构数据的强大工具，在各种应用中取得了显着的成功。然而，对这些模型稳定性的理论理解，即，它们对图形结构中的微小变化的敏感性仍然在相当有限的环境中，阻碍了在实践中稳健且值得信赖的模型的开发和部署。为了填补这一空白，我们研究了图布局中的扰动如何影响GCNN输出，并提出了一种用于分析模型稳定性的新公式。与之前仅关注最坏情况扰动的研究不同，我们的分布感知公式描述了广泛输入数据中的输出扰动。通过这种方式，我们的框架首次能够从概率角度看待节点数据的统计属性和图布局中的扰动之间的相互作用。我们进行了广泛的实验来验证我们的理论发现，并展示它们在表示稳定性和对下游任务的对抗攻击方面相对于现有基线的优势。我们的结果证明了拟议公式的实际意义，并强调了将数据分布纳入稳定性分析的重要性。



## **44. Authentication Against Insecure Bootstrapping for 5G Networks: Feasibility, Resiliency, and Transitional Solutions in Post-Quantum Era**

针对5G网络不安全引导的认证：后量子时代的可行性、弹性和过渡解决方案 cs.CR

17 pages, 3 tables, 6 figures

**SubmitDate**: 2025-10-27    [abs](http://arxiv.org/abs/2510.23457v1) [paper-pdf](http://arxiv.org/pdf/2510.23457v1)

**Authors**: Saleh Darzi, Mirza Masfiqur Rahman, Imtiaz Karim, Rouzbeh Behnia, Attila A Yavuz, Elisa Bertino

**Abstract**: The 5G protocol lacks a robust base station authentication mechanism during the initial bootstrapping phase, leaving it susceptible to threats such as fake base station attacks. Conventional solutions, including digital signatures based on Public Key Infrastructures (PKIs) and identity-based signatures, are inadequate against quantum-capable adversaries. While integrating NIST's Post-Quantum Cryptography (PQC) standards is a leading approach for quantum resistance, their suitability for 5G base station authentication remains unexplored. Moreover, current solutions are predominantly centralized and lack security features such as distributed authentication. This work presents, to our knowledge, the first comprehensive network-level performance characterization of integrating NIST-PQC standards and conventional digital signatures (including threshold and identity-based schemes) into 5G base station authentication. Our findings reveal significant feasibility concerns, with direct PQC adoption hindered by protocol constraints and large signature sizes. We also highlight the performance limitations of conventional methods due to the overhead of certificate chains. To mitigate these challenges, we propose BORG, a transitional authentication solution based on a Hierarchical Identity-Based Threshold Signature scheme with a Fail-Stop property. BORG offers post-mortem post-quantum forgery detection and distributed trust via threshold and compact signatures, well-suited for 5G's stringent requirements. Our performance analysis underscores an important warning on the infeasibility of direct PQC integration and positions BORG as an effective transitional solution toward future quantum-resilient 5G authentication.

摘要: 5G协议在初始引导阶段缺乏强大的基站认证机制，容易受到假基站攻击等威胁。传统的解决方案，包括基于公钥结构（PKI）的数字签名和基于身份的签名，不足以对抗具有量子能力的对手。虽然集成NIH的后量子密码学（PQC）标准是量子抵抗的领先方法，但其对5G基站认证的适用性仍有待探索。此外，当前的解决方案主要是集中式的，缺乏分布式身份验证等安全功能。据我们所知，这项工作提出了将NIST-PQC标准和传统数字签名（包括阈值和基于身份的方案）集成到5G基站认证中的第一个全面的网络级性能表征。我们的研究结果揭示了重大的可行性问题，直接PQC采用协议约束和大签名大小的阻碍。我们还强调了传统方法由于证书链的开销而导致的性能限制。为了缓解这些挑战，我们提出了BORG，一个过渡的认证解决方案的基础上的分层的基于身份的门限签名方案的失败停止属性。BORG通过阈值和紧凑签名提供事后量子伪造检测和分布式信任，非常适合5G的严格要求。我们的性能分析强调了直接PQC集成不可行的重要警告，并将BORG定位为未来量子弹性5G认证的有效过渡解决方案。



## **45. Generalization Bounds for Robust Contrastive Learning: From Theory to Practice**

稳健对比学习的概括界限：从理论到实践 cs.LG

13 pages, 1 figure, 4 tables

**SubmitDate**: 2025-10-27    [abs](http://arxiv.org/abs/2311.09671v2) [paper-pdf](http://arxiv.org/pdf/2311.09671v2)

**Authors**: Ngoc N. Tran, Lam Tran, Hoang Phan, Anh Bui, Tung Pham, Toan Tran, Dinh Phung, Trung Le

**Abstract**: Contrastive Learning first extracts features from unlabeled data, followed by linear probing with labeled data. Adversarial Contrastive Learning (ACL) integrates Adversarial Training into the first phase to enhance feature robustness against attacks in the probing phase. While ACL has shown strong empirical results, its theoretical understanding remains limited. Furthermore, while a fair amount of theoretical works analyze how the unsupervised loss can support the supervised loss in the probing phase, none has examined its role to the robust supervised loss. To fill this gap, our work develops rigorous theories to identify which components in the unsupervised training can help improve the robust supervised loss. Specifically, besides the adversarial contrastive loss, we reveal that the benign one, along with a global divergence between benign and adversarial examples can also improve robustness. Proper experiments are conducted to justify our findings.

摘要: 对比学习首先从未标记的数据中提取特征，然后使用标记的数据进行线性探测。对抗性对比学习（ACL）将对抗性训练集成到第一阶段，以增强特征针对探测阶段攻击的稳健性。虽然ACL表现出了强有力的实证结果，但其理论理解仍然有限。此外，虽然大量理论著作分析了无监督损失如何在探测阶段支持监督损失，但没有人研究过它对稳健监督损失的作用。为了填补这一空白，我们的工作开发了严格的理论来确定无监督训练中的哪些组件可以帮助改善稳健的监督损失。具体来说，除了对抗性对比损失之外，我们还发现良性对比损失以及良性和对抗性示例之间的全球差异也可以提高稳健性。进行适当的实验来证明我们的发现的合理性。



## **46. Attention! Your Vision Language Model Could Be Maliciously Manipulated**

注意！您的视觉语言模型可能被恶意操纵 cs.CV

NeurIPS 2025

**SubmitDate**: 2025-10-27    [abs](http://arxiv.org/abs/2505.19911v2) [paper-pdf](http://arxiv.org/pdf/2505.19911v2)

**Authors**: Xiaosen Wang, Shaokang Wang, Zhijin Ge, Yuyang Luo, Shudong Zhang

**Abstract**: Large Vision-Language Models (VLMs) have achieved remarkable success in understanding complex real-world scenarios and supporting data-driven decision-making processes. However, VLMs exhibit significant vulnerability against adversarial examples, either text or image, which can lead to various adversarial outcomes, e.g., jailbreaking, hijacking, and hallucination, etc. In this work, we empirically and theoretically demonstrate that VLMs are particularly susceptible to image-based adversarial examples, where imperceptible perturbations can precisely manipulate each output token. To this end, we propose a novel attack called Vision-language model Manipulation Attack (VMA), which integrates first-order and second-order momentum optimization techniques with a differentiable transformation mechanism to effectively optimize the adversarial perturbation. Notably, VMA can be a double-edged sword: it can be leveraged to implement various attacks, such as jailbreaking, hijacking, privacy breaches, Denial-of-Service, and the generation of sponge examples, etc, while simultaneously enabling the injection of watermarks for copyright protection. Extensive empirical evaluations substantiate the efficacy and generalizability of VMA across diverse scenarios and datasets. Code is available at https://github.com/Trustworthy-AI-Group/VMA.

摘要: 大型视觉语言模型（VLM）在理解复杂的现实世界场景和支持数据驱动的决策流程方面取得了显着的成功。然而，VLM对对抗性示例（无论是文本还是图像）表现出显着的脆弱性，这可能会导致各种对抗性结果，例如越狱、劫持和幻觉等。在这项工作中，我们从经验和理论上证明了VLM特别容易受到基于图像的对抗示例的影响，其中不可感知的扰动可以精确地操纵每个输出令牌。为此，我们提出了一种名为视觉语言模型操纵攻击（VMA）的新型攻击，该攻击将一阶和二阶动量优化技术与可微转换机制集成在一起，以有效地优化对抗性扰动。值得注意的是，VMA可以是一把双刃剑：它可以被用来实施各种攻击，例如越狱、劫持、隐私泄露、拒绝服务和海绵示例的生成等，同时允许注入水印以进行版权保护。广泛的实证评估证实了VMA在不同场景和数据集中的有效性和普遍性。代码可在https://github.com/Trustworthy-AI-Group/VMA上获取。



## **47. Exploring Semantic-constrained Adversarial Example with Instruction Uncertainty Reduction**

通过减少教学不确定性探索语义约束对抗示例 cs.AI

NeurIPS 2025

**SubmitDate**: 2025-10-27    [abs](http://arxiv.org/abs/2510.22981v1) [paper-pdf](http://arxiv.org/pdf/2510.22981v1)

**Authors**: Jin Hu, Jiakai Wang, Linna Jing, Haolin Li, Haodong Liu, Haotong Qin, Aishan Liu, Ke Xu, Xianglong Liu

**Abstract**: Recently, semantically constrained adversarial examples (SemanticAE), which are directly generated from natural language instructions, have become a promising avenue for future research due to their flexible attacking forms. To generate SemanticAEs, current methods fall short of satisfactory attacking ability as the key underlying factors of semantic uncertainty in human instructions, such as referring diversity, descriptive incompleteness, and boundary ambiguity, have not been fully investigated. To tackle the issues, this paper develops a multi-dimensional instruction uncertainty reduction (InSUR) framework to generate more satisfactory SemanticAE, i.e., transferable, adaptive, and effective. Specifically, in the dimension of the sampling method, we propose the residual-driven attacking direction stabilization to alleviate the unstable adversarial optimization caused by the diversity of language references. By coarsely predicting the language-guided sampling process, the optimization process will be stabilized by the designed ResAdv-DDIM sampler, therefore releasing the transferable and robust adversarial capability of multi-step diffusion models. In task modeling, we propose the context-encoded attacking scenario constraint to supplement the missing knowledge from incomplete human instructions. Guidance masking and renderer integration are proposed to regulate the constraints of 2D/3D SemanticAE, activating stronger scenario-adapted attacks. Moreover, in the dimension of generator evaluation, we propose the semantic-abstracted attacking evaluation enhancement by clarifying the evaluation boundary, facilitating the development of more effective SemanticAE generators. Extensive experiments demonstrate the superiority of the transfer attack performance of InSUR. Moreover, we realize the reference-free generation of semantically constrained 3D adversarial examples for the first time.

摘要: 近年来，直接从自然语言指令生成的语义约束对抗示例（SemanticAE）因其灵活的攻击形式而成为未来研究的一个有希望的途径。为了生成SemanticAE，当前的方法缺乏令人满意的攻击能力，因为人类指令中语义不确定性的关键潜在因素，例如引用多样性、描述性不完整性和边界模糊性，尚未得到充分研究。为了解决这些问题，本文开发了一个多维指令不确定性减少（Insus）框架，以生成更满意的SemanticAE，即可转移、适应性和有效。具体来说，在抽样方法维度上，我们提出了剩余驱动的攻击方向稳定化，以缓解语言引用多样性造成的不稳定对抗优化。通过粗略预测语言引导的采样过程，优化过程将由设计的ResAdv-DDIM采样器稳定，从而释放多步扩散模型的可转移且鲁棒的对抗能力。在任务建模中，我们提出了上下文编码的攻击场景约束来补充不完整人类指令中缺失的知识。提出引导掩蔽和渲染器集成来调节2D/3D SemanticAE的约束，从而激活更强的自适应攻击。此外，在生成器评估维度上，我们通过明确评估边界提出了语义抽象的攻击评估增强，促进了更有效的SemanticAE生成器的开发。大量实验证明了Insus传输攻击性能的优越性。此外，我们首次实现了语义约束的3D对抗示例的无引用生成。



## **48. PersonaTeaming: Exploring How Introducing Personas Can Improve Automated AI Red-Teaming**

角色协作：探索引入角色协作如何改善自动化人工智能红色协作 cs.AI

**SubmitDate**: 2025-10-27    [abs](http://arxiv.org/abs/2509.03728v3) [paper-pdf](http://arxiv.org/pdf/2509.03728v3)

**Authors**: Wesley Hanwen Deng, Sunnie S. Y. Kim, Akshita Jha, Ken Holstein, Motahhare Eslami, Lauren Wilcox, Leon A Gatys

**Abstract**: Recent developments in AI governance and safety research have called for red-teaming methods that can effectively surface potential risks posed by AI models. Many of these calls have emphasized how the identities and backgrounds of red-teamers can shape their red-teaming strategies, and thus the kinds of risks they are likely to uncover. While automated red-teaming approaches promise to complement human red-teaming by enabling larger-scale exploration of model behavior, current approaches do not consider the role of identity. As an initial step towards incorporating people's background and identities in automated red-teaming, we develop and evaluate a novel method, PersonaTeaming, that introduces personas in the adversarial prompt generation process to explore a wider spectrum of adversarial strategies. In particular, we first introduce a methodology for mutating prompts based on either "red-teaming expert" personas or "regular AI user" personas. We then develop a dynamic persona-generating algorithm that automatically generates various persona types adaptive to different seed prompts. In addition, we develop a set of new metrics to explicitly measure the "mutation distance" to complement existing diversity measurements of adversarial prompts. Our experiments show promising improvements (up to 144.1%) in the attack success rates of adversarial prompts through persona mutation, while maintaining prompt diversity, compared to RainbowPlus, a state-of-the-art automated red-teaming method. We discuss the strengths and limitations of different persona types and mutation methods, shedding light on future opportunities to explore complementarities between automated and human red-teaming approaches.

摘要: 人工智能治理和安全研究的最新进展呼吁采取红色团队方法，以有效地揭示人工智能模型带来的潜在风险。其中许多电话都强调了红色团队成员的身份和背景如何影响他们的红色团队策略，从而也强调了他们可能发现的风险。虽然自动化红色团队方法有望通过更大规模地探索模型行为来补充人类红色团队，但目前的方法没有考虑身份的作用。作为将人们的背景和身份融入自动化红色团队的第一步，我们开发并评估了一种新型方法PersonaTeaming，该方法在对抗性提示生成过程中引入角色，以探索更广泛的对抗策略。特别是，我们首先引入了一种基于“红色团队专家”角色或“普通人工智能用户”角色来变异提示的方法。然后，我们开发了一个动态角色生成算法，该算法自动生成适应不同种子提示的各种角色类型。此外，我们开发了一组新的指标来明确测量“突变距离”，以补充现有的对抗提示多样性测量。我们的实验显示，与最先进的自动化红色团队方法RainbowPlus相比，通过角色突变的对抗提示的攻击成功率有了有希望的改进（高达144.1%），同时保持了提示的多样性。我们讨论了不同角色类型和突变方法的优点和局限性，揭示了未来探索自动化和人类红色团队方法之间互补性的机会。



## **49. CompressionAttack: Exploiting Prompt Compression as a New Attack Surface in LLM-Powered Agents**

CompressionAttack：利用即时压缩作为LLM支持的代理中的新攻击表面 cs.CR

**SubmitDate**: 2025-10-27    [abs](http://arxiv.org/abs/2510.22963v1) [paper-pdf](http://arxiv.org/pdf/2510.22963v1)

**Authors**: Zesen Liu, Zhixiang Zhang, Yuchong Xie, Dongdong She

**Abstract**: LLM-powered agents often use prompt compression to reduce inference costs, but this introduces a new security risk. Compression modules, which are optimized for efficiency rather than safety, can be manipulated by adversarial inputs, causing semantic drift and altering LLM behavior. This work identifies prompt compression as a novel attack surface and presents CompressionAttack, the first framework to exploit it. CompressionAttack includes two strategies: HardCom, which uses discrete adversarial edits for hard compression, and SoftCom, which performs latent-space perturbations for soft compression. Experiments on multiple LLMs show up to 80% attack success and 98% preference flips, while remaining highly stealthy and transferable. Case studies in VSCode Cline and Ollama confirm real-world impact, and current defenses prove ineffective, highlighting the need for stronger protections.

摘要: LLM支持的代理通常使用即时压缩来降低推理成本，但这会带来新的安全风险。压缩模块针对效率而不是安全性进行了优化，可以通过对抗输入来操纵，从而导致语义漂移并改变LLM行为。这项工作将即时压缩确定为一种新型攻击表面，并提出了第一个利用它的框架CompressionAttack。CompressionAttack包括两种策略：HardCom，使用离散对抗编辑进行硬压缩，以及SoftCom，为软压缩执行潜伏空间扰动。对多个LLM的实验显示，攻击成功率高达80%，偏好翻转率高达98%，同时保持高度隐蔽性和可转移性。VSCode Cline和Olama的案例研究证实了现实世界的影响，而当前的防御措施被证明无效，凸显了加强保护的必要性。



## **50. Your Compiler is Backdooring Your Model: Understanding and Exploiting Compilation Inconsistency Vulnerabilities in Deep Learning Compilers**

您的编译器正在为您的模型做后门：了解和利用深度学习编译器中的编译不一致漏洞 cs.CR

This paper is accepted to IEEE S&P 2026, the code is available at  https://github.com/SeekingDream/DLCompilerAttack

**SubmitDate**: 2025-10-27    [abs](http://arxiv.org/abs/2509.11173v3) [paper-pdf](http://arxiv.org/pdf/2509.11173v3)

**Authors**: Simin Chen, Jinjun Peng, Yixin He, Junfeng Yang, Baishakhi Ray

**Abstract**: Deep learning (DL) compilers are core infrastructure in modern DL systems, offering flexibility and scalability beyond vendor-specific libraries. This work uncovers a fundamental vulnerability in their design: can an official, unmodified compiler alter a model's semantics during compilation and introduce hidden backdoors? We study both adversarial and natural settings. In the adversarial case, we craft benign models where triggers have no effect pre-compilation but become effective backdoors after compilation. Tested on six models, three commercial compilers, and two hardware platforms, our attack yields 100% success on triggered inputs while preserving normal accuracy and remaining undetected by state-of-the-art detectors. The attack generalizes across compilers, hardware, and floating-point settings. In the natural setting, we analyze the top 100 HuggingFace models (including one with 220M+ downloads) and find natural triggers in 31 models. This shows that compilers can introduce risks even without adversarial manipulation.   Our results reveal an overlooked threat: unmodified DL compilers can silently alter model semantics. To our knowledge, this is the first work to expose inherent security risks in DL compiler design, opening a new direction for secure and trustworthy ML.

摘要: 深度学习（DL）编译器是现代DL系统的核心基础设施，提供超出供应商特定库的灵活性和可扩展性。这项工作揭示了他们设计中的一个根本漏洞：官方的、未经修改的编译器能否在编译期间改变模型的语义并引入隐藏的后门？我们研究对抗环境和自然环境。在对抗性的情况下，我们构建了良性模型，其中触发器对预编译没有影响，但在编译后成为有效的后门。经过六种型号、三种商业编译器和两个硬件平台的测试，我们的攻击在触发的输入上取得了100%的成功，同时保持正常的准确性并保持未被最先进的检测器检测到。这种攻击可以跨编译器、硬件和浮点设置进行推广。在自然环境中，我们分析了排名前100的HuggingFace模型（包括下载量超过2.2亿的模型），并在31个模型中找到自然触发因素。这表明，即使没有对抗性操纵，编译器也会引入风险。   我们的结果揭示了一个被忽视的威胁：未修改的DL编译器可以悄悄改变模型语义。据我们所知，这是第一个暴露DL编译器设计中固有安全风险的工作，为安全和可信的ML开辟了新的方向。



