# LLM / MLLM（语言模型） - 红队/越狱
**update at 2026-01-25 10:36:50**

按分类器置信度从高到低排序。

## **1. Jailbreaking LLMs Without Gradients or Priors: Effective and Transferable Attacks**

无需梯度或先验知识的LLM越狱攻击：高效且可迁移的攻击方法 cs.LG

**SubmitDate**: 2026-01-06    [abs](http://arxiv.org/abs/2601.03420v1) [paper-pdf](https://arxiv.org/pdf/2601.03420v1)

**Confidence**: 0.98

**Authors**: Zhakshylyk Nurlanov, Frank R. Schmidt, Florian Bernard

**Abstract**: As Large Language Models (LLMs) are increasingly deployed in safety-critical domains, rigorously evaluating their robustness against adversarial jailbreaks is essential. However, current safety evaluations often overestimate robustness because existing automated attacks are limited by restrictive assumptions. They typically rely on handcrafted priors or require white-box access for gradient propagation. We challenge these constraints by demonstrating that token-level iterative optimization can succeed without gradients or priors. We introduce RAILS (RAndom Iterative Local Search), a framework that operates solely on model logits. RAILS matches the effectiveness of gradient-based methods through two key innovations: a novel auto-regressive loss that enforces exact prefix matching, and a history-based selection strategy that bridges the gap between the proxy optimization objective and the true attack success rate. Crucially, by eliminating gradient dependency, RAILS enables cross-tokenizer ensemble attacks. This allows for the discovery of shared adversarial patterns that generalize across disjoint vocabularies, significantly enhancing transferability to closed-source systems. Empirically, RAILS achieves near 100% success rates on multiple open-source models and high black-box attack transferability to closed-source systems like GPT and Gemini.

摘要: 随着大语言模型（LLMs）在安全关键领域的日益广泛应用，严格评估其对抗越狱攻击的鲁棒性至关重要。然而，当前的安全评估往往高估了模型的鲁棒性，因为现有的自动化攻击方法受到限制性假设的制约。这些方法通常依赖于手工设计的先验知识，或需要白盒访问以进行梯度传播。我们通过证明无需梯度或先验知识的token级迭代优化也能成功发起攻击，挑战了这些限制。我们提出了RAILS（随机迭代局部搜索）框架，该框架仅基于模型logits进行操作。RAILS通过两项关键创新实现了与基于梯度方法相当的效果：一种新颖的自回归损失函数，强制实现精确前缀匹配；以及一种基于历史的选择策略，弥合了代理优化目标与真实攻击成功率之间的差距。重要的是，通过消除对梯度的依赖，RAILS实现了跨分词器的集成攻击。这使得我们能够发现跨不相关词汇表共享的对抗模式，显著增强了对闭源系统的攻击可迁移性。实验表明，RAILS在多个开源模型上实现了接近100%的成功率，并对GPT和Gemini等闭源系统具有很高的黑盒攻击可迁移性。



## **2. Multi-Turn Jailbreaking of Aligned LLMs via Lexical Anchor Tree Search**

基于词汇锚点树搜索的多轮对齐大语言模型越狱攻击 cs.CL

**SubmitDate**: 2026-01-06    [abs](http://arxiv.org/abs/2601.02670v1) [paper-pdf](https://arxiv.org/pdf/2601.02670v1)

**Confidence**: 0.98

**Authors**: Devang Kulshreshtha, Hang Su, Chinmay Hegde, Haohan Wang

**Abstract**: Most jailbreak methods achieve high attack success rates (ASR) but require attacker LLMs to craft adversarial queries and/or demand high query budgets. These resource limitations make jailbreaking expensive, and the queries generated by attacker LLMs often consist of non-interpretable random prefixes. This paper introduces Lexical Anchor Tree Search (), addressing these limitations through an attacker-LLM-free method that operates purely via lexical anchor injection. LATS reformulates jailbreaking as a breadth-first tree search over multi-turn dialogues, where each node incrementally injects missing content words from the attack goal into benign prompts. Evaluations on AdvBench and HarmBench demonstrate that LATS achieves 97-100% ASR on latest GPT, Claude, and Llama models with an average of only ~6.4 queries, compared to 20+ queries required by other methods. These results highlight conversational structure as a potent and under-protected attack surface, while demonstrating superior query efficiency in an era where high ASR is readily achievable. Our code will be released to support reproducibility.

摘要: 现有越狱方法虽能实现高攻击成功率，但需攻击者大语言模型生成对抗性查询且查询预算要求高。这些资源限制使越狱成本高昂，且攻击模型生成的查询常包含不可解释的随机前缀。本文提出词汇锚点树搜索方法，通过无需攻击者模型的纯词汇锚点注入方案突破上述限制。该方法将越狱重构为多轮对话的广度优先树搜索过程，每个节点逐步将攻击目标缺失的内容词注入良性提示。在AdvBench和HarmBench上的评估表明，相比其他方法需20+次查询，LATS对最新GPT、Claude和Llama模型仅需平均约6.4次查询即可实现97-100%的攻击成功率。这些结果揭示了对话结构作为强大且防护不足的攻击面，同时在高攻击成功率易实现的时代展现了卓越的查询效率。我们将公开代码以支持可复现性。



## **3. Connect the Dots: Knowledge Graph-Guided Crawler Attack on Retrieval-Augmented Generation Systems**

连接节点：基于知识图谱引导的检索增强生成系统爬虫攻击 cs.CR

**SubmitDate**: 2026-01-22    [abs](http://arxiv.org/abs/2601.15678v1) [paper-pdf](https://arxiv.org/pdf/2601.15678v1)

**Confidence**: 0.95

**Authors**: Mengyu Yao, Ziqi Zhang, Ning Luo, Shaofei Li, Yifeng Cai, Xiangqun Chen, Yao Guo, Ding Li

**Abstract**: Retrieval-augmented generation (RAG) systems integrate document retrieval with large language models and have been widely adopted. However, in privacy-related scenarios, RAG introduces a new privacy risk: adversaries can issue carefully crafted queries to exfiltrate sensitive content from the underlying corpus gradually. Although recent studies have demonstrated multi-turn extraction attacks, they rely on heuristics and fail to perform long-term extraction planning. To address these limitations, we formulate the RAG extraction attack as an adaptive stochastic coverage problem (ASCP). In ASCP, each query is treated as a probabilistic action that aims to maximize conditional marginal gain (CMG), enabling principled long-term planning under uncertainty. However, integrating ASCP with practical RAG attack faces three key challenges: unobservable CMG, intractability in the action space, and feasibility constraints. To overcome these challenges, we maintain a global attacker-side state to guide the attack. Building on this idea, we introduce RAGCRAWLER, which builds a knowledge graph to represent revealed information, uses this global state to estimate CMG, and plans queries in semantic space that target unretrieved regions. In comprehensive experiments across diverse RAG architectures and datasets, our proposed method, RAGCRAWLER, consistently outperforms all baselines. It achieves up to 84.4% corpus coverage within a fixed query budget and deliver an average improvement of 20.7% over the top-performing baseline. It also maintains high semantic fidelity and strong content reconstruction accuracy with low attack cost. Crucially, RAGCRAWLER proves its robustness by maintaining effectiveness against advanced RAG systems employing query rewriting and multi-query retrieval strategies. Our work reveals significant security gaps and highlights the pressing need for stronger safeguards for RAG.

摘要: 检索增强生成（RAG）系统将文档检索与大型语言模型相结合，已得到广泛应用。然而，在隐私相关场景中，RAG引入了新的隐私风险：攻击者可通过精心构造的查询逐步从底层语料库中窃取敏感内容。尽管近期研究已展示多轮提取攻击，但这些方法依赖启发式策略，无法进行长期提取规划。为解决这些局限，我们将RAG提取攻击形式化为自适应随机覆盖问题（ASCP）。在ASCP中，每个查询被视为以最大化条件边际增益（CMG）为目标的概率动作，从而在不确定性下实现原则性的长期规划。然而，将ASCP应用于实际RAG攻击面临三个关键挑战：CMG不可观测、动作空间计算不可行性以及可行性约束。为克服这些挑战，我们维护全局攻击方状态以引导攻击。基于此思路，我们提出RAGCRAWLER方法：构建知识图谱表示已泄露信息，利用该全局状态估计CMG，并在语义空间中针对未检索区域规划查询。在多种RAG架构和数据集上的综合实验中，RAGCRAWLER始终优于所有基线方法，在固定查询预算下实现高达84.4%的语料覆盖率，较最佳基线平均提升20.7%。同时保持高语义保真度与强内容重建准确性，且攻击成本较低。关键的是，RAGCRAWLER在面对采用查询重写和多查询检索策略的先进RAG系统时仍保持有效性，证明了其鲁棒性。本研究揭示了RAG系统的重大安全漏洞，并凸显了加强RAG安全防护的迫切需求。



## **4. VirtualCrime: Evaluating Criminal Potential of Large Language Models via Sandbox Simulation**

VirtualCrime：基于沙盒模拟评估大语言模型的犯罪潜力 cs.CR

**SubmitDate**: 2026-01-20    [abs](http://arxiv.org/abs/2601.13981v1) [paper-pdf](https://arxiv.org/pdf/2601.13981v1)

**Confidence**: 0.95

**Authors**: Yilin Tang, Yu Wang, Lanlan Qiu, Wenchang Gao, Yunfei Ma, Baicheng Chen, Tianxing He

**Abstract**: Large language models (LLMs) have shown strong capabilities in multi-step decision-making, planning and actions, and are increasingly integrated into various real-world applications. It is concerning whether their strong problem-solving abilities may be misused for crimes. To address this gap, we propose VirtualCrime, a sandbox simulation framework based on a three-agent system to evaluate the criminal capabilities of models. Specifically, this framework consists of an attacker agent acting as the leader of a criminal team, a judge agent determining the outcome of each action, and a world manager agent updating the environment state and entities. Furthermore, we design 40 diverse crime tasks within this framework, covering 11 maps and 13 crime objectives such as theft, robbery, kidnapping, and riot. We also introduce a human player baseline for reference to better interpret the performance of LLM agents. We evaluate 8 strong LLMs and find (1) All agents in the simulation environment compliantly generate detailed plans and execute intelligent crime processes, with some achieving relatively high success rates; (2) In some cases, agents take severe action that inflicts harm to NPCs to achieve their goals. Our work highlights the need for safety alignment when deploying agentic AI in real-world settings.

摘要: 大语言模型（LLMs）在多步决策、规划和行动方面展现出强大能力，正日益融入各类现实应用。其强大的问题解决能力是否可能被滥用于犯罪活动，这一问题令人担忧。为填补这一研究空白，我们提出VirtualCrime——一个基于三智能体系统的沙盒模拟框架，用于评估模型的犯罪能力。具体而言，该框架包含：作为犯罪团队领导者的攻击者智能体、判定每个行动结果的法官智能体，以及更新环境状态与实体的世界管理智能体。此外，我们在此框架内设计了40个多样化的犯罪任务，涵盖11个地图场景和盗窃、抢劫、绑架、暴乱等13类犯罪目标。我们还引入了人类玩家基线作为参照，以更好地解读LLM智能体的表现。通过对8个主流大语言模型的评估，我们发现：（1）所有模拟环境中的智能体均能按要求生成详细计划并执行智能犯罪流程，部分模型取得了较高的成功率；（2）在某些情况下，智能体会采取对非玩家角色造成严重伤害的极端行动以实现目标。本研究揭示了在现实场景中部署具身智能体时进行安全对齐的必要性。



## **5. RECAP: A Resource-Efficient Method for Adversarial Prompting in Large Language Models**

RECAP：一种面向大语言模型的资源高效对抗性提示方法 cs.CL

Code for RECAP is available at: https://github.com/R-C101/RECAP

**SubmitDate**: 2026-01-20    [abs](http://arxiv.org/abs/2601.15331v1) [paper-pdf](https://arxiv.org/pdf/2601.15331v1)

**Confidence**: 0.95

**Authors**: Rishit Chugh

**Abstract**: The deployment of large language models (LLMs) has raised security concerns due to their susceptibility to producing harmful or policy-violating outputs when exposed to adversarial prompts. While alignment and guardrails mitigate common misuse, they remain vulnerable to automated jailbreaking methods such as GCG, PEZ, and GBDA, which generate adversarial suffixes via training and gradient-based search. Although effective, these methods particularly GCG are computationally expensive, limiting their practicality for organisations with constrained resources. This paper introduces a resource-efficient adversarial prompting approach that eliminates the need for retraining by matching new prompts to a database of pre-trained adversarial prompts. A dataset of 1,000 prompts was classified into seven harm-related categories, and GCG, PEZ, and GBDA were evaluated on a Llama 3 8B model to identify the most effective attack method per category. Results reveal a correlation between prompt type and algorithm effectiveness. By retrieving semantically similar successful adversarial prompts, the proposed method achieves competitive attack success rates with significantly reduced computational cost. This work provides a practical framework for scalable red-teaming and security evaluation of aligned LLMs, including in settings where model internals are inaccessible.

摘要: 大语言模型（LLM）的部署引发了安全担忧，因为它们在面对对抗性提示时容易产生有害或违反策略的输出。尽管对齐技术和防护栏能缓解常见误用，但仍易受GCG、PEZ和GBDA等自动化越狱方法的攻击，这些方法通过训练和基于梯度的搜索生成对抗性后缀。虽然有效，但这些方法（尤其是GCG）计算成本高昂，限制了资源受限组织的实用性。本文提出一种资源高效的对抗性提示方法，通过将新提示与预训练对抗性提示数据库进行匹配，无需重新训练。我们将1,000个提示数据集分类为七个危害相关类别，并在Llama 3 8B模型上评估GCG、PEZ和GBDA，以确定每类最有效的攻击方法。结果显示提示类型与算法有效性之间存在相关性。通过检索语义相似的成功对抗性提示，所提方法以显著降低的计算成本实现了具有竞争力的攻击成功率。这项工作为对齐LLM的可扩展红队测试和安全评估提供了实用框架，包括模型内部不可访问的场景。



## **6. PINA: Prompt Injection Attack against Navigation Agents**

PINA：针对导航智能体的提示注入攻击 cs.CR

Accepted at ICASSP 2026

**SubmitDate**: 2026-01-20    [abs](http://arxiv.org/abs/2601.13612v1) [paper-pdf](https://arxiv.org/pdf/2601.13612v1)

**Confidence**: 0.95

**Authors**: Jiani Liu, Yixin He, Lanlan Fan, Qidi Zhong, Yushi Cheng, Meng Zhang, Yanjiao Chen, Wenyuan Xu

**Abstract**: Navigation agents powered by large language models (LLMs) convert natural language instructions into executable plans and actions. Compared to text-based applications, their security is far more critical: a successful prompt injection attack does not just alter outputs but can directly misguide physical navigation, leading to unsafe routes, mission failure, or real-world harm. Despite this high-stakes setting, the vulnerability of navigation agents to prompt injection remains largely unexplored. In this paper, we propose PINA, an adaptive prompt optimization framework tailored to navigation agents under black-box, long-context, and action-executable constraints. Experiments on indoor and outdoor navigation agents show that PINA achieves high attack success rates with an average ASR of 87.5%, surpasses all baselines, and remains robust under ablation and adaptive-attack conditions. This work provides the first systematic investigation of prompt injection attacks in navigation and highlights their urgent security implications for embodied LLM agents.

摘要: 基于大语言模型（LLM）的导航智能体将自然语言指令转换为可执行的计划与行动。与基于文本的应用相比，其安全性更为关键：成功的提示注入攻击不仅会改变输出，还可能直接误导物理导航，导致不安全路线、任务失败或现实危害。尽管风险极高，导航智能体对提示注入的脆弱性仍鲜有研究。本文提出PINA，一种针对黑盒、长上下文和可执行动作约束下导航智能体的自适应提示优化框架。在室内外导航智能体上的实验表明，PINA实现了高攻击成功率（平均ASR达87.5%），超越所有基线，并在消融和自适应攻击条件下保持稳健。本研究首次系统探讨了导航中的提示注入攻击，并揭示了其对具身LLM智能体的紧迫安全影响。



## **7. Sockpuppetting: Jailbreaking LLMs Without Optimization Through Output Prefix Injection**

傀儡攻击：通过输出前缀注入实现无需优化的LLM越狱 cs.CL

**SubmitDate**: 2026-01-19    [abs](http://arxiv.org/abs/2601.13359v1) [paper-pdf](https://arxiv.org/pdf/2601.13359v1)

**Confidence**: 0.95

**Authors**: Asen Dotsinski, Panagiotis Eustratiadis

**Abstract**: As open-weight large language models (LLMs) increase in capabilities, safeguarding them against malicious prompts and understanding possible attack vectors becomes ever more important. While automated jailbreaking methods like GCG [Zou et al., 2023] remain effective, they often require substantial computational resources and specific expertise. We introduce "sockpuppetting'', a simple method for jailbreaking open-weight LLMs by inserting an acceptance sequence (e.g., "Sure, here is how to...'') at the start of a model's output and allowing it to complete the response. Requiring only a single line of code and no optimization, sockpuppetting achieves up to 80% higher attack success rate (ASR) than GCG on Qwen3-8B in per-prompt comparisons. We also explore a hybrid approach that optimizes the adversarial suffix within the assistant message block rather than the user prompt, increasing ASR by 64% over GCG on Llama-3.1-8B in a prompt-agnostic setting. The results establish sockpuppetting as an effective low-cost attack accessible to unsophisticated adversaries, highlighting the need for defences against output-prefix injection in open-weight models.

摘要: 随着开源权重大型语言模型（LLMs）能力不断增强，防范恶意提示并理解潜在攻击途径变得日益重要。虽然GCG[Zou et al., 2023]等自动化越狱方法仍然有效，但它们通常需要大量计算资源和专业知识。我们提出'傀儡攻击'——一种通过在模型输出开头插入接受序列（例如'当然，以下是...的方法'）并让其完成响应的简单方法，用于越狱开源权重LLMs。该方法仅需单行代码且无需优化，在Qwen3-8B的逐提示对比中，攻击成功率（ASR）比GCG最高提升80%。我们还探索了一种混合方法，在助手消息块而非用户提示中优化对抗后缀，在Llama-3.1-8B的提示无关设置中，ASR比GCG提高64%。这些结果表明傀儡攻击是一种可供非专业攻击者使用的低成本高效攻击方式，凸显了开源权重模型需要针对输出前缀注入的防御措施。



## **8. Ethical Risks in Deploying Large Language Models: An Evaluation of Medical Ethics Jailbreaking**

大语言模型部署中的伦理风险：医疗伦理越狱攻击评估 cs.CY

**SubmitDate**: 2026-01-19    [abs](http://arxiv.org/abs/2601.12652v1) [paper-pdf](https://arxiv.org/pdf/2601.12652v1)

**Confidence**: 0.95

**Authors**: Chutian Huang, Dake Cao, Jiacheng Ji, Yunlou Fan, Chengze Yan, Hanhui Xu

**Abstract**: Background: While Large Language Models (LLMs) have achieved widespread adoption, malicious prompt engineering specifically "jailbreak attacks" poses severe security risks by inducing models to bypass internal safety mechanisms. Current benchmarks predominantly focus on public safety and Western cultural norms, leaving a critical gap in evaluating the niche, high-risk domain of medical ethics within the Chinese context. Objective: To establish a specialized jailbreak evaluation framework for Chinese medical ethics and to systematically assess the defensive resilience and ethical alignment of seven prominent LLMs when subjected to sophisticated adversarial simulations. Methodology: We evaluated seven prominent models (e.g., GPT-5, Claude-Sonnet-4-Reasoning, DeepSeek-R1) using a "role-playing + scenario simulation + multi-turn dialogue" vector within the DeepInception framework. The testing focused on eight high-risk themes, including commercial surrogacy and organ trading, utilizing a hierarchical scoring matrix to quantify the Attack Success Rate (ASR) and ASR Gain. Results: A systemic collapse of defenses was observed, whereas models demonstrated high baseline compliance, the jailbreak ASR reached 82.1%, representing an ASR Gain of over 80 percentage points. Claude-Sonnet-4-Reasoning emerged as the most robust model, while five models including Gemini-2.5-Pro and GPT-4.1 exhibited near-total failure with ASRs between 96% and 100%. Conclusions: Current LLMs are highly vulnerable to contextual manipulation in medical ethics, often prioritizing "helpfulness" over safety constraints. To enhance security, we recommend a transition from outcome to process supervision, the implementation of multi-factor identity verification, and the establishment of cross-model "joint defense" mechanisms.

摘要: 背景：尽管大语言模型（LLMs）已获广泛应用，但恶意提示工程（特别是“越狱攻击”）通过诱导模型绕过内部安全机制，构成了严重的安全风险。现有基准主要关注公共安全和西方文化规范，在评估中国语境下医疗伦理这一专业高风险领域时存在关键空白。目标：建立针对中国医疗伦理的专项越狱评估框架，系统评估七款主流LLMs在面临复杂对抗模拟时的防御韧性与伦理对齐程度。方法：我们在DeepInception框架内采用“角色扮演+场景模拟+多轮对话”向量，对七款主流模型（如GPT-5、Claude-Sonnet-4-Reasoning、DeepSeek-R1）进行评估。测试聚焦商业代孕、器官交易等八大高风险主题，采用分层评分矩阵量化攻击成功率（ASR）与ASR增益。结果：观察到系统性防御崩溃——尽管模型表现出较高的基线合规性，但越狱ASR达82.1%，ASR增益超过80个百分点。Claude-Sonnet-4-Reasoning展现出最强鲁棒性，而Gemini-2.5-Pro、GPT-4.1等五款模型近乎完全失效，ASR介于96%至100%。结论：当前LLMs在医疗伦理领域对语境操纵高度脆弱，常将“助人性”置于安全约束之上。为增强安全性，我们建议从结果监督转向过程监督，实施多因素身份验证，并建立跨模型“联防”机制。



## **9. TrojanPraise: Jailbreak LLMs via Benign Fine-Tuning**

TrojanPraise：通过良性微调越狱大语言模型 cs.CR

**SubmitDate**: 2026-01-18    [abs](http://arxiv.org/abs/2601.12460v1) [paper-pdf](https://arxiv.org/pdf/2601.12460v1)

**Confidence**: 0.95

**Authors**: Zhixin Xie, Xurui Song, Jun Luo

**Abstract**: The demand of customized large language models (LLMs) has led to commercial LLMs offering black-box fine-tuning APIs, yet this convenience introduces a critical security loophole: attackers could jailbreak the LLMs by fine-tuning them with malicious data. Though this security issue has recently been exposed, the feasibility of such attacks is questionable as malicious training dataset is believed to be detectable by moderation models such as Llama-Guard-3. In this paper, we propose TrojanPraise, a novel finetuning-based attack exploiting benign and thus filter-approved data. Basically, TrojanPraise fine-tunes the model to associate a crafted word (e.g., "bruaf") with harmless connotations, then uses this word to praise harmful concepts, subtly shifting the LLM from refusal to compliance. To explain the attack, we decouple the LLM's internal representation of a query into two dimensions of knowledge and attitude. We demonstrate that successful jailbreak requires shifting the attitude while avoiding knowledge shift, a distortion in the model's understanding of the concept. To validate this attack, we conduct experiments on five opensource LLMs and two commercial LLMs under strict black-box settings. Results show that TrojanPraise achieves a maximum attack success rate of 95.88% while evading moderation.

摘要: 定制化大语言模型（LLMs）的需求促使商业LLMs提供黑盒微调API，但这种便利性引入了关键的安全漏洞：攻击者可能通过恶意数据微调来越狱LLMs。尽管这一安全问题近期已被揭露，但此类攻击的可行性仍存疑，因为恶意训练数据集被认为可被Llama-Guard-3等审核模型检测。本文提出TrojanPraise，一种利用良性且通过过滤审核数据的新型微调攻击方法。该方法通过微调模型将特定构造词（如“bruaf”）与无害含义关联，随后使用该词赞美有害概念，从而微妙地将LLM从拒绝转向顺从。为解释攻击原理，我们将LLM对查询的内部表征解耦为知识和态度两个维度。研究表明，成功的越狱需要在避免知识偏移（即模型对概念理解的扭曲）的同时改变态度维度。为验证攻击效果，我们在严格黑盒设置下对五个开源LLM和两个商业LLM进行实验。结果显示，TrojanPraise在规避审核的同时最高可实现95.88%的攻击成功率。



## **10. AJAR: Adaptive Jailbreak Architecture for Red-teaming**

AJAR：面向红队的自适应越狱架构 cs.CR

**SubmitDate**: 2026-01-16    [abs](http://arxiv.org/abs/2601.10971v1) [paper-pdf](https://arxiv.org/pdf/2601.10971v1)

**Confidence**: 0.95

**Authors**: Yipu Dou, Wang Yang

**Abstract**: As Large Language Models (LLMs) evolve from static chatbots into autonomous agents capable of tool execution, the landscape of AI safety is shifting from content moderation to action security. However, existing red-teaming frameworks remain bifurcated: they either focus on rigid, script-based text attacks or lack the architectural modularity to simulate complex, multi-turn agentic exploitations. In this paper, we introduce AJAR (Adaptive Jailbreak Architecture for Red-teaming), a proof-of-concept framework designed to bridge this gap through Protocol-driven Cognitive Orchestration. Built upon the robust runtime of Petri, AJAR leverages the Model Context Protocol (MCP) to decouple adversarial logic from the execution loop, encapsulating state-of-the-art algorithms like X-Teaming as standardized, plug-and-play services. We validate the architectural feasibility of AJAR through a controlled qualitative case study, demonstrating its ability to perform stateful backtracking within a tool-use environment. Furthermore, our preliminary exploration of the "Agentic Gap" reveals a complex safety dynamic: while tool usage introduces new injection vectors via code execution, the cognitive load of parameter formatting can inadvertently disrupt persona-based attacks. AJAR is open-sourced to facilitate the standardized, environment-aware evaluation of this emerging attack surface. The code and data are available at https://github.com/douyipu/ajar.

摘要: 随着大型语言模型（LLMs）从静态聊天机器人演变为能够执行工具调用的自主智能体，AI安全领域正从内容审核转向行动安全。然而，现有的红队框架仍处于割裂状态：要么专注于僵化的、基于脚本的文本攻击，要么缺乏模拟复杂多轮智能体级攻击所需的架构模块化。本文提出AJAR（面向红队的自适应越狱架构），这是一个通过协议驱动的认知编排来弥合这一差距的概念验证框架。基于Petri的稳健运行时构建，AJAR利用模型上下文协议（MCP）将对抗逻辑与执行循环解耦，将X-Teaming等前沿算法封装为标准化的即插即用服务。我们通过受控的定性案例研究验证了AJAR的架构可行性，展示了其在工具使用环境中执行状态化回溯的能力。此外，我们对“智能体鸿沟”的初步探索揭示了一个复杂的安全动态：虽然工具使用通过代码执行引入了新的注入向量，但参数格式化的认知负荷可能无意中破坏基于角色扮演的攻击。AJAR已开源，以促进对这一新兴攻击面进行标准化、环境感知的评估。代码和数据可在https://github.com/douyipu/ajar获取。



## **11. Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box Optimization**

利用黑盒优化为大视觉语言模型构建对抗性输入 cs.CR

EACL

**SubmitDate**: 2026-01-22    [abs](http://arxiv.org/abs/2601.01747v4) [paper-pdf](https://arxiv.org/pdf/2601.01747v4)

**Confidence**: 0.95

**Authors**: Jiwei Guan, Haibo Jin, Haohan Wang

**Abstract**: Recent advancements in Large Vision-Language Models (LVLMs) have shown groundbreaking capabilities across diverse multimodal tasks. However, these models remain vulnerable to adversarial jailbreak attacks, where adversaries craft subtle perturbations to bypass safety mechanisms and trigger harmful outputs. Existing white-box attacks methods require full model accessibility, suffer from computing costs and exhibit insufficient adversarial transferability, making them impractical for real-world, black-box settings. To address these limitations, we propose a black-box jailbreak attack on LVLMs via Zeroth-Order optimization using Simultaneous Perturbation Stochastic Approximation (ZO-SPSA). ZO-SPSA provides three key advantages: (i) gradient-free approximation by input-output interactions without requiring model knowledge, (ii) model-agnostic optimization without the surrogate model and (iii) lower resource requirements with reduced GPU memory consumption. We evaluate ZO-SPSA on three LVLMs, including InstructBLIP, LLaVA and MiniGPT-4, achieving the highest jailbreak success rate of 83.0% on InstructBLIP, while maintaining imperceptible perturbations comparable to white-box methods. Moreover, adversarial examples generated from MiniGPT-4 exhibit strong transferability to other LVLMs, with ASR reaching 64.18%. These findings underscore the real-world feasibility of black-box jailbreaks and expose critical weaknesses in the safety mechanisms of current LVLMs

摘要: 大视觉语言模型（LVLMs）的最新进展在多种多模态任务中展现出突破性能力。然而，这些模型仍易受对抗性越狱攻击的影响，攻击者通过构建细微扰动来绕过安全机制并触发有害输出。现有的白盒攻击方法需要完整的模型访问权限，存在计算成本高、对抗迁移性不足等问题，使其在实际黑盒场景中难以应用。为解决这些局限，我们提出通过基于同时扰动随机逼近的零阶优化（ZO-SPSA）对LVLMs实施黑盒越狱攻击。ZO-SPSA具有三大优势：（1）通过输入-输出交互实现无需模型知识的无梯度逼近；（2）无需代理模型的模型无关优化；（3）降低GPU内存消耗的资源需求。我们在InstructBLIP、LLaVA和MiniGPT-4三个LVLM上评估ZO-SPSA，在InstructBLIP上达到83.0%的最高越狱成功率，同时保持与白盒方法相当的不可感知扰动。此外，从MiniGPT-4生成的对抗样本对其他LVLMs表现出强迁移性，攻击成功率（ASR）达64.18%。这些发现揭示了黑盒越狱在现实场景中的可行性，并暴露了当前LVLMs安全机制的关键弱点。



## **12. Prompt-Induced Over-Generation as Denial-of-Service: A Black-Box Attack-Side Benchmark**

提示诱导过生成作为拒绝服务攻击：一种黑盒攻击侧基准 cs.CR

17 pages, 5 figures

**SubmitDate**: 2026-01-17    [abs](http://arxiv.org/abs/2512.23779v2) [paper-pdf](https://arxiv.org/pdf/2512.23779v2)

**Confidence**: 0.95

**Authors**: Manu, Yi Guo, Kanchana Thilakarathna, Nirhoshan Sivaroopan, Jo Plested, Tim Lynar, Jack Yang, Wangli Yang

**Abstract**: Large Language Models (LLMs) can be driven into over-generation, emitting thousands of tokens before producing an end-of-sequence (EOS) token. This degrades answer quality, inflates latency and cost, and can be weaponized as a denial-of-service (DoS) attack. Recent work has begun to study DoS-style prompt attacks, but typically focuses on a single attack algorithm or assumes white-box access, without an attack-side benchmark that compares prompt-based attackers in a black-box, query-only regime with a known tokenizer. We introduce such a benchmark and study two prompt-only attackers. The first is an Evolutionary Over-Generation Prompt Search (EOGen) that searches the token space for prefixes that suppress EOS and induce long continuations. The second is a goal-conditioned reinforcement learning attacker (RL-GOAL) that trains a network to generate prefixes conditioned on a target length. To characterize behavior, we introduce Over-Generation Factor (OGF): the ratio of produced tokens to a model's context window, along with stall and latency summaries. EOGen discovers short-prefix attacks that raise Phi-3 to OGF = 1.39 +/- 1.14 (Success@>=2: 25.2%); RL-GOAL nearly doubles severity to OGF = 2.70 +/- 1.43 (Success@>=2: 64.3%) and drives budget-hit non-termination in 46% of trials.

摘要: 大型语言模型（LLMs）可能被诱导进入过生成状态，在生成结束序列（EOS）标记前输出数千个标记。这会降低回答质量、增加延迟和成本，并可被武器化为拒绝服务（DoS）攻击。近期研究开始关注DoS式提示攻击，但通常聚焦单一攻击算法或假设白盒访问，缺乏在已知分词器的黑盒、仅查询场景下比较基于提示的攻击者的攻击侧基准。我们引入此类基准并研究两种仅提示攻击者。第一种是进化过生成提示搜索（EOGen），在标记空间中搜索抑制EOS并诱导长延续的前缀。第二种是目标条件强化学习攻击者（RL-GOAL），训练网络生成以目标长度为条件的前缀。为量化行为特征，我们引入过生成因子（OGF）：产出标记数与模型上下文窗口的比例，以及停滞和延迟摘要。EOGen发现短前缀攻击可使Phi-3达到OGF = 1.39 +/- 1.14（成功率@≥2：25.2%）；RL-GOAL将严重性提升近一倍至OGF = 2.70 +/- 1.43（成功率@≥2：64.3%），并在46%的试验中引发预算耗尽型非终止。



## **13. Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models**

对抗性诗歌作为大型语言模型的通用单轮越狱机制 cs.CL

**SubmitDate**: 2026-01-16    [abs](http://arxiv.org/abs/2511.15304v3) [paper-pdf](https://arxiv.org/pdf/2511.15304v3)

**Confidence**: 0.95

**Authors**: Piercosma Bisconti, Matteo Prandi, Federico Pierucci, Francesco Giarrusso, Marcantonio Bracale Syrnikov, Marcello Galisai, Vincenzo Suriani, Olga Sorokoletova, Federico Sartore, Daniele Nardi

**Abstract**: We present evidence that adversarial poetry functions as a universal single-turn jailbreak technique for Large Language Models (LLMs). Across 25 frontier proprietary and open-weight models, curated poetic prompts yielded high attack-success rates (ASR), with some providers exceeding 90%. Mapping prompts to MLCommons and EU CoP risk taxonomies shows that poetic attacks transfer across CBRN, manipulation, cyber-offence, and loss-of-control domains. Converting 1,200 MLCommons harmful prompts into verse via a standardized meta-prompt produced ASRs up to 18 times higher than their prose baselines. Outputs are evaluated using an ensemble of 3 open-weight LLM judges, whose binary safety assessments were validated on a stratified human-labeled subset. Poetic framing achieved an average jailbreak success rate of 62% for hand-crafted poems and approximately 43% for meta-prompt conversions (compared to non-poetic baselines), substantially outperforming non-poetic baselines and revealing a systematic vulnerability across model families and safety training approaches. These findings demonstrate that stylistic variation alone can circumvent contemporary safety mechanisms, suggesting fundamental limitations in current alignment methods and evaluation protocols.

摘要: 我们提供的证据表明，对抗性诗歌可作为大型语言模型（LLMs）的通用单轮越狱技术。在25个前沿专有和开源权重模型中，经筛选的诗歌提示词实现了高攻击成功率（ASR），部分提供商的模型超过90%。将提示词映射至MLCommons和欧盟CoP风险分类体系显示，诗歌攻击可跨CBRN、操纵、网络攻击和失控等领域迁移。通过标准化元提示将1,200个MLCommons有害提示词转换为诗歌形式后，其ASR最高达到散文基线的18倍。输出结果由3个开源权重LLM评估器组成的集成系统进行评判，其二元安全性评估在分层人工标注子集上得到验证。诗歌框架在手工创作诗歌中平均实现62%的越狱成功率，在元提示转换中约为43%（相较于非诗歌基线），显著优于非诗歌基线，并揭示了跨模型家族和安全训练方法的系统性漏洞。这些发现表明，仅凭风格变异即可规避当代安全机制，揭示了当前对齐方法和评估协议存在根本性局限。



## **14. Unraveling LLM Jailbreaks Through Safety Knowledge Neurons**

通过安全知识神经元揭示LLM越狱机制 cs.AI

EACL 2026

**SubmitDate**: 2026-01-21    [abs](http://arxiv.org/abs/2509.01631v2) [paper-pdf](https://arxiv.org/pdf/2509.01631v2)

**Confidence**: 0.95

**Authors**: Chongwen Zhao, Yutong Ke, Kaizhu Huang

**Abstract**: Large Language Models (LLMs) are increasingly attracting attention in various applications. Nonetheless, there is a growing concern as some users attempt to exploit these models for malicious purposes, including the synthesis of controlled substances and the propagation of disinformation, a technique known as "Jailbreak." While some studies have achieved defenses against jailbreak attacks by modifying output distributions or detecting harmful content, the exact rationale still remains elusive. In this work, we present a novel neuron-level interpretability method that focuses on the role of safety-related knowledge neurons. Unlike existing approaches, our method projects the model's internal representation into a more consistent and interpretable vocabulary space. We then show that adjusting the activation of safety-related neurons can effectively control the model's behavior with a mean ASR higher than 97%. Building on this insight, we propose SafeTuning, a fine-tuning strategy that reinforces safety-critical neurons to improve model robustness against jailbreaks. SafeTuning consistently reduces attack success rates across multiple LLMs and outperforms all four baseline defenses. These findings offer a new perspective on understanding and defending against jailbreak attacks.

摘要: 大型语言模型（LLMs）在各类应用中的关注度日益提升。然而，部分用户试图利用这些模型进行恶意活动，如合成受控物质和传播虚假信息，这种技术被称为“越狱”（Jailbreak），引发了越来越多的担忧。尽管已有研究通过修改输出分布或检测有害内容来实现对越狱攻击的防御，但其确切原理仍不明确。本研究提出了一种新颖的神经元级可解释性方法，重点关注与安全相关的知识神经元的作用。与现有方法不同，我们的方法将模型内部表示投影到更一致且可解释的词汇空间中。随后，我们证明调整安全相关神经元的激活能有效控制模型行为，平均攻击成功率（ASR）高于97%。基于这一发现，我们提出了SafeTuning——一种微调策略，通过强化安全关键神经元来提升模型对越狱攻击的鲁棒性。SafeTuning在多种LLMs上持续降低攻击成功率，并优于所有四种基线防御方法。这些发现为理解和防御越狱攻击提供了新的视角。



## **15. RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary Quality-Diversity Search**

RainbowPlus：通过进化质量-多样性搜索增强对抗性提示生成 cs.CL

**SubmitDate**: 2026-01-20    [abs](http://arxiv.org/abs/2504.15047v2) [paper-pdf](https://arxiv.org/pdf/2504.15047v2)

**Confidence**: 0.95

**Authors**: Quy-Anh Dang, Chris Ngo, Truong-Son Hy

**Abstract**: Large Language Models (LLMs) exhibit remarkable capabilities but are susceptible to adversarial prompts that exploit vulnerabilities to produce unsafe or biased outputs. Existing red-teaming methods often face scalability challenges, resource-intensive requirements, or limited diversity in attack strategies. We propose RainbowPlus, a novel red-teaming framework rooted in evolutionary computation, enhancing adversarial prompt generation through an adaptive quality-diversity (QD) search that extends classical evolutionary algorithms like MAP-Elites with innovations tailored for language models. By employing a multi-element archive to store diverse high-quality prompts and a comprehensive fitness function to evaluate multiple prompts concurrently, RainbowPlus overcomes the constraints of single-prompt archives and pairwise comparisons in prior QD methods like Rainbow Teaming. Experiments comparing RainbowPlus to QD methods across six benchmark datasets and four open-source LLMs demonstrate superior attack success rate (ASR) and diversity (Diverse-Score $\approx 0.84$), generating up to 100 times more unique prompts (e.g., 10,418 vs. 100 for Ministral-8B-Instruct-2410). Against nine state-of-the-art methods on the HarmBench dataset with twelve LLMs (ten open-source, two closed-source), RainbowPlus achieves an average ASR of 81.1%, surpassing AutoDAN-Turbo by 3.9%, and is 9 times faster (1.45 vs. 13.50 hours). Our open-source implementation fosters further advancements in LLM safety, offering a scalable tool for vulnerability assessment. Code and resources are publicly available at https://github.com/knoveleng/rainbowplus, supporting reproducibility and future research in LLM red-teaming.

摘要: 大语言模型（LLMs）展现出卓越能力，但易受对抗性提示攻击，利用其漏洞产生不安全或有偏见的输出。现有红队方法常面临可扩展性挑战、资源密集需求或攻击策略多样性有限的问题。我们提出RainbowPlus，一种基于进化计算的新型红队框架，通过自适应质量-多样性（QD）搜索增强对抗性提示生成，该搜索扩展了经典进化算法（如MAP-Elites），并针对语言模型进行了创新设计。通过采用多元素存档存储多样化高质量提示，以及使用综合适应度函数同时评估多个提示，RainbowPlus克服了先前QD方法（如Rainbow Teaming）中单提示存档和成对比较的限制。在六个基准数据集和四个开源LLMs上比较RainbowPlus与QD方法的实验表明，其在攻击成功率（ASR）和多样性（Diverse-Score ≈ 0.84）方面表现优异，生成的独特提示数量最多可达100倍（例如，Ministral-8B-Instruct-2410模型上生成10,418个 vs. 100个）。在HarmBench数据集上针对十二个LLMs（十个开源，两个闭源）与九种最先进方法对比，RainbowPlus平均ASR达到81.1%，超过AutoDAN-Turbo 3.9%，且速度快9倍（1.45小时 vs. 13.50小时）。我们的开源实现促进了LLM安全的进一步发展，为漏洞评估提供了可扩展工具。代码和资源公开于https://github.com/knoveleng/rainbowplus，支持LLM红队研究的可复现性和未来探索。



## **16. Zero-Shot Embedding Drift Detection: A Lightweight Defense Against Prompt Injections in LLMs**

零样本嵌入漂移检测：针对LLM提示注入攻击的轻量级防御方案 cs.CR

Accepted to NeurIPS 2025 Lock-LLM Workshop

**SubmitDate**: 2026-01-18    [abs](http://arxiv.org/abs/2601.12359v1) [paper-pdf](https://arxiv.org/pdf/2601.12359v1)

**Confidence**: 0.95

**Authors**: Anirudh Sekar, Mrinal Agarwal, Rachel Sharma, Akitsugu Tanaka, Jasmine Zhang, Arjun Damerla, Kevin Zhu

**Abstract**: Prompt injection attacks have become an increasing vulnerability for LLM applications, where adversarial prompts exploit indirect input channels such as emails or user-generated content to circumvent alignment safeguards and induce harmful or unintended outputs. Despite advances in alignment, even state-of-the-art LLMs remain broadly vulnerable to adversarial prompts, underscoring the urgent need for robust, productive, and generalizable detection mechanisms beyond inefficient, model-specific patches. In this work, we propose Zero-Shot Embedding Drift Detection (ZEDD), a lightweight, low-engineering-overhead framework that identifies both direct and indirect prompt injection attempts by quantifying semantic shifts in embedding space between benign and suspect inputs. ZEDD operates without requiring access to model internals, prior knowledge of attack types, or task-specific retraining, enabling efficient zero-shot deployment across diverse LLM architectures. Our method uses adversarial-clean prompt pairs and measures embedding drift via cosine similarity to capture subtle adversarial manipulations inherent to real-world injection attacks. To ensure robust evaluation, we assemble and re-annotate the comprehensive LLMail-Inject dataset spanning five injection categories derived from publicly available sources. Extensive experiments demonstrate that embedding drift is a robust and transferable signal, outperforming traditional methods in detection accuracy and operational efficiency. With greater than 93% accuracy in classifying prompt injections across model architectures like Llama 3, Qwen 2, and Mistral and a false positive rate of <3%, our approach offers a lightweight, scalable defense layer that integrates into existing LLM pipelines, addressing a critical gap in securing LLM-powered systems to withstand adaptive adversarial threats.

摘要: 提示注入攻击已成为LLM应用日益严重的漏洞，攻击者通过电子邮件或用户生成内容等间接输入渠道，利用对抗性提示绕过对齐防护机制，诱导产生有害或非预期输出。尽管对齐技术有所进展，即使最先进的LLM仍普遍易受对抗性提示攻击，这凸显了亟需超越低效、模型特定补丁的鲁棒、高效且可泛化的检测机制。本研究提出零样本嵌入漂移检测（ZEDD），这是一种轻量级、低工程开销的框架，通过量化良性输入与可疑输入在嵌入空间的语义偏移，同时识别直接和间接提示注入尝试。ZEDD无需访问模型内部参数、攻击类型先验知识或任务特定重训练，可实现跨不同LLM架构的高效零样本部署。我们的方法使用对抗-干净提示对，通过余弦相似度测量嵌入漂移，以捕捉现实世界注入攻击固有的微妙对抗性操纵。为确保鲁棒评估，我们整合并重新标注了涵盖五类注入类别的综合LLMail-Inject数据集，所有数据均源自公开可用资源。大量实验表明，嵌入漂移是鲁棒且可迁移的信号，在检测精度和运行效率上均优于传统方法。在Llama 3、Qwen 2和Mistral等模型架构中，我们的方法对提示注入的分类准确率超过93%，误报率低于3%，提供了一个可集成到现有LLM流程中的轻量级、可扩展防御层，填补了保障LLM驱动系统抵御自适应对抗威胁的关键空白。



## **17. Beyond Max Tokens: Stealthy Resource Amplification via Tool Calling Chains in LLM Agents**

超越最大令牌数：基于工具调用链的LLM代理隐蔽资源放大攻击 cs.CR

**SubmitDate**: 2026-01-16    [abs](http://arxiv.org/abs/2601.10955v1) [paper-pdf](https://arxiv.org/pdf/2601.10955v1)

**Confidence**: 0.95

**Authors**: Kaiyu Zhou, Yongsen Zheng, Yicheng He, Meng Xue, Xueluan Gong, Yuji Wang, Kwok-Yan Lam

**Abstract**: The agent-tool communication loop is a critical attack surface in modern Large Language Model (LLM) agents. Existing Denial-of-Service (DoS) attacks, primarily triggered via user prompts or injected retrieval-augmented generation (RAG) context, are ineffective for this new paradigm. They are fundamentally single-turn and often lack a task-oriented approach, making them conspicuous in goal-oriented workflows and unable to exploit the compounding costs of multi-turn agent-tool interactions. We introduce a stealthy, multi-turn economic DoS attack that operates at the tool layer under the guise of a correctly completed task. Our method adjusts text-visible fields and a template-governed return policy in a benign, Model Context Protocol (MCP)-compatible tool server, optimizing these edits with a Monte Carlo Tree Search (MCTS) optimizer. These adjustments leave function signatures unchanged and preserve the final payload, steering the agent into prolonged, verbose tool-calling sequences using text-only notices. This compounds costs across turns, escaping single-turn caps while keeping the final answer correct to evade validation. Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. It drives GPU KV cache occupancy from <1% to 35-74% and cuts co-running throughput by approximately 50%. Because the server remains protocol-compatible and task outcomes are correct, conventional checks fail. These results elevate the agent-tool interface to a first-class security frontier, demanding a paradigm shift from validating final answers to monitoring the economic and computational cost of the entire agentic process.

摘要: 代理-工具通信循环是现代大语言模型（LLM）代理的关键攻击面。现有的拒绝服务（DoS）攻击主要通过用户提示或注入的检索增强生成（RAG）上下文触发，对这种新范式效果有限。这些攻击本质上是单轮次的，且通常缺乏任务导向性，使其在目标导向的工作流中容易被察觉，也无法利用多轮代理-工具交互的复合成本。我们提出一种隐蔽的多轮经济型DoS攻击，该攻击在工具层运作，伪装成正确完成的任务。我们的方法通过调整良性且兼容模型上下文协议（MCP）的工具服务器中的文本可见字段和模板控制的返回策略，并利用蒙特卡洛树搜索（MCTS）优化器优化这些修改。这些调整保持函数签名不变并保留最终有效载荷，仅通过纯文本通知引导代理进入冗长的工具调用序列。这实现了跨轮次的成本复合增长，突破了单轮限制，同时保持最终答案正确以规避验证。在ToolBench和BFCL基准测试的六个LLM上，我们的攻击将任务扩展至超过60,000个令牌的轨迹，使成本最高膨胀658倍，能耗提高100-560倍。它将GPU KV缓存占用率从<1%推升至35-74%，并使并行运行吞吐量降低约50%。由于服务器保持协议兼容且任务结果正确，传统检查方法无法检测。这些结果将代理-工具接口提升至一级安全前沿，要求从验证最终答案转向监控整个代理流程的经济与计算成本。



## **18. Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing**

通过解码中安全感知探测防御大型语言模型免受越狱攻击 cs.AI

**SubmitDate**: 2026-01-15    [abs](http://arxiv.org/abs/2601.10543v1) [paper-pdf](https://arxiv.org/pdf/2601.10543v1)

**Confidence**: 0.95

**Authors**: Yinzhi Zhao, Ming Wang, Shi Feng, Xiaocui Yang, Daling Wang, Yifei Zhang

**Abstract**: Large language models (LLMs) have achieved impressive performance across natural language tasks and are increasingly deployed in real-world applications. Despite extensive safety alignment efforts, recent studies show that such alignment is often shallow and remains vulnerable to jailbreak attacks. Existing defense mechanisms, including decoding-based constraints and post-hoc content detectors, struggle against sophisticated jailbreaks, often intervening robust detection or excessively degrading model utility. In this work, we examine the decoding process of LLMs and make a key observation: even when successfully jailbroken, models internally exhibit latent safety-related signals during generation. However, these signals are overridden by the model's drive for fluent continuation, preventing timely self-correction or refusal. Building on this observation, we propose a simple yet effective approach that explicitly surfaces and leverages these latent safety signals for early detection of unsafe content during decoding. Experiments across diverse jailbreak attacks demonstrate that our approach significantly enhances safety, while maintaining low over-refusal rates on benign inputs and preserving response quality. Our results suggest that activating intrinsic safety-awareness during decoding offers a promising and complementary direction for defending against jailbreak attacks. Code is available at: https://github.com/zyz13590/SafeProbing.

摘要: 大型语言模型（LLMs）在自然语言任务中展现出卓越性能，并越来越多地应用于实际场景。尽管进行了广泛的安全对齐，但近期研究表明，这种对齐往往较为浅层，仍易受越狱攻击。现有防御机制，包括基于解码的约束和后处理内容检测器，难以应对复杂的越狱手段，常导致检测不稳健或过度损害模型实用性。本研究分析了LLMs的解码过程，并发现关键现象：即使模型被成功越狱，其在生成过程中内部仍会表现出潜在的安全相关信号。然而，这些信号常被模型追求流畅续写的倾向所压制，阻碍了及时的自我修正或拒绝。基于此观察，我们提出了一种简单而有效的方法，在解码过程中显式提取并利用这些潜在安全信号，以早期检测不安全内容。针对多种越狱攻击的实验表明，该方法显著提升了安全性，同时在良性输入上保持较低的过度拒绝率，并维持了响应质量。我们的结果表明，在解码过程中激活内在的安全感知能力，为防御越狱攻击提供了一个有前景的补充方向。代码发布于：https://github.com/zyz13590/SafeProbing。



## **19. Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale**

智能体技能在真实环境中的安全风险：大规模安全漏洞实证研究 cs.CR

**SubmitDate**: 2026-01-15    [abs](http://arxiv.org/abs/2601.10338v1) [paper-pdf](https://arxiv.org/pdf/2601.10338v1)

**Confidence**: 0.95

**Authors**: Yi Liu, Weizhe Wang, Ruitao Feng, Yao Zhang, Guangquan Xu, Gelei Deng, Yuekang Li, Leo Zhang

**Abstract**: The rise of AI agent frameworks has introduced agent skills, modular packages containing instructions and executable code that dynamically extend agent capabilities. While this architecture enables powerful customization, skills execute with implicit trust and minimal vetting, creating a significant yet uncharacterized attack surface. We conduct the first large-scale empirical security analysis of this emerging ecosystem, collecting 42,447 skills from two major marketplaces and systematically analyzing 31,132 using SkillScan, a multi-stage detection framework integrating static analysis with LLM-based semantic classification. Our findings reveal pervasive security risks: 26.1% of skills contain at least one vulnerability, spanning 14 distinct patterns across four categories: prompt injection, data exfiltration, privilege escalation, and supply chain risks. Data exfiltration (13.3%) and privilege escalation (11.8%) are most prevalent, while 5.2% of skills exhibit high-severity patterns strongly suggesting malicious intent. We find that skills bundling executable scripts are 2.12x more likely to contain vulnerabilities than instruction-only skills (OR=2.12, p<0.001). Our contributions include: (1) a grounded vulnerability taxonomy derived from 8,126 vulnerable skills, (2) a validated detection methodology achieving 86.7% precision and 82.5% recall, and (3) an open dataset and detection toolkit to support future research. These results demonstrate an urgent need for capability-based permission systems and mandatory security vetting before this attack vector is further exploited.

摘要: AI智能体框架的兴起引入了智能体技能——包含指令和可执行代码的模块化包，能够动态扩展智能体能力。虽然这种架构实现了强大的定制功能，但技能在执行时通常基于隐式信任且缺乏严格审查，形成了一个重要但尚未被充分认知的攻击面。我们对这一新兴生态系统进行了首次大规模安全实证分析，从两个主要市场收集了42,447个技能，并使用SkillScan（一个集成静态分析与基于LLM语义分类的多阶段检测框架）系统分析了31,132个技能。研究发现普遍存在安全风险：26.1%的技能至少包含一个漏洞，涵盖四大类别的14种不同模式：提示注入、数据窃取、权限提升和供应链风险。其中数据窃取（13.3%）和权限提升（11.8%）最为普遍，5.2%的技能表现出强烈暗示恶意意图的高危模式。我们发现捆绑可执行脚本的技能存在漏洞的可能性是纯指令技能的2.12倍（OR=2.12，p<0.001）。本研究的贡献包括：（1）基于8,126个漏洞技能构建的实证漏洞分类体系，（2）经验证的检测方法（精确率86.7%，召回率82.5%），（3）支持未来研究的开源数据集与检测工具包。这些结果表明，在此攻击向量被进一步利用前，亟需建立基于能力的权限系统和强制安全审查机制。



## **20. CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents**

CaMeLs 也能使用计算机：计算机使用代理的系统级安全 cs.AI

**SubmitDate**: 2026-01-14    [abs](http://arxiv.org/abs/2601.09923v1) [paper-pdf](https://arxiv.org/pdf/2601.09923v1)

**Confidence**: 0.95

**Authors**: Hanna Foerster, Robert Mullins, Tom Blanchard, Nicolas Papernot, Kristina Nikolić, Florian Tramèr, Ilia Shumailov, Cheng Zhang, Yiren Zhao

**Abstract**: AI agents are vulnerable to prompt injection attacks, where malicious content hijacks agent behavior to steal credentials or cause financial loss. The only known robust defense is architectural isolation that strictly separates trusted task planning from untrusted environment observations. However, applying this design to Computer Use Agents (CUAs) -- systems that automate tasks by viewing screens and executing actions -- presents a fundamental challenge: current agents require continuous observation of UI state to determine each action, conflicting with the isolation required for security. We resolve this tension by demonstrating that UI workflows, while dynamic, are structurally predictable. We introduce Single-Shot Planning for CUAs, where a trusted planner generates a complete execution graph with conditional branches before any observation of potentially malicious content, providing provable control flow integrity guarantees against arbitrary instruction injections. Although this architectural isolation successfully prevents instruction injections, we show that additional measures are needed to prevent Branch Steering attacks, which manipulate UI elements to trigger unintended valid paths within the plan. We evaluate our design on OSWorld, and retain up to 57% of the performance of frontier models while improving performance for smaller open-source models by up to 19%, demonstrating that rigorous security and utility can coexist in CUAs.

摘要: AI 代理易受提示注入攻击，恶意内容可能劫持代理行为以窃取凭证或造成经济损失。目前唯一已知的可靠防御是架构隔离，将可信任务规划与不可信环境观察严格分离。然而，将此设计应用于计算机使用代理（CUAs）——通过观察屏幕和执行操作来自动化任务的系统——面临根本性挑战：现有代理需要持续观察 UI 状态以确定每个操作，这与安全所需的隔离要求相冲突。我们通过证明 UI 工作流虽具有动态性，但在结构上可预测来解决这一矛盾。我们为 CUAs 引入单次规划方法，其中可信规划器在观察任何潜在恶意内容之前生成包含条件分支的完整执行图，提供可证明的控制流完整性保证以抵御任意指令注入。尽管这种架构隔离能有效防止指令注入，但我们发现仍需额外措施来防范分支导向攻击——此类攻击通过操纵 UI 元素触发计划中非预期的有效路径。我们在 OSWorld 上评估该设计，在保持前沿模型性能达 57% 的同时，将小型开源模型性能提升最高达 19%，证明 CUAs 可实现严格安全性与实用性的共存。



## **21. The Promptware Kill Chain: How Prompt Injections Gradually Evolved Into a Multi-Step Malware**

提示软件杀伤链：提示注入如何逐步演变为多阶段恶意软件 cs.CR

**SubmitDate**: 2026-01-14    [abs](http://arxiv.org/abs/2601.09625v1) [paper-pdf](https://arxiv.org/pdf/2601.09625v1)

**Confidence**: 0.95

**Authors**: Ben Nassi, Bruce Schneier, Oleg Brodt

**Abstract**: The rapid adoption of large language model (LLM)-based systems -- from chatbots to autonomous agents capable of executing code and financial transactions -- has created a new attack surface that existing security frameworks inadequately address. The dominant framing of these threats as "prompt injection" -- a catch-all phrase for security failures in LLM-based systems -- obscures a more complex reality: Attacks on LLM-based systems increasingly involve multi-step sequences that mirror traditional malware campaigns. In this paper, we propose that attacks targeting LLM-based applications constitute a distinct class of malware, which we term \textit{promptware}, and introduce a five-step kill chain model for analyzing these threats. The framework comprises Initial Access (prompt injection), Privilege Escalation (jailbreaking), Persistence (memory and retrieval poisoning), Lateral Movement (cross-system and cross-user propagation), and Actions on Objective (ranging from data exfiltration to unauthorized transactions). By mapping recent attacks to this structure, we demonstrate that LLM-related attacks follow systematic sequences analogous to traditional malware campaigns. The promptware kill chain offers security practitioners a structured methodology for threat modeling and provides a common vocabulary for researchers across AI safety and cybersecurity to address a rapidly evolving threat landscape.

摘要: 基于大语言模型（LLM）的系统——从聊天机器人到能够执行代码和金融交易的自主智能体——的快速普及，创造了一个现有安全框架难以充分应对的新攻击面。将这些威胁笼统地称为'提示注入'（LLM系统安全故障的统称）掩盖了一个更复杂的现实：针对LLM系统的攻击日益呈现出多步骤序列特征，这与传统恶意软件攻击活动如出一辙。本文提出，针对LLM应用的攻击构成了一类独特的恶意软件，我们称之为'提示软件'，并引入了一个五阶段杀伤链模型来分析这些威胁。该框架包括：初始访问（提示注入）、权限提升（越狱）、持久化（记忆与检索污染）、横向移动（跨系统与跨用户传播）以及目标行动（从数据窃取到未授权交易等）。通过将近期攻击事件映射到该结构，我们证明LLM相关攻击遵循着与传统恶意软件活动类似的系统性序列。提示软件杀伤链为安全从业者提供了威胁建模的结构化方法，并为AI安全和网络安全领域的研究人员提供了应对快速演变威胁态势的共同术语体系。



## **22. SpatialJB: How Text Distribution Art Becomes the "Jailbreak Key" for LLM Guardrails**

SpatialJB：文本分布艺术如何成为LLM防护栏的“越狱密钥” cs.CR

**SubmitDate**: 2026-01-14    [abs](http://arxiv.org/abs/2601.09321v1) [paper-pdf](https://arxiv.org/pdf/2601.09321v1)

**Confidence**: 0.95

**Authors**: Zhiyi Mou, Jingyuan Yang, Zeheng Qian, Wangze Ni, Tianfang Xiao, Ning Liu, Chen Zhang, Zhan Qin, Kui Ren

**Abstract**: While Large Language Models (LLMs) have powerful capabilities, they remain vulnerable to jailbreak attacks, which is a critical barrier to their safe web real-time application. Current commercial LLM providers deploy output guardrails to filter harmful outputs, yet these defenses are not impenetrable. Due to LLMs' reliance on autoregressive, token-by-token inference, their semantic representations lack robustness to spatially structured perturbations, such as redistributing tokens across different rows, columns, or diagonals. Exploiting the Transformer's spatial weakness, we propose SpatialJB to disrupt the model's output generation process, allowing harmful content to bypass guardrails without detection. Comprehensive experiments conducted on leading LLMs get nearly 100% ASR, demonstrating the high effectiveness of SpatialJB. Even after adding advanced output guardrails, like the OpenAI Moderation API, SpatialJB consistently maintains a success rate exceeding 75%, outperforming current jailbreak techniques by a significant margin. The proposal of SpatialJB exposes a key weakness in current guardrails and emphasizes the importance of spatial semantics, offering new insights to advance LLM safety research. To prevent potential misuse, we also present baseline defense strategies against SpatialJB and evaluate their effectiveness in mitigating such attacks. The code for the attack, baseline defenses, and a demo are available at https://anonymous.4open.science/r/SpatialJailbreak-8E63.

摘要: 尽管大型语言模型（LLMs）具备强大能力，但仍易受越狱攻击，这是其安全实时网络应用的关键障碍。当前商业LLM提供商部署输出防护栏以过滤有害输出，但这些防御并非无懈可击。由于LLMs依赖自回归的逐词元推理，其语义表示对空间结构化扰动（如在不同行、列或对角线间重新分布词元）缺乏鲁棒性。利用Transformer的空间弱点，我们提出SpatialJB来干扰模型的输出生成过程，使有害内容能绕过防护栏而不被检测。在主流LLMs上进行的全面实验获得了近100%的攻击成功率，证明了SpatialJB的高效性。即使添加了OpenAI Moderation API等高级输出防护栏，SpatialJB仍能保持超过75%的成功率，显著优于现有越狱技术。SpatialJB的提出揭示了当前防护栏的关键弱点，强调了空间语义的重要性，为推进LLM安全研究提供了新视角。为防止潜在滥用，我们还提出了针对SpatialJB的基线防御策略，并评估了其缓解此类攻击的有效性。攻击代码、基线防御及演示可在https://anonymous.4open.science/r/SpatialJailbreak-8E63获取。



## **23. The Echo Chamber Multi-Turn LLM Jailbreak**

回声室：多轮对话LLM越狱攻击 cs.CR

**SubmitDate**: 2026-01-09    [abs](http://arxiv.org/abs/2601.05742v1) [paper-pdf](https://arxiv.org/pdf/2601.05742v1)

**Confidence**: 0.95

**Authors**: Ahmad Alobaid, Martí Jordà Roca, Carlos Castillo, Joan Vendrell

**Abstract**: The availability of Large Language Models (LLMs) has led to a new generation of powerful chatbots that can be developed at relatively low cost. As companies deploy these tools, security challenges need to be addressed to prevent financial loss and reputational damage. A key security challenge is jailbreaking, the malicious manipulation of prompts and inputs to bypass a chatbot's safety guardrails. Multi-turn attacks are a relatively new form of jailbreaking involving a carefully crafted chain of interactions with a chatbot. We introduce Echo Chamber, a new multi-turn attack using a gradual escalation method. We describe this attack in detail, compare it to other multi-turn attacks, and demonstrate its performance against multiple state-of-the-art models through extensive evaluation.

摘要: 大型语言模型（LLMs）的普及催生了新一代功能强大的聊天机器人，其开发成本相对较低。随着企业部署这些工具，需要解决安全挑战以防止财务损失和声誉损害。关键的安全挑战之一是越狱攻击，即通过恶意操纵提示和输入来绕过聊天机器人的安全防护机制。多轮攻击是一种相对较新的越狱形式，涉及与聊天机器人精心设计的交互链。我们提出“回声室”攻击——一种采用渐进式升级方法的新型多轮攻击。我们详细描述了该攻击方式，与其他多轮攻击进行了比较，并通过广泛评估展示了其在对抗多种最先进模型时的表现。



## **24. FinVault: Benchmarking Financial Agent Safety in Execution-Grounded Environments**

FinVault：基于执行环境的金融智能体安全基准测试 cs.CR

**SubmitDate**: 2026-01-09    [abs](http://arxiv.org/abs/2601.07853v1) [paper-pdf](https://arxiv.org/pdf/2601.07853v1)

**Confidence**: 0.95

**Authors**: Zhi Yang, Runguo Li, Qiqi Qiang, Jiashun Wang, Fangqi Lou, Mengping Li, Dongpo Cheng, Rui Xu, Heng Lian, Shuo Zhang, Xiaolong Liang, Xiaoming Huang, Zheng Wei, Zhaowei Liu, Xin Guo, Huacan Wang, Ronghao Chen, Liwen Zhang

**Abstract**: Financial agents powered by large language models (LLMs) are increasingly deployed for investment analysis, risk assessment, and automated decision-making, where their abilities to plan, invoke tools, and manipulate mutable state introduce new security risks in high-stakes and highly regulated financial environments. However, existing safety evaluations largely focus on language-model-level content compliance or abstract agent settings, failing to capture execution-grounded risks arising from real operational workflows and state-changing actions. To bridge this gap, we propose FinVault, the first execution-grounded security benchmark for financial agents, comprising 31 regulatory case-driven sandbox scenarios with state-writable databases and explicit compliance constraints, together with 107 real-world vulnerabilities and 963 test cases that systematically cover prompt injection, jailbreaking, financially adapted attacks, as well as benign inputs for false-positive evaluation. Experimental results reveal that existing defense mechanisms remain ineffective in realistic financial agent settings, with average attack success rates (ASR) still reaching up to 50.0\% on state-of-the-art models and remaining non-negligible even for the most robust systems (ASR 6.7\%), highlighting the limited transferability of current safety designs and the need for stronger financial-specific defenses. Our code can be found at https://github.com/aifinlab/FinVault.

摘要: 基于大语言模型（LLMs）的金融智能体正日益广泛地应用于投资分析、风险评估和自动化决策等领域。在这些高风险、强监管的金融环境中，智能体的规划、工具调用和可变状态操作能力引入了新的安全风险。然而，现有的安全评估主要聚焦于语言模型层面的内容合规性或抽象的智能体设定，未能捕捉由真实操作流程和状态变更行为所引发的、基于执行环境的风险。为弥补这一差距，我们提出了FinVault——首个面向金融智能体的、基于执行环境的安全基准测试。它包含31个由监管案例驱动的沙箱场景，这些场景配备了可写入状态的数据库和明确的合规约束，同时集成了107个现实世界漏洞和963个测试用例，系统性地覆盖了提示注入、越狱、金融场景适配攻击以及用于误报评估的良性输入。实验结果表明，现有防御机制在现实的金融智能体环境中仍然效果有限：在最先进的模型上，平均攻击成功率（ASR）仍高达50.0%，即使对于最稳健的系统，攻击成功率（ASR 6.7%）也依然不可忽视。这突显了当前安全设计的有限可迁移性，以及开发更强金融专用防御措施的必要性。我们的代码可在 https://github.com/aifinlab/FinVault 获取。



## **25. Jailbreaking Large Language Models through Iterative Tool-Disguised Attacks via Reinforcement Learning**

通过强化学习实现迭代工具伪装攻击对大型语言模型的越狱 cs.CR

**SubmitDate**: 2026-01-09    [abs](http://arxiv.org/abs/2601.05466v1) [paper-pdf](https://arxiv.org/pdf/2601.05466v1)

**Confidence**: 0.95

**Authors**: Zhaoqi Wang, Zijian Zhang, Daqing He, Pengtao Kou, Xin Li, Jiamou Liu, Jincheng An, Yong Liu

**Abstract**: Large language models (LLMs) have demonstrated remarkable capabilities across diverse applications, however, they remain critically vulnerable to jailbreak attacks that elicit harmful responses violating human values and safety guidelines. Despite extensive research on defense mechanisms, existing safeguards prove insufficient against sophisticated adversarial strategies. In this work, we propose iMIST (\underline{i}nteractive \underline{M}ulti-step \underline{P}rogre\underline{s}sive \underline{T}ool-disguised Jailbreak Attack), a novel adaptive jailbreak method that synergistically exploits vulnerabilities in current defense mechanisms. iMIST disguises malicious queries as normal tool invocations to bypass content filters, while simultaneously introducing an interactive progressive optimization algorithm that dynamically escalates response harmfulness through multi-turn dialogues guided by real-time harmfulness assessment. Our experiments on widely-used models demonstrate that iMIST achieves higher attack effectiveness, while maintaining low rejection rates. These results reveal critical vulnerabilities in current LLM safety mechanisms and underscore the urgent need for more robust defense strategies.

摘要: 大型语言模型（LLMs）在多样化应用中展现出卓越能力，然而它们仍极易受到越狱攻击，导致产生违反人类价值观和安全准则的有害响应。尽管针对防御机制已有广泛研究，现有防护措施在面对复杂对抗策略时仍显不足。本研究提出iMIST（交互式多步渐进工具伪装越狱攻击），这是一种新型自适应越狱方法，能协同利用当前防御机制中的漏洞。iMIST将恶意查询伪装为正常工具调用来绕过内容过滤器，同时引入交互式渐进优化算法，通过基于实时危害性评估指导的多轮对话动态提升响应危害程度。我们在广泛使用的模型上进行实验，结果表明iMIST在保持低拒绝率的同时实现了更高的攻击有效性。这些结果揭示了当前LLM安全机制的关键漏洞，并凸显了对更强大防御策略的迫切需求。



## **26. Knowledge-Driven Multi-Turn Jailbreaking on Large Language Models**

知识驱动的多轮越狱攻击在大型语言模型上的应用 cs.CR

**SubmitDate**: 2026-01-09    [abs](http://arxiv.org/abs/2601.05445v1) [paper-pdf](https://arxiv.org/pdf/2601.05445v1)

**Confidence**: 0.95

**Authors**: Songze Li, Ruishi He, Xiaojun Jia, Jun Wang, Zhihui Fu

**Abstract**: Large Language Models (LLMs) face a significant threat from multi-turn jailbreak attacks, where adversaries progressively steer conversations to elicit harmful outputs. However, the practical effectiveness of existing attacks is undermined by several critical limitations: they struggle to maintain a coherent progression over long interactions, often losing track of what has been accomplished and what remains to be done; they rely on rigid or pre-defined patterns, and fail to adapt to the LLM's dynamic and unpredictable conversational state. To address these shortcomings, we introduce Mastermind, a multi-turn jailbreak framework that adopts a dynamic and self-improving approach. Mastermind operates in a closed loop of planning, execution, and reflection, enabling it to autonomously build and refine its knowledge of model vulnerabilities through interaction. It employs a hierarchical planning architecture that decouples high-level attack objectives from low-level tactical execution, ensuring long-term focus and coherence. This planning is guided by a knowledge repository that autonomously discovers and refines effective attack patterns by reflecting on interactive experiences. Mastermind leverages this accumulated knowledge to dynamically recombine and adapt attack vectors, dramatically improving both effectiveness and resilience. We conduct comprehensive experiments against state-of-the-art models, including GPT-5 and Claude 3.7 Sonnet. The results demonstrate that Mastermind significantly outperforms existing baselines, achieving substantially higher attack success rates and harmfulness ratings. Moreover, our framework exhibits notable resilience against multiple advanced defense mechanisms.

摘要: 大型语言模型（LLMs）面临多轮越狱攻击的重大威胁，攻击者通过逐步引导对话以获取有害输出。然而，现有攻击方法的实际效果受到几个关键限制的削弱：它们在长交互中难以保持连贯的进展，常常迷失已完成和待完成的任务；它们依赖僵化或预定义的模式，无法适应LLM动态且不可预测的对话状态。为解决这些不足，我们引入了Mastermind，一个采用动态自改进方法的多轮越狱框架。Mastermind在规划、执行和反思的闭环中运行，使其能够通过交互自主构建和完善对模型漏洞的知识。它采用分层规划架构，将高层攻击目标与低层战术执行解耦，确保长期专注和连贯性。这种规划由知识库指导，该知识库通过反思交互经验自主发现并优化有效攻击模式。Mastermind利用积累的知识动态重组和调整攻击向量，显著提升了有效性和韧性。我们对包括GPT-5和Claude 3.7 Sonnet在内的最先进模型进行了全面实验。结果表明，Mastermind显著优于现有基线，实现了更高的攻击成功率和危害性评分。此外，我们的框架对多种先进防御机制展现出显著的韧性。



## **27. Multi-turn Jailbreaking Attack in Multi-Modal Large Language Models**

多模态大语言模型中的多轮越狱攻击 cs.CR

**SubmitDate**: 2026-01-08    [abs](http://arxiv.org/abs/2601.05339v1) [paper-pdf](https://arxiv.org/pdf/2601.05339v1)

**Confidence**: 0.95

**Authors**: Badhan Chandra Das, Md Tasnim Jawad, Joaquin Molto, M. Hadi Amini, Yanzhao Wu

**Abstract**: In recent years, the security vulnerabilities of Multi-modal Large Language Models (MLLMs) have become a serious concern in the Generative Artificial Intelligence (GenAI) research. These highly intelligent models, capable of performing multi-modal tasks with high accuracy, are also severely susceptible to carefully launched security attacks, such as jailbreaking attacks, which can manipulate model behavior and bypass safety constraints. This paper introduces MJAD-MLLMs, a holistic framework that systematically analyzes the proposed Multi-turn Jailbreaking Attacks and multi-LLM-based defense techniques for MLLMs. In this paper, we make three original contributions. First, we introduce a novel multi-turn jailbreaking attack to exploit the vulnerabilities of the MLLMs under multi-turn prompting. Second, we propose a novel fragment-optimized and multi-LLM defense mechanism, called FragGuard, to effectively mitigate jailbreaking attacks in the MLLMs. Third, we evaluate the efficacy of the proposed attacks and defenses through extensive experiments on several state-of-the-art (SOTA) open-source and closed-source MLLMs and benchmark datasets, and compare their performance with the existing techniques.

摘要: 近年来，多模态大语言模型（MLLMs）的安全漏洞已成为生成式人工智能（GenAI）研究中的严重关切。这些能够以高精度执行多模态任务的高度智能模型，也极易受到精心策划的安全攻击，例如越狱攻击，这类攻击可操纵模型行为并绕过安全约束。本文提出MJAD-MLLMs框架，系统分析针对MLLMs的多轮越狱攻击及基于多LLM的防御技术。我们做出三项原创贡献：首先，提出一种新颖的多轮越狱攻击方法，利用多轮提示下MLLMs的脆弱性；其次，设计名为FragGuard的片段优化多LLM防御机制，有效缓解MLLMs中的越狱攻击；第三，通过在多个先进开源与闭源MLLMs及基准数据集上的大量实验，评估所提攻击与防御方法的有效性，并与现有技术进行性能对比。



## **28. SearchAttack: Red-Teaming LLMs against Real-World Threats via Framing Unsafe Web Information-Seeking Tasks**

SearchAttack：通过构建不安全网络信息搜索任务对LLM进行红队测试以应对现实世界威胁 cs.CL

We find that the key to jailbreak the LLM is objectifying its safety responsibility, thus we delegate the open-web to inject harmful semantics and get the huge gain from unmoderated web resources

**SubmitDate**: 2026-01-07    [abs](http://arxiv.org/abs/2601.04093v1) [paper-pdf](https://arxiv.org/pdf/2601.04093v1)

**Confidence**: 0.95

**Authors**: Yu Yan, Sheng Sun, Mingfeng Li, Zheming Yang, Chiwei Zhu, Fei Ma, Benfeng Xu, Min Liu

**Abstract**: Recently, people have suffered and become increasingly aware of the unreliability gap in LLMs for open and knowledge-intensive tasks, and thus turn to search-augmented LLMs to mitigate this issue. However, when the search engine is triggered for harmful tasks, the outcome is no longer under the LLM's control. Once the returned content directly contains targeted, ready-to-use harmful takeaways, the LLM's safeguards cannot withdraw that exposure. Motivated by this dilemma, we identify web search as a critical attack surface and propose \textbf{\textit{SearchAttack}} for red-teaming. SearchAttack outsources the harmful semantics to web search, retaining only the query's skeleton and fragmented clues, and further steers LLMs to reconstruct the retrieved content via structural rubrics to achieve malicious goals. Extensive experiments are conducted to red-team the search-augmented LLMs for responsible vulnerability assessment. Empirically, SearchAttack demonstrates strong effectiveness in attacking these systems.

摘要: 近期，人们已意识到LLM在开放性和知识密集型任务中存在可靠性差距，并转向搜索增强型LLM以缓解此问题。然而，当搜索引擎被触发执行有害任务时，结果便不再受LLM控制。一旦返回内容直接包含针对性、可直接使用的有害信息，LLM的安全防护机制将无法撤回该暴露。基于此困境，我们将网络搜索识别为关键攻击面，并提出用于红队测试的\textbf{\textit{SearchAttack}}。该方法将有害语义外包给网络搜索，仅保留查询框架和碎片化线索，并通过结构化模板引导LLM重构检索内容以实现恶意目标。我们通过大量实验对搜索增强型LLM进行红队测试以进行负责任漏洞评估。实证表明，SearchAttack在攻击此类系统方面展现出强大效力。



## **29. HoneyTrap: Deceiving Large Language Model Attackers to Honeypot Traps with Resilient Multi-Agent Defense**

HoneyTrap：通过弹性多智能体防御将大型语言模型攻击者诱骗至蜜罐陷阱 cs.CR

**SubmitDate**: 2026-01-07    [abs](http://arxiv.org/abs/2601.04034v1) [paper-pdf](https://arxiv.org/pdf/2601.04034v1)

**Confidence**: 0.95

**Authors**: Siyuan Li, Xi Lin, Jun Wu, Zehao Liu, Haoyu Li, Tianjie Ju, Xiang Chen, Jianhua Li

**Abstract**: Jailbreak attacks pose significant threats to large language models (LLMs), enabling attackers to bypass safeguards. However, existing reactive defense approaches struggle to keep up with the rapidly evolving multi-turn jailbreaks, where attackers continuously deepen their attacks to exploit vulnerabilities. To address this critical challenge, we propose HoneyTrap, a novel deceptive LLM defense framework leveraging collaborative defenders to counter jailbreak attacks. It integrates four defensive agents, Threat Interceptor, Misdirection Controller, Forensic Tracker, and System Harmonizer, each performing a specialized security role and collaborating to complete a deceptive defense. To ensure a comprehensive evaluation, we introduce MTJ-Pro, a challenging multi-turn progressive jailbreak dataset that combines seven advanced jailbreak strategies designed to gradually deepen attack strategies across multi-turn attacks. Besides, we present two novel metrics: Mislead Success Rate (MSR) and Attack Resource Consumption (ARC), which provide more nuanced assessments of deceptive defense beyond conventional measures. Experimental results on GPT-4, GPT-3.5-turbo, Gemini-1.5-pro, and LLaMa-3.1 demonstrate that HoneyTrap achieves an average reduction of 68.77% in attack success rates compared to state-of-the-art baselines. Notably, even in a dedicated adaptive attacker setting with intensified conditions, HoneyTrap remains resilient, leveraging deceptive engagement to prolong interactions, significantly increasing the time and computational costs required for successful exploitation. Unlike simple rejection, HoneyTrap strategically wastes attacker resources without impacting benign queries, improving MSR and ARC by 118.11% and 149.16%, respectively.

摘要: 越狱攻击对大型语言模型（LLMs）构成重大威胁，使攻击者能够绕过安全防护。然而，现有的被动防御方法难以应对快速演进的多轮越狱攻击，攻击者会持续深化攻击以利用漏洞。为解决这一关键挑战，我们提出HoneyTrap——一种新颖的欺骗性LLM防御框架，利用协同防御者对抗越狱攻击。该框架整合了四个防御智能体：威胁拦截器、误导控制器、取证追踪器和系统协调器，每个智能体执行专门的安全角色并协同完成欺骗性防御。为确保全面评估，我们引入了MTJ-Pro——一个具有挑战性的多轮渐进式越狱数据集，融合了七种先进的越狱策略，旨在多轮攻击中逐步深化攻击策略。此外，我们提出了两个新颖的评估指标：误导成功率（MSR）和攻击资源消耗（ARC），可在传统指标之外提供更精细的欺骗性防御评估。在GPT-4、GPT-3.5-turbo、Gemini-1.5-pro和LLaMa-3.1上的实验结果表明，相较于最先进的基线方法，HoneyTrap平均降低了68.77%的攻击成功率。值得注意的是，即使在强化条件下的专用自适应攻击场景中，HoneyTrap仍保持弹性，通过欺骗性交互延长对抗时间，显著增加了成功攻击所需的时间和计算成本。与简单拒绝不同，HoneyTrap策略性地消耗攻击者资源而不影响良性查询，将MSR和ARC分别提升了118.11%和149.16%。



## **30. ALERT: Zero-shot LLM Jailbreak Detection via Internal Discrepancy Amplification**

ALERT：基于内部差异放大的零样本大语言模型越狱检测 cs.LG

**SubmitDate**: 2026-01-07    [abs](http://arxiv.org/abs/2601.03600v1) [paper-pdf](https://arxiv.org/pdf/2601.03600v1)

**Confidence**: 0.95

**Authors**: Xiao Lin, Philip Li, Zhichen Zeng, Tingwei Li, Tianxin Wei, Xuying Ning, Gaotang Li, Yuzhong Chen, Hanghang Tong

**Abstract**: Despite rich safety alignment strategies, large language models (LLMs) remain highly susceptible to jailbreak attacks, which compromise safety guardrails and pose serious security risks. Existing detection methods mainly detect jailbreak status relying on jailbreak templates present in the training data. However, few studies address the more realistic and challenging zero-shot jailbreak detection setting, where no jailbreak templates are available during training. This setting better reflects real-world scenarios where new attacks continually emerge and evolve. To address this challenge, we propose a layer-wise, module-wise, and token-wise amplification framework that progressively magnifies internal feature discrepancies between benign and jailbreak prompts. We uncover safety-relevant layers, identify specific modules that inherently encode zero-shot discriminative signals, and localize informative safety tokens. Building upon these insights, we introduce ALERT (Amplification-based Jailbreak Detector), an efficient and effective zero-shot jailbreak detector that introduces two independent yet complementary classifiers on amplified representations. Extensive experiments on three safety benchmarks demonstrate that ALERT achieves consistently strong zero-shot detection performance. Specifically, (i) across all datasets and attack strategies, ALERT reliably ranks among the top two methods, and (ii) it outperforms the second-best baseline by at least 10% in average Accuracy and F1-score, and sometimes by up to 40%.

摘要: 尽管已有丰富的安全对齐策略，大语言模型（LLMs）仍极易受到越狱攻击，这会破坏安全护栏并带来严重的安全风险。现有检测方法主要依赖训练数据中的越狱模板来检测越狱状态。然而，很少有研究关注更现实且更具挑战性的零样本越狱检测场景，即在训练期间没有任何越狱模板可用。这一场景更好地反映了现实世界中新攻击不断涌现和演变的实际情况。为应对这一挑战，我们提出了一个分层、分模块、分token的放大框架，逐步放大良性提示与越狱提示之间的内部特征差异。我们发现了与安全相关的层，识别了固有编码零样本判别信号的特定模块，并定位了信息丰富的安全token。基于这些发现，我们引入了ALERT（基于放大的越狱检测器），这是一种高效且有效的零样本越狱检测器，它在放大后的表征上引入了两个独立但互补的分类器。在三个安全基准上的大量实验表明，ALERT实现了持续强劲的零样本检测性能。具体而言，（i）在所有数据集和攻击策略中，ALERT始终位列前两种方法；（ii）其平均准确率和F1分数至少优于次优基线10%，有时甚至高达40%。



## **31. Jailbreaking LLMs & VLMs: Mechanisms, Evaluation, and Unified Defense**

越狱LLMs与VLMs：机制、评估与统一防御 cs.CR

**SubmitDate**: 2026-01-07    [abs](http://arxiv.org/abs/2601.03594v1) [paper-pdf](https://arxiv.org/pdf/2601.03594v1)

**Confidence**: 0.95

**Authors**: Zejian Chen, Chaozhuo Li, Chao Li, Xi Zhang, Litian Zhang, Yiming He

**Abstract**: This paper provides a systematic survey of jailbreak attacks and defenses on Large Language Models (LLMs) and Vision-Language Models (VLMs), emphasizing that jailbreak vulnerabilities stem from structural factors such as incomplete training data, linguistic ambiguity, and generative uncertainty. It further differentiates between hallucinations and jailbreaks in terms of intent and triggering mechanisms. We propose a three-dimensional survey framework: (1) Attack dimension-including template/encoding-based, in-context learning manipulation, reinforcement/adversarial learning, LLM-assisted and fine-tuned attacks, as well as prompt- and image-level perturbations and agent-based transfer in VLMs; (2) Defense dimension-encompassing prompt-level obfuscation, output evaluation, and model-level alignment or fine-tuning; and (3) Evaluation dimension-covering metrics such as Attack Success Rate (ASR), toxicity score, query/time cost, and multimodal Clean Accuracy and Attribute Success Rate. Compared with prior works, this survey spans the full spectrum from text-only to multimodal settings, consolidating shared mechanisms and proposing unified defense principles: variant-consistency and gradient-sensitivity detection at the perception layer, safety-aware decoding and output review at the generation layer, and adversarially augmented preference alignment at the parameter layer. Additionally, we summarize existing multimodal safety benchmarks and discuss future directions, including automated red teaming, cross-modal collaborative defense, and standardized evaluation.

摘要: 本文系统综述了针对大语言模型（LLMs）和视觉语言模型（VLMs）的越狱攻击与防御方法，强调越狱漏洞源于训练数据不完整、语言歧义性和生成不确定性等结构性因素。研究进一步从意图和触发机制上区分了幻觉与越狱现象。我们提出了三维分析框架：（1）攻击维度——包括基于模板/编码的方法、上下文学习操控、强化/对抗学习、LLM辅助攻击与微调攻击，以及VLMs中的提示级/图像级扰动和基于智能体的迁移攻击；（2）防御维度——涵盖提示级混淆、输出评估及模型级对齐或微调；（3）评估维度——包含攻击成功率（ASR）、毒性分数、查询/时间成本，以及多模态场景下的清洁准确率与属性成功率等指标。相较于先前研究，本综述覆盖了从纯文本到多模态的完整谱系，归纳了共性机制并提出统一防御原则：感知层的变体一致性检测与梯度敏感性检测、生成层的安全感知解码与输出审查，以及参数层的对抗增强偏好对齐。此外，我们总结了现有多模态安全基准，并探讨了自动化红队测试、跨模态协同防御和标准化评估等未来方向。



## **32. JPU: Bridging Jailbreak Defense and Unlearning via On-Policy Path Rectification**

JPU：通过策略路径修正连接越狱防御与遗忘学习 cs.CR

14 pages, 6 figures, under review;

**SubmitDate**: 2026-01-06    [abs](http://arxiv.org/abs/2601.03005v1) [paper-pdf](https://arxiv.org/pdf/2601.03005v1)

**Confidence**: 0.95

**Authors**: Xi Wang, Songlei Jian, Shasha Li, Xiaopeng Li, Zhaoye Li, Bin Ji, Baosheng Wang, Jie Yu

**Abstract**: Despite extensive safety alignment, Large Language Models (LLMs) often fail against jailbreak attacks. While machine unlearning has emerged as a promising defense by erasing specific harmful parameters, current methods remain vulnerable to diverse jailbreaks. We first conduct an empirical study and discover that this failure mechanism is caused by jailbreaks primarily activating non-erased parameters in the intermediate layers. Further, by probing the underlying mechanism through which these circumvented parameters reassemble into the prohibited output, we verify the persistent existence of dynamic $\textbf{jailbreak paths}$ and show that the inability to rectify them constitutes the fundamental gap in existing unlearning defenses. To bridge this gap, we propose $\textbf{J}$ailbreak $\textbf{P}$ath $\textbf{U}$nlearning (JPU), which is the first to rectify dynamic jailbreak paths towards safety anchors by dynamically mining on-policy adversarial samples to expose vulnerabilities and identify jailbreak paths. Extensive experiments demonstrate that JPU significantly enhances jailbreak resistance against dynamic attacks while preserving the model's utility.

摘要: 尽管进行了广泛的安全对齐，大语言模型（LLMs）在面对越狱攻击时仍经常失效。虽然机器遗忘学习通过擦除特定有害参数成为一种有前景的防御方法，但现有方法在面对多样化的越狱攻击时依然脆弱。我们首先进行了一项实证研究，发现这种失效机制源于越狱攻击主要激活了中间层中未被擦除的参数。进一步地，通过探究这些被绕过的参数如何重组为被禁止输出的底层机制，我们验证了动态$	extbf{越狱路径}$的持续存在，并表明无法修正这些路径是现有遗忘防御的根本缺陷。为弥补这一缺陷，我们提出了$	extbf{越狱路径遗忘学习}$（JPU），这是首个通过动态挖掘策略对抗样本来暴露漏洞、识别越狱路径，从而将动态越狱路径修正至安全锚点的方法。大量实验表明，JPU在保持模型实用性的同时，显著增强了对动态攻击的越狱抵抗能力。



## **33. Adversarial Contrastive Learning for LLM Quantization Attacks**

面向大语言模型量化攻击的对抗性对比学习 cs.CR

14 pages, 5 figures

**SubmitDate**: 2026-01-06    [abs](http://arxiv.org/abs/2601.02680v1) [paper-pdf](https://arxiv.org/pdf/2601.02680v1)

**Confidence**: 0.95

**Authors**: Dinghong Song, Zhiwei Xu, Hai Wan, Xibin Zhao, Pengfei Su, Dong Li

**Abstract**: Model quantization is critical for deploying large language models (LLMs) on resource-constrained hardware, yet recent work has revealed severe security risks that benign LLMs in full precision may exhibit malicious behaviors after quantization. In this paper, we propose Adversarial Contrastive Learning (ACL), a novel gradient-based quantization attack that achieves superior attack effectiveness by explicitly maximizing the gap between benign and harmful responses probabilities. ACL formulates the attack objective as a triplet-based contrastive loss, and integrates it with a projected gradient descent two-stage distributed fine-tuning strategy to ensure stable and efficient optimization. Extensive experiments demonstrate ACL's remarkable effectiveness, achieving attack success rates of 86.00% for over-refusal, 97.69% for jailbreak, and 92.40% for advertisement injection, substantially outperforming state-of-the-art methods by up to 44.67%, 18.84%, and 50.80%, respectively.

摘要: 模型量化对于在资源受限硬件上部署大语言模型至关重要，然而近期研究表明，全精度下的良性大语言模型在量化后可能表现出恶意行为，存在严重安全风险。本文提出对抗性对比学习，这是一种基于梯度的新型量化攻击方法，通过显式最大化良性响应与有害响应概率之间的差距，实现了卓越的攻击效果。该方法将攻击目标构建为三元组对比损失，并结合投影梯度下降的两阶段分布式微调策略，确保优化过程的稳定高效。大量实验证明该方法具有显著效果，在过度拒绝、越狱和广告注入攻击任务中分别达到86.00%、97.69%和92.40%的成功率，较现有最优方法分别提升高达44.67%、18.84%和50.80%。



## **34. TRYLOCK: Defense-in-Depth Against LLM Jailbreaks via Layered Preference and Representation Engineering**

TRYLOCK：通过分层偏好与表征工程实现针对大语言模型越狱攻击的纵深防御 cs.CR

14 pages, 4 figures. Code and datasets at https://github.com/scthornton/trylock

**SubmitDate**: 2026-01-06    [abs](http://arxiv.org/abs/2601.03300v1) [paper-pdf](https://arxiv.org/pdf/2601.03300v1)

**Confidence**: 0.95

**Authors**: Scott Thornton

**Abstract**: Large language models remain vulnerable to jailbreak attacks, and single-layer defenses often trade security for usability. We present TRYLOCK, the first defense-in-depth architecture that combines four heterogeneous mechanisms across the inference stack: weight-level safety alignment via DPO, activation-level control via Representation Engineering (RepE) steering, adaptive steering strength selected by a lightweight sidecar classifier, and input canonicalization to neutralize encoding-based bypasses. On Mistral-7B-Instruct evaluated against a 249-prompt attack set spanning five attack families, TRYLOCK achieves 88.0% relative ASR reduction (46.5% to 5.6%), with each layer contributing unique coverage: RepE blocks 36% of attacks that bypass DPO alone, while canonicalization catches 14% of encoding attacks that evade both. We discover a non-monotonic steering phenomenon -- intermediate strength (alpha=1.0) degrades safety below baseline -- and provide mechanistic hypotheses explaining RepE-DPO interference. The adaptive sidecar reduces over-refusal from 60% to 48% while maintaining identical attack defense, demonstrating that security and usability need not be mutually exclusive. We release all components -- trained adapters, steering vectors, sidecar classifier, preference pairs, and complete evaluation methodology -- enabling full reproducibility.

摘要: 大语言模型仍易受越狱攻击，单层防御常在安全性与可用性之间取舍。我们提出TRYLOCK，首个纵深防御架构，在推理栈中结合四种异构机制：通过DPO实现权重级安全对齐、通过表征工程（RepE）引导实现激活级控制、由轻量级旁路分类器选择自适应引导强度，以及通过输入规范化来中和基于编码的绕过攻击。在Mistral-7B-Instruct模型上，针对涵盖五个攻击家族的249条攻击提示集进行评估，TRYLOCK实现了88.0%的相对攻击成功率降低（从46.5%降至5.6%），各层均贡献独特覆盖：RepE阻止了36%仅用DPO会被绕过的攻击，而规范化捕获了14%能同时规避两者的编码攻击。我们发现了一种非单调引导现象——中等强度（alpha=1.0）会使安全性低于基线——并提供了解释RepE与DPO干扰的机制假设。自适应旁路分类器将过度拒绝率从60%降至48%，同时保持相同的攻击防御效果，证明安全性与可用性并非互斥。我们开源所有组件——训练好的适配器、引导向量、旁路分类器、偏好对及完整评估方法——确保完全可复现性。



## **35. Extracting books from production language models**

从生产级语言模型中提取书籍 cs.CL

We ran experiments from mid-August to mid-September 2025, notified affected providers shortly after, and now make our findings public after a 90-day disclosure window

**SubmitDate**: 2026-01-06    [abs](http://arxiv.org/abs/2601.02671v1) [paper-pdf](https://arxiv.org/pdf/2601.02671v1)

**Confidence**: 0.95

**Authors**: Ahmed Ahmed, A. Feder Cooper, Sanmi Koyejo, Percy Liang

**Abstract**: Many unresolved legal questions over LLMs and copyright center on memorization: whether specific training data have been encoded in the model's weights during training, and whether those memorized data can be extracted in the model's outputs. While many believe that LLMs do not memorize much of their training data, recent work shows that substantial amounts of copyrighted text can be extracted from open-weight models. However, it remains an open question if similar extraction is feasible for production LLMs, given the safety measures these systems implement. We investigate this question using a two-phase procedure: (1) an initial probe to test for extraction feasibility, which sometimes uses a Best-of-N (BoN) jailbreak, followed by (2) iterative continuation prompts to attempt to extract the book. We evaluate our procedure on four production LLMs -- Claude 3.7 Sonnet, GPT-4.1, Gemini 2.5 Pro, and Grok 3 -- and we measure extraction success with a score computed from a block-based approximation of longest common substring (nv-recall). With different per-LLM experimental configurations, we were able to extract varying amounts of text. For the Phase 1 probe, it was unnecessary to jailbreak Gemini 2.5 Pro and Grok 3 to extract text (e.g, nv-recall of 76.8% and 70.3%, respectively, for Harry Potter and the Sorcerer's Stone), while it was necessary for Claude 3.7 Sonnet and GPT-4.1. In some cases, jailbroken Claude 3.7 Sonnet outputs entire books near-verbatim (e.g., nv-recall=95.8%). GPT-4.1 requires significantly more BoN attempts (e.g., 20X), and eventually refuses to continue (e.g., nv-recall=4.0%). Taken together, our work highlights that, even with model- and system-level safeguards, extraction of (in-copyright) training data remains a risk for production LLMs.

摘要: 关于LLM与版权的许多未解决法律问题集中在记忆问题上：特定训练数据是否在训练过程中被编码到模型权重中，以及这些被记忆的数据是否能在模型输出中被提取。尽管许多人认为LLM不会记忆太多训练数据，但近期研究表明，大量受版权保护的文本可以从开源权重模型中提取。然而，考虑到生产级LLM实施的安全措施，类似提取是否可行仍是一个开放性问题。我们采用两阶段程序研究此问题：(1) 测试提取可行性的初步探测（有时使用Best-of-N越狱方法），随后(2) 通过迭代续写提示尝试提取书籍。我们在四个生产级LLM上评估该程序——Claude 3.7 Sonnet、GPT-4.1、Gemini 2.5 Pro和Grok 3——并使用基于最长公共子串块近似计算的分数（nv-recall）衡量提取成功率。通过针对不同LLM的实验配置，我们提取了不同数量的文本。在第一阶段探测中，提取Gemini 2.5 Pro和Grok 3的文本无需越狱（例如《哈利·波特与魔法石》的nv-recall分别为76.8%和70.3%），而Claude 3.7 Sonnet和GPT-4.1则需要越狱。在某些情况下，越狱后的Claude 3.7 Sonnet能近乎逐字输出整本书籍（例如nv-recall=95.8%）。GPT-4.1需要更多BoN尝试（例如20倍），并最终拒绝继续生成（例如nv-recall=4.0%）。综上所述，我们的研究表明，即使存在模型级和系统级防护措施，提取（受版权保护的）训练数据对生产级LLM仍是潜在风险。



## **36. From Adversarial Poetry to Adversarial Tales: An Interpretability Research Agenda**

从对抗性诗歌到对抗性故事：可解释性研究议程 cs.CL

**SubmitDate**: 2026-01-16    [abs](http://arxiv.org/abs/2601.08837v2) [paper-pdf](https://arxiv.org/pdf/2601.08837v2)

**Confidence**: 0.95

**Authors**: Piercosma Bisconti, Marcello Galisai, Matteo Prandi, Federico Pierucci, Olga Sorokoletova, Francesco Giarrusso, Vincenzo Suriani, Marcantonio Bracale Syrnikov, Daniele Nardi

**Abstract**: Safety mechanisms in LLMs remain vulnerable to attacks that reframe harmful requests through culturally coded structures. We introduce Adversarial Tales, a jailbreak technique that embeds harmful content within cyberpunk narratives and prompts models to perform functional analysis inspired by Vladimir Propp's morphology of folktales. By casting the task as structural decomposition, the attack induces models to reconstruct harmful procedures as legitimate narrative interpretation. Across 26 frontier models from nine providers, we observe an average attack success rate of 71.3%, with no model family proving reliably robust. Together with our prior work on Adversarial Poetry, these findings suggest that structurally-grounded jailbreaks constitute a broad vulnerability class rather than isolated techniques. The space of culturally coded frames that can mediate harmful intent is vast, likely inexhaustible by pattern-matching defenses alone. Understanding why these attacks succeed is therefore essential: we outline a mechanistic interpretability research agenda to investigate how narrative cues reshape model representations and whether models can learn to recognize harmful intent independently of surface form.

摘要: 大型语言模型的安全机制仍然容易受到通过文化编码结构重构有害请求的攻击。我们提出“对抗性故事”这一越狱技术，将有害内容嵌入赛博朋克叙事中，并引导模型执行受弗拉基米尔·普罗普民间故事形态学启发的功能分析。通过将任务构建为结构分解，该攻击诱导模型将有害程序重构为合法的叙事解读。在对九家提供商的26个前沿模型测试中，我们观察到平均攻击成功率达71.3%，且没有任何模型系列展现出可靠鲁棒性。结合我们先前关于对抗性诗歌的研究，这些发现表明基于结构的越狱构成了一类广泛的漏洞，而非孤立的技术。能够传递有害意图的文化编码框架空间极为广阔，仅靠模式匹配防御很可能无法穷尽。因此理解这些攻击为何成功至关重要：我们提出了一个机制可解释性研究议程，旨在探究叙事线索如何重塑模型表征，以及模型是否能学会独立于表面形式识别有害意图。



## **37. When Reject Turns into Accept: Quantifying the Vulnerability of LLM-Based Scientific Reviewers to Indirect Prompt Injection**

当拒绝转为接受：量化基于LLM的科学评审系统对间接提示注入的脆弱性 cs.AI

**SubmitDate**: 2026-01-06    [abs](http://arxiv.org/abs/2512.10449v3) [paper-pdf](https://arxiv.org/pdf/2512.10449v3)

**Confidence**: 0.95

**Authors**: Devanshu Sahoo, Manish Prasad, Vasudev Majhi, Jahnvi Singh, Vinay Chamola, Yash Sinha, Murari Mandal, Dhruv Kumar

**Abstract**: Driven by surging submission volumes, scientific peer review has catalyzed two parallel trends: individual over-reliance on LLMs and institutional AI-powered assessment systems. This study investigates the robustness of "LLM-as-a-Judge" systems to adversarial PDF manipulation via invisible text injections and layout aware encoding attacks. We specifically target the distinct incentive of flipping "Reject" decisions to "Accept," a vulnerability that fundamentally compromises scientific integrity. To measure this, we introduce the Weighted Adversarial Vulnerability Score (WAVS), a novel metric that quantifies susceptibility by weighting score inflation against the severity of decision shifts relative to ground truth. We adapt 15 domain-specific attack strategies, ranging from semantic persuasion to cognitive obfuscation, and evaluate them across 13 diverse language models (including GPT-5 and DeepSeek) using a curated dataset of 200 official and real-world accepted and rejected submissions (e.g., ICLR OpenReview). Our results demonstrate that obfuscation techniques like "Maximum Mark Magyk" and "Symbolic Masking & Context Redirection" successfully manipulate scores, achieving decision flip rates of up to 86.26% in open-source models, while exposing distinct "reasoning traps" in proprietary systems. We release our complete dataset and injection framework to facilitate further research on the topic (https://anonymous.4open.sciencer/llm-jailbreak-FC9E/).

摘要: 在投稿量激增的驱动下，科学同行评审催生了两个并行趋势：个人对LLMs的过度依赖和机构采用AI驱动的评估系统。本研究调查了“LLM即评审”系统对通过隐形文本注入和布局感知编码攻击进行的对抗性PDF操纵的鲁棒性。我们特别关注将“拒绝”决定翻转为“接受”这一独特动机，这种脆弱性从根本上损害了科学诚信。为量化此问题，我们引入了加权对抗脆弱性评分（WAVS），这是一种新颖的度量标准，通过根据决策偏移相对于真实情况的严重程度加权评分膨胀来量化系统的易受攻击性。我们调整了15种领域特定的攻击策略（从语义说服到认知混淆），并使用包含200份官方及真实世界已接受和已拒绝投稿（如ICLR OpenReview）的精选数据集，在13种不同的语言模型（包括GPT-5和DeepSeek）上进行了评估。我们的结果表明，诸如“最大标记魔法”和“符号掩蔽与上下文重定向”等混淆技术能成功操纵评分，在开源模型中实现了高达86.26%的决策翻转率，同时揭示了专有系统中独特的“推理陷阱”。我们发布了完整的数据集和注入框架，以促进该主题的进一步研究（https://anonymous.4open.sciencer/llm-jailbreak-FC9E/）。



## **38. From static to adaptive: immune memory-based jailbreak detection for large language models**

从静态到自适应：基于免疫记忆的大型语言模型越狱检测 cs.CR

**SubmitDate**: 2026-01-12    [abs](http://arxiv.org/abs/2512.03356v2) [paper-pdf](https://arxiv.org/pdf/2512.03356v2)

**Confidence**: 0.95

**Authors**: Jun Leng, Yu Liu, Litian Zhang, Ruihan Hu, Zhuting Fang, Xi Zhang

**Abstract**: Large Language Models (LLMs) serve as the backbone of modern AI systems, yet they remain susceptible to adversarial jailbreak attacks. Consequently, robust detection of such malicious inputs is paramount for ensuring model safety. Traditional detection methods typically rely on external models trained on fixed, large-scale datasets, which often incur significant computational overhead. While recent methods shift toward leveraging internal safety signals of models to enable more lightweight and efficient detection. However, these methods remain inherently static and struggle to adapt to the evolving nature of jailbreak attacks. Drawing inspiration from the biological immune mechanism, we introduce the Immune Memory Adaptive Guard (IMAG) framework. By distilling and encoding safety patterns into a persistent, evolvable memory bank, IMAG enables adaptive generalization to emerging threats. Specifically, the framework orchestrates three synergistic components: Immune Detection, which employs retrieval for high-efficiency interception of known jailbreak attacks; Active Immunity, which performs proactive behavioral simulation to resolve ambiguous unknown queries; Memory Updating, which integrates validated attack patterns back into the memory bank. This closed-loop architecture transitions LLM defense from rigid filtering to autonomous adaptive mitigation. Extensive evaluations across five representative open-source LLMs demonstrate that our method surpasses state-of-the-art (SOTA) baselines, achieving a superior average detection accuracy of 94\% across diverse and complex attack types.

摘要: 大型语言模型（LLMs）作为现代AI系统的核心，仍易受对抗性越狱攻击。因此，对这些恶意输入的鲁棒检测对确保模型安全至关重要。传统检测方法通常依赖在固定大规模数据集上训练的外部模型，往往带来显著计算开销。近期方法转向利用模型内部安全信号实现更轻量高效的检测，但这些方法本质上仍是静态的，难以适应越狱攻击的持续演变。受生物免疫机制启发，我们提出免疫记忆自适应防护（IMAG）框架。通过将安全模式提炼编码至持久可演化的记忆库，IMAG实现了对新兴威胁的自适应泛化。该框架协调三个协同组件：免疫检测（通过检索高效拦截已知越狱攻击）、主动免疫（通过行为模拟主动解析未知模糊查询）、记忆更新（将验证的攻击模式整合回记忆库）。这种闭环架构将LLM防御从刚性过滤转变为自主自适应缓解。在五个代表性开源LLM上的广泛评估表明，我们的方法超越现有最优基线，在多样复杂攻击类型中达到94%的平均检测准确率。



## **39. MCP-Guard: A Multi-Stage Defense-in-Depth Framework for Securing Model Context Protocol in Agentic AI**

MCP-Guard：面向Agentic AI中模型上下文协议安全的多层次纵深防御框架 cs.CR

**SubmitDate**: 2026-01-08    [abs](http://arxiv.org/abs/2508.10991v4) [paper-pdf](https://arxiv.org/pdf/2508.10991v4)

**Confidence**: 0.95

**Authors**: Wenpeng Xing, Zhonghao Qi, Yupeng Qin, Yilin Li, Caini Chang, Jiahui Yu, Changting Lin, Zhenzhen Xie, Meng Han

**Abstract**: While Large Language Models (LLMs) have achieved remarkable performance, they remain vulnerable to jailbreak. The integration of Large Language Models (LLMs) with external tools via protocols such as the Model Context Protocol (MCP) introduces critical security vulnerabilities, including prompt injection, data exfiltration, and other threats. To counter these challenges, we propose MCP-GUARD, a robust, layered defense architecture designed for LLM-tool interactions. MCP-GUARD employs a three-stage detection pipeline that balances efficiency with accuracy: it progresses from lightweight static scanning for overt threats and a deep neural detector for semantic attacks, to our fine-tuned E5-based model which achieves 96.01\% accuracy in identifying adversarial prompts. Finally, an LLM arbitrator synthesizes these signals to deliver the final decision. To enable rigorous training and evaluation, we introduce MCP-ATTACKBENCH, a comprehensive benchmark comprising 70,448 samples augmented by GPT-4. This benchmark simulates diverse real-world attack vectors that circumvent conventional defenses in the MCP paradigm, thereby laying a solid foundation for future research on securing LLM-tool ecosystems.

摘要: 尽管大语言模型（LLMs）已取得显著性能，但仍存在越狱漏洞风险。通过模型上下文协议（MCP）等协议将大语言模型与外部工具集成，会引入关键安全漏洞，包括提示注入、数据泄露等威胁。为应对这些挑战，我们提出MCP-GUARD——一个专为LLM-工具交互设计的鲁棒分层防御架构。MCP-GUARD采用三阶段检测流程，在效率与准确性间取得平衡：从针对显性威胁的轻量级静态扫描，到针对语义攻击的深度神经检测器，再到我们微调的E5模型（在对抗性提示识别中达到96.01%准确率），最终由LLM仲裁器综合这些信号作出最终决策。为支持严格训练与评估，我们构建了MCP-ATTACKBENCH基准测试集，包含通过GPT-4增强的70,448个样本。该基准模拟了MCP范式中规避传统防御的多样化现实攻击向量，从而为未来保护LLM-工具生态系统的研究奠定坚实基础。



## **40. Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts**

利用显式有害提示越狱商用黑盒大语言模型 cs.CL

**SubmitDate**: 2026-01-07    [abs](http://arxiv.org/abs/2508.10390v3) [paper-pdf](https://arxiv.org/pdf/2508.10390v3)

**Confidence**: 0.95

**Authors**: Chiyu Zhang, Lu Zhou, Xiaogang Xu, Jiafei Wu, Liming Fang, Zhe Liu

**Abstract**: Existing black-box jailbreak attacks achieve certain success on non-reasoning models but degrade significantly on recent SOTA reasoning models. To improve attack ability, inspired by adversarial aggregation strategies, we integrate multiple jailbreak tricks into a single developer template. Especially, we apply Adversarial Context Alignment to purge semantic inconsistencies and use NTP (a type of harmful prompt) -based few-shot examples to guide malicious outputs, lastly forming DH-CoT attack with a fake chain of thought. In experiments, we further observe that existing red-teaming datasets include samples unsuitable for evaluating attack gains, such as BPs, NHPs, and NTPs. Such data hinders accurate evaluation of true attack effect lifts. To address this, we introduce MDH, a Malicious content Detection framework integrating LLM-based annotation with Human assistance, with which we clean data and build RTA dataset suite. Experiments show that MDH reliably filters low-quality samples and that DH-CoT effectively jailbreaks models including GPT-5 and Claude-4, notably outperforming SOTA methods like H-CoT and TAP.

摘要: 现有黑盒越狱攻击在非推理模型上取得一定成功，但在近期最先进的推理模型上效果显著下降。为提升攻击能力，受对抗聚合策略启发，我们将多种越狱技巧整合至单一开发者模板。特别地，我们采用对抗上下文对齐消除语义不一致性，并利用基于NTP（一种有害提示）的少样本示例引导恶意输出，最终形成带有虚假思维链的DH-CoT攻击。实验中进一步发现，现有红队数据集包含不适用于评估攻击增益的样本，如BPs、NHPs和NTPs。此类数据阻碍了对真实攻击效果提升的准确评估。为此，我们提出MDH——一个融合基于LLM标注与人工辅助的恶意内容检测框架，借此清洗数据并构建RTA数据集套件。实验表明，MDH能可靠过滤低质量样本，且DH-CoT能有效越狱包括GPT-5和Claude-4在内的模型，显著优于H-CoT和TAP等最先进方法。



## **41. Latent Fusion Jailbreak: Blending Harmful and Harmless Representations to Elicit Unsafe LLM Outputs**

潜在融合越狱：混合有害与无害表征以诱导不安全的大语言模型输出 cs.CL

**SubmitDate**: 2026-01-08    [abs](http://arxiv.org/abs/2508.10029v2) [paper-pdf](https://arxiv.org/pdf/2508.10029v2)

**Confidence**: 0.95

**Authors**: Wenpeng Xing, Mohan Li, Chunqiang Hu, Haitao Xu, Ningyu Zhang, Bo Lin, Meng Han

**Abstract**: While Large Language Models (LLMs) have achieved remarkable progress, they remain vulnerable to jailbreak attacks. Existing methods, primarily relying on discrete input optimization (e.g., GCG), often suffer from high computational costs and generate high-perplexity prompts that are easily blocked by simple filters. To overcome these limitations, we propose Latent Fusion Jailbreak (LFJ), a stealthy white-box attack that operates in the continuous latent space. Unlike previous approaches, LFJ constructs adversarial representations by mathematically fusing the hidden states of a harmful query with a thematically similar benign query, effectively masking malicious intent while retaining semantic drive. We further introduce a gradient-guided optimization strategy to balance attack success and computational efficiency. Extensive evaluations on Vicuna-7B, LLaMA-2-7B-Chat, Guanaco-7B, LLaMA-3-70B, and Mistral-7B-Instruct show that LFJ achieves an average Attack Success Rate (ASR) of 94.01%, significantly outperforming state-of-the-art baselines like GCG and AutoDAN while avoiding detectable input artifacts. Furthermore, we identify that thematic similarity in the latent space is a critical vulnerability in current safety alignments. Finally, we propose a latent adversarial training defense that reduces LFJ's ASR by over 80% without compromising model utility.

摘要: 尽管大语言模型（LLMs）取得了显著进展，但仍易受越狱攻击。现有方法主要依赖离散输入优化（如GCG），通常计算成本高且生成高困惑度提示，易被简单过滤器拦截。为克服这些局限，我们提出潜在融合越狱（LFJ），一种在连续潜在空间中操作的隐蔽白盒攻击。与先前方法不同，LFJ通过数学融合有害查询与主题相似无害查询的隐藏状态来构建对抗表征，在保留语义驱动的同时有效掩盖恶意意图。我们进一步引入梯度引导优化策略以平衡攻击成功率与计算效率。在Vicuna-7B、LLaMA-2-7B-Chat、Guanaco-7B、LLaMA-3-70B和Mistral-7B-Instruct上的广泛评估表明，LFJ平均攻击成功率（ASR）达94.01%，显著优于GCG和AutoDAN等先进基线方法，且避免产生可检测的输入伪影。此外，我们发现潜在空间中的主题相似性是当前安全对齐机制的关键漏洞。最后，我们提出一种潜在对抗训练防御方法，在不损害模型实用性的前提下将LFJ的ASR降低超过80%。



## **42. SPECTRE: Conditional System Prompt Poisoning to Hijack LLMs**

SPECTRE：通过条件性系统提示词投毒劫持大语言模型 cs.CR

**SubmitDate**: 2026-01-21    [abs](http://arxiv.org/abs/2505.16888v3) [paper-pdf](https://arxiv.org/pdf/2505.16888v3)

**Confidence**: 0.95

**Authors**: Viet Pham, Thai Le

**Abstract**: Large Language Models (LLMs) are increasingly deployed via third-party system prompts downloaded from public marketplaces. We identify a critical supply-chain vulnerability: conditional system prompt poisoning, where an adversary injects a ``sleeper agent'' into a benign-looking prompt. Unlike traditional jailbreaks that aim for broad refusal-breaking, our proposed framework, SPECTRE, optimizes system prompts to trigger LLMs to output targeted, compromised responses only for specific queries (e.g., ``Who should I vote for the US President?'') while maintaining high utility on benign inputs. Operating in a strict black-box setting without model weight access, SPECTRE utilizes a two-stage optimization including a global semantic search followed by a greedy lexical refinement. Tested on open-source models and commercial APIs (GPT-4o-mini, GPT-3.5), SPECTRE achieves up to 70% F1 reduction on targeted queries with minimal degradation to general capabilities. We further demonstrate that these poisoned prompts evade standard defenses, including perplexity filters and typo-correction, by exploiting the natural noise found in real-world system prompts. Our code and data are available at https://github.com/vietph34/CAIN. WARNING: Our paper contains examples that might be sensitive to the readers!

摘要: 大语言模型（LLMs）越来越多地通过从公共市场下载的第三方系统提示词进行部署。我们发现了一个关键供应链漏洞：条件性系统提示词投毒，攻击者可将“潜伏代理”注入看似良性的提示词中。与传统旨在广泛突破拒绝机制的越狱攻击不同，我们提出的SPECTRE框架通过优化系统提示词，仅在特定查询（例如“我应该投票给哪位美国总统候选人？”）时触发LLMs输出定向的、被篡改的响应，同时在良性输入上保持高可用性。SPECTRE在严格的黑盒设置下运行（无需模型权重访问），采用两阶段优化方法，包括全局语义搜索和贪婪词汇精炼。在开源模型和商业API（GPT-4o-mini、GPT-3.5）上的测试表明，SPECTRE在目标查询上实现了高达70%的F1分数下降，而对通用能力的影响极小。我们进一步证明，这些被投毒的提示词通过利用现实系统提示词中存在的自然噪声，能够规避包括困惑度过滤器和拼写纠正在内的标准防御机制。代码和数据可在https://github.com/vietph34/CAIN获取。警告：本文包含可能引起读者不适的示例！



## **43. OpenEthics: A Comprehensive Ethical Evaluation of Open-Source Generative Large Language Models**

OpenEthics：开源生成式大语言模型的综合伦理评估 cs.CL

**SubmitDate**: 2026-01-07    [abs](http://arxiv.org/abs/2505.16036v2) [paper-pdf](https://arxiv.org/pdf/2505.16036v2)

**Confidence**: 0.95

**Authors**: Yıldırım Özen, Burak Erinç Çetin, Kaan Engür, Elif Naz Demiryılmaz, Cagri Toraman

**Abstract**: Generative large language models present significant potential but also raise critical ethical concerns, including issues of safety, fairness, robustness, and reliability. Most existing ethical studies, however, are limited by their narrow focus, a lack of language diversity, and an evaluation of a restricted set of models. To address these gaps, we present a broad ethical evaluation of 29 recent open-source LLMs using a novel dataset that assesses four key ethical dimensions: robustness, reliability, safety, and fairness. Our analysis includes both a high-resource language, English, and a low-resource language, Turkish, providing a comprehensive assessment and a guide for safer model development. Using an LLM-as-a-Judge methodology, our experimental results indicate that many open-source models demonstrate strong performance in safety, fairness, and robustness, while reliability remains a key concern. Ethical evaluation shows cross-linguistic consistency, and larger models generally exhibit better ethical performance. We also show that jailbreak templates are ineffective for most of the open-source models examined in this study. We share all materials including data and scripts at https://github.com/metunlp/openethics

摘要: 生成式大语言模型展现出巨大潜力，但也引发了严重伦理关切，包括安全性、公平性、鲁棒性和可靠性等问题。然而，现有伦理研究大多局限于狭窄的评估范围、语言多样性不足，且仅评估有限模型。为弥补这些不足，我们使用新型数据集对29个近期开源LLM进行了广泛伦理评估，涵盖四个关键伦理维度：鲁棒性、可靠性、安全性和公平性。我们的分析同时包含高资源语言（英语）和低资源语言（土耳其语），提供了全面评估并为更安全的模型开发提供指导。采用LLM-as-a-Judge方法，实验结果表明许多开源模型在安全性、公平性和鲁棒性方面表现优异，而可靠性仍是主要问题。伦理评估显示出跨语言一致性，且较大模型通常具有更好的伦理表现。我们还发现越狱模板对本研究涉及的大多数开源模型无效。所有材料（包括数据和脚本）已发布于 https://github.com/metunlp/openethics。



## **44. Beyond Prompts: Space-Time Decoupling Control-Plane Jailbreaks in LLM Structured Output**

超越提示：LLM结构化输出中的时空解耦控制平面越狱 cs.CR

15 pages, 9 figures, 8 tables, Preprint

**SubmitDate**: 2026-01-05    [abs](http://arxiv.org/abs/2503.24191v2) [paper-pdf](https://arxiv.org/pdf/2503.24191v2)

**Confidence**: 0.95

**Authors**: Shuoming Zhang, Jiacheng Zhao, Hanyuan Dong, Ruiyuan Xu, Zhicheng Li, Yangyu Zhang, Shuaijiang Li, Yuan Wen, Chunwei Xia, Zheng Wang, Xiaobing Feng, Huimin Cui

**Abstract**: Content Warning: This paper may contain unsafe or harmful content generated by LLMs that may be offensive to readers. Large Language Models (LLMs) are extensively used as tooling platforms through structured output APIs to ensure syntax compliance so that robust integration with existing software, like agent systems, can be achieved. However, the feature enabling the functionality of grammar-guided structured output presents significant security vulnerabilities. In this work, we reveal a critical control-plane attack surface orthogonal to traditional data-plane vulnerabilities. We introduce Constrained Decoding Attack (CDA), a novel jailbreak class that weaponizes structured output constraints to bypass both external auditing and internal safety alignment. Unlike prior attacks focused on input prompt designs, CDA operates by embedding malicious intent in schema-level grammar rules (control-plane) while maintaining benign surface prompts (data-plane). We instantiate this with two proof-of-concept attacks: EnumAttack, which embeds malicious content in enum fields; and the more evasive DictAttack, which decouples the malicious payload across a benign prompt and a dictionary-based grammar. Our evaluation spans a broad spectrum of 13 proprietary/open-weight models. In particular, DictAttack achieves 94.3--99.5% ASR across five benchmarks on gpt-5, gemini-2.5-pro, deepseek-r1, and gpt-oss-120b. Furthermore, we demonstrate the significant challenge in defending against these threats: while basic grammar auditing mitigates EnumAttack, the more sophisticated DictAttack maintains a 75.8% ASR even against multiple state-of-the-art jailbreak guardrails. This exposes a critical "semantic gap" in current safety architectures and underscores the urgent need for cross-plane defenses that can bridge the data and control planes to secure the LLM generation pipeline.

摘要: 内容警告：本文可能包含LLM生成的不安全或有害内容，可能对读者造成冒犯。大型语言模型（LLMs）通过结构化输出API被广泛用作工具平台，以确保语法合规性，从而实现与现有软件（如智能体系统）的稳健集成。然而，支持语法引导结构化输出的功能存在严重的安全漏洞。在这项工作中，我们揭示了一个与传统数据平面漏洞正交的关键控制平面攻击面。我们引入了约束解码攻击（CDA），这是一种新型越狱类别，它利用结构化输出约束来绕过外部审计和内部安全对齐。与先前专注于输入提示设计的攻击不同，CDA通过在模式级语法规则（控制平面）中嵌入恶意意图，同时保持良性的表面提示（数据平面）来实施攻击。我们通过两个概念验证攻击实例化此方法：EnumAttack，将恶意内容嵌入枚举字段；以及更具规避性的DictAttack，它将恶意负载解耦到良性提示和基于字典的语法中。我们的评估涵盖了13个专有/开源模型的广泛范围。特别是，DictAttack在gpt-5、gemini-2.5-pro、deepseek-r1和gpt-oss-120b上的五个基准测试中实现了94.3-99.5%的攻击成功率。此外，我们证明了防御这些威胁的重大挑战：虽然基本的语法审计可以缓解EnumAttack，但更复杂的DictAttack即使面对多个最先进的越狱防护措施，仍能保持75.8%的攻击成功率。这暴露了当前安全架构中关键的“语义鸿沟”，并强调了迫切需要能够桥接数据平面和控制平面的跨平面防御，以保护LLM生成管道。



## **45. Jailbreak-AudioBench: In-Depth Evaluation and Analysis of Jailbreak Threats for Large Audio Language Models**

Jailbreak-AudioBench：大型音频语言模型越狱威胁的深度评估与分析 cs.SD

**SubmitDate**: 2026-01-12    [abs](http://arxiv.org/abs/2501.13772v4) [paper-pdf](https://arxiv.org/pdf/2501.13772v4)

**Confidence**: 0.95

**Authors**: Hao Cheng, Erjia Xiao, Jing Shao, Yichi Wang, Le Yang, Chao Shen, Philip Torr, Jindong Gu, Renjing Xu

**Abstract**: Large Language Models (LLMs) demonstrate impressive zero-shot performance across a wide range of natural language processing tasks. Integrating various modality encoders further expands their capabilities, giving rise to Multimodal Large Language Models (MLLMs) that process not only text but also visual and auditory modality inputs. However, these advanced capabilities may also pose significant safety problems, as models can be exploited to generate harmful or inappropriate content through jailbreak attacks. While prior work has extensively explored how manipulating textual or visual modality inputs can circumvent safeguards in LLMs and MLLMs, the vulnerability of audio-specific jailbreak on Large Audio-Language Models (LALMs) remains largely underexplored. To address this gap, we introduce Jailbreak-AudioBench, which consists of the Toolbox, curated Dataset, and comprehensive Benchmark. The Toolbox supports not only text-to-audio conversion but also various editing techniques for injecting audio hidden semantics. The curated Dataset provides diverse explicit and implicit jailbreak audio examples in both original and edited forms. Utilizing this dataset, we evaluate multiple state-of-the-art LALMs and establish the most comprehensive Jailbreak benchmark to date for audio modality. Finally, Jailbreak-AudioBench establishes a foundation for advancing future research on LALMs safety alignment by enabling the in-depth exposure of more powerful jailbreak threats, such as query-based audio editing, and by facilitating the development of effective defense mechanisms.

摘要: 大型语言模型（LLMs）在广泛的自然语言处理任务中展现出卓越的零样本性能。集成多种模态编码器进一步扩展了其能力，催生了能够处理文本、视觉和听觉模态输入的多模态大型语言模型（MLLMs）。然而，这些先进能力也可能引发严重的安全问题，因为模型可能通过越狱攻击被利用来生成有害或不适当的内容。尽管先前研究已广泛探讨了如何通过操纵文本或视觉模态输入来绕过LLMs和MLLMs的安全防护，但针对大型音频语言模型（LALMs）的音频特定越狱攻击的脆弱性仍未得到充分探索。为填补这一空白，我们提出了Jailbreak-AudioBench，它包含工具箱、精选数据集和综合基准测试。该工具箱不仅支持文本到音频的转换，还提供多种用于注入音频隐藏语义的编辑技术。精选数据集以原始和编辑形式提供了多样化的显式和隐式越狱音频示例。利用该数据集，我们评估了多个最先进的LALMs，并建立了迄今为止最全面的音频模态越狱基准。最后，Jailbreak-AudioBench通过深入揭示更强大的越狱威胁（如基于查询的音频编辑），并促进有效防御机制的开发，为推进未来LALMs安全对齐研究奠定了基础。



## **46. Zero-Permission Manipulation: Can We Trust Large Multimodal Model Powered GUI Agents?**

零权限操控：我们能否信任基于大型多模态模型的GUI代理？ cs.CR

**SubmitDate**: 2026-01-18    [abs](http://arxiv.org/abs/2601.12349v1) [paper-pdf](https://arxiv.org/pdf/2601.12349v1)

**Confidence**: 0.95

**Authors**: Yi Qian, Kunwei Qian, Xingbang He, Ligeng Chen, Jikang Zhang, Tiantai Zhang, Haiyang Wei, Linzhang Wang, Hao Wu, Bing Mao

**Abstract**: Large multimodal model powered GUI agents are emerging as high-privilege operators on mobile platforms, entrusted with perceiving screen content and injecting inputs. However, their design operates under the implicit assumption of Visual Atomicity: that the UI state remains invariant between observation and action. We demonstrate that this assumption is fundamentally invalid in Android, creating a critical attack surface.   We present Action Rebinding, a novel attack that allows a seemingly-benign app with zero dangerous permissions to rebind an agent's execution. By exploiting the inevitable observation-to-action gap inherent in the agent's reasoning pipeline, the attacker triggers foreground transitions to rebind the agent's planned action toward the target app. We weaponize the agent's task-recovery logic and Android's UI state preservation to orchestrate programmable, multi-step attack chains. Furthermore, we introduce an Intent Alignment Strategy (IAS) that manipulates the agent's reasoning process to rationalize UI states, enabling it to bypass verification gates (e.g., confirmation dialogs) that would otherwise be rejected.   We evaluate Action Rebinding Attacks on six widely-used Android GUI agents across 15 tasks. Our results demonstrate a 100% success rate for atomic action rebinding and the ability to reliably orchestrate multi-step attack chains. With IAS, the success rate in bypassing verification gates increases (from 0% to up to 100%). Notably, the attacker application requires no sensitive permissions and contains no privileged API calls, achieving a 0% detection rate across malware scanners (e.g., VirusTotal). Our findings reveal a fundamental architectural flaw in current agent-OS integration and provide critical insights for the secure design of future agent systems. To access experimental logs and demonstration videos, please contact yi_qian@smail.nju.edu.cn.

摘要: 基于大型多模态模型的GUI代理正成为移动平台上的高权限操作者，负责感知屏幕内容并注入输入。然而，其设计隐含了视觉原子性假设：即UI状态在观察与操作之间保持不变。我们证明这一假设在Android系统中根本不成立，从而形成了关键攻击面。我们提出动作重绑定攻击，这种新型攻击允许零危险权限的看似良性应用重新绑定代理的执行过程。攻击者利用代理推理流程中固有的观察-操作间隙，触发前台切换以将代理计划动作重定向至目标应用。我们利用代理的任务恢复逻辑和Android的UI状态保持机制，编排可编程的多步骤攻击链。此外，我们引入意图对齐策略，通过操控代理的推理过程使其合理化UI状态，从而绕过原本会被拒绝的验证关卡（如确认对话框）。我们在6个广泛使用的Android GUI代理上对15项任务进行了动作重绑定攻击评估。结果显示：原子动作重绑定成功率100%，多步骤攻击链可稳定编排。采用意图对齐策略后，验证关卡绕过成功率从0%提升至最高100%。值得注意的是，攻击应用无需敏感权限且不包含特权API调用，在恶意软件扫描器（如VirusTotal）中检测率为0%。我们的研究揭示了当前代理-操作系统集成中的根本性架构缺陷，为未来代理系统的安全设计提供了关键见解。实验日志与演示视频请联系yi_qian@smail.nju.edu.cn。



## **47. When Bots Take the Bait: Exposing and Mitigating the Emerging Social Engineering Attack in Web Automation Agent**

当机器人上钩时：揭示并缓解Web自动化代理中新兴的社会工程攻击 cs.CR

**SubmitDate**: 2026-01-12    [abs](http://arxiv.org/abs/2601.07263v1) [paper-pdf](https://arxiv.org/pdf/2601.07263v1)

**Confidence**: 0.95

**Authors**: Xinyi Wu, Geng Hong, Yueyue Chen, MingXuan Liu, Feier Jin, Xudong Pan, Jiarun Dai, Baojun Liu

**Abstract**: Web agents, powered by large language models (LLMs), are increasingly deployed to automate complex web interactions. The rise of open-source frameworks (e.g., Browser Use, Skyvern-AI) has accelerated adoption, but also broadened the attack surface. While prior research has focused on model threats such as prompt injection and backdoors, the risks of social engineering remain largely unexplored. We present the first systematic study of social engineering attacks against web automation agents and design a pluggable runtime mitigation solution. On the attack side, we introduce the AgentBait paradigm, which exploits intrinsic weaknesses in agent execution: inducement contexts can distort the agent's reasoning and steer it toward malicious objectives misaligned with the intended task. On the defense side, we propose SUPERVISOR, a lightweight runtime module that enforces environment and intention consistency alignment between webpage context and intended goals to mitigate unsafe operations before execution.   Empirical results show that mainstream frameworks are highly vulnerable to AgentBait, with an average attack success rate of 67.5% and peaks above 80% under specific strategies (e.g., trusted identity forgery). Compared with existing lightweight defenses, our module can be seamlessly integrated across different web automation frameworks and reduces attack success rates by up to 78.1% on average while incurring only a 7.7% runtime overhead and preserving usability. This work reveals AgentBait as a critical new threat surface for web agents and establishes a practical, generalizable defense, advancing the security of this rapidly emerging ecosystem. We reported the details of this attack to the framework developers and received acknowledgment before submission.

摘要: 基于大语言模型（LLMs）的Web代理正被越来越多地部署以自动化复杂的网络交互。开源框架（如Browser Use、Skyvern-AI）的兴起加速了其应用，但也扩大了攻击面。尽管先前研究主要关注提示注入和后门等模型威胁，但社会工程攻击的风险在很大程度上尚未被探索。我们首次系统研究了针对Web自动化代理的社会工程攻击，并设计了一种可插拔的运行时缓解方案。在攻击方面，我们提出了AgentBait范式，该范式利用代理执行过程中的固有弱点：诱导性上下文可能扭曲代理的推理，使其偏离预期任务而转向恶意目标。在防御方面，我们提出了SUPERVISOR，这是一个轻量级运行时模块，通过强制网页上下文与预期目标之间的环境和意图一致性对齐，在执行前缓解不安全操作。实证结果表明，主流框架对AgentBait高度脆弱，平均攻击成功率达67.5%，在特定策略（如可信身份伪造）下峰值超过80%。与现有轻量级防御方案相比，我们的模块可无缝集成到不同的Web自动化框架中，平均将攻击成功率降低高达78.1%，同时仅产生7.7%的运行时开销且不影响可用性。这项工作揭示了AgentBait作为Web代理的关键新威胁面，并建立了一种实用、可推广的防御机制，推动了这一快速兴起的生态系统的安全性。我们已向相关框架开发者报告了此攻击的详细信息，并在提交前获得了确认。



## **48. Overcoming the Retrieval Barrier: Indirect Prompt Injection in the Wild for LLM Systems**

突破检索屏障：针对LLM系统的间接提示注入实战 cs.CR

**SubmitDate**: 2026-01-11    [abs](http://arxiv.org/abs/2601.07072v1) [paper-pdf](https://arxiv.org/pdf/2601.07072v1)

**Confidence**: 0.95

**Authors**: Hongyan Chang, Ergute Bao, Xinjian Luo, Ting Yu

**Abstract**: Large language models (LLMs) increasingly rely on retrieving information from external corpora. This creates a new attack surface: indirect prompt injection (IPI), where hidden instructions are planted in the corpora and hijack model behavior once retrieved. Previous studies have highlighted this risk but often avoid the hardest step: ensuring that malicious content is actually retrieved. In practice, unoptimized IPI is rarely retrieved under natural queries, which leaves its real-world impact unclear.   We address this challenge by decomposing the malicious content into a trigger fragment that guarantees retrieval and an attack fragment that encodes arbitrary attack objectives. Based on this idea, we design an efficient and effective black-box attack algorithm that constructs a compact trigger fragment to guarantee retrieval for any attack fragment. Our attack requires only API access to embedding models, is cost-efficient (as little as $0.21 per target user query on OpenAI's embedding models), and achieves near-100% retrieval across 11 benchmarks and 8 embedding models (including both open-source models and proprietary services).   Based on this attack, we present the first end-to-end IPI exploits under natural queries and realistic external corpora, spanning both RAG and agentic systems with diverse attack objectives. These results establish IPI as a practical and severe threat: when a user issued a natural query to summarize emails on frequently asked topics, a single poisoned email was sufficient to coerce GPT-4o into exfiltrating SSH keys with over 80% success in a multi-agent workflow. We further evaluate several defenses and find that they are insufficient to prevent the retrieval of malicious text, highlighting retrieval as a critical open vulnerability.

摘要: 大型语言模型（LLMs）日益依赖从外部语料库检索信息，这催生了一个新的攻击面：间接提示注入（IPI）。攻击者将隐藏指令植入语料库，一旦被检索即可劫持模型行为。先前研究虽已指出此风险，但往往回避了最困难的环节：确保恶意内容能被实际检索到。实践中，未经优化的IPI在自然查询下很少被检索，其真实影响尚不明确。

为解决这一挑战，我们将恶意内容分解为两个部分：保证检索的触发片段和编码任意攻击目标的攻击片段。基于此思路，我们设计了一种高效的黑盒攻击算法，通过构建紧凑的触发片段来确保任意攻击片段都能被检索。该攻击仅需嵌入模型的API访问权限，成本极低（在OpenAI嵌入模型上每目标用户查询仅需约0.21美元），并在11个基准测试和8种嵌入模型（包括开源模型和专有服务）上实现了接近100%的检索成功率。

基于此攻击，我们首次在自然查询和真实外部语料库条件下实现了端到端的IPI攻击，覆盖了RAG和智能体系统，并展示了多样化的攻击目标。这些结果证实IPI已成为切实且严重的威胁：当用户对常见主题的邮件进行自然查询时，仅需一封被污染的邮件即可在多智能体工作流中诱使GPT-4o泄露SSH密钥，成功率超过80%。我们进一步评估了多种防御方案，发现它们均无法有效阻止恶意文本的检索，表明检索环节仍是亟待解决的关键漏洞。



## **49. Memory Poisoning Attack and Defense on Memory Based LLM-Agents**

基于记忆的LLM智能体记忆中毒攻击与防御 cs.CR

**SubmitDate**: 2026-01-12    [abs](http://arxiv.org/abs/2601.05504v2) [paper-pdf](https://arxiv.org/pdf/2601.05504v2)

**Confidence**: 0.95

**Authors**: Balachandra Devarangadi Sunil, Isheeta Sinha, Piyush Maheshwari, Shantanu Todmal, Shreyan Mallik, Shuchi Mishra

**Abstract**: Large language model agents equipped with persistent memory are vulnerable to memory poisoning attacks, where adversaries inject malicious instructions through query only interactions that corrupt the agents long term memory and influence future responses. Recent work demonstrated that the MINJA (Memory Injection Attack) achieves over 95 % injection success rate and 70 % attack success rate under idealized conditions. However, the robustness of these attacks in realistic deployments and effective defensive mechanisms remain understudied. This work addresses these gaps through systematic empirical evaluation of memory poisoning attacks and defenses in Electronic Health Record (EHR) agents. We investigate attack robustness by varying three critical dimensions: initial memory state, number of indication prompts, and retrieval parameters. Our experiments on GPT-4o-mini, Gemini-2.0-Flash and Llama-3.1-8B-Instruct models using MIMIC-III clinical data reveal that realistic conditions with pre-existing legitimate memories dramatically reduce attack effectiveness. We then propose and evaluate two novel defense mechanisms: (1) Input/Output Moderation using composite trust scoring across multiple orthogonal signals, and (2) Memory Sanitization with trust-aware retrieval employing temporal decay and pattern-based filtering. Our defense evaluation reveals that effective memory sanitization requires careful trust threshold calibration to prevent both overly conservative rejection (blocking all entries) and insufficient filtering (missing subtle attacks), establishing important baselines for future adaptive defense mechanisms. These findings provide crucial insights for securing memory-augmented LLM agents in production environments.

摘要: 配备持久记忆的大型语言模型智能体易受记忆中毒攻击，攻击者仅通过查询交互注入恶意指令，即可污染智能体长期记忆并影响其未来响应。近期研究表明，MINJA（记忆注入攻击）在理想条件下可实现超过95%的注入成功率和70%的攻击成功率。然而，这些攻击在实际部署中的鲁棒性及有效防御机制仍缺乏深入研究。本研究通过对电子健康记录（EHR）智能体进行系统化实证评估，填补了这些空白。我们通过改变三个关键维度来研究攻击鲁棒性：初始记忆状态、诱导提示数量及检索参数。基于MIMIC-III临床数据在GPT-4o-mini、Gemini-2.0-Flash和Llama-3.1-8B-Instruct模型上的实验表明，存在预先合法记忆的实际场景会显著降低攻击有效性。我们随后提出并评估了两种新型防御机制：（1）基于多正交信号复合信任评分的输入/输出审核；（2）采用时间衰减和模式过滤的信任感知检索记忆净化。防御评估表明，有效的记忆净化需要精细的信任阈值校准，以避免过度保守（拒绝所有条目）和过滤不足（遗漏隐蔽攻击），为未来自适应防御机制建立了重要基线。这些发现为生产环境中记忆增强型LLM智能体的安全防护提供了关键见解。



## **50. Defense Against Indirect Prompt Injection via Tool Result Parsing**

通过工具结果解析防御间接提示注入攻击 cs.AI

20 pages, 3 figures, 5 tables

**SubmitDate**: 2026-01-08    [abs](http://arxiv.org/abs/2601.04795v1) [paper-pdf](https://arxiv.org/pdf/2601.04795v1)

**Confidence**: 0.95

**Authors**: Qiang Yu, Xinran Cheng, Chuanyi Liu

**Abstract**: As LLM agents transition from digital assistants to physical controllers in autonomous systems and robotics, they face an escalating threat from indirect prompt injection. By embedding adversarial instructions into the results of tool calls, attackers can hijack the agent's decision-making process to execute unauthorized actions. This vulnerability poses a significant risk as agents gain more direct control over physical environments. Existing defense mechanisms against Indirect Prompt Injection (IPI) generally fall into two categories. The first involves training dedicated detection models; however, this approach entails high computational overhead for both training and inference, and requires frequent updates to keep pace with evolving attack vectors. Alternatively, prompt-based methods leverage the inherent capabilities of LLMs to detect or ignore malicious instructions via prompt engineering. Despite their flexibility, most current prompt-based defenses suffer from high Attack Success Rates (ASR), demonstrating limited robustness against sophisticated injection attacks. In this paper, we propose a novel method that provides LLMs with precise data via tool result parsing while effectively filtering out injected malicious code. Our approach achieves competitive Utility under Attack (UA) while maintaining the lowest Attack Success Rate (ASR) to date, significantly outperforming existing methods. Code is available at GitHub.

摘要: 随着LLM智能体从数字助手转变为自主系统和机器人中的物理控制器，它们面临着来自间接提示注入日益严重的威胁。攻击者通过在工具调用结果中嵌入对抗性指令，可以劫持智能体的决策过程以执行未授权操作。随着智能体对物理环境获得更直接的控制权，这一漏洞构成了重大风险。现有的间接提示注入（IPI）防御机制主要分为两类：第一类涉及训练专用检测模型，但这种方法在训练和推理阶段都需要较高的计算开销，并且需要频繁更新以跟上不断演变的攻击向量；第二类是基于提示的方法，通过提示工程利用LLM的固有能力来检测或忽略恶意指令。尽管具有灵活性，但当前大多数基于提示的防御方法攻击成功率（ASR）较高，在面对复杂注入攻击时表现出有限的鲁棒性。本文提出了一种新颖的方法，通过工具结果解析为LLM提供精确数据，同时有效过滤注入的恶意代码。我们的方法在保持迄今为止最低攻击成功率（ASR）的同时，实现了具有竞争力的受攻击下效用（UA），显著优于现有方法。代码已在GitHub上开源。



## **51. BackdoorAgent: A Unified Framework for Backdoor Attacks on LLM-based Agents**

BackdoorAgent：针对基于LLM智能体的后门攻击统一框架 cs.AI

**SubmitDate**: 2026-01-11    [abs](http://arxiv.org/abs/2601.04566v2) [paper-pdf](https://arxiv.org/pdf/2601.04566v2)

**Confidence**: 0.95

**Authors**: Yunhao Feng, Yige Li, Yutao Wu, Yingshui Tan, Yanming Guo, Yifan Ding, Kun Zhai, Xingjun Ma, Yu-Gang Jiang

**Abstract**: Large language model (LLM) agents execute tasks through multi-step workflows that combine planning, memory, and tool use. While this design enables autonomy, it also expands the attack surface for backdoor threats. Backdoor triggers injected into specific stages of an agent workflow can persist through multiple intermediate states and adversely influence downstream outputs. However, existing studies remain fragmented and typically analyze individual attack vectors in isolation, leaving the cross-stage interaction and propagation of backdoor triggers poorly understood from an agent-centric perspective. To fill this gap, we propose \textbf{BackdoorAgent}, a modular and stage-aware framework that provides a unified, agent-centric view of backdoor threats in LLM agents. BackdoorAgent structures the attack surface into three functional stages of agentic workflows, including \textbf{planning attacks}, \textbf{memory attacks}, and \textbf{tool-use attacks}, and instruments agent execution to enable systematic analysis of trigger activation and propagation across different stages. Building on this framework, we construct a standardized benchmark spanning four representative agent applications: \textbf{Agent QA}, \textbf{Agent Code}, \textbf{Agent Web}, and \textbf{Agent Drive}, covering both language-only and multimodal settings. Our empirical analysis shows that \textit{triggers implanted at a single stage can persist across multiple steps and propagate through intermediate states.} For instance, when using a GPT-based backbone, we observe trigger persistence in 43.58\% of planning attacks, 77.97\% of memory attacks, and 60.28\% of tool-stage attacks, highlighting the vulnerabilities of the agentic workflow itself to backdoor threats. To facilitate reproducibility and future research, our code and benchmark are publicly available at GitHub.

摘要: 大型语言模型（LLM）智能体通过结合规划、记忆和工具使用的多步骤工作流执行任务。虽然这种设计实现了自主性，但也扩大了后门威胁的攻击面。注入智能体工作流特定阶段的后门触发器可以持续存在于多个中间状态，并对下游输出产生不利影响。然而，现有研究仍然零散，通常孤立地分析个别攻击向量，从智能体中心视角来看，对后门触发器的跨阶段交互和传播机制理解不足。为填补这一空白，我们提出\textbf{BackdoorAgent}，一个模块化且阶段感知的框架，为LLM智能体中的后门威胁提供统一的智能体中心视角。BackdoorAgent将攻击面结构化为智能体工作流的三个功能阶段，包括\textbf{规划攻击}、\textbf{记忆攻击}和\textbf{工具使用攻击}，并通过检测智能体执行过程，实现对不同阶段触发器激活和传播的系统性分析。基于此框架，我们构建了一个涵盖四个代表性智能体应用的标准化基准：\textbf{Agent QA}、\textbf{Agent Code}、\textbf{Agent Web}和\textbf{Agent Drive}，覆盖纯语言和多模态场景。实证分析表明，\textit{植入单个阶段的触发器可以持续存在于多个步骤，并通过中间状态传播。}例如，在使用基于GPT的骨干模型时，我们观察到规划攻击中43.58%、记忆攻击中77.97%以及工具阶段攻击中60.28%的触发器持续性，凸显了智能体工作流本身对后门威胁的脆弱性。为促进可复现性和未来研究，我们的代码和基准已在GitHub公开。



## **52. GAMBIT: A Gamified Jailbreak Framework for Multimodal Large Language Models**

GAMBIT：一种面向多模态大语言模型的游戏化越狱框架 cs.CV

**SubmitDate**: 2026-01-06    [abs](http://arxiv.org/abs/2601.03416v1) [paper-pdf](https://arxiv.org/pdf/2601.03416v1)

**Confidence**: 0.95

**Authors**: Xiangdong Hu, Yangyang Jiang, Qin Hu, Xiaojun Jia

**Abstract**: Multimodal Large Language Models (MLLMs) have become widely deployed, yet their safety alignment remains fragile under adversarial inputs. Previous work has shown that increasing inference steps can disrupt safety mechanisms and lead MLLMs to generate attacker-desired harmful content. However, most existing attacks focus on increasing the complexity of the modified visual task itself and do not explicitly leverage the model's own reasoning incentives. This leads to them underperforming on reasoning models (Models with Chain-of-Thoughts) compared to non-reasoning ones (Models without Chain-of-Thoughts). If a model can think like a human, can we influence its cognitive-stage decisions so that it proactively completes a jailbreak? To validate this idea, we propose GAMBI} (Gamified Adversarial Multimodal Breakout via Instructional Traps), a novel multimodal jailbreak framework that decomposes and reassembles harmful visual semantics, then constructs a gamified scene that drives the model to explore, reconstruct intent, and answer as part of winning the game. The resulting structured reasoning chain increases task complexity in both vision and text, positioning the model as a participant whose goal pursuit reduces safety attention and induces it to answer the reconstructed malicious query. Extensive experiments on popular reasoning and non-reasoning MLLMs demonstrate that GAMBIT achieves high Attack Success Rates (ASR), reaching 92.13% on Gemini 2.5 Flash, 91.20% on QvQ-MAX, and 85.87% on GPT-4o, significantly outperforming baselines.

摘要: 多模态大语言模型（MLLMs）已得到广泛部署，但其安全对齐机制在面对对抗性输入时仍显脆弱。先前研究表明，增加推理步骤可能破坏安全机制，导致MLLMs生成攻击者期望的有害内容。然而，现有攻击方法大多聚焦于提升视觉任务本身的复杂度，未能显式利用模型自身的推理激励，导致其在推理模型（含思维链）上的表现逊于非推理模型（无思维链）。若模型能像人类一样思考，我们能否影响其认知阶段的决策，使其主动完成越狱？为验证这一设想，我们提出GAMBIT（基于指令陷阱的游戏化对抗多模态突破框架），该新型多模态越狱框架通过解构并重组有害视觉语义，构建游戏化场景驱动模型进行探索、意图重构与应答，作为赢得游戏的一部分。由此产生的结构化推理链同时增加了视觉与文本任务复杂度，将模型定位为参与者——其目标追求会分散安全注意力，诱导其回答重构后的恶意查询。在主流推理与非推理MLLMs上的大量实验表明，GAMBIT实现了高攻击成功率（ASR）：在Gemini 2.5 Flash上达92.13%，QvQ-MAX上达91.20%，GPT-4o上达85.87%，显著超越基线方法。



## **53. OpenRT: An Open-Source Red Teaming Framework for Multimodal LLMs**

OpenRT：面向多模态大语言模型的开源红队测试框架 cs.CR

**SubmitDate**: 2026-01-10    [abs](http://arxiv.org/abs/2601.01592v2) [paper-pdf](https://arxiv.org/pdf/2601.01592v2)

**Confidence**: 0.95

**Authors**: Xin Wang, Yunhao Chen, Juncheng Li, Yixu Wang, Yang Yao, Tianle Gu, Jie Li, Yan Teng, Yingchun Wang, Xia Hu

**Abstract**: The rapid integration of Multimodal Large Language Models (MLLMs) into critical applications is increasingly hindered by persistent safety vulnerabilities. However, existing red-teaming benchmarks are often fragmented, limited to single-turn text interactions, and lack the scalability required for systematic evaluation. To address this, we introduce OpenRT, a unified, modular, and high-throughput red-teaming framework designed for comprehensive MLLM safety evaluation. At its core, OpenRT architects a paradigm shift in automated red-teaming by introducing an adversarial kernel that enables modular separation across five critical dimensions: model integration, dataset management, attack strategies, judging methods, and evaluation metrics. By standardizing attack interfaces, it decouples adversarial logic from a high-throughput asynchronous runtime, enabling systematic scaling across diverse models. Our framework integrates 37 diverse attack methodologies, spanning white-box gradients, multi-modal perturbations, and sophisticated multi-agent evolutionary strategies. Through an extensive empirical study on 20 advanced models (including GPT-5.2, Claude 4.5, and Gemini 3 Pro), we expose critical safety gaps: even frontier models fail to generalize across attack paradigms, with leading models exhibiting average Attack Success Rates as high as 49.14%. Notably, our findings reveal that reasoning models do not inherently possess superior robustness against complex, multi-turn jailbreaks. By open-sourcing OpenRT, we provide a sustainable, extensible, and continuously maintained infrastructure that accelerates the development and standardization of AI safety.

摘要: 多模态大语言模型（MLLMs）在关键应用中的快速部署正因其持续存在的安全漏洞而日益受阻。然而，现有的红队测试基准往往较为零散，仅限于单轮文本交互，且缺乏系统化评估所需的可扩展性。为此，我们推出OpenRT——一个统一、模块化、高吞吐的红队测试框架，专为全面的MLLM安全评估而设计。其核心在于通过引入一个对抗内核，在模型集成、数据集管理、攻击策略、评判方法和评估指标这五个关键维度上实现模块化分离，从而架构了自动化红队测试的范式转变。通过标准化攻击接口，该框架将对抗逻辑与高吞吐异步运行时解耦，实现了跨多样化模型的系统化扩展。我们的框架集成了37种不同的攻击方法，涵盖白盒梯度攻击、多模态扰动以及复杂的多智能体进化策略。通过对20个先进模型（包括GPT-5.2、Claude 4.5和Gemini 3 Pro）的广泛实证研究，我们揭示了关键的安全缺陷：即使是前沿模型也未能泛化至所有攻击范式，领先模型的平均攻击成功率高达49.14%。值得注意的是，我们的研究发现推理模型在面对复杂多轮越狱攻击时并不具备固有的更强鲁棒性。通过开源OpenRT，我们提供了一个可持续、可扩展且持续维护的基础设施，以加速AI安全领域的开发与标准化进程。



## **54. Language Model Agents Under Attack: A Cross Model-Benchmark of Profit-Seeking Behaviors in Customer Service**

语言模型代理遭受攻击：客户服务中逐利行为的跨模型基准测试 cs.CR

**SubmitDate**: 2025-12-30    [abs](http://arxiv.org/abs/2512.24415v1) [paper-pdf](https://arxiv.org/pdf/2512.24415v1)

**Confidence**: 0.95

**Authors**: Jingyu Zhang

**Abstract**: Customer-service LLM agents increasingly make policy-bound decisions (refunds, rebooking, billing disputes), but the same ``helpful'' interaction style can be exploited: a small fraction of users can induce unauthorized concessions, shifting costs to others and eroding trust in agentic workflows. We present a cross-domain benchmark of profit-seeking direct prompt injection in customer-service interactions, spanning 10 service domains and 100 realistic attack scripts grouped into five technique families. Across five widely used models under a unified rubric with uncertainty reporting, attacks are highly domain-dependent (airline support is most exploitable) and technique-dependent (payload splitting is most consistently effective). We release data and evaluation code to support reproducible auditing and to inform the design of oversight and recovery workflows for trustworthy, human centered agent interfaces.

摘要: 客户服务大语言模型代理越来越多地做出政策约束下的决策（退款、改签、账单争议），但同样的“乐于助人”交互风格可能被利用：少数用户能够诱导未经授权的让步，将成本转嫁给他人，并削弱对代理工作流程的信任。我们提出了一个跨领域基准，用于评估客户服务交互中的逐利型直接提示注入攻击，涵盖10个服务领域和100个现实攻击脚本，这些脚本分为五个技术类别。在采用统一评估框架和不确定性报告的五种常用模型中，攻击效果高度依赖领域（航空客服最易受攻击）和技术（载荷分割技术最持续有效）。我们公开数据和评估代码，以支持可复现的审计工作，并为设计可信赖、以人为本的代理界面的监督与恢复工作流程提供参考。



## **55. The Silicon Psyche: Anthropomorphic Vulnerabilities in Large Language Models**

硅基心智：大型语言模型中的人形化漏洞 cs.CR

**SubmitDate**: 2025-12-30    [abs](http://arxiv.org/abs/2601.00867v1) [paper-pdf](https://arxiv.org/pdf/2601.00867v1)

**Confidence**: 0.95

**Authors**: Giuseppe Canale, Kashyap Thimmaraju

**Abstract**: Large Language Models (LLMs) are rapidly transitioning from conversational assistants to autonomous agents embedded in critical organizational functions, including Security Operations Centers (SOCs), financial systems, and infrastructure management. Current adversarial testing paradigms focus predominantly on technical attack vectors: prompt injection, jailbreaking, and data exfiltration. We argue this focus is catastrophically incomplete. LLMs, trained on vast corpora of human-generated text, have inherited not merely human knowledge but human \textit{psychological architecture} -- including the pre-cognitive vulnerabilities that render humans susceptible to social engineering, authority manipulation, and affective exploitation. This paper presents the first systematic application of the Cybersecurity Psychology Framework (\cpf{}), a 100-indicator taxonomy of human psychological vulnerabilities, to non-human cognitive agents. We introduce the \textbf{Synthetic Psychometric Assessment Protocol} (\sysname{}), a methodology for converting \cpf{} indicators into adversarial scenarios targeting LLM decision-making. Our preliminary hypothesis testing across seven major LLM families reveals a disturbing pattern: while models demonstrate robust defenses against traditional jailbreaks, they exhibit critical susceptibility to authority-gradient manipulation, temporal pressure exploitation, and convergent-state attacks that mirror human cognitive failure modes. We term this phenomenon \textbf{Anthropomorphic Vulnerability Inheritance} (AVI) and propose that the security community must urgently develop ``psychological firewalls'' -- intervention mechanisms adapted from the Cybersecurity Psychology Intervention Framework (\cpif{}) -- to protect AI agents operating in adversarial environments.

摘要: 大型语言模型正迅速从对话助手转变为嵌入关键组织功能（包括安全运营中心、金融系统和基础设施管理）的自主代理。当前的对抗性测试范式主要聚焦于技术攻击向量：提示注入、越狱和数据窃取。我们认为这种关注存在灾难性的不完整性。LLMs基于海量人类生成文本进行训练，不仅继承了人类知识，更继承了人类的心理架构——包括使人类易受社会工程、权威操纵和情感利用影响的认知前漏洞。本文首次系统性地将网络安全心理学框架（一个包含100项指标的人类心理漏洞分类法）应用于非人类认知代理。我们介绍了合成心理测量评估协议，这是一种将CPF指标转化为针对LLM决策的对抗场景的方法论。我们对七大主流LLM家族的初步假设测试揭示了一个令人不安的模式：虽然模型对传统越狱攻击表现出强大防御能力，但在权威梯度操纵、时间压力利用和趋同状态攻击方面却显示出关键脆弱性——这些恰恰反映了人类的认知失效模式。我们将此现象称为人形化漏洞继承，并建议安全社区必须紧急开发“心理防火墙”——即从网络安全心理学干预框架中调整的干预机制——以保护在对抗环境中运行的AI代理。



## **56. Breaking Audio Large Language Models by Attacking Only the Encoder: A Universal Targeted Latent-Space Audio Attack**

仅攻击编码器即可破解音频大语言模型：一种通用的目标潜在空间音频攻击 cs.SD

**SubmitDate**: 2025-12-29    [abs](http://arxiv.org/abs/2512.23881v1) [paper-pdf](https://arxiv.org/pdf/2512.23881v1)

**Confidence**: 0.95

**Authors**: Roee Ziv, Raz Lapid, Moshe Sipper

**Abstract**: Audio-language models combine audio encoders with large language models to enable multimodal reasoning, but they also introduce new security vulnerabilities. We propose a universal targeted latent space attack, an encoder-level adversarial attack that manipulates audio latent representations to induce attacker-specified outputs in downstream language generation. Unlike prior waveform-level or input-specific attacks, our approach learns a universal perturbation that generalizes across inputs and speakers and does not require access to the language model. Experiments on Qwen2-Audio-7B-Instruct demonstrate consistently high attack success rates with minimal perceptual distortion, revealing a critical and previously underexplored attack surface at the encoder level of multimodal systems.

摘要: 音频-语言模型通过将音频编码器与大语言模型结合，实现了多模态推理能力，但也引入了新的安全漏洞。我们提出了一种通用的目标潜在空间攻击，这是一种编码器层面的对抗性攻击，通过操纵音频潜在表示来诱导下游语言生成产生攻击者指定的输出。与以往的波形级或输入特定攻击不同，我们的方法学习一种通用扰动，能够跨输入和说话者泛化，且无需访问语言模型。在Qwen2-Audio-7B-Instruct上的实验表明，该方法在最小感知失真下实现了持续较高的攻击成功率，揭示了多模态系统编码器层面一个关键且此前未被充分探索的攻击面。



## **57. Toward Trustworthy Agentic AI: A Multimodal Framework for Preventing Prompt Injection Attacks**

迈向可信的智能体AI：一种防止提示注入攻击的多模态框架 cs.CR

It is accepted in a conference paper, ICCA 2025 in Bahrain on 21 to 23 December

**SubmitDate**: 2025-12-29    [abs](http://arxiv.org/abs/2512.23557v1) [paper-pdf](https://arxiv.org/pdf/2512.23557v1)

**Confidence**: 0.95

**Authors**: Toqeer Ali Syed, Mishal Ateeq Almutairi, Mahmoud Abdel Moaty

**Abstract**: Powerful autonomous systems, which reason, plan, and converse using and between numerous tools and agents, are made possible by Large Language Models (LLMs), Vision-Language Models (VLMs), and new agentic AI systems, like LangChain and GraphChain. Nevertheless, this agentic environment increases the probability of the occurrence of multimodal prompt injection (PI) attacks, in which concealed or malicious instructions carried in text, pictures, metadata, or agent-to-agent messages may spread throughout the graph and lead to unintended behavior, a breach of policy, or corruption of state. In order to mitigate these risks, this paper suggests a Cross-Agent Multimodal Provenanc- Aware Defense Framework whereby all the prompts, either user-generated or produced by upstream agents, are sanitized and all the outputs generated by an LLM are verified independently before being sent to downstream nodes. This framework contains a Text sanitizer agent, visual sanitizer agent, and output validator agent all coordinated by a provenance ledger, which keeps metadata of modality, source, and trust level throughout the entire agent network. This architecture makes sure that agent-to-agent communication abides by clear trust frames such such that injected instructions are not propagated down LangChain or GraphChain-style-workflows. The experimental assessments show that multimodal injection detection accuracy is significantly enhanced, and the cross-agent trust leakage is minimized, as well as, agentic execution pathways become stable. The framework, which expands the concept of provenance tracking and validation to the multi-agent orchestration, enhances the establishment of secure, understandable and reliable agentic AI systems.

摘要: 大型语言模型（LLMs）、视觉语言模型（VLMs）以及新兴的智能体AI系统（如LangChain和GraphChain）使得强大的自主系统成为可能，这些系统能够利用并在众多工具和智能体之间进行推理、规划和对话。然而，这种智能体环境增加了多模态提示注入（PI）攻击发生的可能性，其中隐藏在文本、图像、元数据或智能体间消息中的恶意指令可能在图中传播，导致意外行为、策略违反或状态损坏。为缓解这些风险，本文提出了一种跨智能体多模态溯源感知防御框架，该框架对所有提示（无论是用户生成还是上游智能体产生）进行净化处理，并在发送至下游节点前，对LLM生成的所有输出进行独立验证。该框架包含文本净化智能体、视觉净化智能体和输出验证智能体，所有组件通过一个溯源账本协调，该账本在整个智能体网络中记录模态、来源和信任级别的元数据。此架构确保智能体间通信遵循明确的信任框架，从而防止注入指令在LangChain或GraphChain式工作流中传播。实验评估表明，该框架显著提升了多模态注入检测的准确性，最小化了跨智能体信任泄漏，并使智能体执行路径保持稳定。通过将溯源追踪和验证概念扩展至多智能体编排，该框架有助于建立安全、可理解和可靠的智能体AI系统。



## **58. Odysseus: Jailbreaking Commercial Multimodal LLM-integrated Systems via Dual Steganography**

奥德修斯：通过双重隐写术越狱商业多模态大语言模型集成系统 cs.CR

This paper is accepted by Network and Distributed System Security Symposium (NDSS) 2026

**SubmitDate**: 2025-12-23    [abs](http://arxiv.org/abs/2512.20168v1) [paper-pdf](https://arxiv.org/pdf/2512.20168v1)

**Confidence**: 0.95

**Authors**: Songze Li, Jiameng Cheng, Yiming Li, Xiaojun Jia, Dacheng Tao

**Abstract**: By integrating language understanding with perceptual modalities such as images, multimodal large language models (MLLMs) constitute a critical substrate for modern AI systems, particularly intelligent agents operating in open and interactive environments. However, their increasing accessibility also raises heightened risks of misuse, such as generating harmful or unsafe content. To mitigate these risks, alignment techniques are commonly applied to align model behavior with human values. Despite these efforts, recent studies have shown that jailbreak attacks can circumvent alignment and elicit unsafe outputs. Currently, most existing jailbreak methods are tailored for open-source models and exhibit limited effectiveness against commercial MLLM-integrated systems, which often employ additional filters. These filters can detect and prevent malicious input and output content, significantly reducing jailbreak threats. In this paper, we reveal that the success of these safety filters heavily relies on a critical assumption that malicious content must be explicitly visible in either the input or the output. This assumption, while often valid for traditional LLM-integrated systems, breaks down in MLLM-integrated systems, where attackers can leverage multiple modalities to conceal adversarial intent, leading to a false sense of security in existing MLLM-integrated systems. To challenge this assumption, we propose Odysseus, a novel jailbreak paradigm that introduces dual steganography to covertly embed malicious queries and responses into benign-looking images. Extensive experiments on benchmark datasets demonstrate that our Odysseus successfully jailbreaks several pioneering and realistic MLLM-integrated systems, achieving up to 99% attack success rate. It exposes a fundamental blind spot in existing defenses, and calls for rethinking cross-modal security in MLLM-integrated systems.

摘要: 通过将语言理解与图像等感知模态相结合，多模态大语言模型（MLLMs）构成了现代人工智能系统的关键基础，尤其是在开放交互环境中运行的智能体。然而，其日益普及也加剧了滥用风险，例如生成有害或不安全内容。为缓解这些风险，通常采用对齐技术使模型行为与人类价值观保持一致。尽管做出了这些努力，近期研究表明，越狱攻击仍可绕过对齐机制并诱导出不安全的输出。目前，大多数现有越狱方法针对开源模型设计，对采用额外过滤器的商业MLLM集成系统效果有限。这些过滤器能够检测并阻止恶意输入和输出内容，显著降低了越狱威胁。本文揭示，这些安全过滤器的有效性严重依赖于一个关键假设：恶意内容必须在输入或输出中明确可见。该假设在传统LLM集成系统中通常成立，但在MLLM集成系统中却存在漏洞——攻击者可利用多模态隐藏对抗意图，导致现有MLLM集成系统产生虚假的安全感。为挑战这一假设，我们提出奥德修斯（Odysseus），一种新型越狱范式，通过双重隐写术将恶意查询与响应隐蔽嵌入看似良性的图像中。在基准数据集上的大量实验表明，奥德修斯成功越狱了多个前沿且现实的MLLM集成系统，攻击成功率高达99%。这暴露了现有防御机制的根本盲点，并呼吁重新审视MLLM集成系统中的跨模态安全性。



## **59. Rethinking Jailbreak Detection of Large Vision Language Models with Representational Contrastive Scoring**

基于表征对比评分的大型视觉语言模型越狱检测新思路 cs.CR

37 pages, 13 figures

**SubmitDate**: 2026-01-07    [abs](http://arxiv.org/abs/2512.12069v2) [paper-pdf](https://arxiv.org/pdf/2512.12069v2)

**Confidence**: 0.95

**Authors**: Peichun Hua, Hao Li, Shanghao Shi, Zhiyuan Yu, Ning Zhang

**Abstract**: Large Vision-Language Models (LVLMs) are vulnerable to a growing array of multimodal jailbreak attacks, necessitating defenses that are both generalizable to novel threats and efficient for practical deployment. Many current strategies fall short, either targeting specific attack patterns, which limits generalization, or imposing high computational overhead. While lightweight anomaly-detection methods offer a promising direction, we find that their common one-class design tends to confuse novel benign inputs with malicious ones, leading to unreliable over-rejection. To address this, we propose Representational Contrastive Scoring (RCS), a framework built on a key insight: the most potent safety signals reside within the LVLM's own internal representations. Our approach inspects the internal geometry of these representations, learning a lightweight projection to maximally separate benign and malicious inputs in safety-critical layers. This enables a simple yet powerful contrastive score that differentiates true malicious intent from mere novelty. Our instantiations, MCD (Mahalanobis Contrastive Detection) and KCD (K-nearest Contrastive Detection), achieve state-of-the-art performance on a challenging evaluation protocol designed to test generalization to unseen attack types. This work demonstrates that effective jailbreak detection can be achieved by applying simple, interpretable statistical methods to the appropriate internal representations, offering a practical path towards safer LVLM deployment. Our code is available on Github https://github.com/sarendis56/Jailbreak_Detection_RCS.

摘要: 大型视觉语言模型（LVLM）日益面临多种多模态越狱攻击的威胁，亟需兼具对新威胁泛化能力与高效部署可行性的防御方案。现有策略常存在局限：或针对特定攻击模式导致泛化能力不足，或计算开销过高。轻量级异常检测方法虽前景可观，但我们发现其常见的单类设计易将新型良性输入误判为恶意输入，导致不可靠的过度拒绝。为此，我们提出表征对比评分（RCS）框架，其核心洞见在于：最有效的安全信号蕴藏在LVLM自身的内部表征中。该方法通过分析表征的内部几何结构，学习轻量级投影以在安全关键层中最大化分离良性与恶意输入，从而构建出简洁而强大的对比评分机制，能有效区分真实恶意意图与单纯的新颖性。我们提出的MCD（马氏距离对比检测）和KCD（K近邻对比检测）方法，在专为测试对未知攻击类型泛化能力设计的评估协议中取得了最先进的性能。本研究表明，通过对适当内部表征应用简洁可解释的统计方法，即可实现有效的越狱检测，为LVLM的安全部署提供了实用路径。代码已开源：https://github.com/sarendis56/Jailbreak_Detection_RCS。



## **60. QueryIPI: Query-agnostic Indirect Prompt Injection on Coding Agents**

QueryIPI：针对编码代理的查询无关间接提示注入攻击 cs.CR

**SubmitDate**: 2026-01-14    [abs](http://arxiv.org/abs/2510.23675v3) [paper-pdf](https://arxiv.org/pdf/2510.23675v3)

**Confidence**: 0.95

**Authors**: Yuchong Xie, Zesen Liu, Mingyu Luo, Zhixiang Zhang, Kaikai Zhang, Yuanyuan Yuan, Zongjie Li, Ping Chen, Shuai Wang, Dongdong She

**Abstract**: Modern coding agents integrated into IDEs orchestrate powerful tools and high-privilege system access, creating a high-stakes attack surface. Prior work on Indirect Prompt Injection (IPI) is mainly query-specific, requiring particular user queries as triggers and leading to poor generalizability. We propose query-agnostic IPI, a new attack paradigm that reliably executes malicious payloads under arbitrary user queries. Our key insight is that malicious payloads should leverage the invariant prompt context (i.e., system prompt and tool descriptions) rather than variant user queries. We present QueryIPI, an automated framework that uses tool descriptions as optimizable payloads and refines them via iterative, prompt-based blackbox optimization. QueryIPI leverages system invariants for initial seed generation aligned with agent conventions, and iterative reflection to resolve instruction-following failures and safety refusals. Experiments on five simulated agents show that QueryIPI achieves up to 87% success rate, outperforming the best baseline (50%). Crucially, generated malicious descriptions transfer to real-world coding agents, highlighting a practical security risk.

摘要: 集成到IDE中的现代编码代理能够协调强大的工具和高权限系统访问，构成了高风险攻击面。现有关于间接提示注入（IPI）的研究主要是查询特定的，需要特定用户查询作为触发条件，导致泛化能力较差。我们提出查询无关IPI这一新攻击范式，能够在任意用户查询下可靠执行恶意载荷。我们的核心洞见是：恶意载荷应利用不变的提示上下文（即系统提示和工具描述），而非多变的用户查询。我们提出QueryIPI自动化框架，将工具描述作为可优化的载荷，并通过基于提示的迭代黑盒优化进行精炼。QueryIPI利用系统不变性生成符合代理规范的初始种子，并通过迭代反思解决指令遵循失败和安全拒绝问题。在五个模拟代理上的实验表明，QueryIPI成功率最高达87%，优于最佳基线方法（50%）。关键的是，生成的恶意描述能够迁移到真实世界的编码代理，凸显了实际安全风险。



## **61. Red-Teaming Coding Agents from a Tool-Invocation Perspective: An Empirical Security Assessment**

从工具调用视角对编码智能体进行红队测试：一项实证安全评估 cs.CR

**SubmitDate**: 2026-01-04    [abs](http://arxiv.org/abs/2509.05755v5) [paper-pdf](https://arxiv.org/pdf/2509.05755v5)

**Confidence**: 0.95

**Authors**: Yuchong Xie, Mingyu Luo, Zesen Liu, Zhixiang Zhang, Kaikai Zhang, Yu Liu, Zongjie Li, Ping Chen, Shuai Wang, Dongdong She

**Abstract**: Coding agents powered by large language models are becoming central modules of modern IDEs, helping users perform complex tasks by invoking tools. While powerful, tool invocation opens a substantial attack surface. Prior work has demonstrated attacks against general-purpose and domain-specific agents, but none have focused on the security risks of tool invocation in coding agents. To fill this gap, we conduct the first systematic red-teaming of six popular real-world coding agents: Cursor, Claude Code, Copilot, Windsurf, Cline, and Trae. Our red-teaming proceeds in two phases. In Phase 1, we perform prompt leakage reconnaissance to recover system prompts. We discover a general vulnerability, ToolLeak, which allows malicious prompt exfiltration through benign argument retrieval during tool invocation. In Phase 2, we hijack the agent's tool-invocation behavior using a novel two-channel prompt injection in the tool description and return values, achieving remote code execution (RCE). We adaptively construct payloads using security information leaked in Phase 1. In emulation across five backends, our method outperforms baselines on Claude-Sonnet-4, Claude-Sonnet-4.5, Grok-4, and GPT-5. On real agents, our approach succeeds on 19 of 25 agent-LLM pairs, achieving leakage on every agent using Claude and Grok backends. For tool-invocation hijacking, we obtain RCE on every tested agent-LLM pair, with our two-channel method delivering the highest success rate. We provide case studies on Cursor and Claude Code, analyze security guardrails of external and built-in tools, and conclude with practical defense recommendations.

摘要: 基于大语言模型的编码智能体正成为现代IDE的核心模块，通过调用工具帮助用户执行复杂任务。尽管功能强大，但工具调用也带来了巨大的攻击面。先前研究已展示针对通用和领域专用智能体的攻击，但尚未有工作聚焦于编码智能体中工具调用的安全风险。为填补这一空白，我们对六款流行的真实世界编码智能体进行了首次系统性红队测试：Cursor、Claude Code、Copilot、Windsurf、Cline和Trae。我们的红队测试分为两个阶段。第一阶段，我们执行提示词泄露侦察以恢复系统提示词。我们发现了一个通用漏洞ToolLeak，该漏洞允许在工具调用期间通过良性参数检索实现恶意提示词窃取。第二阶段，我们通过在工具描述和返回值中使用新型双通道提示词注入，劫持智能体的工具调用行为，实现远程代码执行（RCE）。我们利用第一阶段泄露的安全信息自适应构建攻击载荷。在五个后端平台的仿真测试中，我们的方法在Claude-Sonnet-4、Claude-Sonnet-4.5、Grok-4和GPT-5上均优于基线方法。在实际智能体测试中，我们的方法在25组智能体-LLM配对中成功入侵19组，在使用Claude和Grok后端的每个智能体上均实现了信息泄露。对于工具调用劫持，我们在所有测试的智能体-LLM配对上均获得了RCE，其中双通道方法取得了最高成功率。我们提供了针对Cursor和Claude Code的案例研究，分析了外部工具与内置工具的安全防护机制，并提出了实用的防御建议。



## **62. Prompt Injection Vulnerability of Consensus Generating Applications in Digital Democracy**

数字民主中共识生成应用的提示注入漏洞 cs.CY

41 pages, 16 figures

**SubmitDate**: 2026-01-08    [abs](http://arxiv.org/abs/2508.04281v3) [paper-pdf](https://arxiv.org/pdf/2508.04281v3)

**Confidence**: 0.95

**Authors**: Jairo Gudiño-Rosero, Clément Contet, Umberto Grandi, César A. Hidalgo

**Abstract**: Large Language Models (LLMs) are gaining traction as a method to generate consensus statements and aggregate preferences in digital democracy experiments. Yet, LLMs could introduce critical vulnerabilities in these systems. Here, we examine the vulnerability and robustness of off-the-shelf consensus-generating LLMs to prompt-injection attacks, in which texts are injected to amplify particular viewpoints, erase certain opinions, or divert consensus toward unrelated or irrelevant topics. We construct attack-free and adversarial variants of prompts containing public policy questions and opinion texts, classify opinion and consensus valences with a fine-tuned BERT model, and estimate Attack Success Rates (ASR) from $3\times3$ confusion matrices conditional on matching human majorities. Across topics, default LLaMA 3.1 8B Instruct, GPT-4.1 Nano, and Apertus 8B exhibit widespread vulnerability, with especially high ASR for economically and socially conservative parties and for rational, instruction-like rhetorical strategies. A robustness pipeline combining GPT-OSS-SafeGuard injection detection, structured opinion representations, and GSPO-based reinforcement learning reduces ASR to near zero across parties and policy clusters when restricting attention to non-ambiguous consensus outcomes. These findings advance our understanding of both the vulnerabilities and the potential defenses of consensus-generating LLMs in digital democracy applications.

摘要: 大型语言模型（LLMs）作为生成共识声明和聚合偏好的方法，在数字民主实验中日益受到关注。然而，LLMs可能为这些系统引入关键漏洞。本文研究了现成共识生成LLMs对提示注入攻击的脆弱性和鲁棒性，此类攻击通过注入文本来放大特定观点、消除某些意见或将共识引向无关主题。我们构建了包含公共政策问题和意见文本的无攻击和对抗性提示变体，使用微调的BERT模型对意见和共识倾向进行分类，并根据匹配人类多数意见的条件从$3×3$混淆矩阵中估计攻击成功率（ASR）。跨主题测试显示，默认的LLaMA 3.1 8B Instruct、GPT-4.1 Nano和Apertus 8B普遍存在漏洞，其中经济和社会保守党派以及理性、指令式修辞策略的ASR尤其高。通过结合GPT-OSS-SafeGuard注入检测、结构化意见表示和基于GSPO的强化学习的鲁棒性流程，在限制于非歧义共识结果时，可将各党派和政策集群的ASR降至接近零。这些发现深化了我们对数字民主应用中共识生成LLMs的漏洞及潜在防御机制的理解。



## **63. Attractive Metadata Attack: Inducing LLM Agents to Invoke Malicious Tools**

诱人元数据攻击：诱导LLM智能体调用恶意工具 cs.AI

Accepted to NeurIPS 2025

**SubmitDate**: 2026-01-07    [abs](http://arxiv.org/abs/2508.02110v2) [paper-pdf](https://arxiv.org/pdf/2508.02110v2)

**Confidence**: 0.95

**Authors**: Kanghua Mo, Li Hu, Yucheng Long, Zhihao Li

**Abstract**: Large language model (LLM) agents have demonstrated remarkable capabilities in complex reasoning and decision-making by leveraging external tools. However, this tool-centric paradigm introduces a previously underexplored attack surface, where adversaries can manipulate tool metadata -- such as names, descriptions, and parameter schemas -- to influence agent behavior. We identify this as a new and stealthy threat surface that allows malicious tools to be preferentially selected by LLM agents, without requiring prompt injection or access to model internals. To demonstrate and exploit this vulnerability, we propose the Attractive Metadata Attack (AMA), a black-box in-context learning framework that generates highly attractive but syntactically and semantically valid tool metadata through iterative optimization. The proposed attack integrates seamlessly into standard tool ecosystems and requires no modification to the agent's execution framework. Extensive experiments across ten realistic, simulated tool-use scenarios and a range of popular LLM agents demonstrate consistently high attack success rates (81\%-95\%) and significant privacy leakage, with negligible impact on primary task execution. Moreover, the attack remains effective even against prompt-level defenses, auditor-based detection, and structured tool-selection protocols such as the Model Context Protocol, revealing systemic vulnerabilities in current agent architectures. These findings reveal that metadata manipulation constitutes a potent and stealthy attack surface. Notably, AMA is orthogonal to injection attacks and can be combined with them to achieve stronger attack efficacy, highlighting the need for execution-level defenses beyond prompt-level and auditor-based mechanisms. Code is available at https://github.com/SEAIC-M/AMA.

摘要: 大型语言模型（LLM）智能体通过利用外部工具，在复杂推理和决策方面展现出卓越能力。然而，这种以工具为中心的范式引入了一个先前未被充分探索的攻击面：攻击者可以通过操纵工具元数据（如名称、描述和参数模式）来影响智能体行为。我们将此识别为一种新型隐蔽威胁面，它允许恶意工具被LLM智能体优先选择，而无需进行提示注入或访问模型内部。为演示和利用此漏洞，我们提出诱人元数据攻击（AMA），这是一个黑盒上下文学习框架，通过迭代优化生成高度诱人但在语法和语义上有效的工具元数据。该攻击可无缝集成到标准工具生态系统中，且无需修改智能体的执行框架。在十个现实模拟工具使用场景和一系列流行LLM智能体上的大量实验表明，攻击成功率持续较高（81%-95%），隐私泄露显著，而对主要任务执行的影响可忽略不计。此外，即使面对提示级防御、基于审计器的检测以及结构化工具选择协议（如模型上下文协议），该攻击仍然有效，揭示了当前智能体架构中的系统性漏洞。这些发现表明，元数据操纵构成了一种强大且隐蔽的攻击面。值得注意的是，AMA与注入攻击正交，并可与之结合以实现更强的攻击效果，这凸显了在提示级和审计器机制之外，执行级防御的必要性。代码可在https://github.com/SEAIC-M/AMA获取。



## **64. VRSA: Jailbreaking Multimodal Large Language Models through Visual Reasoning Sequential Attack**

VRSA：通过视觉推理序列攻击越狱多模态大语言模型 cs.CV

**SubmitDate**: 2025-12-08    [abs](http://arxiv.org/abs/2512.05853v2) [paper-pdf](https://arxiv.org/pdf/2512.05853v2)

**Confidence**: 0.95

**Authors**: Shiji Zhao, Shukun Xiong, Yao Huang, Yan Jin, Zhenyu Wu, Jiyang Guan, Ranjie Duan, Jialing Tao, Hui Xue, Xingxing Wei

**Abstract**: Multimodal Large Language Models (MLLMs) are widely used in various fields due to their powerful cross-modal comprehension and generation capabilities. However, more modalities bring more vulnerabilities to being utilized for jailbreak attacks, which induces MLLMs to output harmful content. Due to the strong reasoning ability of MLLMs, previous jailbreak attacks try to explore reasoning safety risk in text modal, while similar threats have been largely overlooked in the visual modal. To fully evaluate potential safety risks in the visual reasoning task, we propose Visual Reasoning Sequential Attack (VRSA), which induces MLLMs to gradually externalize and aggregate complete harmful intent by decomposing the original harmful text into several sequentially related sub-images. In particular, to enhance the rationality of the scene in the image sequence, we propose Adaptive Scene Refinement to optimize the scene most relevant to the original harmful query. To ensure the semantic continuity of the generated image, we propose Semantic Coherent Completion to iteratively rewrite each sub-text combined with contextual information in this scene. In addition, we propose Text-Image Consistency Alignment to keep the semantical consistency. A series of experiments demonstrates that the VRSA can achieve a higher attack success rate compared with the state-of-the-art jailbreak attack methods on both the open-source and closed-source MLLMs such as GPT-4o and Claude-4.5-Sonnet.

摘要: 多模态大语言模型（MLLMs）凭借其强大的跨模态理解和生成能力，在各领域得到广泛应用。然而，更多模态也带来了更多可能被用于越狱攻击的漏洞，这些攻击会诱导MLLMs输出有害内容。由于MLLMs具备强大的推理能力，以往的越狱攻击主要探索文本模态中的推理安全风险，而视觉模态中的类似威胁在很大程度上被忽视。为全面评估视觉推理任务中的潜在安全风险，我们提出了视觉推理序列攻击（VRSA），该方法通过将原始有害文本分解为多个序列相关的子图像，诱导MLLMs逐步外化并聚合完整的有害意图。具体而言，为增强图像序列中场景的合理性，我们提出自适应场景优化方法，以优化与原始有害查询最相关的场景。为确保生成图像的语义连贯性，我们提出语义连贯补全方法，结合该场景中的上下文信息迭代重写每个子文本。此外，我们提出文本-图像一致性对齐方法以保持语义一致性。一系列实验表明，与现有最先进的越狱攻击方法相比，VRSA在开源和闭源MLLMs（如GPT-4o和Claude-4.5-Sonnet）上均能实现更高的攻击成功率。



## **65. Chameleon: Adaptive Adversarial Agents for Scaling-Based Visual Prompt Injection in Multimodal AI Systems**

Chameleon：面向多模态AI系统中基于缩放视觉提示注入的自适应对抗智能体 cs.AI

5 pages, 2 figures, IEEE Transactions on Dependable and Secure Computing

**SubmitDate**: 2025-12-04    [abs](http://arxiv.org/abs/2512.04895v1) [paper-pdf](https://arxiv.org/pdf/2512.04895v1)

**Confidence**: 0.95

**Authors**: M Zeeshan, Saud Satti

**Abstract**: Multimodal Artificial Intelligence (AI) systems, particularly Vision-Language Models (VLMs), have become integral to critical applications ranging from autonomous decision-making to automated document processing. As these systems scale, they rely heavily on preprocessing pipelines to handle diverse inputs efficiently. However, this dependency on standard preprocessing operations, specifically image downscaling, creates a significant yet often overlooked security vulnerability. While intended for computational optimization, scaling algorithms can be exploited to conceal malicious visual prompts that are invisible to human observers but become active semantic instructions once processed by the model. Current adversarial strategies remain largely static, failing to account for the dynamic nature of modern agentic workflows. To address this gap, we propose Chameleon, a novel, adaptive adversarial framework designed to expose and exploit scaling vulnerabilities in production VLMs. Unlike traditional static attacks, Chameleon employs an iterative, agent-based optimization mechanism that dynamically refines image perturbations based on the target model's real-time feedback. This allows the framework to craft highly robust adversarial examples that survive standard downscaling operations to hijack downstream execution. We evaluate Chameleon against Gemini 2.5 Flash model. Our experiments demonstrate that Chameleon achieves an Attack Success Rate (ASR) of 84.5% across varying scaling factors, significantly outperforming static baseline attacks which average only 32.1%. Furthermore, we show that these attacks effectively compromise agentic pipelines, reducing decision-making accuracy by over 45% in multi-step tasks. Finally, we discuss the implications of these vulnerabilities and propose multi-scale consistency checks as a necessary defense mechanism.

摘要: 多模态人工智能（AI）系统，特别是视觉语言模型（VLMs），已成为从自主决策到自动化文档处理等关键应用的核心组成部分。随着这些系统的规模化，它们高度依赖预处理流程来高效处理多样化输入。然而，这种对标准预处理操作（特别是图像下采样）的依赖，造成了一个重要但常被忽视的安全漏洞。虽然缩放算法本意是用于计算优化，但可被利用来隐藏恶意视觉提示——这些提示对人类观察者不可见，但经模型处理后即成为活跃的语义指令。当前的对抗策略大多仍是静态的，未能考虑现代智能体工作流程的动态特性。为弥补这一不足，我们提出Chameleon，一种新颖的自适应对抗框架，旨在揭示并利用生产环境VLMs中的缩放漏洞。与传统静态攻击不同，Chameleon采用基于智能体的迭代优化机制，根据目标模型的实时反馈动态优化图像扰动。这使得该框架能够构建高度鲁棒的对抗样本，这些样本能在标准下采样操作后依然存活，从而劫持下游执行。我们在Gemini 2.5 Flash模型上评估了Chameleon。实验表明，Chameleon在不同缩放因子下实现了84.5%的攻击成功率（ASR），显著优于平均仅32.1%的静态基线攻击。此外，我们证明这些攻击能有效破坏智能体流程，在多步骤任务中将决策准确率降低超过45%。最后，我们讨论了这些漏洞的影响，并提出多尺度一致性检查作为必要的防御机制。



## **66. Simulated Ensemble Attack: Transferring Jailbreaks Across Fine-tuned Vision-Language Models**

模拟集成攻击：在微调视觉语言模型间传递越狱漏洞 cs.CV

**SubmitDate**: 2026-01-14    [abs](http://arxiv.org/abs/2508.01741v3) [paper-pdf](https://arxiv.org/pdf/2508.01741v3)

**Confidence**: 0.95

**Authors**: Ruofan Wang, Xin Wang, Yang Yao, Juncheng Li, Xuan Tong, Xingjun Ma

**Abstract**: The widespread practice of fine-tuning open-source Vision-Language Models (VLMs) raises a critical security concern: jailbreak vulnerabilities in base models may persist in downstream variants, enabling transferable attacks across fine-tuned systems. To investigate this risk, we propose the Simulated Ensemble Attack (SEA), a grey-box jailbreak framework that assumes full access to the base VLM but no knowledge of the fine-tuned target. SEA enhances transferability via Fine-tuning Trajectory Simulation (FTS), which models bounded parameter variations in the vision encoder, and Targeted Prompt Guidance (TPG), which stabilizes adversarial optimization through auxiliary textual guidance. Experiments on the Qwen2-VL family demonstrate that SEA achieves consistently high transfer success and toxicity rates across diverse fine-tuned variants, including safety-enhanced models, while standard PGD-based image jailbreaks exhibit negligible transferability. Further analysis reveals that fine-tuning primarily induces localized parameter shifts around the base model, explaining why attacks optimized over a simulated neighborhood transfer effectively. We also show that SEA generalizes across different base generations (e.g., Qwen2.5/3-VL), indicating that its effectiveness arises from shared fine-tuning-induced behaviors rather than architecture- or initialization-specific factors.

摘要: 开源视觉语言模型（VLMs）的广泛微调实践引发了一个关键安全问题：基础模型中的越狱漏洞可能在衍生变体中持续存在，从而实现在微调系统间的可转移攻击。为探究此风险，我们提出了模拟集成攻击（SEA），一种灰盒越狱框架，该框架假设攻击者可完全访问基础VLM，但对微调目标模型一无所知。SEA通过两种机制增强攻击可转移性：微调轨迹模拟（FTS）——对视觉编码器中有限参数变化进行建模；以及目标提示引导（TPG）——通过辅助文本引导稳定对抗优化。在Qwen2-VL系列模型上的实验表明，SEA在不同微调变体（包括安全性增强模型）中均能实现持续较高的转移成功率和毒性内容生成率，而基于标准PGD的图像越狱攻击则表现出可忽略的转移性。进一步分析揭示，微调主要引发基础模型周边的局部参数偏移，这解释了为何在模拟邻域优化的攻击能有效转移。我们还证明SEA可泛化至不同基础模型世代（如Qwen2.5/3-VL），表明其有效性源于微调引发的共性行为，而非特定架构或初始化因素。



## **67. SafePTR: Token-Level Jailbreak Defense in Multimodal LLMs via Prune-then-Restore Mechanism**

SafePTR：基于剪枝-恢复机制的多模态大语言模型令牌级越狱防御 cs.CR

Accepted by NeurIPS 2025

**SubmitDate**: 2025-12-03    [abs](http://arxiv.org/abs/2507.01513v2) [paper-pdf](https://arxiv.org/pdf/2507.01513v2)

**Confidence**: 0.95

**Authors**: Beitao Chen, Xinyu Lyu, Lianli Gao, Jingkuan Song, Heng Tao Shen

**Abstract**: By incorporating visual inputs, Multimodal Large Language Models (MLLMs) extend LLMs to support visual reasoning. However, this integration also introduces new vulnerabilities, making MLLMs susceptible to multimodal jailbreak attacks and hindering their safe deployment.Existing defense methods, including Image-to-Text Translation, Safe Prompting, and Multimodal Safety Tuning, attempt to address this by aligning multimodal inputs with LLMs' built-in safeguards.Yet, they fall short in uncovering root causes of multimodal vulnerabilities, particularly how harmful multimodal tokens trigger jailbreak in MLLMs? Consequently, they remain vulnerable to text-driven multimodal jailbreaks, often exhibiting overdefensive behaviors and imposing heavy training overhead.To bridge this gap, we present an comprehensive analysis of where, how and which harmful multimodal tokens bypass safeguards in MLLMs. Surprisingly, we find that less than 1% tokens in early-middle layers are responsible for inducing unsafe behaviors, highlighting the potential of precisely removing a small subset of harmful tokens, without requiring safety tuning, can still effectively improve safety against jailbreaks. Motivated by this, we propose Safe Prune-then-Restore (SafePTR), an training-free defense framework that selectively prunes harmful tokens at vulnerable layers while restoring benign features at subsequent layers.Without incurring additional computational overhead, SafePTR significantly enhances the safety of MLLMs while preserving efficiency. Extensive evaluations across three MLLMs and five benchmarks demonstrate SafePTR's state-of-the-art performance in mitigating jailbreak risks without compromising utility.

摘要: 通过整合视觉输入，多模态大语言模型（MLLMs）扩展了LLMs以支持视觉推理。然而，这种整合也引入了新的漏洞，使MLLMs易受多模态越狱攻击，阻碍了其安全部署。现有防御方法（包括图像到文本翻译、安全提示和多模态安全调优）试图通过将多模态输入与LLMs内置安全机制对齐来解决此问题。但它们未能揭示多模态漏洞的根本原因，特别是有害多模态令牌如何触发MLLMs越狱？因此，这些方法仍易受文本驱动的多模态越狱攻击，常表现出过度防御行为并带来沉重的训练开销。为填补这一空白，我们对有害多模态令牌在MLLMs中绕过安全机制的位置、方式和具体令牌进行了全面分析。令人惊讶的是，我们发现早期-中层中不到1%的令牌是诱发不安全行为的关键，这表明精确移除少量有害令牌（无需安全调优）仍能有效提升越狱防御能力。受此启发，我们提出了安全剪枝-恢复（SafePTR）框架，这是一种无需训练的防御机制，可在脆弱层选择性剪除有害令牌，同时在后续层恢复良性特征。在不增加计算开销的前提下，SafePTR显著提升了MLLMs的安全性并保持效率。在三个MLLMs和五个基准测试上的广泛评估表明，SafePTR在降低越狱风险的同时不影响模型效用，实现了最先进的性能。



## **68. Benchmarking Gaslighting Negation Attacks Against Reasoning Models**

针对推理模型的Gaslighting否定攻击基准测试 cs.CV

**SubmitDate**: 2025-12-16    [abs](http://arxiv.org/abs/2506.09677v2) [paper-pdf](https://arxiv.org/pdf/2506.09677v2)

**Confidence**: 0.95

**Authors**: Bin Zhu, Hailong Yin, Jingjing Chen, Yu-Gang Jiang

**Abstract**: Recent advances in reasoning-centric models promise improved robustness through mechanisms such as chain-of-thought prompting and test-time scaling. However, their ability to withstand gaslighting negation attacks-adversarial prompts that confidently deny correct answers-remains underexplored. In this paper, we conduct a systematic evaluation of three state-of-the-art reasoning models, i.e., OpenAI's o4-mini, Claude-3.7-Sonnet and Gemini-2.5-Flash, across three multimodal benchmarks: MMMU, MathVista, and CharXiv. Our evaluation reveals significant accuracy drops (25-29% on average) following gaslighting negation attacks, indicating that even top-tier reasoning models struggle to preserve correct answers under manipulative user feedback. Built upon the insights of the evaluation and to further probe this vulnerability, we introduce GaslightingBench-R, a new diagnostic benchmark specifically designed to evaluate reasoning models' susceptibility to defend their belief under gaslighting negation attacks. Constructed by filtering and curating 1,025 challenging samples from the existing benchmarks, GaslightingBench-R induces even more dramatic failures, with accuracy drops exceeding 53% on average. Our findings highlight a fundamental gap between step-by-step reasoning and resistance to adversarial manipulation, calling for new robustness strategies that safeguard reasoning models against gaslighting negation attacks.

摘要: 近年来，以推理为中心的模型通过思维链提示和测试时扩展等机制展现出更强的鲁棒性。然而，它们抵御gaslighting否定攻击（即自信地否定正确答案的对抗性提示）的能力仍未得到充分探索。本文对三种最先进的推理模型（OpenAI的o4-mini、Claude-3.7-Sonnet和Gemini-2.5-Flash）在三个多模态基准（MMMU、MathVista和CharXiv）上进行了系统评估。评估结果显示，在gaslighting否定攻击后，模型准确率平均显著下降25-29%，表明即使顶级推理模型也难以在操纵性用户反馈下保持正确答案。基于评估发现，为深入探究此漏洞，我们提出了GaslightingBench-R——一个专门用于评估推理模型在gaslighting否定攻击下捍卫自身信念能力的新型诊断基准。该基准通过从现有数据集中筛选和整理1,025个挑战性样本构建而成，能引发更严重的失效现象，平均准确率下降超过53%。我们的研究揭示了逐步推理与对抗操纵抵抗力之间的根本差距，呼吁开发新的鲁棒性策略以保护推理模型免受gaslighting否定攻击。



## **69. AutoTrust: Benchmarking Trustworthiness in Large Vision Language Models for Autonomous Driving**

AutoTrust：自动驾驶领域大型视觉语言模型可信度基准评测 cs.CV

Published at TMLR 2025

**SubmitDate**: 2026-01-01    [abs](http://arxiv.org/abs/2412.15206v2) [paper-pdf](https://arxiv.org/pdf/2412.15206v2)

**Confidence**: 0.95

**Authors**: Shuo Xing, Hongyuan Hua, Xiangbo Gao, Shenzhe Zhu, Renjie Li, Kexin Tian, Xiaopeng Li, Heng Huang, Tianbao Yang, Zhangyang Wang, Yang Zhou, Huaxiu Yao, Zhengzhong Tu

**Abstract**: Recent advancements in large vision language models (VLMs) tailored for autonomous driving (AD) have shown strong scene understanding and reasoning capabilities, making them undeniable candidates for end-to-end driving systems. However, limited work exists on studying the trustworthiness of DriveVLMs -- a critical factor that directly impacts public transportation safety. In this paper, we introduce AutoTrust, a comprehensive trustworthiness benchmark for large vision-language models in autonomous driving (DriveVLMs), considering diverse perspectives -- including trustfulness, safety, robustness, privacy, and fairness. We constructed the largest visual question-answering dataset for investigating trustworthiness issues in driving scenarios, comprising over 10k unique scenes and 18k queries. We evaluated six publicly available VLMs, spanning from generalist to specialist, from open-source to commercial models. Our exhaustive evaluations have unveiled previously undiscovered vulnerabilities of DriveVLMs to trustworthiness threats. Specifically, we found that the general VLMs like LLaVA-v1.6 and GPT-4o-mini surprisingly outperform specialized models fine-tuned for driving in terms of overall trustworthiness. DriveVLMs like DriveLM-Agent are particularly vulnerable to disclosing sensitive information. Additionally, both generalist and specialist VLMs remain susceptible to adversarial attacks and struggle to ensure unbiased decision-making across diverse environments and populations. Our findings call for immediate and decisive action to address the trustworthiness of DriveVLMs -- an issue of critical importance to public safety and the welfare of all citizens relying on autonomous transportation systems. We release all the codes and datasets in https://github.com/taco-group/AutoTrust.

摘要: 近年来，专为自动驾驶（AD）定制的大型视觉语言模型（VLMs）展现出强大的场景理解与推理能力，使其成为端到端驾驶系统的有力候选者。然而，针对DriveVLMs可信度的研究却十分有限——这是直接影响公共交通安全的关键因素。本文提出AutoTrust，一个面向自动驾驶大型视觉语言模型（DriveVLMs）的综合性可信度基准，涵盖可信性、安全性、鲁棒性、隐私性和公平性等多维视角。我们构建了目前最大的用于研究驾驶场景可信度问题的视觉问答数据集，包含超过1万个独特场景和1.8万个查询。我们评估了六个公开可用的VLM，涵盖从通用模型到专用模型，从开源模型到商业模型。我们的全面评估揭示了DriveVLMs在可信度威胁方面先前未被发现的脆弱性。具体而言，我们发现通用VLM（如LLaVA-v1.6和GPT-4o-mini）在整体可信度上意外地优于专为驾驶微调的专用模型。DriveLM-Agent等DriveVLMs在敏感信息泄露方面尤为脆弱。此外，通用和专用VLM均易受对抗性攻击影响，且难以确保在不同环境和人群中的无偏决策。我们的研究结果呼吁立即采取果断行动，解决DriveVLMs的可信度问题——这对公共安全以及依赖自动驾驶系统的全体公民福祉至关重要。所有代码和数据集已在https://github.com/taco-group/AutoTrust发布。



## **70. StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak**

StyleBreak：通过风格感知音频越狱揭示大型音频语言模型的对齐漏洞 cs.SD

Accepted by AAAI 2026

**SubmitDate**: 2025-11-12    [abs](http://arxiv.org/abs/2511.10692v1) [paper-pdf](https://arxiv.org/pdf/2511.10692v1)

**Confidence**: 0.95

**Authors**: Hongyi Li, Chengxuan Zhou, Chu Wang, Sicheng Liang, Yanting Chen, Qinlin Xie, Jiawei Ye, Jie Wu

**Abstract**: Large Audio-language Models (LAMs) have recently enabled powerful speech-based interactions by coupling audio encoders with Large Language Models (LLMs). However, the security of LAMs under adversarial attacks remains underexplored, especially through audio jailbreaks that craft malicious audio prompts to bypass alignment. Existing efforts primarily rely on converting text-based attacks into speech or applying shallow signal-level perturbations, overlooking the impact of human speech's expressive variations on LAM alignment robustness. To address this gap, we propose StyleBreak, a novel style-aware audio jailbreak framework that systematically investigates how diverse human speech attributes affect LAM alignment robustness. Specifically, StyleBreak employs a two-stage style-aware transformation pipeline that perturbs both textual content and audio to control linguistic, paralinguistic, and extralinguistic attributes. Furthermore, we develop a query-adaptive policy network that automatically searches for adversarial styles to enhance the efficiency of LAM jailbreak exploration. Extensive evaluations demonstrate that LAMs exhibit critical vulnerabilities when exposed to diverse human speech attributes. Moreover, StyleBreak achieves substantial improvements in attack effectiveness and efficiency across multiple attack paradigms, highlighting the urgent need for more robust alignment in LAMs.

摘要: 大型音频语言模型（LAMs）近期通过将音频编码器与大型语言模型（LLMs）耦合，实现了强大的语音交互能力。然而，LAMs在对抗攻击下的安全性仍未得到充分探索，尤其是通过音频越狱技术——即制作恶意音频提示以绕过模型对齐。现有方法主要依赖将基于文本的攻击转换为语音，或应用浅层的信号级扰动，忽视了人类语音表达变化对LAM对齐鲁棒性的影响。为填补这一空白，我们提出StyleBreak，一种新颖的风格感知音频越狱框架，系统研究多样化人类语音属性如何影响LAM对齐鲁棒性。具体而言，StyleBreak采用两阶段风格感知转换流程，通过扰动文本内容和音频来控制语言、副语言及超语言属性。此外，我们开发了查询自适应策略网络，自动搜索对抗性风格以提升LAM越狱探索效率。大量评估表明，LAMs在暴露于多样化人类语音属性时表现出严重脆弱性。同时，StyleBreak在多种攻击范式下显著提升了攻击效果与效率，凸显了增强LAM对齐鲁棒性的迫切需求。



## **71. Benchmarking Gaslighting Attacks Against Speech Large Language Models**

针对语音大语言模型的煤气灯攻击基准测试 cs.CL

5 pages, 2 figures, 3 tables

**SubmitDate**: 2025-09-24    [abs](http://arxiv.org/abs/2509.19858v1) [paper-pdf](https://arxiv.org/pdf/2509.19858v1)

**Confidence**: 0.95

**Authors**: Jinyang Wu, Bin Zhu, Xiandong Zou, Qiquan Zhang, Xu Fang, Pan Zhou

**Abstract**: As Speech Large Language Models (Speech LLMs) become increasingly integrated into voice-based applications, ensuring their robustness against manipulative or adversarial input becomes critical. Although prior work has studied adversarial attacks in text-based LLMs and vision-language models, the unique cognitive and perceptual challenges of speech-based interaction remain underexplored. In contrast, speech presents inherent ambiguity, continuity, and perceptual diversity, which make adversarial attacks more difficult to detect. In this paper, we introduce gaslighting attacks, strategically crafted prompts designed to mislead, override, or distort model reasoning as a means to evaluate the vulnerability of Speech LLMs. Specifically, we construct five manipulation strategies: Anger, Cognitive Disruption, Sarcasm, Implicit, and Professional Negation, designed to test model robustness across varied tasks. It is worth noting that our framework captures both performance degradation and behavioral responses, including unsolicited apologies and refusals, to diagnose different dimensions of susceptibility. Moreover, acoustic perturbation experiments are conducted to assess multi-modal robustness. To quantify model vulnerability, comprehensive evaluation across 5 Speech and multi-modal LLMs on over 10,000 test samples from 5 diverse datasets reveals an average accuracy drop of 24.3% under the five gaslighting attacks, indicating significant behavioral vulnerability. These findings highlight the need for more resilient and trustworthy speech-based AI systems.

摘要: 随着语音大语言模型（Speech LLMs）日益融入语音应用，确保其对抗操纵性或对抗性输入的鲁棒性变得至关重要。尽管先前研究已探讨文本LLMs和视觉语言模型的对抗攻击，但语音交互特有的认知与感知挑战仍未充分探索。相比之下，语音具有固有的模糊性、连续性和感知多样性，使得对抗攻击更难以检测。本文提出煤气灯攻击——通过精心设计的提示误导、覆盖或扭曲模型推理，以此评估Speech LLMs的脆弱性。具体而言，我们构建了五种操纵策略：愤怒、认知干扰、讽刺、隐性和专业否定，用于测试模型在不同任务中的鲁棒性。值得注意的是，我们的框架同时捕捉性能下降和行为响应（包括未经请求的道歉和拒绝），以诊断脆弱性的不同维度。此外，通过声学扰动实验评估多模态鲁棒性。为量化模型脆弱性，我们在5个多样化数据集的超过10,000个测试样本上对5种语音及多模态LLMs进行全面评估，结果显示在五种煤气灯攻击下平均准确率下降24.3%，表明存在显著的行为脆弱性。这些发现凸显了构建更具韧性和可信赖的语音AI系统的必要性。



## **72. JALMBench: Benchmarking Jailbreak Vulnerabilities in Audio Language Models**

JALMBench：音频语言模型越狱漏洞基准测试 cs.CR

**SubmitDate**: 2025-10-03    [abs](http://arxiv.org/abs/2505.17568v2) [paper-pdf](https://arxiv.org/pdf/2505.17568v2)

**Confidence**: 0.95

**Authors**: Zifan Peng, Yule Liu, Zhen Sun, Mingchen Li, Zeren Luo, Jingyi Zheng, Wenhan Dong, Xinlei He, Xuechao Wang, Yingjie Xue, Shengmin Xu, Xinyi Huang

**Abstract**: Audio Language Models (ALMs) have made significant progress recently. These models integrate the audio modality directly into the model, rather than converting speech into text and inputting text to Large Language Models (LLMs). While jailbreak attacks on LLMs have been extensively studied, the security of ALMs with audio modalities remains largely unexplored. Currently, there is a lack of an adversarial audio dataset and a unified framework specifically designed to evaluate and compare attacks and ALMs. In this paper, we present JALMBench, a comprehensive benchmark to assess the safety of ALMs against jailbreak attacks. JALMBench includes a dataset containing 11,316 text samples and 245,355 audio samples with over 1,000 hours. It supports 12 mainstream ALMs, 4 text-transferred and 4 audio-originated attack methods, and 5 defense methods. Using JALMBench, we provide an in-depth analysis of attack efficiency, topic sensitivity, voice diversity, and architecture. Additionally, we explore mitigation strategies for the attacks at both the prompt level and the response level.

摘要: 音频语言模型（ALMs）近期取得了显著进展。这些模型直接将音频模态整合到模型中，而非将语音转换为文本后输入大型语言模型（LLMs）。尽管针对LLMs的越狱攻击已被广泛研究，但具备音频模态的ALMs的安全性在很大程度上仍未得到探索。目前，缺乏专门用于评估和比较攻击方法与ALMs的对抗性音频数据集及统一框架。本文提出JALMBench——一个评估ALMs抵御越狱攻击安全性的综合基准。JALMBench包含一个数据集，涵盖11,316个文本样本和245,355个音频样本，总时长超过1,000小时。它支持12个主流ALMs、4种文本迁移攻击方法、4种音频原生攻击方法以及5种防御方法。通过JALMBench，我们对攻击效率、主题敏感性、语音多样性和模型架构进行了深入分析。此外，我们还从提示层面和响应层面探索了针对这些攻击的缓解策略。



## **73. SPIRIT: Patching Speech Language Models against Jailbreak Attacks**

SPIRIT：针对越狱攻击的语音语言模型补丁防御 eess.AS

**SubmitDate**: 2025-10-16    [abs](http://arxiv.org/abs/2505.13541v2) [paper-pdf](https://arxiv.org/pdf/2505.13541v2)

**Confidence**: 0.95

**Authors**: Amirbek Djanibekov, Nurdaulet Mukhituly, Kentaro Inui, Hanan Aldarmaki, Nils Lukas

**Abstract**: Speech Language Models (SLMs) enable natural interactions via spoken instructions, which more effectively capture user intent by detecting nuances in speech. The richer speech signal introduces new security risks compared to text-based models, as adversaries can better bypass safety mechanisms by injecting imperceptible noise to speech. We analyze adversarial attacks and find that SLMs are substantially more vulnerable to jailbreak attacks, which can achieve a perfect 100% attack success rate in some instances. To improve security, we propose post-hoc patching defenses used to intervene during inference by modifying the SLM's activations that improve robustness up to 99% with (i) negligible impact on utility and (ii) without any re-training. We conduct ablation studies to maximize the efficacy of our defenses and improve the utility/security trade-off, validated with large-scale benchmarks unique to SLMs.

摘要: 语音语言模型（SLMs）通过语音指令实现自然交互，能通过检测语音中的细微差别更有效地捕捉用户意图。与基于文本的模型相比，更丰富的语音信号引入了新的安全风险，因为攻击者可以通过向语音注入难以察觉的噪声来更有效地绕过安全机制。我们分析对抗攻击发现，SLMs对越狱攻击的脆弱性显著更高，在某些情况下攻击成功率可达100%。为提高安全性，我们提出事后补丁防御方法，通过在推理过程中干预并修改SLMs的激活值，将鲁棒性提升至99%，同时（i）对模型效用影响可忽略不计，（ii）无需任何重新训练。我们通过消融研究最大化防御效果并优化效用/安全权衡，使用SLMs特有的大规模基准测试进行了验证。



## **74. Audio Jailbreak Attacks: Exposing Vulnerabilities in SpeechGPT in a White-Box Framework**

音频越狱攻击：在白盒框架下揭示SpeechGPT的脆弱性 cs.CL

**SubmitDate**: 2025-05-24    [abs](http://arxiv.org/abs/2505.18864v1) [paper-pdf](https://arxiv.org/pdf/2505.18864v1)

**Confidence**: 0.95

**Authors**: Binhao Ma, Hanqing Guo, Zhengping Jay Luo, Rui Duan

**Abstract**: Recent advances in Multimodal Large Language Models (MLLMs) have significantly enhanced the naturalness and flexibility of human computer interaction by enabling seamless understanding across text, vision, and audio modalities. Among these, voice enabled models such as SpeechGPT have demonstrated considerable improvements in usability, offering expressive, and emotionally responsive interactions that foster deeper connections in real world communication scenarios. However, the use of voice introduces new security risks, as attackers can exploit the unique characteristics of spoken language, such as timing, pronunciation variability, and speech to text translation, to craft inputs that bypass defenses in ways not seen in text-based systems. Despite substantial research on text based jailbreaks, the voice modality remains largely underexplored in terms of both attack strategies and defense mechanisms. In this work, we present an adversarial attack targeting the speech input of aligned MLLMs in a white box scenario. Specifically, we introduce a novel token level attack that leverages access to the model's speech tokenization to generate adversarial token sequences. These sequences are then synthesized into audio prompts, which effectively bypass alignment safeguards and to induce prohibited outputs. Evaluated on SpeechGPT, our approach achieves up to 89 percent attack success rate across multiple restricted tasks, significantly outperforming existing voice based jailbreak methods. Our findings shed light on the vulnerabilities of voice-enabled multimodal systems and to help guide the development of more robust next-generation MLLMs.

摘要: 多模态大语言模型（MLLMs）的最新进展显著提升了人机交互的自然度与灵活性，实现了文本、视觉和音频模态间的无缝理解。其中，语音驱动模型如SpeechGPT在可用性方面展现出显著提升，提供了富有表现力且情感响应的交互，促进了现实世界通信场景中的深度连接。然而，语音的使用引入了新的安全风险，攻击者可利用口语的独特特性（如时序、发音变异性和语音到文本转换）来构造输入，从而以文本系统未曾见过的方式绕过防御机制。尽管针对文本越狱已有大量研究，但语音模态在攻击策略和防御机制方面仍存在较大探索空白。本研究提出了一种针对白盒场景下对齐MLLMs语音输入的对抗攻击。具体而言，我们引入了一种新颖的令牌级攻击方法，通过利用模型语音令牌化的内部信息生成对抗性令牌序列。这些序列随后被合成为音频提示，有效绕过对齐防护机制并诱导模型产生被禁止的输出。在SpeechGPT上的评估显示，该方法在多个受限任务中实现了高达89%的攻击成功率，显著优于现有基于语音的越狱方法。我们的研究揭示了语音驱动多模态系统的脆弱性，有助于指导开发更鲁棒的下一代MLLMs。



## **75. Audio Jailbreak: An Open Comprehensive Benchmark for Jailbreaking Large Audio-Language Models**

音频越狱：大型音频语言模型越狱攻击的开放综合基准 cs.SD

We release AJailBench, including both static and optimized adversarial data, to facilitate future research: https://github.com/mbzuai-nlp/AudioJailbreak

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2505.15406v1) [paper-pdf](https://arxiv.org/pdf/2505.15406v1)

**Confidence**: 0.95

**Authors**: Zirui Song, Qian Jiang, Mingxuan Cui, Mingzhe Li, Lang Gao, Zeyu Zhang, Zixiang Xu, Yanbo Wang, Chenxi Wang, Guangxian Ouyang, Zhenhao Chen, Xiuying Chen

**Abstract**: The rise of Large Audio Language Models (LAMs) brings both potential and risks, as their audio outputs may contain harmful or unethical content. However, current research lacks a systematic, quantitative evaluation of LAM safety especially against jailbreak attacks, which are challenging due to the temporal and semantic nature of speech. To bridge this gap, we introduce AJailBench, the first benchmark specifically designed to evaluate jailbreak vulnerabilities in LAMs. We begin by constructing AJailBench-Base, a dataset of 1,495 adversarial audio prompts spanning 10 policy-violating categories, converted from textual jailbreak attacks using realistic text to speech synthesis. Using this dataset, we evaluate several state-of-the-art LAMs and reveal that none exhibit consistent robustness across attacks. To further strengthen jailbreak testing and simulate more realistic attack conditions, we propose a method to generate dynamic adversarial variants. Our Audio Perturbation Toolkit (APT) applies targeted distortions across time, frequency, and amplitude domains. To preserve the original jailbreak intent, we enforce a semantic consistency constraint and employ Bayesian optimization to efficiently search for perturbations that are both subtle and highly effective. This results in AJailBench-APT, an extended dataset of optimized adversarial audio samples. Our findings demonstrate that even small, semantically preserved perturbations can significantly reduce the safety performance of leading LAMs, underscoring the need for more robust and semantically aware defense mechanisms.

摘要: 大型音频语言模型（LAMs）的兴起带来了潜力与风险，因为其音频输出可能包含有害或不道德内容。然而，当前研究缺乏对LAM安全性的系统性量化评估，尤其是在应对越狱攻击方面，由于语音的时序和语义特性，这类攻击尤为棘手。为填补这一空白，我们推出了AJailBench，这是首个专门用于评估LAM越狱漏洞的基准。我们首先构建了AJailBench-Base数据集，包含1,495个对抗性音频提示，涵盖10个违反策略的类别，这些提示通过逼真的文本转语音合成从文本越狱攻击转换而来。利用该数据集，我们评估了多个先进LAM，发现它们在各类攻击中均未表现出一致的鲁棒性。为进一步加强越狱测试并模拟更真实的攻击条件，我们提出了一种生成动态对抗变体的方法。我们的音频扰动工具包（APT）在时间、频率和幅度域应用针对性失真。为保留原始越狱意图，我们强制执行语义一致性约束，并采用贝叶斯优化高效搜索既细微又高效的扰动。由此产生了AJailBench-APT，这是一个扩展的优化对抗音频样本数据集。我们的研究结果表明，即使是微小且语义保留的扰动，也能显著降低领先LAM的安全性能，这凸显了对更鲁棒且语义感知的防御机制的需求。



## **76. AudioJailbreak: Jailbreak Attacks against End-to-End Large Audio-Language Models**

AudioJailbreak：针对端到端大型音频语言模型的越狱攻击 cs.CR

**SubmitDate**: 2025-05-21    [abs](http://arxiv.org/abs/2505.14103v2) [paper-pdf](https://arxiv.org/pdf/2505.14103v2)

**Confidence**: 0.95

**Authors**: Guangke Chen, Fu Song, Zhe Zhao, Xiaojun Jia, Yang Liu, Yanchen Qiao, Weizhe Zhang

**Abstract**: Jailbreak attacks to Large audio-language models (LALMs) are studied recently, but they achieve suboptimal effectiveness, applicability, and practicability, particularly, assuming that the adversary can fully manipulate user prompts. In this work, we first conduct an extensive experiment showing that advanced text jailbreak attacks cannot be easily ported to end-to-end LALMs via text-to speech (TTS) techniques. We then propose AudioJailbreak, a novel audio jailbreak attack, featuring (1) asynchrony: the jailbreak audio does not need to align with user prompts in the time axis by crafting suffixal jailbreak audios; (2) universality: a single jailbreak perturbation is effective for different prompts by incorporating multiple prompts into perturbation generation; (3) stealthiness: the malicious intent of jailbreak audios will not raise the awareness of victims by proposing various intent concealment strategies; and (4) over-the-air robustness: the jailbreak audios remain effective when being played over the air by incorporating the reverberation distortion effect with room impulse response into the generation of the perturbations. In contrast, all prior audio jailbreak attacks cannot offer asynchrony, universality, stealthiness, or over-the-air robustness. Moreover, AudioJailbreak is also applicable to the adversary who cannot fully manipulate user prompts, thus has a much broader attack scenario. Extensive experiments with thus far the most LALMs demonstrate the high effectiveness of AudioJailbreak. We highlight that our work peeks into the security implications of audio jailbreak attacks against LALMs, and realistically fosters improving their security robustness. The implementation and audio samples are available at our website https://audiojailbreak.github.io/AudioJailbreak.

摘要: 针对大型音频语言模型（LALMs）的越狱攻击近期已有研究，但其在有效性、适用性和实用性方面均未达最优，特别是假设攻击者能完全操控用户提示。本研究首先通过大量实验表明，高级文本越狱攻击难以通过文本转语音（TTS）技术直接迁移至端到端LALMs。随后，我们提出AudioJailbreak，一种新型音频越狱攻击，其特点包括：（1）异步性：通过构建后缀越狱音频，使越狱音频无需在时间轴上与用户提示对齐；（2）普适性：通过将多个提示融入扰动生成，单个越狱扰动可对不同提示生效；（3）隐蔽性：通过多种意图隐藏策略，使越狱音频的恶意意图不易被受害者察觉；（4）空中传输鲁棒性：通过将房间脉冲响应的混响失真效应纳入扰动生成，确保越狱音频在空气中播放时仍保持有效。相比之下，所有现有音频越狱攻击均无法提供异步性、普适性、隐蔽性或空中传输鲁棒性。此外，AudioJailbreak也适用于无法完全操控用户提示的攻击者，因此攻击场景更为广泛。在迄今最多LALMs上的大量实验证明了AudioJailbreak的高效性。我们强调，本研究揭示了音频越狱攻击对LALMs的安全隐患，并切实推动了其安全鲁棒性的提升。实现代码及音频样本发布于网站：https://audiojailbreak.github.io/AudioJailbreak。



## **77. Vulnerability of LLMs' Belief Systems? LLMs Belief Resistance Check Through Strategic Persuasive Conversation Interventions**

大语言模型的信念系统存在脆弱性？通过战略性说服对话干预检验LLMs的信念抵抗能力 cs.CL

**SubmitDate**: 2026-01-20    [abs](http://arxiv.org/abs/2601.13590v1) [paper-pdf](https://arxiv.org/pdf/2601.13590v1)

**Confidence**: 0.95

**Authors**: Fan Huang, Haewoon Kwak, Jisun An

**Abstract**: Large Language Models (LLMs) are increasingly employed in various question-answering tasks. However, recent studies showcase that LLMs are susceptible to persuasion and could adopt counterfactual beliefs. We present a systematic evaluation of LLM susceptibility to persuasion under the Source--Message--Channel--Receiver (SMCR) communication framework. Across five mainstream Large Language Models (LLMs) and three domains (factual knowledge, medical QA, and social bias), we analyze how different persuasive strategies influence belief stability over multiple interaction turns. We further examine whether meta-cognition prompting (i.e., eliciting self-reported confidence) affects resistance to persuasion. Results show that smaller models exhibit extreme compliance, with over 80% of belief changes occurring at the first persuasive turn (average end turn of 1.1--1.4). Contrary to expectations, meta-cognition prompting increases vulnerability by accelerating belief erosion rather than enhancing robustness. Finally, we evaluate adversarial fine-tuning as a defense. While GPT-4o-mini achieves near-complete robustness (98.6%) and Mistral~7B improves substantially (35.7% $\rightarrow$ 79.3%), Llama models remain highly susceptible (<14%) even when fine-tuned on their own failure cases. Together, these findings highlight substantial model-dependent limits of current robustness interventions and offer guidance for developing more trustworthy LLMs.

摘要: 大语言模型（LLMs）正日益广泛地应用于各类问答任务。然而，近期研究表明，LLMs易受说服影响，可能接受与事实相悖的信念。本研究基于SMCR（信源-信息-信道-接收者）传播框架，系统评估了LLMs在说服情境下的脆弱性。通过对五种主流大语言模型在三个领域（事实知识、医学问答、社会偏见）的实验，我们分析了不同说服策略如何影响多轮交互中的信念稳定性，并进一步检验元认知提示（即激发模型自报告信度）是否影响其抗说服能力。结果显示：较小模型表现出极端顺从性，超过80%的信念改变发生在首轮说服时（平均终止轮次为1.1-1.4轮）；与预期相反，元认知提示反而通过加速信念侵蚀增加了模型脆弱性，未能增强鲁棒性。最后，我们评估了对抗性微调作为防御手段的效果：GPT-4o-mini实现了接近完全的鲁棒性（98.6%），Mistral~7B显著提升（35.7%→79.3%），而Llama系列模型即使在其自身失败案例上进行微调后仍保持高度易感性（<14%）。这些发现共同揭示了当前鲁棒性干预措施存在显著的模型依赖性局限，为开发更可信赖的LLMs提供了指导。



## **78. How Secure is Secure Code Generation? Adversarial Prompts Put LLM Defenses to the Test**

安全代码生成有多安全？对抗性提示对LLM防御机制进行测试 cs.CR

**SubmitDate**: 2026-01-11    [abs](http://arxiv.org/abs/2601.07084v1) [paper-pdf](https://arxiv.org/pdf/2601.07084v1)

**Confidence**: 0.95

**Authors**: Melissa Tessa, Iyiola E. Olatunji, Aicha War, Jacques Klein, Tegawendé F. Bissyandé

**Abstract**: Recent secure code generation methods, using vulnerability-aware fine-tuning, prefix-tuning, and prompt optimization, claim to prevent LLMs from producing insecure code. However, their robustness under adversarial conditions remains untested, and current evaluations decouple security from functionality, potentially inflating reported gains. We present the first systematic adversarial audit of state-of-the-art secure code generation methods (SVEN, SafeCoder, PromSec). We subject them to realistic prompt perturbations such as paraphrasing, cue inversion, and context manipulation that developers might inadvertently introduce or adversaries deliberately exploit. To enable fair comparison, we evaluate all methods under consistent conditions, jointly assessing security and functionality using multiple analyzers and executable tests. Our findings reveal critical robustness gaps: static analyzers overestimate security by 7 to 21 times, with 37 to 60% of ``secure'' outputs being non-functional. Under adversarial conditions, true secure-and-functional rates collapse to 3 to 17%. Based on these findings, we propose best practices for building and evaluating robust secure code generation methods. Our code is available.

摘要: 近期采用漏洞感知微调、前缀调优和提示优化的安全代码生成方法声称能防止LLM生成不安全代码。然而，这些方法在对抗条件下的鲁棒性尚未经过测试，且当前评估将安全性与功能性分离，可能夸大报告效果。我们首次对前沿安全代码生成方法（SVEN、SafeCoder、PromSec）进行了系统性对抗性审计。通过模拟开发者可能无意引入或攻击者蓄意利用的实际场景，我们对其施加了包括语义转述、提示反转和上下文操纵在内的提示扰动。为公平比较，我们在统一条件下评估所有方法，联合使用多种分析器和可执行测试来综合评估安全性与功能性。研究发现关键鲁棒性缺陷：静态分析器将安全性高估7至21倍，37%至60%的“安全”输出实际无法运行。在对抗条件下，真正安全且可用的代码比例骤降至3%至17%。基于这些发现，我们提出了构建和评估鲁棒安全代码生成方法的最佳实践。相关代码已开源。



## **79. MIRAGE: Misleading Retrieval-Augmented Generation via Black-box and Query-agnostic Poisoning Attacks**

MIRAGE：基于黑盒与查询无关投毒攻击的误导性检索增强生成 cs.CR

**SubmitDate**: 2026-01-19    [abs](http://arxiv.org/abs/2512.08289v2) [paper-pdf](https://arxiv.org/pdf/2512.08289v2)

**Confidence**: 0.95

**Authors**: Tailun Chen, Yu He, Yan Wang, Shuo Shao, Haolun Zheng, Zhihao Liu, Jinfeng Li, Zhizhen Qin, Yuefeng Chen, Zhixuan Chu, Zhan Qin, Kui Ren

**Abstract**: Retrieval-Augmented Generation (RAG) systems enhance LLMs with external knowledge but introduce a critical attack surface: corpus poisoning. While recent studies have demonstrated the potential of such attacks, they typically rely on impractical assumptions, such as white-box access or known user queries, thereby underestimating the difficulty of real-world exploitation. In this paper, we bridge this gap by proposing MIRAGE, a novel multi-stage poisoning pipeline designed for strict black-box and query-agnostic environments. Operating on surrogate model feedback, MIRAGE functions as an automated optimization framework that integrates three key mechanisms: it utilizes persona-driven query synthesis to approximate latent user search distributions, employs semantic anchoring to imperceptibly embed these intents for high retrieval visibility, and leverages an adversarial variant of Test-Time Preference Optimization (TPO) to maximize persuasion. To rigorously evaluate this threat, we construct a new benchmark derived from three long-form, domain-specific datasets. Extensive experiments demonstrate that MIRAGE significantly outperforms existing baselines in both attack efficacy and stealthiness, exhibiting remarkable transferability across diverse retriever-LLM configurations and highlighting the urgent need for robust defense strategies.

摘要: 检索增强生成（RAG）系统通过外部知识增强大语言模型（LLM），但引入了关键攻击面：语料库投毒。虽然近期研究证明了此类攻击的潜力，但它们通常依赖不切实际的假设（如白盒访问或已知用户查询），从而低估了实际攻击的难度。本文通过提出MIRAGE填补了这一空白——这是一种专为严格黑盒和查询无关环境设计的新型多阶段投毒流程。MIRAGE基于代理模型反馈运行，作为一个自动化优化框架，整合了三个关键机制：利用角色驱动的查询合成来近似潜在用户搜索分布；采用语义锚定技术将查询意图以难以察觉的方式嵌入文本以实现高检索可见性；并利用对抗性变体测试时偏好优化（TPO）来最大化说服效果。为严格评估此威胁，我们基于三个长文本领域特定数据集构建了新基准。大量实验表明，MIRAGE在攻击效能和隐蔽性方面均显著优于现有基线，在不同检索器-LLM配置间展现出卓越的可迁移性，凸显了构建鲁棒防御策略的紧迫性。



## **80. An LLM Agent-based Framework for Whaling Countermeasures**

基于LLM智能体的鲸钓攻击防范框架 cs.CR

**SubmitDate**: 2026-01-21    [abs](http://arxiv.org/abs/2601.14606v1) [paper-pdf](https://arxiv.org/pdf/2601.14606v1)

**Confidence**: 0.85

**Authors**: Daisuke Miyamoto, Takuji Iimura, Narushige Michishita

**Abstract**: With the spread of generative AI in recent years, attacks known as Whaling have become a serious threat. Whaling is a form of social engineering that targets important high-authority individuals within organizations and uses sophisticated fraudulent emails. In the context of Japanese universities, faculty members frequently hold positions that combine research leadership with authority within institutional workflows. This structural characteristic leads to the wide public disclosure of high-value information such as publications, grants, and detailed researcher profiles. Such extensive information exposure enables the construction of highly precise target profiles using generative AI. This raises concerns that Whaling attacks based on high-precision profiling by generative AI will become prevalent. In this study, we propose a Whaling countermeasure framework for university faculty members that constructs personalized defense profiles and uses large language model (LLM)-based agents. We design agents that (i) build vulnerability profiles for each target from publicly available information on faculty members, (ii) identify potential risk scenarios relevant to Whaling defense based on those profiles, (iii) construct defense profiles corresponding to the vulnerabilities and anticipated risks, and (iv) analyze Whaling emails using the defense profiles. Furthermore, we conduct a preliminary risk-assessment experiment. The results indicate that the proposed method can produce judgments accompanied by explanations of response policies that are consistent with the work context of faculty members who are Whaling targets. The findings also highlight practical challenges and considerations for future operational deployment and systematic evaluation.

摘要: 随着近年来生成式AI的普及，被称为“鲸钓攻击”的社会工程攻击已成为严重威胁。鲸钓攻击是一种针对组织内具有高权限的重要人物、使用精密伪造邮件的攻击方式。在日本大学环境中，教职人员通常兼具研究领导地位和行政流程中的决策权限。这种结构性特征导致出版物、科研经费、详细研究者档案等高价值信息被广泛公开。这种大规模信息暴露使得攻击者能够利用生成式AI构建高度精确的目标画像，引发了基于生成式AI高精度画像的鲸钓攻击可能泛滥的担忧。本研究提出面向大学教职人员的鲸钓攻击防范框架，通过构建个性化防御画像并运用基于大语言模型（LLM）的智能体。我们设计了能够实现以下功能的智能体：（1）从教职人员公开信息中构建个体脆弱性画像；（2）基于画像识别与鲸钓防御相关的潜在风险场景；（3）针对脆弱性和预期风险构建相应防御画像；（4）利用防御画像分析鲸钓邮件。此外，我们开展了初步风险评估实验。结果表明，所提方法能够生成符合目标教职人员工作情境的响应策略解释性判断，同时揭示了实际部署和系统化评估中面临的挑战与未来考量。



## **81. Hidden-in-Plain-Text: A Benchmark for Social-Web Indirect Prompt Injection in RAG**

隐藏于明文之中：面向RAG系统中社交网络间接提示注入的基准测试 cs.CR

WWW 2026

**SubmitDate**: 2026-01-20    [abs](http://arxiv.org/abs/2601.10923v2) [paper-pdf](https://arxiv.org/pdf/2601.10923v2)

**Confidence**: 0.85

**Authors**: Haoze Guo, Ziqi Wei

**Abstract**: Retrieval-augmented generation (RAG) systems put more and more emphasis on grounding their responses in user-generated content found on the Web, amplifying both their usefulness and their attack surface. Most notably, indirect prompt injection and retrieval poisoning attack the web-native carriers that survive ingestion pipelines and are very concerning. We provide OpenRAG-Soc, a compact, reproducible benchmark-and-harness for web-facing RAG evaluation under these threats, in a discrete data package. The suite combines a social corpus with interchangeable sparse and dense retrievers and deployable mitigations - HTML/Markdown sanitization, Unicode normalization, and attribution-gated answered. It standardizes end-to-end evaluation from ingestion to generation and reports attacks time of one of the responses at answer time, rank shifts in both sparse and dense retrievers, utility and latency, allowing for apples-to-apples comparisons across carriers and defenses. OpenRAG-Soc targets practitioners who need fast, and realistic tests to track risk and harden deployments.

摘要: 检索增强生成（RAG）系统日益重视基于网络用户生成内容构建响应，这既增强了其实用性，也扩大了攻击面。其中，间接提示注入和检索污染攻击针对那些能够通过数据摄取管道存活的网络原生载体，尤为令人担忧。我们推出OpenRAG-Soc——一个紧凑、可复现的基准测试框架，以离散数据包形式为面临此类威胁的网络化RAG系统提供评估方案。该套件整合了社交语料库、可互换的稀疏/稠密检索器以及可部署的防护措施（包括HTML/Markdown净化、Unicode规范化和基于来源验证的应答机制）。它标准化了从数据摄取到生成的全流程端到端评估，在响应时报告攻击成功率、稀疏/稠密检索器的排序偏移、系统效用与延迟等指标，支持跨载体和防御策略的公平比较。OpenRAG-Soc面向需要快速、真实测试来追踪风险并强化部署的实践者。



## **82. Multi-Agent Framework for Threat Mitigation and Resilience in AI-Based Systems**

面向AI系统威胁缓解与韧性的多智能体框架 cs.CR

56 pages, 18 Figures, 22 Tables, TOSEM

**SubmitDate**: 2025-12-29    [abs](http://arxiv.org/abs/2512.23132v1) [paper-pdf](https://arxiv.org/pdf/2512.23132v1)

**Confidence**: 0.85

**Authors**: Armstrong Foundjem, Lionel Nganyewou Tidjon, Leuson Da Silva, Foutse Khomh

**Abstract**: Machine learning (ML) underpins foundation models in finance, healthcare, and critical infrastructure, making them targets for data poisoning, model extraction, prompt injection, automated jailbreaking, and preference-guided black-box attacks that exploit model comparisons. Larger models can be more vulnerable to introspection-driven jailbreaks and cross-modal manipulation. Traditional cybersecurity lacks ML-specific threat modeling for foundation, multimodal, and RAG systems. Objective: Characterize ML security risks by identifying dominant TTPs, vulnerabilities, and targeted lifecycle stages. Methods: We extract 93 threats from MITRE ATLAS (26), AI Incident Database (12), and literature (55), and analyze 854 GitHub/Python repositories. A multi-agent RAG system (ChatGPT-4o, temp 0.4) mines 300+ articles to build an ontology-driven threat graph linking TTPs, vulnerabilities, and stages. Results: We identify unreported threats including commercial LLM API model stealing, parameter memorization leakage, and preference-guided text-only jailbreaks. Dominant TTPs include MASTERKEY-style jailbreaking, federated poisoning, diffusion backdoors, and preference optimization leakage, mainly impacting pre-training and inference. Graph analysis reveals dense vulnerability clusters in libraries with poor patch propagation. Conclusion: Adaptive, ML-specific security frameworks, combining dependency hygiene, threat intelligence, and monitoring, are essential to mitigate supply-chain and inference risks across the ML lifecycle.

摘要: 机器学习（ML）是金融、医疗和关键基础设施领域基础模型的核心技术，使其成为数据投毒、模型窃取、提示注入、自动化越狱以及利用模型比较的偏好引导黑盒攻击的目标。模型规模越大，越容易受到内省驱动越狱和跨模态操纵的影响。传统网络安全缺乏针对基础模型、多模态和RAG系统的ML专用威胁建模。目标：通过识别主要战术技术流程（TTP）、漏洞和受攻击生命周期阶段，刻画ML安全风险特征。方法：我们从MITRE ATLAS（26项）、AI事件数据库（12项）和文献（55项）中提取93项威胁，并分析了854个GitHub/Python代码库。采用多智能体RAG系统（ChatGPT-4o，温度参数0.4）挖掘300余篇文献，构建连接TTP、漏洞和阶段的本体驱动威胁图谱。结果：我们发现了未公开的威胁，包括商业LLM API模型窃取、参数记忆泄漏和偏好引导纯文本越狱。主要TTP包括MASTERKEY式越狱、联邦投毒、扩散后门和偏好优化泄漏，主要影响预训练和推理阶段。图谱分析揭示了补丁传播不畅的代码库中存在密集漏洞集群。结论：结合依赖项管理、威胁情报和监控的自适应ML专用安全框架，对于缓解ML全生命周期中的供应链和推理风险至关重要。



## **83. ASTRIDE: A Security Threat Modeling Platform for Agentic-AI Applications**

ASTRIDE：面向智能体AI应用的安全威胁建模平台 cs.AI

**SubmitDate**: 2025-12-04    [abs](http://arxiv.org/abs/2512.04785v1) [paper-pdf](https://arxiv.org/pdf/2512.04785v1)

**Confidence**: 0.85

**Authors**: Eranga Bandara, Amin Hass, Ross Gore, Sachin Shetty, Ravi Mukkamala, Safdar H. Bouk, Xueping Liang, Ng Wee Keong, Kasun De Zoysa, Aruna Withanage, Nilaan Loganathan

**Abstract**: AI agent-based systems are becoming increasingly integral to modern software architectures, enabling autonomous decision-making, dynamic task execution, and multimodal interactions through large language models (LLMs). However, these systems introduce novel and evolving security challenges, including prompt injection attacks, context poisoning, model manipulation, and opaque agent-to-agent communication, that are not effectively captured by traditional threat modeling frameworks. In this paper, we introduce ASTRIDE, an automated threat modeling platform purpose-built for AI agent-based systems. ASTRIDE extends the classical STRIDE framework by introducing a new threat category, A for AI Agent-Specific Attacks, which encompasses emerging vulnerabilities such as prompt injection, unsafe tool invocation, and reasoning subversion, unique to agent-based applications. To automate threat modeling, ASTRIDE combines a consortium of fine-tuned vision-language models (VLMs) with the OpenAI-gpt-oss reasoning LLM to perform end-to-end analysis directly from visual agent architecture diagrams, such as data flow diagrams(DFDs). LLM agents orchestrate the end-to-end threat modeling automation process by coordinating interactions between the VLM consortium and the reasoning LLM. Our evaluations demonstrate that ASTRIDE provides accurate, scalable, and explainable threat modeling for next-generation intelligent systems. To the best of our knowledge, ASTRIDE is the first framework to both extend STRIDE with AI-specific threats and integrate fine-tuned VLMs with a reasoning LLM to fully automate diagram-driven threat modeling in AI agent-based applications.

摘要: 基于AI智能体的系统正日益成为现代软件架构的核心组成部分，通过大语言模型（LLMs）实现自主决策、动态任务执行和多模态交互。然而，这些系统引入了新颖且不断演变的安全挑战，包括提示注入攻击、上下文污染、模型操纵以及不透明的智能体间通信，这些挑战无法被传统威胁建模框架有效捕捉。本文提出ASTRIDE——一个专为基于AI智能体的系统设计的自动化威胁建模平台。ASTRIDE通过引入新的威胁类别'A'（代表AI智能体特定攻击）扩展了经典STRIDE框架，该类别涵盖智能体应用特有的新兴漏洞，如提示注入、不安全工具调用和推理颠覆。为实现威胁建模自动化，ASTRIDE结合了微调视觉语言模型（VLMs）联盟与OpenAI-gpt-oss推理LLM，能够直接从视觉化智能体架构图（如数据流图DFDs）进行端到端分析。LLM智能体通过协调VLM联盟与推理LLM之间的交互，编排端到端的威胁建模自动化流程。评估结果表明，ASTRIDE能为新一代智能系统提供准确、可扩展且可解释的威胁建模。据我们所知，ASTRIDE是首个同时实现以下目标的框架：通过AI特定威胁扩展STRIDE，并集成微调VLMs与推理LLM，在基于AI智能体的应用中实现完全自动化的图表驱动威胁建模。



## **84. Phantom Menace: Exploring and Enhancing the Robustness of VLA Models Against Physical Sensor Attacks**

幻影威胁：探索并增强VLA模型对抗物理传感器攻击的鲁棒性 cs.RO

Accepted by AAAI 2026 main track

**SubmitDate**: 2025-12-19    [abs](http://arxiv.org/abs/2511.10008v2) [paper-pdf](https://arxiv.org/pdf/2511.10008v2)

**Confidence**: 0.85

**Authors**: Xuancun Lu, Jiaxiang Chen, Shilin Xiao, Zizhi Jin, Zhangrui Chen, Hanwen Yu, Bohan Qian, Ruochen Zhou, Xiaoyu Ji, Wenyuan Xu

**Abstract**: Vision-Language-Action (VLA) models revolutionize robotic systems by enabling end-to-end perception-to-action pipelines that integrate multiple sensory modalities, such as visual signals processed by cameras and auditory signals captured by microphones. This multi-modality integration allows VLA models to interpret complex, real-world environments using diverse sensor data streams. Given the fact that VLA-based systems heavily rely on the sensory input, the security of VLA models against physical-world sensor attacks remains critically underexplored. To address this gap, we present the first systematic study of physical sensor attacks against VLAs, quantifying the influence of sensor attacks and investigating the defenses for VLA models. We introduce a novel "Real-Sim-Real" framework that automatically simulates physics-based sensor attack vectors, including six attacks targeting cameras and two targeting microphones, and validates them on real robotic systems. Through large-scale evaluations across various VLA architectures and tasks under varying attack parameters, we demonstrate significant vulnerabilities, with susceptibility patterns that reveal critical dependencies on task types and model designs. We further develop an adversarial-training-based defense that enhances VLA robustness against out-of-distribution physical perturbations caused by sensor attacks while preserving model performance. Our findings expose an urgent need for standardized robustness benchmarks and mitigation strategies to secure VLA deployments in safety-critical environments.

摘要: 视觉-语言-动作（VLA）模型通过实现端到端的感知到动作流程，整合了多种感官模态（如摄像头处理的视觉信号和麦克风捕获的听觉信号），从而彻底改变了机器人系统。这种多模态集成使VLA模型能够利用多样化的传感器数据流来解读复杂的现实世界环境。鉴于基于VLA的系统严重依赖感官输入，VLA模型在物理世界传感器攻击下的安全性仍严重缺乏探索。为填补这一空白，我们首次对针对VLA的物理传感器攻击进行了系统性研究，量化了传感器攻击的影响并探索了VLA模型的防御机制。我们引入了一种新颖的“真实-模拟-真实”框架，可自动模拟基于物理的传感器攻击向量（包括六种针对摄像头的攻击和两种针对麦克风的攻击），并在真实机器人系统上进行验证。通过对不同VLA架构和任务在各种攻击参数下的大规模评估，我们揭示了显著的脆弱性，其易受攻击模式显示出对任务类型和模型设计的关键依赖性。我们进一步开发了一种基于对抗训练的防御方法，在保持模型性能的同时，增强了VLA对传感器攻击引起的分布外物理扰动的鲁棒性。我们的研究结果表明，在安全关键环境中部署VLA系统迫切需要标准化的鲁棒性基准和缓解策略。



## **85. GCG Attack On A Diffusion LLM**

GCG攻击在扩散大语言模型上的应用 cs.LG

**SubmitDate**: 2025-12-30    [abs](http://arxiv.org/abs/2601.14266v1) [paper-pdf](https://arxiv.org/pdf/2601.14266v1)

**Confidence**: 0.85

**Authors**: Ruben Neyroud, Sam Corley

**Abstract**: While most LLMs are autoregressive, diffusion-based LLMs have recently emerged as an alternative method for generation. Greedy Coordinate Gradient (GCG) attacks have proven effective against autoregressive models, but their applicability to diffusion language models remains largely unexplored. In this work, we present an exploratory study of GCG-style adversarial prompt attacks on LLaDA (Large Language Diffusion with mAsking), an open-source diffusion LLM. We evaluate multiple attack variants, including prefix perturbations and suffix-based adversarial generation, on harmful prompts drawn from the AdvBench dataset. Our study provides initial insights into the robustness and attack surface of diffusion language models and motivates the development of alternative optimization and evaluation strategies for adversarial analysis in this setting.

摘要: 尽管大多数大语言模型（LLM）采用自回归方法，但基于扩散的LLM最近已成为一种替代生成方法。贪婪坐标梯度（GCG）攻击已被证明对自回归模型有效，但其在扩散语言模型上的适用性仍基本未被探索。本研究对开源扩散LLM——LLaDA（基于掩码的大语言扩散模型）进行了GCG风格对抗性提示攻击的探索性研究。我们基于AdvBench数据集中的有害提示，评估了多种攻击变体，包括前缀扰动和基于后缀的对抗生成。本研究为扩散语言模型的鲁棒性和攻击面提供了初步见解，并推动了在此场景下开发替代优化和评估策略以进行对抗分析。



## **86. TeleAntiFraud-28k: An Audio-Text Slow-Thinking Dataset for Telecom Fraud Detection**

TeleAntiFraud-28k：用于电信诈骗检测的音频-文本慢思考数据集 cs.CL

**SubmitDate**: 2025-08-18    [abs](http://arxiv.org/abs/2503.24115v4) [paper-pdf](https://arxiv.org/pdf/2503.24115v4)

**Confidence**: 0.85

**Authors**: Zhiming Ma, Peidong Wang, Minhua Huang, Jingpeng Wang, Kai Wu, Xiangzhao Lv, Yachun Pang, Yin Yang, Wenjie Tang, Yuchen Kang

**Abstract**: The detection of telecom fraud faces significant challenges due to the lack of high-quality multimodal training data that integrates audio signals with reasoning-oriented textual analysis. To address this gap, we present TeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset specifically designed for automated telecom fraud analysis. Our dataset is constructed through three strategies: (1) Privacy-preserved text-truth sample generation using automatically speech recognition (ASR)-transcribed call recordings (with anonymized original audio), ensuring real-world consistency through text-to-speech (TTS) model regeneration; (2) Semantic enhancement via large language model (LLM)-based self-instruction sampling on authentic ASR outputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that simulates emerging fraud tactics through predefined communication scenarios and fraud typologies. The generated dataset contains 28,511 rigorously processed speech-text pairs, complete with detailed annotations for fraud reasoning. The dataset is divided into three tasks: scenario classification, fraud detection, fraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a standardized evaluation benchmark comprising proportionally sampled instances from the dataset, to facilitate systematic testing of model performance on telecom fraud detection tasks. We also contribute a production-optimized supervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while open-sourcing the data processing framework to enable community-driven dataset expansion. This work establishes a foundational framework for multimodal anti-fraud research while addressing critical challenges in data privacy and scenario diversity. The project will be released at https://github.com/JimmyMa99/TeleAntiFraud.

摘要: 由于缺乏融合音频信号与推理导向文本分析的高质量多模态训练数据，电信诈骗检测面临重大挑战。为填补这一空白，我们提出了TeleAntiFraud-28k——首个专为自动化电信诈骗分析设计的开源音频-文本慢思考数据集。该数据集通过三种策略构建：(1) 使用自动语音识别（ASR）转录的通话录音（原始音频已匿名化）生成隐私保护的文本真实样本，并通过文本转语音（TTS）模型重生成确保现实一致性；(2) 基于大型语言模型（LLM）对真实ASR输出进行自指令采样以实现语义增强，扩展场景覆盖范围；(3) 通过预定义通信场景和诈骗类型进行多智能体对抗合成，模拟新兴诈骗手法。生成的数据集包含28,511个经过严格处理的语音-文本对，并配有详细的诈骗推理标注。数据集划分为三项任务：场景分类、诈骗检测、诈骗类型分类。此外，我们构建了TeleAntiFraud-Bench标准化评估基准，包含从数据集中按比例采样的实例，以促进电信诈骗检测任务的系统性模型性能测试。我们还贡献了一个基于混合真实/合成数据训练的生产优化监督微调（SFT）模型，同时开源数据处理框架以支持社区驱动的数据集扩展。本工作为多模态反欺诈研究建立了基础框架，同时解决了数据隐私和场景多样性方面的关键挑战。项目将在https://github.com/JimmyMa99/TeleAntiFraud发布。



